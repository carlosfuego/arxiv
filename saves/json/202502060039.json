[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v1",
                "updated": "2025-02-04T03:13:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "14 pages, 11 figures, the first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v1",
                "updated": "2025-02-04T02:23:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v2",
                "updated": "2025-02-03T21:45:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    45,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v1",
                "updated": "2025-02-03T20:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01637v1",
                "updated": "2025-02-03T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Scaling Embedding Layers in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Embedding Layers in Language Models"
                },
                "summary": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS."
                },
                "authors": [
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Edith Cohen"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v1",
                "updated": "2025-02-03T15:38:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v1",
                "updated": "2025-02-03T05:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00527v1",
                "updated": "2025-02-01T18:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T18:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration"
                },
                "summary": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models."
                },
                "authors": [
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00439v1",
                "updated": "2025-02-01T14:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T14:16:31Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs"
                },
                "summary": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "11 pages, 4 figures. Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00433v1",
                "updated": "2025-02-01T13:46:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T13:46:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models"
                },
                "summary": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning"
                },
                "authors": [
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00382v1",
                "updated": "2025-02-01T09:41:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T09:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "Masked Generative Nested Transformers with Decode Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Nested Transformers with Decode Time Scaling"
                },
                "summary": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Debapriya Tula"
                    },
                    {
                        "name": "Gagan Jain"
                    },
                    {
                        "name": "Pradeep Shenoy"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Sujoy Paul"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Paul"
                },
                "author": "Sujoy Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v1",
                "updated": "2025-02-01T03:49:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v1",
                "updated": "2025-01-31T18:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v1",
                "updated": "2025-01-31T16:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Mao Xun Huang"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernández"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivanković Nizić"
                    },
                    {
                        "name": "Milko Jaksić"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaquín Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gómez-Albarrán"
                    },
                    {
                        "name": "José-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "José-Luis Sierra"
                },
                "author": "José-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Bürgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jiménez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. García-León"
                    },
                    {
                        "name": "R. García-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqué"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elsässer"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Anaïs Dréau"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Gérard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Gérard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.09864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09864v3",
                "updated": "2025-02-04T18:59:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    59,
                    27,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-15T01:01:44Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    1,
                    44,
                    4,
                    320,
                    0
                ],
                "title": "Uncertainty Propagation within Chained Models for Machine Learning\n  Reconstruction of Neutrino-LAr Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Propagation within Chained Models for Machine Learning\n  Reconstruction of Neutrino-LAr Interactions"
                },
                "summary": "Sequential or chained models are increasingly prevalent in machine learning\nfor scientific applications, due to their flexibility and ease of development.\nChained models are particularly useful when a task is separable into distinct\nsteps with a hierarchy of meaningful intermediate representations. In\nreliability-critical tasks, it is important to quantify the confidence of model\ninferences. However, chained models pose an additional challenge for\nuncertainty quantification, especially when input uncertainties need to be\npropagated. In such cases, a fully uncertainty-aware chain of models is\nrequired, where each step accepts a probability distribution over the input\nspace, and produces a probability distribution over the output space. In this\nwork, we present a case study for adapting a single model within an existing\nchain, designed for reconstruction within neutrino-Argon interactions,\ndeveloped for neutrino oscillation experiments such as MicroBooNE, ICARUS, and\nthe future DUNE experiment. We test the performance of an input\nuncertainty-enabled model against an uncertainty-blinded model using a method\nfor generating synthetic noise. By comparing these two, we assess the increase\nin inference quality achieved by exposing models to upstream uncertainty\nestimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential or chained models are increasingly prevalent in machine learning\nfor scientific applications, due to their flexibility and ease of development.\nChained models are particularly useful when a task is separable into distinct\nsteps with a hierarchy of meaningful intermediate representations. In\nreliability-critical tasks, it is important to quantify the confidence of model\ninferences. However, chained models pose an additional challenge for\nuncertainty quantification, especially when input uncertainties need to be\npropagated. In such cases, a fully uncertainty-aware chain of models is\nrequired, where each step accepts a probability distribution over the input\nspace, and produces a probability distribution over the output space. In this\nwork, we present a case study for adapting a single model within an existing\nchain, designed for reconstruction within neutrino-Argon interactions,\ndeveloped for neutrino oscillation experiments such as MicroBooNE, ICARUS, and\nthe future DUNE experiment. We test the performance of an input\nuncertainty-enabled model against an uncertainty-blinded model using a method\nfor generating synthetic noise. By comparing these two, we assess the increase\nin inference quality achieved by exposing models to upstream uncertainty\nestimates."
                },
                "authors": [
                    {
                        "name": "Daniel Douglas"
                    },
                    {
                        "name": "Aashwin Mishra"
                    },
                    {
                        "name": "Daniel Ratner"
                    },
                    {
                        "name": "Felix Petersen"
                    },
                    {
                        "name": "Kazuhiro Terao"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Terao"
                },
                "author": "Kazuhiro Terao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02584v1",
                "updated": "2025-02-04T18:58:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    58,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:58:31Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    58,
                    31,
                    1,
                    35,
                    0
                ],
                "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search"
                },
                "summary": "Language agents have become a promising solution to complex interactive\ntasks. One of the key ingredients to the success of language agents is the\nreward model on the trajectory of the agentic workflow, which provides valuable\nguidance during training or inference. However, due to the lack of annotations\nof intermediate interactions, most existing works use an outcome reward model\nto optimize policies across entire trajectories. This may lead to sub-optimal\npolicies and hinder the overall performance. To address this, we propose QLASS\n(Q-guided Language Agent Stepwise Search), to automatically generate\nannotations by estimating Q-values in a stepwise manner for open language\nagents. By introducing a reasoning tree and performing process reward modeling,\nQLASS provides effective intermediate guidance for each step. With the stepwise\nguidance, we propose a Q-guided generation strategy to enable language agents\nto better adapt to long-term value, resulting in significant performance\nimprovement during model inference on complex interactive agent tasks. Notably,\neven with almost half the annotated data, QLASS retains strong performance,\ndemonstrating its efficiency in handling limited supervision. We also\nempirically demonstrate that QLASS can lead to more effective decision making\nthrough qualitative analysis. We will release our code and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language agents have become a promising solution to complex interactive\ntasks. One of the key ingredients to the success of language agents is the\nreward model on the trajectory of the agentic workflow, which provides valuable\nguidance during training or inference. However, due to the lack of annotations\nof intermediate interactions, most existing works use an outcome reward model\nto optimize policies across entire trajectories. This may lead to sub-optimal\npolicies and hinder the overall performance. To address this, we propose QLASS\n(Q-guided Language Agent Stepwise Search), to automatically generate\nannotations by estimating Q-values in a stepwise manner for open language\nagents. By introducing a reasoning tree and performing process reward modeling,\nQLASS provides effective intermediate guidance for each step. With the stepwise\nguidance, we propose a Q-guided generation strategy to enable language agents\nto better adapt to long-term value, resulting in significant performance\nimprovement during model inference on complex interactive agent tasks. Notably,\neven with almost half the annotated data, QLASS retains strong performance,\ndemonstrating its efficiency in handling limited supervision. We also\nempirically demonstrate that QLASS can lead to more effective decision making\nthrough qualitative analysis. We will release our code and data."
                },
                "authors": [
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Yao Tang"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02577v1",
                "updated": "2025-02-04T18:53:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    53,
                    42,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:53:42Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    53,
                    42,
                    1,
                    35,
                    0
                ],
                "title": "A comparison of translation performance between DeepL and Supertext",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparison of translation performance between DeepL and Supertext"
                },
                "summary": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext."
                },
                "authors": [
                    {
                        "name": "Alex Flückiger"
                    },
                    {
                        "name": "Chantal Amrhein"
                    },
                    {
                        "name": "Tim Graf"
                    },
                    {
                        "name": "Philippe Schläpfer"
                    },
                    {
                        "name": "Florian Schottmann"
                    },
                    {
                        "name": "Samuel Läubli"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Läubli"
                },
                "author": "Samuel Läubli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14299v3",
                "updated": "2025-02-04T18:52:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    52,
                    39,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-21T16:50:11Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "title": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n  Harnessing AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n  Harnessing AI"
                },
                "summary": "Masala-CHAI is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masala-CHAI is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development."
                },
                "authors": [
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Yuheng He"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Hamed Rahmani"
                    },
                    {
                        "name": "Ramesh Karri"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Karri"
                },
                "author": "Ramesh Karri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02574v1",
                "updated": "2025-02-04T18:47:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    47,
                    35,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:47:35Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    47,
                    35,
                    1,
                    35,
                    0
                ],
                "title": "Probing large-scale structures with the 2-point function and the power\n  spectrum: insights into cosmic clustering evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing large-scale structures with the 2-point function and the power\n  spectrum: insights into cosmic clustering evolution"
                },
                "summary": "Understanding the large-scale structure of the Universe requires analysis of\ncosmic clustering and its evolution over time. In this work, we investigate the\nclustering properties of SDSS blue galaxies, which are excellent tracers of\ndark matter, along two distinct epochs of the Universe, utilizing estimators\nlike the 2-point angular correlation function (2PACF), the angular power\nspectra, among others. Considering a model-independent approach, we perform\nanalyses in two disjoint redshift shells, $0 \\leq z < 0.06$ and $0.06 \\leq z <\n0.12$, to investigate the distribution of large cosmic structures. Using\nBayesian inference methods, we constrain the parameter that quantifies the\ngalaxy clustering in the 2PACF, enabling us to perform comparisons among\ndifferent regions on the sky and between different epochs in the Universe\nregarding the gravitational action on matter structures. Our analyses\ncomplement previous efforts to map large-scale structures in the Local\nUniverse. In addition, this study reveals differences regarding the clustering\nof large cosmic structures comparing two epochs of the Universe, analyses done\nwith diverse estimators. Results reveal, clearly, distinct evolutionary\nsignatures between the two redshift shells. Moreover, we had the opportunity to\ntest the concordance cosmological model under extreme conditions in the highly\nnon-linear Local Universe, computing the amplitude of the angular power\nspectrum at very small scales. Ultimately, all our analyses serve as a set of\nconsistency tests of the concordance cosmological model, the $\\Lambda$CDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the large-scale structure of the Universe requires analysis of\ncosmic clustering and its evolution over time. In this work, we investigate the\nclustering properties of SDSS blue galaxies, which are excellent tracers of\ndark matter, along two distinct epochs of the Universe, utilizing estimators\nlike the 2-point angular correlation function (2PACF), the angular power\nspectra, among others. Considering a model-independent approach, we perform\nanalyses in two disjoint redshift shells, $0 \\leq z < 0.06$ and $0.06 \\leq z <\n0.12$, to investigate the distribution of large cosmic structures. Using\nBayesian inference methods, we constrain the parameter that quantifies the\ngalaxy clustering in the 2PACF, enabling us to perform comparisons among\ndifferent regions on the sky and between different epochs in the Universe\nregarding the gravitational action on matter structures. Our analyses\ncomplement previous efforts to map large-scale structures in the Local\nUniverse. In addition, this study reveals differences regarding the clustering\nof large cosmic structures comparing two epochs of the Universe, analyses done\nwith diverse estimators. Results reveal, clearly, distinct evolutionary\nsignatures between the two redshift shells. Moreover, we had the opportunity to\ntest the concordance cosmological model under extreme conditions in the highly\nnon-linear Local Universe, computing the amplitude of the angular power\nspectrum at very small scales. Ultimately, all our analyses serve as a set of\nconsistency tests of the concordance cosmological model, the $\\Lambda$CDM."
                },
                "authors": [
                    {
                        "name": "Camila Franco"
                    },
                    {
                        "name": "Felipe Avila"
                    },
                    {
                        "name": "Armando Bernui"
                    }
                ],
                "author_detail": {
                    "name": "Armando Bernui"
                },
                "author": "Armando Bernui",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02573v1",
                "updated": "2025-02-04T18:47:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    47,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:47:31Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    47,
                    31,
                    1,
                    35,
                    0
                ],
                "title": "Are Language Models Up to Sequential Optimization Problems? From\n  Evaluation to a Hegelian-Inspired Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Language Models Up to Sequential Optimization Problems? From\n  Evaluation to a Hegelian-Inspired Enhancement"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nnumerous fields, presenting an opportunity to revolutionize optimization\nproblem-solving, a crucial, ubiquitous, and complex domain. This paper explores\nthe proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We\nintroduce WorldGen, a dynamic framework for generating unseen SOPs with\ncontrollable complexities, to evaluate LLM performance. Our initial\nobservations reveal that while LLMs perform well on simple SOPs, their\nperformance significantly degrades with increased complexity. Motivated by\nthis, we revisit philosophical hypotheses on reasoning to enhance LLM\nperformance. Inspired by the influential framework of Hegelian Dialectics, we\npropose ACE, demonstrating how the performance of LLMs in SOP contexts can be\nsignificantly improved without any retraining or further fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nnumerous fields, presenting an opportunity to revolutionize optimization\nproblem-solving, a crucial, ubiquitous, and complex domain. This paper explores\nthe proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We\nintroduce WorldGen, a dynamic framework for generating unseen SOPs with\ncontrollable complexities, to evaluate LLM performance. Our initial\nobservations reveal that while LLMs perform well on simple SOPs, their\nperformance significantly degrades with increased complexity. Motivated by\nthis, we revisit philosophical hypotheses on reasoning to enhance LLM\nperformance. Inspired by the influential framework of Hegelian Dialectics, we\npropose ACE, demonstrating how the performance of LLMs in SOP contexts can be\nsignificantly improved without any retraining or further fine-tuning."
                },
                "authors": [
                    {
                        "name": "Soheil Abbasloo"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Abbasloo"
                },
                "author": "Soheil Abbasloo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02552v1",
                "updated": "2025-02-04T18:23:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    23,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:23:22Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    23,
                    22,
                    1,
                    35,
                    0
                ],
                "title": "Hierarchical Sparse Bayesian Multitask Model with Scalable Inference for\n  Microbiome Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Sparse Bayesian Multitask Model with Scalable Inference for\n  Microbiome Analysis"
                },
                "summary": "This paper proposes a hierarchical Bayesian multitask learning model that is\napplicable to the general multi-task binary classification learning problem\nwhere the model assumes a shared sparsity structure across different tasks. We\nderive a computationally efficient inference algorithm based on variational\ninference to approximate the posterior distribution. We demonstrate the\npotential of the new approach on various synthetic datasets and for predicting\nhuman health status based on microbiome profile. Our analysis incorporates data\npooled from multiple microbiome studies, along with a comprehensive comparison\nwith other benchmark methods. Results in synthetic datasets show that the\nproposed approach has superior support recovery property when the underlying\nregression coefficients share a common sparsity structure across different\ntasks. Our experiments on microbiome classification demonstrate the utility of\nthe method in extracting informative taxa while providing well-calibrated\npredictions with uncertainty quantification and achieving competitive\nperformance in terms of prediction metrics. Notably, despite the heterogeneity\nof the pooled datasets (e.g., different experimental objectives, laboratory\nsetups, sequencing equipment, patient demographics), our method delivers robust\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a hierarchical Bayesian multitask learning model that is\napplicable to the general multi-task binary classification learning problem\nwhere the model assumes a shared sparsity structure across different tasks. We\nderive a computationally efficient inference algorithm based on variational\ninference to approximate the posterior distribution. We demonstrate the\npotential of the new approach on various synthetic datasets and for predicting\nhuman health status based on microbiome profile. Our analysis incorporates data\npooled from multiple microbiome studies, along with a comprehensive comparison\nwith other benchmark methods. Results in synthetic datasets show that the\nproposed approach has superior support recovery property when the underlying\nregression coefficients share a common sparsity structure across different\ntasks. Our experiments on microbiome classification demonstrate the utility of\nthe method in extracting informative taxa while providing well-calibrated\npredictions with uncertainty quantification and achieving competitive\nperformance in terms of prediction metrics. Notably, despite the heterogeneity\nof the pooled datasets (e.g., different experimental objectives, laboratory\nsetups, sequencing equipment, patient demographics), our method delivers robust\nresults."
                },
                "authors": [
                    {
                        "name": "Haonan Zhu"
                    },
                    {
                        "name": "Andre R. Goncalves"
                    },
                    {
                        "name": "Camilo Valdes"
                    },
                    {
                        "name": "Hiranmayi Ranganathan"
                    },
                    {
                        "name": "Boya Zhang"
                    },
                    {
                        "name": "Jose Manuel Martí"
                    },
                    {
                        "name": "Car Reen Kok"
                    },
                    {
                        "name": "Monica K. Borucki"
                    },
                    {
                        "name": "Nisha J. Mulakken"
                    },
                    {
                        "name": "James B. Thissen"
                    },
                    {
                        "name": "Crystal Jaing"
                    },
                    {
                        "name": "Alfred Hero"
                    },
                    {
                        "name": "Nicholas A. Be"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas A. Be"
                },
                "author": "Nicholas A. Be",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01101v2",
                "updated": "2025-02-04T18:18:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    18,
                    40,
                    1,
                    35,
                    0
                ],
                "published": "2024-04-01T13:21:05Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    13,
                    21,
                    5,
                    0,
                    92,
                    0
                ],
                "title": "UFID: A Unified Framework for Input-level Backdoor Detection on\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UFID: A Unified Framework for Input-level Backdoor Detection on\n  Diffusion Models"
                },
                "summary": "Diffusion models are vulnerable to backdoor attacks, where malicious\nattackers inject backdoors by poisoning certain training samples during the\ntraining stage. This poses a significant threat to real-world applications in\nthe Model-as-a-Service (MaaS) scenario, where users query diffusion models\nthrough APIs or directly download them from the internet. To mitigate the\nthreat of backdoor attacks under MaaS, black-box input-level backdoor detection\nhas drawn recent interest, where defenders aim to build a firewall that filters\nout backdoor samples in the inference stage, with access only to input queries\nand the generated results from diffusion models. Despite some preliminary\nexplorations on the traditional classification tasks, these methods cannot be\ndirectly applied to the generative tasks due to two major challenges: (1) more\ndiverse failures and (2) a multi-modality attack surface. In this paper, we\npropose a black-box input-level backdoor detection framework on diffusion\nmodels, called UFID. Our defense is motivated by an insightful causal analysis:\nBackdoor attacks serve as the confounder, introducing a spurious path from\ninput to target images, which remains consistent even when we perturb the input\nsamples with Gaussian noise. We further validate the intuition with theoretical\nanalysis. Extensive experiments across different datasets on both conditional\nand unconditional diffusion models show that our method achieves superb\nperformance on detection effectiveness and run-time efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are vulnerable to backdoor attacks, where malicious\nattackers inject backdoors by poisoning certain training samples during the\ntraining stage. This poses a significant threat to real-world applications in\nthe Model-as-a-Service (MaaS) scenario, where users query diffusion models\nthrough APIs or directly download them from the internet. To mitigate the\nthreat of backdoor attacks under MaaS, black-box input-level backdoor detection\nhas drawn recent interest, where defenders aim to build a firewall that filters\nout backdoor samples in the inference stage, with access only to input queries\nand the generated results from diffusion models. Despite some preliminary\nexplorations on the traditional classification tasks, these methods cannot be\ndirectly applied to the generative tasks due to two major challenges: (1) more\ndiverse failures and (2) a multi-modality attack surface. In this paper, we\npropose a black-box input-level backdoor detection framework on diffusion\nmodels, called UFID. Our defense is motivated by an insightful causal analysis:\nBackdoor attacks serve as the confounder, introducing a spurious path from\ninput to target images, which remains consistent even when we perturb the input\nsamples with Gaussian noise. We further validate the intuition with theoretical\nanalysis. Extensive experiments across different datasets on both conditional\nand unconditional diffusion models show that our method achieves superb\nperformance on detection effectiveness and run-time efficiency."
                },
                "authors": [
                    {
                        "name": "Zihan Guan"
                    },
                    {
                        "name": "Mengxuan Hu"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Anil Vullikanti"
                    }
                ],
                "author_detail": {
                    "name": "Anil Vullikanti"
                },
                "author": "Anil Vullikanti",
                "arxiv_comment": "18 pages,24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02542v1",
                "updated": "2025-02-04T18:12:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    12,
                    41,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:12:41Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    12,
                    41,
                    1,
                    35,
                    0
                ],
                "title": "OVERTHINKING: Slowdown Attacks on Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVERTHINKING: Slowdown Attacks on Reasoning LLMs"
                },
                "summary": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 46x slowdown and high transferability of the\nattack across models. To protect applications, we discuss and implement\ndefenses leveraging LLM-based and system design approaches. Finally, we discuss\nsocietal, financial, and energy impacts of OVERTHINK attack which could amplify\nthe costs for third party applications operating reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 46x slowdown and high transferability of the\nattack across models. To protect applications, we discuss and implement\ndefenses leveraging LLM-based and system design approaches. Finally, we discuss\nsocietal, financial, and energy impacts of OVERTHINK attack which could amplify\nthe costs for third party applications operating reasoning models."
                },
                "authors": [
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Jaechul Roh"
                    },
                    {
                        "name": "Ali Naseh"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Mohit Iyyer"
                    },
                    {
                        "name": "Amir Houmansadr"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Bagdasarian"
                },
                "author": "Eugene Bagdasarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12100v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12100v4",
                "updated": "2025-02-04T18:08:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    8,
                    55,
                    1,
                    35,
                    0
                ],
                "published": "2024-06-17T21:25:36Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    25,
                    36,
                    0,
                    169,
                    0
                ],
                "title": "CUQDS: Conformal Uncertainty Quantification under Distribution Shift for\n  Trajectory Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUQDS: Conformal Uncertainty Quantification under Distribution Shift for\n  Trajectory Prediction"
                },
                "summary": "Trajectory prediction models that can infer both finite future trajectories\nand their associated uncertainties of the target vehicles in an online setting\n(e.g., real-world application scenarios) is crucial for ensuring the safe and\nrobust navigation and path planning of autonomous vehicle motion. However, the\nmajority of existing trajectory prediction models have neither considered\nreducing the uncertainty as one objective during the training stage nor\nprovided reliable uncertainty quantification during inference stage under\npotential distribution shift. Therefore, in this paper, we propose the\nConformal Uncertainty Quantification under Distribution Shift framework, CUQDS,\nto quantify the uncertainty of the predicted trajectories of existing\ntrajectory prediction models under potential data distribution shift, while\nconsidering improving the prediction accuracy of the models and reducing the\nestimated uncertainty during the training stage. Specifically, CUQDS includes\n1) a learning-based Gaussian process regression module that models the output\ndistribution of the base model (any existing trajectory prediction or time\nseries forecasting neural networks) and reduces the estimated uncertainty by\nadditional loss term, and 2) a statistical-based Conformal P control module to\ncalibrate the estimated uncertainty from the Gaussian process regression module\nin an online setting under potential distribution shift between training and\ntesting data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction models that can infer both finite future trajectories\nand their associated uncertainties of the target vehicles in an online setting\n(e.g., real-world application scenarios) is crucial for ensuring the safe and\nrobust navigation and path planning of autonomous vehicle motion. However, the\nmajority of existing trajectory prediction models have neither considered\nreducing the uncertainty as one objective during the training stage nor\nprovided reliable uncertainty quantification during inference stage under\npotential distribution shift. Therefore, in this paper, we propose the\nConformal Uncertainty Quantification under Distribution Shift framework, CUQDS,\nto quantify the uncertainty of the predicted trajectories of existing\ntrajectory prediction models under potential data distribution shift, while\nconsidering improving the prediction accuracy of the models and reducing the\nestimated uncertainty during the training stage. Specifically, CUQDS includes\n1) a learning-based Gaussian process regression module that models the output\ndistribution of the base model (any existing trajectory prediction or time\nseries forecasting neural networks) and reduces the estimated uncertainty by\nadditional loss term, and 2) a statistical-based Conformal P control module to\ncalibrate the estimated uncertainty from the Gaussian process regression module\nin an online setting under potential distribution shift between training and\ntesting data."
                },
                "authors": [
                    {
                        "name": "Huiqun Huang"
                    },
                    {
                        "name": "Sihong He"
                    },
                    {
                        "name": "Fei Miao"
                    }
                ],
                "author_detail": {
                    "name": "Fei Miao"
                },
                "author": "Fei Miao",
                "arxiv_comment": "9 pages, 2 figures",
                "arxiv_journal_ref": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12100v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12100v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02539v1",
                "updated": "2025-02-04T18:06:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    6,
                    4,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:06:04Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    6,
                    4,
                    1,
                    35,
                    0
                ],
                "title": "LLMs for Generation of Architectural Components: An Exploratory\n  Empirical Study in the Serverless World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Generation of Architectural Components: An Exploratory\n  Empirical Study in the Serverless World"
                },
                "summary": "Recently, the exponential growth in capability and pervasiveness of Large\nLanguage Models (LLMs) has led to significant work done in the field of code\ngeneration. However, this generation has been limited to code snippets. Going\none step further, our desideratum is to automatically generate architectural\ncomponents. This would not only speed up development time, but would also\nenable us to eventually completely skip the development phase, moving directly\nfrom design decisions to deployment. To this end, we conduct an exploratory\nstudy on the capability of LLMs to generate architectural components for\nFunctions as a Service (FaaS), commonly known as serverless functions. The\nsmall size of their architectural components make this architectural style\namenable for generation using current LLMs compared to other styles like\nmonoliths and microservices. We perform the study by systematically selecting\nopen source serverless repositories, masking a serverless function and\nutilizing state of the art LLMs provided with varying levels of context\ninformation about the overall system to generate the masked function. We\nevaluate correctness through existing tests present in the repositories and use\nmetrics from the Software Engineering (SE) and Natural Language Processing\n(NLP) domains to evaluate code quality and the degree of similarity between\nhuman and LLM generated code respectively. Along with our findings, we also\npresent a discussion on the path forward for using GenAI in architectural\ncomponent generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the exponential growth in capability and pervasiveness of Large\nLanguage Models (LLMs) has led to significant work done in the field of code\ngeneration. However, this generation has been limited to code snippets. Going\none step further, our desideratum is to automatically generate architectural\ncomponents. This would not only speed up development time, but would also\nenable us to eventually completely skip the development phase, moving directly\nfrom design decisions to deployment. To this end, we conduct an exploratory\nstudy on the capability of LLMs to generate architectural components for\nFunctions as a Service (FaaS), commonly known as serverless functions. The\nsmall size of their architectural components make this architectural style\namenable for generation using current LLMs compared to other styles like\nmonoliths and microservices. We perform the study by systematically selecting\nopen source serverless repositories, masking a serverless function and\nutilizing state of the art LLMs provided with varying levels of context\ninformation about the overall system to generate the masked function. We\nevaluate correctness through existing tests present in the repositories and use\nmetrics from the Software Engineering (SE) and Natural Language Processing\n(NLP) domains to evaluate code quality and the degree of similarity between\nhuman and LLM generated code respectively. Along with our findings, we also\npresent a discussion on the path forward for using GenAI in architectural\ncomponent generation."
                },
                "authors": [
                    {
                        "name": "Shrikara Arun"
                    },
                    {
                        "name": "Meghana Tedla"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Vaidhyanathan"
                },
                "author": "Karthik Vaidhyanathan",
                "arxiv_comment": "Accepted to IEEE International Conference on Software Architecture\n  (ICSA) 2025 Main Track (https://conf.researchr.org/home/icsa-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02534v1",
                "updated": "2025-02-04T17:57:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    57,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:57:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    57,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Adaptive Self-improvement LLM Agentic System for ML Library Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Self-improvement LLM Agentic System for ML Library Development"
                },
                "summary": "ML libraries, often written in architecture-specific programming languages\n(ASPLs) that target domain-specific architectures, are key to efficient ML\nsystems. However, writing these high-performance ML libraries is challenging\nbecause it requires expert knowledge of ML algorithms and the ASPL. Large\nlanguage models (LLMs), on the other hand, have shown general coding\ncapabilities. However, challenges remain when using LLMs for generating ML\nlibraries using ASPLs because 1) this task is complicated even for experienced\nhuman programmers and 2) there are limited code examples because of the\nesoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning\nwith limited data in order to complete this task. To address these challenges,\nwe introduce an adaptive self-improvement agentic system. In order to evaluate\nthe effectiveness of our system, we construct a benchmark of a typical ML\nlibrary and generate ASPL code with both open and closed-source LLMs on this\nbenchmark. Our results show improvements of up to $3.9\\times$ over a baseline\nsingle LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML libraries, often written in architecture-specific programming languages\n(ASPLs) that target domain-specific architectures, are key to efficient ML\nsystems. However, writing these high-performance ML libraries is challenging\nbecause it requires expert knowledge of ML algorithms and the ASPL. Large\nlanguage models (LLMs), on the other hand, have shown general coding\ncapabilities. However, challenges remain when using LLMs for generating ML\nlibraries using ASPLs because 1) this task is complicated even for experienced\nhuman programmers and 2) there are limited code examples because of the\nesoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning\nwith limited data in order to complete this task. To address these challenges,\nwe introduce an adaptive self-improvement agentic system. In order to evaluate\nthe effectiveness of our system, we construct a benchmark of a typical ML\nlibrary and generate ASPL code with both open and closed-source LLMs on this\nbenchmark. Our results show improvements of up to $3.9\\times$ over a baseline\nsingle LLM."
                },
                "authors": [
                    {
                        "name": "Genghan Zhang"
                    },
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Olivia Hsu"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02527v1",
                "updated": "2025-02-04T17:49:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    49,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:49:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    49,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "TabPFN Unleashed: A Scalable and Effective Solution to Tabular\n  Classification Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabPFN Unleashed: A Scalable and Effective Solution to Tabular\n  Classification Problems"
                },
                "summary": "TabPFN has emerged as a promising in-context learning model for tabular data,\ncapable of directly predicting the labels of test samples given labeled\ntraining examples. It has demonstrated competitive performance, particularly on\nsmall-scale classification tasks. However, despite its effectiveness, TabPFN\nstill requires further refinement in several areas, including handling\nhigh-dimensional features, aligning with downstream datasets, and scaling to\nlarger datasets. In this paper, we revisit existing variants of TabPFN and\nobserve that most approaches focus either on reducing bias or variance, often\nneglecting the need to address the other side, while also increasing inference\noverhead. To fill this gap, we propose Beta (Bagging and Encoder-based\nFine-tuning for TabPFN Adaptation), a novel and effective method designed to\nminimize both bias and variance. To reduce bias, we introduce a lightweight\nencoder to better align downstream tasks with the pre-trained TabPFN. By\nincreasing the number of encoders in a lightweight manner, Beta mitigate\nvariance, thereby further improving the model's performance. Additionally,\nbootstrapped sampling is employed to further reduce the impact of data\nperturbations on the model, all while maintaining computational efficiency\nduring inference. Our approach enhances TabPFN's ability to handle\nhigh-dimensional data and scale to larger datasets. Experimental results on\nover 200 benchmark classification datasets demonstrate that Beta either\noutperforms or matches state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabPFN has emerged as a promising in-context learning model for tabular data,\ncapable of directly predicting the labels of test samples given labeled\ntraining examples. It has demonstrated competitive performance, particularly on\nsmall-scale classification tasks. However, despite its effectiveness, TabPFN\nstill requires further refinement in several areas, including handling\nhigh-dimensional features, aligning with downstream datasets, and scaling to\nlarger datasets. In this paper, we revisit existing variants of TabPFN and\nobserve that most approaches focus either on reducing bias or variance, often\nneglecting the need to address the other side, while also increasing inference\noverhead. To fill this gap, we propose Beta (Bagging and Encoder-based\nFine-tuning for TabPFN Adaptation), a novel and effective method designed to\nminimize both bias and variance. To reduce bias, we introduce a lightweight\nencoder to better align downstream tasks with the pre-trained TabPFN. By\nincreasing the number of encoders in a lightweight manner, Beta mitigate\nvariance, thereby further improving the model's performance. Additionally,\nbootstrapped sampling is employed to further reduce the impact of data\nperturbations on the model, all while maintaining computational efficiency\nduring inference. Our approach enhances TabPFN's ability to handle\nhigh-dimensional data and scale to larger datasets. Experimental results on\nover 200 benchmark classification datasets demonstrate that Beta either\noutperforms or matches state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Si-Yang Liu"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02525v1",
                "updated": "2025-02-04T17:46:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    46,
                    34,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:46:34Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    46,
                    34,
                    1,
                    35,
                    0
                ],
                "title": "Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object\n  Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object\n  Pose Estimation"
                },
                "summary": "Nine-degrees-of-freedom (9-DoF) object pose and size estimation is crucial\nfor enabling augmented reality and robotic manipulation. Category-level methods\nhave received extensive research attention due to their potential for\ngeneralization to intra-class unknown objects. However, these methods require\nmanual collection and labeling of large-scale real-world training data. To\naddress this problem, we introduce a diffusion-based paradigm for\ndomain-generalized category-level 9-DoF object pose estimation. Our motivation\nis to leverage the latent generalization ability of the diffusion model to\naddress the domain generalization challenge in object pose estimation. This\nentails training the model exclusively on rendered synthetic data to achieve\ngeneralization to real-world scenes. We propose an effective diffusion model to\nredefine 9-DoF object pose estimation from a generative perspective. Our model\ndoes not require any 3D shape priors during training or inference. By employing\nthe Denoising Diffusion Implicit Model, we demonstrate that the reverse\ndiffusion process can be executed in as few as 3 steps, achieving near\nreal-time performance. Finally, we design a robotic grasping system comprising\nboth hardware and software components. Through comprehensive experiments on two\nbenchmark datasets and the real-world robotic system, we show that our method\nachieves state-of-the-art domain generalization performance. Our code will be\nmade public at https://github.com/CNJianLiu/Diff9D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nine-degrees-of-freedom (9-DoF) object pose and size estimation is crucial\nfor enabling augmented reality and robotic manipulation. Category-level methods\nhave received extensive research attention due to their potential for\ngeneralization to intra-class unknown objects. However, these methods require\nmanual collection and labeling of large-scale real-world training data. To\naddress this problem, we introduce a diffusion-based paradigm for\ndomain-generalized category-level 9-DoF object pose estimation. Our motivation\nis to leverage the latent generalization ability of the diffusion model to\naddress the domain generalization challenge in object pose estimation. This\nentails training the model exclusively on rendered synthetic data to achieve\ngeneralization to real-world scenes. We propose an effective diffusion model to\nredefine 9-DoF object pose estimation from a generative perspective. Our model\ndoes not require any 3D shape priors during training or inference. By employing\nthe Denoising Diffusion Implicit Model, we demonstrate that the reverse\ndiffusion process can be executed in as few as 3 steps, achieving near\nreal-time performance. Finally, we design a robotic grasping system comprising\nboth hardware and software components. Through comprehensive experiments on two\nbenchmark datasets and the real-world robotic system, we show that our method\nachieves state-of-the-art domain generalization performance. Our code will be\nmade public at https://github.com/CNJianLiu/Diff9D."
                },
                "authors": [
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Hui Yang"
                    },
                    {
                        "name": "Pengchao Deng"
                    },
                    {
                        "name": "Chongpei Liu"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Hossein Rahmani"
                    },
                    {
                        "name": "Ajmal Mian"
                    }
                ],
                "author_detail": {
                    "name": "Ajmal Mian"
                },
                "author": "Ajmal Mian",
                "arxiv_comment": "17 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02514v1",
                "updated": "2025-02-04T17:33:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    33,
                    8,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:33:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    33,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "Privacy Attacks on Image AutoRegressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Attacks on Image AutoRegressive Models"
                },
                "summary": "Image autoregressive (IAR) models have surpassed diffusion models (DMs) in\nboth image quality (FID: 1.48 vs. 1.58) and generation speed. However, their\nprivacy risks remain largely unexplored. To address this, we conduct a\ncomprehensive privacy analysis comparing IARs to DMs. We develop a novel\nmembership inference attack (MIA) that achieves a significantly higher success\nrate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for\nDMs). Using this MIA, we perform dataset inference (DI) and find that IARs\nrequire as few as six samples to detect dataset membership, compared to 200 for\nDMs, indicating higher information leakage. Additionally, we extract hundreds\nof training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight\na fundamental privacy-utility trade-off: while IARs excel in generation quality\nand speed, they are significantly more vulnerable to privacy attacks. This\nsuggests that incorporating techniques from DMs, such as per-token probability\nmodeling using diffusion, could help mitigate IARs' privacy risks. Our code is\navailable at https://github.com/sprintml/privacy_attacks_against_iars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image autoregressive (IAR) models have surpassed diffusion models (DMs) in\nboth image quality (FID: 1.48 vs. 1.58) and generation speed. However, their\nprivacy risks remain largely unexplored. To address this, we conduct a\ncomprehensive privacy analysis comparing IARs to DMs. We develop a novel\nmembership inference attack (MIA) that achieves a significantly higher success\nrate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for\nDMs). Using this MIA, we perform dataset inference (DI) and find that IARs\nrequire as few as six samples to detect dataset membership, compared to 200 for\nDMs, indicating higher information leakage. Additionally, we extract hundreds\nof training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight\na fundamental privacy-utility trade-off: while IARs excel in generation quality\nand speed, they are significantly more vulnerable to privacy attacks. This\nsuggests that incorporating techniques from DMs, such as per-token probability\nmodeling using diffusion, could help mitigate IARs' privacy risks. Our code is\navailable at https://github.com/sprintml/privacy_attacks_against_iars."
                },
                "authors": [
                    {
                        "name": "Antoni Kowalczuk"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Code: https://github.com/sprintml/privacy_attacks_against_iars",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02508v1",
                "updated": "2025-02-04T17:26:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    26,
                    58,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:26:58Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    26,
                    58,
                    1,
                    35,
                    0
                ],
                "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models will be fully open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models will be fully open-sourced."
                },
                "authors": [
                    {
                        "name": "Maohao Shen"
                    },
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Zhang-Wei Hong"
                    },
                    {
                        "name": "Zhenfang Chen"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Gregory Wornell"
                    },
                    {
                        "name": "Subhro Das"
                    },
                    {
                        "name": "David Cox"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03277v2",
                "updated": "2025-02-04T17:22:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    22,
                    34,
                    1,
                    35,
                    0
                ],
                "published": "2024-09-05T06:41:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    41,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding"
                },
                "summary": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark."
                },
                "authors": [
                    {
                        "name": "Zhengzhuo Xu"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Sinan Du"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02501v1",
                "updated": "2025-02-04T17:16:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    16,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:16:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    16,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "Graph-based Document Structure Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Document Structure Analysis"
                },
                "summary": "When reading a document, glancing at the spatial layout of a document is an\ninitial step to understand it roughly. Traditional document layout analysis\n(DLA) methods, however, offer only a superficial parsing of documents, focusing\non basic instance detection and often failing to capture the nuanced spatial\nand logical relations between instances. These limitations hinder DLA-based\nmodels from achieving a gradually deeper comprehension akin to human reading.\nIn this work, we propose a novel graph-based Document Structure Analysis (gDSA)\ntask. This task requires that model not only detects document elements but also\ngenerates spatial and logical relations in form of a graph structure, allowing\nto understand documents in a holistic and intuitive manner. For this new task,\nwe construct a relation graph-based document structure analysis dataset\n(GraphDoc) with 80K document images and 4.13M relation annotations, enabling\ntraining models to complete multiple tasks like reading order, hierarchical\nstructures analysis, and complex inter-element relation inference. Furthermore,\na document relation graph generator (DRGG) is proposed to address the gDSA\ntask, which achieves performance with 57.6% at mAP$_g$@0.5 for a strong\nbenchmark baseline on this novel task and dataset. We hope this graphical\nrepresentation of document structure can mark an innovative advancement in\ndocument structure analysis and understanding. The new dataset and code will be\nmade publicly available at https://yufanchen96.github.io/projects/GraphDoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When reading a document, glancing at the spatial layout of a document is an\ninitial step to understand it roughly. Traditional document layout analysis\n(DLA) methods, however, offer only a superficial parsing of documents, focusing\non basic instance detection and often failing to capture the nuanced spatial\nand logical relations between instances. These limitations hinder DLA-based\nmodels from achieving a gradually deeper comprehension akin to human reading.\nIn this work, we propose a novel graph-based Document Structure Analysis (gDSA)\ntask. This task requires that model not only detects document elements but also\ngenerates spatial and logical relations in form of a graph structure, allowing\nto understand documents in a holistic and intuitive manner. For this new task,\nwe construct a relation graph-based document structure analysis dataset\n(GraphDoc) with 80K document images and 4.13M relation annotations, enabling\ntraining models to complete multiple tasks like reading order, hierarchical\nstructures analysis, and complex inter-element relation inference. Furthermore,\na document relation graph generator (DRGG) is proposed to address the gDSA\ntask, which achieves performance with 57.6% at mAP$_g$@0.5 for a strong\nbenchmark baseline on this novel task and dataset. We hope this graphical\nrepresentation of document structure can mark an innovative advancement in\ndocument structure analysis and understanding. The new dataset and code will be\nmade publicly available at https://yufanchen96.github.io/projects/GraphDoc."
                },
                "authors": [
                    {
                        "name": "Yufan Chen"
                    },
                    {
                        "name": "Ruiping Liu"
                    },
                    {
                        "name": "Junwei Zheng"
                    },
                    {
                        "name": "Di Wen"
                    },
                    {
                        "name": "Kunyu Peng"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Rainer Stiefelhagen"
                    }
                ],
                "author_detail": {
                    "name": "Rainer Stiefelhagen"
                },
                "author": "Rainer Stiefelhagen",
                "arxiv_comment": "Accepted by ICLR 2025. Project page:\n  https://yufanchen96.github.io/projects/GraphDoc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18626v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18626v3",
                "updated": "2025-02-04T17:09:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-27T12:48:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    48,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt\n  Adversarial Attacks on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt\n  Adversarial Attacks on LLMs"
                },
                "summary": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18626v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18626v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02492v1",
                "updated": "2025-02-04T17:07:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    7,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:07:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    7,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion\n  Generation in Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion\n  Generation in Video Models"
                },
                "summary": "Despite tremendous recent progress, generative video models still struggle to\ncapture real-world motion, dynamics, and physics. We show that this limitation\narises from the conventional pixel reconstruction objective, which biases\nmodels toward appearance fidelity at the expense of motion coherence. To\naddress this, we introduce VideoJAM, a novel framework that instills an\neffective motion prior to video generators, by encouraging the model to learn a\njoint appearance-motion representation. VideoJAM is composed of two\ncomplementary units. During training, we extend the objective to predict both\nthe generated pixels and their corresponding motion from a single learned\nrepresentation. During inference, we introduce Inner-Guidance, a mechanism that\nsteers the generation toward coherent motion by leveraging the model's own\nevolving motion prediction as a dynamic guidance signal. Notably, our framework\ncan be applied to any video model with minimal adaptations, requiring no\nmodifications to the training data or scaling of the model. VideoJAM achieves\nstate-of-the-art performance in motion coherence, surpassing highly competitive\nproprietary models while also enhancing the perceived visual quality of the\ngenerations. These findings emphasize that appearance and motion can be\ncomplementary and, when effectively integrated, enhance both the visual quality\nand the coherence of video generation. Project website:\nhttps://hila-chefer.github.io/videojam-paper.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite tremendous recent progress, generative video models still struggle to\ncapture real-world motion, dynamics, and physics. We show that this limitation\narises from the conventional pixel reconstruction objective, which biases\nmodels toward appearance fidelity at the expense of motion coherence. To\naddress this, we introduce VideoJAM, a novel framework that instills an\neffective motion prior to video generators, by encouraging the model to learn a\njoint appearance-motion representation. VideoJAM is composed of two\ncomplementary units. During training, we extend the objective to predict both\nthe generated pixels and their corresponding motion from a single learned\nrepresentation. During inference, we introduce Inner-Guidance, a mechanism that\nsteers the generation toward coherent motion by leveraging the model's own\nevolving motion prediction as a dynamic guidance signal. Notably, our framework\ncan be applied to any video model with minimal adaptations, requiring no\nmodifications to the training data or scaling of the model. VideoJAM achieves\nstate-of-the-art performance in motion coherence, surpassing highly competitive\nproprietary models while also enhancing the perceived visual quality of the\ngenerations. These findings emphasize that appearance and motion can be\ncomplementary and, when effectively integrated, enhance both the visual quality\nand the coherence of video generation. Project website:\nhttps://hila-chefer.github.io/videojam-paper.github.io/"
                },
                "authors": [
                    {
                        "name": "Hila Chefer"
                    },
                    {
                        "name": "Uriel Singer"
                    },
                    {
                        "name": "Amit Zohar"
                    },
                    {
                        "name": "Yuval Kirstain"
                    },
                    {
                        "name": "Adam Polyak"
                    },
                    {
                        "name": "Yaniv Taigman"
                    },
                    {
                        "name": "Lior Wolf"
                    },
                    {
                        "name": "Shelly Sheynin"
                    }
                ],
                "author_detail": {
                    "name": "Shelly Sheynin"
                },
                "author": "Shelly Sheynin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02483v1",
                "updated": "2025-02-04T16:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    59,
                    3,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    59,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Distributional Diffusion Models with Scoring Rules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Diffusion Models with Scoring Rules"
                },
                "summary": "Diffusion models generate high-quality synthetic data. They operate by\ndefining a continuous-time forward process which gradually adds Gaussian noise\nto data until fully corrupted. The corresponding reverse process progressively\n\"denoises\" a Gaussian sample into a sample from the data distribution. However,\ngenerating high-quality outputs requires many discretization steps to obtain a\nfaithful approximation of the reverse process. This is expensive and has\nmotivated the development of many acceleration methods. We propose to\naccomplish sample generation by learning the posterior {\\em distribution} of\nclean data samples given their noisy versions, instead of only the mean of this\ndistribution. This allows us to sample from the probability transitions of the\nreverse process on a coarse time scale, significantly accelerating inference\nwith minimal degradation of the quality of the output. This is accomplished by\nreplacing the standard regression loss used to estimate conditional means with\na scoring rule. We validate our method on image and robot trajectory\ngeneration, where we consistently outperform standard diffusion models at few\ndiscretization steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models generate high-quality synthetic data. They operate by\ndefining a continuous-time forward process which gradually adds Gaussian noise\nto data until fully corrupted. The corresponding reverse process progressively\n\"denoises\" a Gaussian sample into a sample from the data distribution. However,\ngenerating high-quality outputs requires many discretization steps to obtain a\nfaithful approximation of the reverse process. This is expensive and has\nmotivated the development of many acceleration methods. We propose to\naccomplish sample generation by learning the posterior {\\em distribution} of\nclean data samples given their noisy versions, instead of only the mean of this\ndistribution. This allows us to sample from the probability transitions of the\nreverse process on a coarse time scale, significantly accelerating inference\nwith minimal degradation of the quality of the output. This is accomplished by\nreplacing the standard regression loss used to estimate conditional means with\na scoring rule. We validate our method on image and robot trajectory\ngeneration, where we consistently outperform standard diffusion models at few\ndiscretization steps."
                },
                "authors": [
                    {
                        "name": "Valentin De Bortoli"
                    },
                    {
                        "name": "Alexandre Galashov"
                    },
                    {
                        "name": "J. Swaroop Guntupalli"
                    },
                    {
                        "name": "Guangyao Zhou"
                    },
                    {
                        "name": "Kevin Murphy"
                    },
                    {
                        "name": "Arthur Gretton"
                    },
                    {
                        "name": "Arnaud Doucet"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud Doucet"
                },
                "author": "Arnaud Doucet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02481v1",
                "updated": "2025-02-04T16:57:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    57,
                    3,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:57:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    57,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study"
                },
                "summary": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo."
                },
                "authors": [
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Pengzhi Gao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "BinWang"
                    }
                ],
                "author_detail": {
                    "name": "BinWang"
                },
                "author": "BinWang",
                "arxiv_comment": "Accept to NAACL2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01042v2",
                "updated": "2025-02-04T16:47:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    47,
                    38,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-03T04:23:33Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    23,
                    33,
                    0,
                    34,
                    0
                ],
                "title": "Internal Activation as the Polar Star for Steering Unsafe LLM Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Activation as the Polar Star for Steering Unsafe LLM Behavior"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross a wide range of tasks but also pose significant risks due to their\npotential to generate harmful content. Although existing safety mechanisms can\nimprove model safety, they often lead to overly cautious behavior and fail to\nfully utilize LLMs' internal cognitive processes. Drawing inspiration from\ncognitive science, where humans rely on reflective reasoning (System 2\nthinking) to regulate language and behavior, we empirically demonstrate that\nLLMs also possess a similar capacity for internal assessment and regulation,\nwhich can be actively detected.\n  Building on this insight, we introduce SafeSwitch, a framework that\ndynamically regulates unsafe outputs by monitoring and utilizing the model's\ninternal states. Our empirical results show that SafeSwitch reduces harmful\noutputs by over 80% on safety benchmarks while maintaining strong utility.\nCompared to traditional safety alignment methods, SafeSwitch delivers more\ninformative and context-aware refusals, demonstrates resilience to unseen\nqueries, and achieves these benefits while only tuning less than 6% of the\noriginal parameters. These features make SafeSwitch a promising approach for\nimplementing nuanced safety controls in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross a wide range of tasks but also pose significant risks due to their\npotential to generate harmful content. Although existing safety mechanisms can\nimprove model safety, they often lead to overly cautious behavior and fail to\nfully utilize LLMs' internal cognitive processes. Drawing inspiration from\ncognitive science, where humans rely on reflective reasoning (System 2\nthinking) to regulate language and behavior, we empirically demonstrate that\nLLMs also possess a similar capacity for internal assessment and regulation,\nwhich can be actively detected.\n  Building on this insight, we introduce SafeSwitch, a framework that\ndynamically regulates unsafe outputs by monitoring and utilizing the model's\ninternal states. Our empirical results show that SafeSwitch reduces harmful\noutputs by over 80% on safety benchmarks while maintaining strong utility.\nCompared to traditional safety alignment methods, SafeSwitch delivers more\ninformative and context-aware refusals, demonstrates resilience to unseen\nqueries, and achieves these benefits while only tuning less than 6% of the\noriginal parameters. These features make SafeSwitch a promising approach for\nimplementing nuanced safety controls in LLMs."
                },
                "authors": [
                    {
                        "name": "Peixuan Han"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Yuji Zhang"
                    },
                    {
                        "name": "Denghui Zhang"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09521v2",
                "updated": "2025-02-04T16:42:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    42,
                    33,
                    1,
                    35,
                    0
                ],
                "published": "2024-06-13T18:17:03Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    18,
                    17,
                    3,
                    3,
                    165,
                    0
                ],
                "title": "Randomization Inference: Theory and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomization Inference: Theory and Applications"
                },
                "summary": "We review approaches to statistical inference based on randomization.\nPermutation tests are treated as an important special case. Under a certain\ngroup invariance property, referred to as the ``randomization hypothesis,''\nrandomization tests achieve exact control of the Type I error rate in finite\nsamples. Although this unequivocal precision is very appealing, the range of\nproblems that satisfy the randomization hypothesis is somewhat limited. We show\nthat randomization tests are often asymptotically, or approximately, valid and\nefficient in settings that deviate from the conditions required for\nfinite-sample error control. When randomization tests fail to offer even\nasymptotic Type 1 error control, their asymptotic validity may be restored by\nconstructing an asymptotically pivotal test statistic. Randomization tests can\nthen provide exact error control for tests of highly structured hypotheses with\ngood performance in a wider class of problems. We give a detailed overview of\nseveral prominent applications of randomization tests, including two-sample\npermutation tests, regression, and conformal inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We review approaches to statistical inference based on randomization.\nPermutation tests are treated as an important special case. Under a certain\ngroup invariance property, referred to as the ``randomization hypothesis,''\nrandomization tests achieve exact control of the Type I error rate in finite\nsamples. Although this unequivocal precision is very appealing, the range of\nproblems that satisfy the randomization hypothesis is somewhat limited. We show\nthat randomization tests are often asymptotically, or approximately, valid and\nefficient in settings that deviate from the conditions required for\nfinite-sample error control. When randomization tests fail to offer even\nasymptotic Type 1 error control, their asymptotic validity may be restored by\nconstructing an asymptotically pivotal test statistic. Randomization tests can\nthen provide exact error control for tests of highly structured hypotheses with\ngood performance in a wider class of problems. We give a detailed overview of\nseveral prominent applications of randomization tests, including two-sample\npermutation tests, regression, and conformal inference."
                },
                "authors": [
                    {
                        "name": "David M. Ritzwoller"
                    },
                    {
                        "name": "Joseph P. Romano"
                    },
                    {
                        "name": "Azeem M. Shaikh"
                    }
                ],
                "author_detail": {
                    "name": "Azeem M. Shaikh"
                },
                "author": "Azeem M. Shaikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02463v1",
                "updated": "2025-02-04T16:33:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    33,
                    12,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:33:12Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    33,
                    12,
                    1,
                    35,
                    0
                ],
                "title": "Distribution Transformers: Fast Approximate Bayesian Inference With\n  On-The-Fly Prior Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution Transformers: Fast Approximate Bayesian Inference With\n  On-The-Fly Prior Adaptation"
                },
                "summary": "While Bayesian inference provides a principled framework for reasoning under\nuncertainty, its widespread adoption is limited by the intractability of exact\nposterior computation, necessitating the use of approximate inference. However,\nexisting methods are often computationally expensive, or demand costly\nretraining when priors change, limiting their utility, particularly in\nsequential inference problems such as real-time sensor fusion. To address these\nchallenges, we introduce the Distribution Transformer -- a novel architecture\nthat can learn arbitrary distribution-to-distribution mappings. Our method can\nbe trained to map a prior to the corresponding posterior, conditioned on some\ndataset -- thus performing approximate Bayesian inference. Our novel\narchitecture represents a prior distribution as a (universally-approximating)\nGaussian Mixture Model (GMM), and transforms it into a GMM representation of\nthe posterior. The components of the GMM attend to each other via\nself-attention, and to the datapoints via cross-attention. We demonstrate that\nDistribution Transformers both maintain flexibility to vary the prior, and\nsignificantly reduces computation times-from minutes to milliseconds-while\nachieving log-likelihood performance on par with or superior to existing\napproximate inference methods across tasks such as sequential inference,\nquantum system parameter inference, and Gaussian Process predictive posterior\ninference with hyperpriors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Bayesian inference provides a principled framework for reasoning under\nuncertainty, its widespread adoption is limited by the intractability of exact\nposterior computation, necessitating the use of approximate inference. However,\nexisting methods are often computationally expensive, or demand costly\nretraining when priors change, limiting their utility, particularly in\nsequential inference problems such as real-time sensor fusion. To address these\nchallenges, we introduce the Distribution Transformer -- a novel architecture\nthat can learn arbitrary distribution-to-distribution mappings. Our method can\nbe trained to map a prior to the corresponding posterior, conditioned on some\ndataset -- thus performing approximate Bayesian inference. Our novel\narchitecture represents a prior distribution as a (universally-approximating)\nGaussian Mixture Model (GMM), and transforms it into a GMM representation of\nthe posterior. The components of the GMM attend to each other via\nself-attention, and to the datapoints via cross-attention. We demonstrate that\nDistribution Transformers both maintain flexibility to vary the prior, and\nsignificantly reduces computation times-from minutes to milliseconds-while\nachieving log-likelihood performance on par with or superior to existing\napproximate inference methods across tasks such as sequential inference,\nquantum system parameter inference, and Gaussian Process predictive posterior\ninference with hyperpriors."
                },
                "authors": [
                    {
                        "name": "George Whittle"
                    },
                    {
                        "name": "Juliusz Ziomek"
                    },
                    {
                        "name": "Jacob Rawling"
                    },
                    {
                        "name": "Michael A Osborne"
                    }
                ],
                "author_detail": {
                    "name": "Michael A Osborne"
                },
                "author": "Michael A Osborne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02458v1",
                "updated": "2025-02-04T16:28:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    28,
                    53,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:28:53Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    28,
                    53,
                    1,
                    35,
                    0
                ],
                "title": "SAISA: Towards Multimodal Large Language Models with Both Training and\n  Inference Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAISA: Towards Multimodal Large Language Models with Both Training and\n  Inference Efficiency"
                },
                "summary": "Multimodal Large Language Models (MLLMs) mainly fall into two architectures,\neach involving a trade-off between training and inference efficiency: embedding\nspace alignment (e.g., LLaVA-1.5) is inefficient during inference, while\ncross-attention space alignment (e.g., Flamingo) is inefficient in training. In\nthis paper, we compare these two architectures and identify the key factors for\nbuilding efficient MLLMs. A primary difference between them lies in how\nattention is applied to visual tokens, particularly in their interactions with\neach other. To investigate whether attention among visual tokens is necessary,\nwe propose a new self-attention mechanism, NAAViT (\\textbf{N}o\n\\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens), which\neliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that\nattention among visual tokens is highly redundant. Based on these insights, we\nintroduce SAISA (\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace\n\\textbf{A}lignment), a novel architecture that enhance both training and\ninference efficiency. SAISA directly aligns visual features with the input\nspaces of NAAViT self-attention blocks, reducing computational overhead in both\nself-attention blocks and feed-forward networks (FFNs). Using the same\nconfiguration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\\% and training\nbudget by 26\\%, while achieving superior performance in terms of accuracy.\nComprehensive ablation studies further validate the effectiveness of SAISA\nacross various LLMs and visual encoders. The code and model will be publicly\navailable at https://github.com/icip-cas/SAISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) mainly fall into two architectures,\neach involving a trade-off between training and inference efficiency: embedding\nspace alignment (e.g., LLaVA-1.5) is inefficient during inference, while\ncross-attention space alignment (e.g., Flamingo) is inefficient in training. In\nthis paper, we compare these two architectures and identify the key factors for\nbuilding efficient MLLMs. A primary difference between them lies in how\nattention is applied to visual tokens, particularly in their interactions with\neach other. To investigate whether attention among visual tokens is necessary,\nwe propose a new self-attention mechanism, NAAViT (\\textbf{N}o\n\\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens), which\neliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that\nattention among visual tokens is highly redundant. Based on these insights, we\nintroduce SAISA (\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace\n\\textbf{A}lignment), a novel architecture that enhance both training and\ninference efficiency. SAISA directly aligns visual features with the input\nspaces of NAAViT self-attention blocks, reducing computational overhead in both\nself-attention blocks and feed-forward networks (FFNs). Using the same\nconfiguration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\\% and training\nbudget by 26\\%, while achieving superior performance in terms of accuracy.\nComprehensive ablation studies further validate the effectiveness of SAISA\nacross various LLMs and visual encoders. The code and model will be publicly\navailable at https://github.com/icip-cas/SAISA."
                },
                "authors": [
                    {
                        "name": "Qianhao Yuan"
                    },
                    {
                        "name": "Yanjiang Liu"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19358v2",
                "updated": "2025-02-04T16:22:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    22,
                    43,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-31T18:10:53Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    10,
                    53,
                    4,
                    31,
                    0
                ],
                "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking"
                },
                "summary": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\nEPPO in mitigating reward hacking and improving RLHF performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\nEPPO in mitigating reward hacking and improving RLHF performance."
                },
                "authors": [
                    {
                        "name": "Yuchun Miao"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yuqi Zhang"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "28 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10993v2",
                "updated": "2025-02-04T16:17:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    17,
                    36,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-14T18:20:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    18,
                    20,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "Direct high-resolution observation of feedback and chemical enrichment\n  in the circumgalactic medium at redshift z ~ 2.8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct high-resolution observation of feedback and chemical enrichment\n  in the circumgalactic medium at redshift z ~ 2.8"
                },
                "summary": "The circumgalactic medium (CGM) plays a vital role in galaxy evolution,\nhowever, studying the emission from CGM is challenging due to its low surface\nbrightness and the complexities involved in interpreting resonant lines such as\nLy$\\alpha$. The near-infrared coverage, unprecedented sensitivity, and high\nspatial resolution of JWST enable us to study the optical strong lines\nassociated with the extended Ly$\\alpha$ \"nebulae\" at redshifts of 2--3. These\nlines serve as diagnostic tools to infer the physical conditions in the CGM gas\nreservoir of these systems. In deep medium-band images taken by the JWST, we\nserendipitously discovered the [O III] emission from the CGM around a massive\ninteracting galaxy system at a redshift z~2.8, known to be embedded in a bright\nextended (100 kpc) Ly$\\alpha$ \"nebula.\" This is the first time that the [O III]\nlines have been detected from a Ly$\\alpha$ \"nebula.\" The JWST images reveal\nthat the CGM gas actually resides in narrow (~ 2.5 kpc) filamentary structures\nwith strong [O III] emission, tracing the same extent as the Ly$\\alpha$\nemission. An analysis of the [O III] suggests that the emitting CGM is fully\nionized and is energetically dominated by mechanical heating. We also find that\nthe density and pressure are higher than those commonly predicted by\nsimulations of the CGM. We conclude that the observed CGM emission originates\nfrom the gas expelled by the episodic feedback processes, cooling down and\nenriching the CGM, while traveling a distance of at least 60 kpc. These\nobservations demonstrate how intensive feedback processes shape gas\ndistribution and properties in the CGM around massive halos. While access to\nsuch deep, high-resolution imaging opens up a new discovery space for\ninvestigating the CGM, it also challenges numerical simulations with respect to\nexplaining and reproducing the exquisitely complex structures revealed by the\nobservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The circumgalactic medium (CGM) plays a vital role in galaxy evolution,\nhowever, studying the emission from CGM is challenging due to its low surface\nbrightness and the complexities involved in interpreting resonant lines such as\nLy$\\alpha$. The near-infrared coverage, unprecedented sensitivity, and high\nspatial resolution of JWST enable us to study the optical strong lines\nassociated with the extended Ly$\\alpha$ \"nebulae\" at redshifts of 2--3. These\nlines serve as diagnostic tools to infer the physical conditions in the CGM gas\nreservoir of these systems. In deep medium-band images taken by the JWST, we\nserendipitously discovered the [O III] emission from the CGM around a massive\ninteracting galaxy system at a redshift z~2.8, known to be embedded in a bright\nextended (100 kpc) Ly$\\alpha$ \"nebula.\" This is the first time that the [O III]\nlines have been detected from a Ly$\\alpha$ \"nebula.\" The JWST images reveal\nthat the CGM gas actually resides in narrow (~ 2.5 kpc) filamentary structures\nwith strong [O III] emission, tracing the same extent as the Ly$\\alpha$\nemission. An analysis of the [O III] suggests that the emitting CGM is fully\nionized and is energetically dominated by mechanical heating. We also find that\nthe density and pressure are higher than those commonly predicted by\nsimulations of the CGM. We conclude that the observed CGM emission originates\nfrom the gas expelled by the episodic feedback processes, cooling down and\nenriching the CGM, while traveling a distance of at least 60 kpc. These\nobservations demonstrate how intensive feedback processes shape gas\ndistribution and properties in the CGM around massive halos. While access to\nsuch deep, high-resolution imaging opens up a new discovery space for\ninvestigating the CGM, it also challenges numerical simulations with respect to\nexplaining and reproducing the exquisitely complex structures revealed by the\nobservations."
                },
                "authors": [
                    {
                        "name": "Bo Peng"
                    },
                    {
                        "name": "Fabrizio Arrigoni Battaia"
                    },
                    {
                        "name": "Amit Vishwas"
                    },
                    {
                        "name": "Mingyu Li"
                    },
                    {
                        "name": "Edoardo Iani"
                    },
                    {
                        "name": "Fengwu Sun"
                    },
                    {
                        "name": "Qiong Li"
                    },
                    {
                        "name": "Carl Ferkinhoff"
                    },
                    {
                        "name": "Gordon Stacey"
                    },
                    {
                        "name": "Zheng Cai"
                    },
                    {
                        "name": "Rob Ivison"
                    }
                ],
                "author_detail": {
                    "name": "Rob Ivison"
                },
                "author": "Rob Ivison",
                "arxiv_doi": "10.1051/0004-6361/202452610",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202452610",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.10993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures, 1 table, accepted by A&A Letter",
                "arxiv_journal_ref": "A&A 694, L1 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02451v1",
                "updated": "2025-02-04T16:17:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    17,
                    1,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:17:01Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    17,
                    1,
                    1,
                    35,
                    0
                ],
                "title": "Beyond English: Evaluating Automated Measurement of Moral Foundations in\n  Non-English Discourse with a Chinese Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond English: Evaluating Automated Measurement of Moral Foundations in\n  Non-English Discourse with a Chinese Case Study"
                },
                "summary": "This study explores computational approaches for measuring moral foundations\n(MFs) in non-English corpora. Since most resources are developed primarily for\nEnglish, cross-linguistic applications of moral foundation theory remain\nlimited. Using Chinese as a case study, this paper evaluates the effectiveness\nof applying English resources to machine translated text, local language\nlexicons, multilingual language models, and large language models (LLMs) in\nmeasuring MFs in non-English texts. The results indicate that machine\ntranslation and local lexicon approaches are insufficient for complex moral\nassessments, frequently resulting in a substantial loss of cultural\ninformation. In contrast, multilingual models and LLMs demonstrate reliable\ncross-language performance with transfer learning, with LLMs excelling in terms\nof data efficiency. Importantly, this study also underscores the need for\nhuman-in-the-loop validation of automated MF assessment, as the most advanced\nmodels may overlook cultural nuances in cross-language measurements. The\nfindings highlight the potential of LLMs for cross-language MF measurements and\nother complex multilingual deductive coding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores computational approaches for measuring moral foundations\n(MFs) in non-English corpora. Since most resources are developed primarily for\nEnglish, cross-linguistic applications of moral foundation theory remain\nlimited. Using Chinese as a case study, this paper evaluates the effectiveness\nof applying English resources to machine translated text, local language\nlexicons, multilingual language models, and large language models (LLMs) in\nmeasuring MFs in non-English texts. The results indicate that machine\ntranslation and local lexicon approaches are insufficient for complex moral\nassessments, frequently resulting in a substantial loss of cultural\ninformation. In contrast, multilingual models and LLMs demonstrate reliable\ncross-language performance with transfer learning, with LLMs excelling in terms\nof data efficiency. Importantly, this study also underscores the need for\nhuman-in-the-loop validation of automated MF assessment, as the most advanced\nmodels may overlook cultural nuances in cross-language measurements. The\nfindings highlight the potential of LLMs for cross-language MF measurements and\nother complex multilingual deductive coding tasks."
                },
                "authors": [
                    {
                        "name": "Calvin Yixiang Cheng"
                    },
                    {
                        "name": "Scott A Hale"
                    }
                ],
                "author_detail": {
                    "name": "Scott A Hale"
                },
                "author": "Scott A Hale",
                "arxiv_comment": "12 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00903v2",
                "updated": "2025-02-04T16:15:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    15,
                    45,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-02T20:29:10Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    20,
                    29,
                    10,
                    6,
                    33,
                    0
                ],
                "title": "Embracing Dialectic Intersubjectivity: Coordination of Different\n  Perspectives in Content Analysis with LLM Persona Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embracing Dialectic Intersubjectivity: Coordination of Different\n  Perspectives in Content Analysis with LLM Persona Simulation"
                },
                "summary": "This study attempts to advancing content analysis methodology from\nconsensus-oriented to coordination-oriented practices, thereby embracing\ndiverse coding outputs and exploring the dynamics among differential\nperspectives. As an exploratory investigation of this approach, we evaluate six\nGPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on\nBiden and Trump during the 2020 U.S. presidential campaign, examining patterns\nacross these models. By assessing each model's alignment with ideological\nperspectives, we explore how partisan selective processing could be identified\nin LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona\nLLMs exhibit stronger ideological biases when processing politically congruent\ncontent. Additionally, intercoder reliability is higher among same-partisan\npersonas compared to cross-partisan pairs. This approach enhances the nuanced\nunderstanding of LLM outputs and advances the integrity of AI-driven social\nscience research, enabling simulations of real-world implications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study attempts to advancing content analysis methodology from\nconsensus-oriented to coordination-oriented practices, thereby embracing\ndiverse coding outputs and exploring the dynamics among differential\nperspectives. As an exploratory investigation of this approach, we evaluate six\nGPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on\nBiden and Trump during the 2020 U.S. presidential campaign, examining patterns\nacross these models. By assessing each model's alignment with ideological\nperspectives, we explore how partisan selective processing could be identified\nin LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona\nLLMs exhibit stronger ideological biases when processing politically congruent\ncontent. Additionally, intercoder reliability is higher among same-partisan\npersonas compared to cross-partisan pairs. This approach enhances the nuanced\nunderstanding of LLM outputs and advances the integrity of AI-driven social\nscience research, enabling simulations of real-world implications."
                },
                "authors": [
                    {
                        "name": "Taewoo Kang"
                    },
                    {
                        "name": "Kjerstin Thorson"
                    },
                    {
                        "name": "Tai-Quan Peng"
                    },
                    {
                        "name": "Dan Hiaeshutter-Rice"
                    },
                    {
                        "name": "Sanguk Lee"
                    },
                    {
                        "name": "Stuart Soroka"
                    }
                ],
                "author_detail": {
                    "name": "Stuart Soroka"
                },
                "author": "Stuart Soroka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02444v1",
                "updated": "2025-02-04T16:10:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    10,
                    55,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:10:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    10,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models"
                },
                "summary": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Tianze Zhang"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Liyuan Zhang"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02441v1",
                "updated": "2025-02-04T16:08:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    8,
                    48,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:08:48Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    8,
                    48,
                    1,
                    35,
                    0
                ],
                "title": "LLMER: Crafting Interactive Extended Reality Worlds with JSON Data\n  Generated by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMER: Crafting Interactive Extended Reality Worlds with JSON Data\n  Generated by Large Language Models"
                },
                "summary": "The integration of Large Language Models (LLMs) like GPT-4 with Extended\nReality (XR) technologies offers the potential to build truly immersive XR\nenvironments that interact with human users through natural language, e.g.,\ngenerating and animating 3D scenes from audio inputs. However, the complexity\nof XR environments makes it difficult to accurately extract relevant contextual\ndata and scene/object parameters from an overwhelming volume of XR artifacts.\nIt leads to not only increased costs with pay-per-use models, but also elevated\nlevels of generation errors. Moreover, existing approaches focusing on coding\nscript generation are often prone to generation errors, resulting in flawed or\ninvalid scripts, application crashes, and ultimately a degraded user\nexperience. To overcome these challenges, we introduce LLMER, a novel framework\nthat creates interactive XR worlds using JSON data generated by LLMs. Unlike\nprior approaches focusing on coding script generation, LLMER translates natural\nlanguage inputs into JSON data, significantly reducing the likelihood of\napplication crashes and processing latency. It employs a multi-stage strategy\nto supply only the essential contextual information adapted to the user's\nrequest and features multiple modules designed for various XR tasks. Our\npreliminary user study reveals the effectiveness of the proposed system, with\nover 80% reduction in consumed tokens and around 60% reduction in task\ncompletion time compared to state-of-the-art approaches. The analysis of users'\nfeedback also illuminates a series of directions for further optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) like GPT-4 with Extended\nReality (XR) technologies offers the potential to build truly immersive XR\nenvironments that interact with human users through natural language, e.g.,\ngenerating and animating 3D scenes from audio inputs. However, the complexity\nof XR environments makes it difficult to accurately extract relevant contextual\ndata and scene/object parameters from an overwhelming volume of XR artifacts.\nIt leads to not only increased costs with pay-per-use models, but also elevated\nlevels of generation errors. Moreover, existing approaches focusing on coding\nscript generation are often prone to generation errors, resulting in flawed or\ninvalid scripts, application crashes, and ultimately a degraded user\nexperience. To overcome these challenges, we introduce LLMER, a novel framework\nthat creates interactive XR worlds using JSON data generated by LLMs. Unlike\nprior approaches focusing on coding script generation, LLMER translates natural\nlanguage inputs into JSON data, significantly reducing the likelihood of\napplication crashes and processing latency. It employs a multi-stage strategy\nto supply only the essential contextual information adapted to the user's\nrequest and features multiple modules designed for various XR tasks. Our\npreliminary user study reveals the effectiveness of the proposed system, with\nover 80% reduction in consumed tokens and around 60% reduction in task\ncompletion time compared to state-of-the-art approaches. The analysis of users'\nfeedback also illuminates a series of directions for further optimization."
                },
                "authors": [
                    {
                        "name": "Jiangong Chen"
                    },
                    {
                        "name": "Xiaoyi Wu"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04032v2",
                "updated": "2025-02-04T16:05:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    5,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-06T16:31:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    31,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Beemo: Benchmark of Expert-edited Machine-generated Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beemo: Benchmark of Expert-edited Machine-generated Outputs"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Jason Lucas"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Jooyoung Lee"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Adaku Uchendu"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    }
                ],
                "author_detail": {
                    "name": "Vladislav Mikhailov"
                },
                "author": "Vladislav Mikhailov",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09321v2",
                "updated": "2025-02-04T16:04:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    4,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-06-13T16:59:43Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    59,
                    43,
                    3,
                    165,
                    0
                ],
                "title": "JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts\n  Against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts\n  Against Large Language Models"
                },
                "summary": "Jailbreak attacks induce Large Language Models (LLMs) to generate harmful\nresponses, posing severe misuse threats. Though research on jailbreak attacks\nand defenses is emerging, there is no consensus on evaluating jailbreaks, i.e.,\nthe methods to assess the harmfulness of an LLM's response are varied. Each\napproach has its own set of strengths and weaknesses, impacting their alignment\nwith human values, as well as the time and financial cost. This diversity\nchallenges researchers in choosing suitable evaluation methods and comparing\ndifferent attacks and defenses. In this paper, we conduct a comprehensive\nanalysis of jailbreak evaluation methodologies, drawing from nearly 90\njailbreak research published between May 2023 and April 2024. Our study\nintroduces a systematic taxonomy of jailbreak evaluators, offering indepth\ninsights into their strengths and weaknesses, along with the current status of\ntheir adaptation. To aid further research, we propose JailbreakEval, a toolkit\nfor evaluating jailbreak attempts. JailbreakEval includes various evaluators\nout-of-the-box, enabling users to obtain results with a single command or\ncustomized evaluation workflows. In summary, we regard JailbreakEval to be a\ncatalyst that simplifies the evaluation process in jailbreak research and\nfosters an inclusive standard for jailbreak evaluation within the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks induce Large Language Models (LLMs) to generate harmful\nresponses, posing severe misuse threats. Though research on jailbreak attacks\nand defenses is emerging, there is no consensus on evaluating jailbreaks, i.e.,\nthe methods to assess the harmfulness of an LLM's response are varied. Each\napproach has its own set of strengths and weaknesses, impacting their alignment\nwith human values, as well as the time and financial cost. This diversity\nchallenges researchers in choosing suitable evaluation methods and comparing\ndifferent attacks and defenses. In this paper, we conduct a comprehensive\nanalysis of jailbreak evaluation methodologies, drawing from nearly 90\njailbreak research published between May 2023 and April 2024. Our study\nintroduces a systematic taxonomy of jailbreak evaluators, offering indepth\ninsights into their strengths and weaknesses, along with the current status of\ntheir adaptation. To aid further research, we propose JailbreakEval, a toolkit\nfor evaluating jailbreak attempts. JailbreakEval includes various evaluators\nout-of-the-box, enabling users to obtain results with a single command or\ncustomized evaluation workflows. In summary, we regard JailbreakEval to be a\ncatalyst that simplifies the evaluation process in jailbreak research and\nfosters an inclusive standard for jailbreak evaluation within the community."
                },
                "authors": [
                    {
                        "name": "Delong Ran"
                    },
                    {
                        "name": "Jinyuan Liu"
                    },
                    {
                        "name": "Yichen Gong"
                    },
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Anyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anyu Wang"
                },
                "author": "Anyu Wang",
                "arxiv_comment": "This is the Extended Version for the Poster at NDSS Symposium 2025,\n  Feb 24-28, 2025. Our code is available at\n  https://github.com/ThuCCSLab/JailbreakEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19368v2",
                "updated": "2025-02-04T16:04:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    4,
                    15,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-31T18:20:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    20,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "The Physics and Metaphysics of Social Powers: Bridging Cognitive\n  Processing and Social Dynamics, a New Perspective on Power through Active\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Physics and Metaphysics of Social Powers: Bridging Cognitive\n  Processing and Social Dynamics, a New Perspective on Power through Active\n  Inference"
                },
                "summary": "The concept of power can be explored at several scales: from physical action\nand process effectuation, all the way to complex social dynamics. A\nspectrum-wide analysis of power requires attention to the fundamental\nprinciples that constrain these processes. In the social realm, the acquisition\nand maintenance of power is intertwined with both social interactions and\ncognitive processing capacity: socially-facilitated empowerment grants agents\nmore information-processing capacities and opportunities, either by relying on\nothers to bring about desired policies or ultimately outcomes, and/or by\nenjoying more information-processing possibilities as a result of relying on\nothers for the reproduction of (material) tasks. The effects of social\nempowerment thus imply an increased ability to harness computation toward\ndesired ends, thereby augmenting the evolution of a specific state space.\nEmpowered individuals attract the attention of others, who contribute to\nincreasing the scale of their access to various policies effectuating these\nstate spaces. The presented argument posits that social power, in the context\nof active inference, is a function of several variables. As a result of its\npower-amplifying effects, this extended computational ability also buffers\nagainst possible vulnerabilities. We propose that individuals wield power not\nonly by associating with others possessing desirable policies, but also by\nenhancing their ability to intake and compute information effectively. This\ndual mechanism is argued to create a cyclical, reinforcing pattern wherein the\nempowered are able to incrementally expand the scope of policies and state\nspaces available to them while minimizing risk-exposure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of power can be explored at several scales: from physical action\nand process effectuation, all the way to complex social dynamics. A\nspectrum-wide analysis of power requires attention to the fundamental\nprinciples that constrain these processes. In the social realm, the acquisition\nand maintenance of power is intertwined with both social interactions and\ncognitive processing capacity: socially-facilitated empowerment grants agents\nmore information-processing capacities and opportunities, either by relying on\nothers to bring about desired policies or ultimately outcomes, and/or by\nenjoying more information-processing possibilities as a result of relying on\nothers for the reproduction of (material) tasks. The effects of social\nempowerment thus imply an increased ability to harness computation toward\ndesired ends, thereby augmenting the evolution of a specific state space.\nEmpowered individuals attract the attention of others, who contribute to\nincreasing the scale of their access to various policies effectuating these\nstate spaces. The presented argument posits that social power, in the context\nof active inference, is a function of several variables. As a result of its\npower-amplifying effects, this extended computational ability also buffers\nagainst possible vulnerabilities. We propose that individuals wield power not\nonly by associating with others possessing desirable policies, but also by\nenhancing their ability to intake and compute information effectively. This\ndual mechanism is argued to create a cyclical, reinforcing pattern wherein the\nempowered are able to incrementally expand the scope of policies and state\nspaces available to them while minimizing risk-exposure."
                },
                "authors": [
                    {
                        "name": "Mahault Albarracin"
                    },
                    {
                        "name": "Sonia de Jager"
                    },
                    {
                        "name": "David Hyland"
                    },
                    {
                        "name": "Sarah Grace Manski"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Grace Manski"
                },
                "author": "Sarah Grace Manski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00883v2",
                "updated": "2025-02-04T16:02:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    2,
                    53,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-02T19:25:41Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    25,
                    41,
                    6,
                    33,
                    0
                ],
                "title": "SimPER: A Minimalist Approach to Preference Alignment without\n  Hyperparameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimPER: A Minimalist Approach to Preference Alignment without\n  Hyperparameters"
                },
                "summary": "Existing preference optimization objectives for language model alignment\nrequire additional hyperparameters that must be extensively tuned to achieve\noptimal performance, increasing both the complexity and time required for\nfine-tuning large language models. In this paper, we propose a simple yet\neffective hyperparameter-free preference optimization algorithm for alignment.\nWe observe that promising performance can be achieved simply by optimizing\ninverse perplexity, which is calculated as the inverse of the exponentiated\naverage log-likelihood of the chosen and rejected responses in the preference\ndataset. The resulting simple learning objective, SimPER, is easy to implement\nand eliminates the need for expensive hyperparameter tuning and a reference\nmodel, making it both computationally and memory efficient. Extensive\nexperiments on widely used real-world benchmarks, including MT-Bench,\nAlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base\nmodels, demonstrate that SimPER consistently and significantly outperforms\nexisting approaches-even without any hyperparameters or a reference model . For\nexample, despite its simplicity, SimPER outperforms state-of-the-art methods by\nup to 5.7 points on AlpacaEval 2 and achieves the highest average ranking\nacross 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is\npublicly available at: https://github.com/tengxiao1/SimPER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing preference optimization objectives for language model alignment\nrequire additional hyperparameters that must be extensively tuned to achieve\noptimal performance, increasing both the complexity and time required for\nfine-tuning large language models. In this paper, we propose a simple yet\neffective hyperparameter-free preference optimization algorithm for alignment.\nWe observe that promising performance can be achieved simply by optimizing\ninverse perplexity, which is calculated as the inverse of the exponentiated\naverage log-likelihood of the chosen and rejected responses in the preference\ndataset. The resulting simple learning objective, SimPER, is easy to implement\nand eliminates the need for expensive hyperparameter tuning and a reference\nmodel, making it both computationally and memory efficient. Extensive\nexperiments on widely used real-world benchmarks, including MT-Bench,\nAlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base\nmodels, demonstrate that SimPER consistently and significantly outperforms\nexisting approaches-even without any hyperparameters or a reference model . For\nexample, despite its simplicity, SimPER outperforms state-of-the-art methods by\nup to 5.7 points on AlpacaEval 2 and achieves the highest average ranking\nacross 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is\npublicly available at: https://github.com/tengxiao1/SimPER."
                },
                "authors": [
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Yige Yuan"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Mingxiao Li"
                    },
                    {
                        "name": "Shangsong Liang"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Vasant G Honavar"
                    }
                ],
                "author_detail": {
                    "name": "Vasant G Honavar"
                },
                "author": "Vasant G Honavar",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11202v2",
                "updated": "2025-02-04T16:00:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    0,
                    8,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-20T00:22:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    0,
                    22,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Online Hybrid-Belief POMDP with Coupled Semantic-Geometric Models and\n  Semantic Safety Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Hybrid-Belief POMDP with Coupled Semantic-Geometric Models and\n  Semantic Safety Awareness"
                },
                "summary": "Robots operating in complex and unknown environments frequently require\ngeometric-semantic representations of the environment to safely perform their\ntasks. While inferring the environment, they must account for many possible\nscenarios when planning future actions. Since objects' class types are discrete\nand the robot's self-pose and the objects' poses are continuous, the\nenvironment can be represented by a hybrid discrete-continuous belief which is\nupdated according to models and incoming data. Prior probabilities and\nobservation models representing the environment can be learned from data using\ndeep learning algorithms. Such models often couple environmental semantic and\ngeometric properties. As a result, semantic variables are interconnected,\ncausing semantic state space dimensionality to increase exponentially. In this\npaper, we consider planning under uncertainty using partially observable Markov\ndecision processes (POMDPs) with hybrid semantic-geometric beliefs. The models\nand priors consider the coupling between semantic and geometric variables.\nWithin POMDP, we introduce the concept of semantically aware safety. Obtaining\nrepresentative samples of the theoretical hybrid belief, required for\nestimating the value function, is very challenging. As a key contribution, we\ndevelop a novel form of the hybrid belief and leverage it to sample\nrepresentative samples. We show that under certain conditions, the value\nfunction and probability of safety can be calculated efficiently with an\nexplicit expectation over all possible semantic mappings. Our simulations show\nthat our estimates of the objective function and probability of safety achieve\nsimilar levels of accuracy compared to estimators that run exhaustively on the\nentire semantic state-space using samples from the theoretical hybrid belief.\nNevertheless, the complexity of our estimators is polynomial rather than\nexponential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots operating in complex and unknown environments frequently require\ngeometric-semantic representations of the environment to safely perform their\ntasks. While inferring the environment, they must account for many possible\nscenarios when planning future actions. Since objects' class types are discrete\nand the robot's self-pose and the objects' poses are continuous, the\nenvironment can be represented by a hybrid discrete-continuous belief which is\nupdated according to models and incoming data. Prior probabilities and\nobservation models representing the environment can be learned from data using\ndeep learning algorithms. Such models often couple environmental semantic and\ngeometric properties. As a result, semantic variables are interconnected,\ncausing semantic state space dimensionality to increase exponentially. In this\npaper, we consider planning under uncertainty using partially observable Markov\ndecision processes (POMDPs) with hybrid semantic-geometric beliefs. The models\nand priors consider the coupling between semantic and geometric variables.\nWithin POMDP, we introduce the concept of semantically aware safety. Obtaining\nrepresentative samples of the theoretical hybrid belief, required for\nestimating the value function, is very challenging. As a key contribution, we\ndevelop a novel form of the hybrid belief and leverage it to sample\nrepresentative samples. We show that under certain conditions, the value\nfunction and probability of safety can be calculated efficiently with an\nexplicit expectation over all possible semantic mappings. Our simulations show\nthat our estimates of the objective function and probability of safety achieve\nsimilar levels of accuracy compared to estimators that run exhaustively on the\nentire semantic state-space using samples from the theoretical hybrid belief.\nNevertheless, the complexity of our estimators is polynomial rather than\nexponential."
                },
                "authors": [
                    {
                        "name": "Tuvy Lemberg"
                    },
                    {
                        "name": "Vadim Indelman"
                    }
                ],
                "author_detail": {
                    "name": "Vadim Indelman"
                },
                "author": "Vadim Indelman",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "None",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02434v1",
                "updated": "2025-02-04T15:58:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    58,
                    12,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:58:12Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    58,
                    12,
                    1,
                    35,
                    0
                ],
                "title": "mPOLICE: Provable Enforcement of Multi-Region Affine Constraints in Deep\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mPOLICE: Provable Enforcement of Multi-Region Affine Constraints in Deep\n  Neural Networks"
                },
                "summary": "Deep neural networks are increasingly employed in fields such as climate\nmodeling, robotics, and industrial control, where strict output constraints\nmust be upheld. Although prior methods like the POLICE algorithm can enforce\naffine constraints in a single convex region by adjusting network parameters,\nthey struggle with multiple disjoint regions, often leading to conflicts or\nunintended affine extensions. We present mPOLICE, a new method that extends\nPOLICE to handle constraints imposed on multiple regions. mPOLICE assigns a\ndistinct activation pattern to each constrained region, preserving exact affine\nbehavior locally while avoiding overreach into other parts of the input domain.\nWe formulate a layer-wise optimization problem that adjusts both the weights\nand biases to assign unique activation patterns to each convex region, ensuring\nthat constraints are met without conflicts, while maintaining the continuity\nand smoothness of the learned function. Our experiments show the enforcement of\nmulti-region constraints for multiple scenarios, including regression and\nclassification, function approximation, and non-convex regions through\napproximation. Notably, mPOLICE adds zero inference overhead and minimal\ntraining overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks are increasingly employed in fields such as climate\nmodeling, robotics, and industrial control, where strict output constraints\nmust be upheld. Although prior methods like the POLICE algorithm can enforce\naffine constraints in a single convex region by adjusting network parameters,\nthey struggle with multiple disjoint regions, often leading to conflicts or\nunintended affine extensions. We present mPOLICE, a new method that extends\nPOLICE to handle constraints imposed on multiple regions. mPOLICE assigns a\ndistinct activation pattern to each constrained region, preserving exact affine\nbehavior locally while avoiding overreach into other parts of the input domain.\nWe formulate a layer-wise optimization problem that adjusts both the weights\nand biases to assign unique activation patterns to each convex region, ensuring\nthat constraints are met without conflicts, while maintaining the continuity\nand smoothness of the learned function. Our experiments show the enforcement of\nmulti-region constraints for multiple scenarios, including regression and\nclassification, function approximation, and non-convex regions through\napproximation. Notably, mPOLICE adds zero inference overhead and minimal\ntraining overhead."
                },
                "authors": [
                    {
                        "name": "Mohammadmehdi Ataei"
                    },
                    {
                        "name": "Hyunmin Cheong"
                    },
                    {
                        "name": "Adrian Butscher"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Butscher"
                },
                "author": "Adrian Butscher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12091v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12091v3",
                "updated": "2025-02-04T15:57:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    57,
                    59,
                    1,
                    35,
                    0
                ],
                "published": "2024-06-17T21:06:00Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    6,
                    0,
                    0,
                    169,
                    0
                ],
                "title": "Is poisoning a real threat to LLM alignment? Maybe more so than you\n  think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is poisoning a real threat to LLM alignment? Maybe more so than you\n  think"
                },
                "summary": "Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have\nsignificantly impacted the alignment of Large Language Models (LLMs). The\nsensitivity of reinforcement learning algorithms such as Proximal Policy\nOptimization (PPO) has led to new line work on Direct Policy Optimization\n(DPO), which treats RLHF in a supervised learning framework. The increased\npractical use of these RLHF methods warrants an analysis of their\nvulnerabilities. In this work, we investigate the vulnerabilities of DPO to\npoisoning attacks under different scenarios and compare the effectiveness of\npreference poisoning, a first of its kind. We comprehensively analyze DPO's\nvulnerabilities under different types of attacks, i.e., backdoor and\nnon-backdoor attacks, and different poisoning methods across a wide array of\nlanguage models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike\nPPO-based methods, which, when it comes to backdoor attacks, require at least\n4\\% of the data to be poisoned to elicit harmful behavior, we exploit the true\nvulnerabilities of DPO more simply so we can poison the model with only as much\nas 0.5\\% of the data. We further investigate the potential reasons behind the\nvulnerability and how well this vulnerability translates into backdoor vs\nnon-backdoor attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have\nsignificantly impacted the alignment of Large Language Models (LLMs). The\nsensitivity of reinforcement learning algorithms such as Proximal Policy\nOptimization (PPO) has led to new line work on Direct Policy Optimization\n(DPO), which treats RLHF in a supervised learning framework. The increased\npractical use of these RLHF methods warrants an analysis of their\nvulnerabilities. In this work, we investigate the vulnerabilities of DPO to\npoisoning attacks under different scenarios and compare the effectiveness of\npreference poisoning, a first of its kind. We comprehensively analyze DPO's\nvulnerabilities under different types of attacks, i.e., backdoor and\nnon-backdoor attacks, and different poisoning methods across a wide array of\nlanguage models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike\nPPO-based methods, which, when it comes to backdoor attacks, require at least\n4\\% of the data to be poisoned to elicit harmful behavior, we exploit the true\nvulnerabilities of DPO more simply so we can poison the model with only as much\nas 0.5\\% of the data. We further investigate the potential reasons behind the\nvulnerability and how well this vulnerability translates into backdoor vs\nnon-backdoor attacks."
                },
                "authors": [
                    {
                        "name": "Pankayaraj Pathmanathan"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Yongyuan Liang"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_journal_ref": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12091v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12091v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02421v1",
                "updated": "2025-02-04T15:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    42,
                    3,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    42,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Activation-Informed Merging of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-Informed Merging of Large Language Models"
                },
                "summary": "Model merging, a method that combines the parameters and embeddings of\nmultiple fine-tuned large language models (LLMs), offers a promising approach\nto enhance model performance across various tasks while maintaining\ncomputational efficiency. This paper introduces Activation-Informed Merging\n(AIM), a technique that integrates the information from the activation space of\nLLMs into the merging process to improve performance and robustness. AIM is\ndesigned as a flexible, complementary solution that is applicable to any\nexisting merging method. It aims to preserve critical weights from the base\nmodel, drawing on principles from continual learning~(CL) and model\ncompression. Utilizing a task-agnostic calibration set, AIM selectively\nprioritizes essential weights during merging. We empirically demonstrate that\nAIM significantly enhances the performance of merged models across multiple\nbenchmarks. Our findings suggest that considering the activation-space\ninformation can provide substantial advancements in the model merging\nstrategies for LLMs with up to 40\\% increase in benchmark performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging, a method that combines the parameters and embeddings of\nmultiple fine-tuned large language models (LLMs), offers a promising approach\nto enhance model performance across various tasks while maintaining\ncomputational efficiency. This paper introduces Activation-Informed Merging\n(AIM), a technique that integrates the information from the activation space of\nLLMs into the merging process to improve performance and robustness. AIM is\ndesigned as a flexible, complementary solution that is applicable to any\nexisting merging method. It aims to preserve critical weights from the base\nmodel, drawing on principles from continual learning~(CL) and model\ncompression. Utilizing a task-agnostic calibration set, AIM selectively\nprioritizes essential weights during merging. We empirically demonstrate that\nAIM significantly enhances the performance of merged models across multiple\nbenchmarks. Our findings suggest that considering the activation-space\ninformation can provide substantial advancements in the model merging\nstrategies for LLMs with up to 40\\% increase in benchmark performance."
                },
                "authors": [
                    {
                        "name": "Amin Heyrani Nobari"
                    },
                    {
                        "name": "Kaveh Alimohammadi"
                    },
                    {
                        "name": "Ali ArjomandBigdeli"
                    },
                    {
                        "name": "Akash Srivastava"
                    },
                    {
                        "name": "Faez Ahmed"
                    },
                    {
                        "name": "Navid Azizan"
                    }
                ],
                "author_detail": {
                    "name": "Navid Azizan"
                },
                "author": "Navid Azizan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02538v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02538v3",
                "updated": "2025-02-04T15:41:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    41,
                    27,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-04T19:17:17Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    19,
                    17,
                    17,
                    0,
                    309,
                    0
                ],
                "title": "MILU: A Multi-task Indic Language Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILU: A Multi-task Indic Language Understanding Benchmark"
                },
                "summary": "Evaluating Large Language Models (LLMs) in low-resource and linguistically\ndiverse languages remains a significant challenge in NLP, particularly for\nlanguages using non-Latin scripts like those spoken in India. Existing\nbenchmarks predominantly focus on English, leaving substantial gaps in\nassessing LLM capabilities in these languages. We introduce MILU, a Multi task\nIndic Language Understanding Benchmark, a comprehensive evaluation benchmark\ndesigned to address this gap. MILU spans 8 domains and 41 subjects across 11\nIndic languages, reflecting both general and culturally specific knowledge.\nWith an India-centric design, incorporates material from regional and\nstate-level examinations, covering topics such as local history, arts,\nfestivals, and laws, alongside standard subjects like science and mathematics.\nWe evaluate over 42 LLMs, and find that current LLMs struggle with MILU, with\nGPT-4o achieving the highest average accuracy at 74 percent. Open multilingual\nmodels outperform language-specific fine-tuned models, which perform only\nslightly better than random baselines. Models also perform better in high\nresource languages as compared to low resource ones. Domain-wise analysis\nindicates that models perform poorly in culturally relevant areas like Arts and\nHumanities, Law and Governance compared to general fields like STEM. To the\nbest of our knowledge, MILU is the first of its kind benchmark focused on Indic\nlanguages, serving as a crucial step towards comprehensive cultural evaluation.\nAll code, benchmarks, and artifacts are publicly available to foster open\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) in low-resource and linguistically\ndiverse languages remains a significant challenge in NLP, particularly for\nlanguages using non-Latin scripts like those spoken in India. Existing\nbenchmarks predominantly focus on English, leaving substantial gaps in\nassessing LLM capabilities in these languages. We introduce MILU, a Multi task\nIndic Language Understanding Benchmark, a comprehensive evaluation benchmark\ndesigned to address this gap. MILU spans 8 domains and 41 subjects across 11\nIndic languages, reflecting both general and culturally specific knowledge.\nWith an India-centric design, incorporates material from regional and\nstate-level examinations, covering topics such as local history, arts,\nfestivals, and laws, alongside standard subjects like science and mathematics.\nWe evaluate over 42 LLMs, and find that current LLMs struggle with MILU, with\nGPT-4o achieving the highest average accuracy at 74 percent. Open multilingual\nmodels outperform language-specific fine-tuned models, which perform only\nslightly better than random baselines. Models also perform better in high\nresource languages as compared to low resource ones. Domain-wise analysis\nindicates that models perform poorly in culturally relevant areas like Arts and\nHumanities, Law and Governance compared to general fields like STEM. To the\nbest of our knowledge, MILU is the first of its kind benchmark focused on Indic\nlanguages, serving as a crucial step towards comprehensive cultural evaluation.\nAll code, benchmarks, and artifacts are publicly available to foster open\nresearch."
                },
                "authors": [
                    {
                        "name": "Sshubam Verma"
                    },
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Vishwajeet Kumar"
                    },
                    {
                        "name": "Rudra Murthy"
                    },
                    {
                        "name": "Jaydeep Sen"
                    }
                ],
                "author_detail": {
                    "name": "Jaydeep Sen"
                },
                "author": "Jaydeep Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02538v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02538v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01618v2",
                "updated": "2025-02-04T15:39:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    39,
                    36,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-03T18:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    50,
                    50,
                    0,
                    34,
                    0
                ],
                "title": "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs\n  using Particle-Based Monte Carlo Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs\n  using Particle-Based Monte Carlo Methods"
                },
                "summary": "Large language models (LLMs) have achieved significant performance gains via\nscaling up model sizes and/or data. However, recent evidence suggests\ndiminishing returns from such approaches, motivating scaling the computation\nspent at inference time. Existing inference-time scaling methods, usually with\nreward models, cast the task as a search problem, which tends to be vulnerable\nto reward hacking as a consequence of approximation errors in reward models. In\nthis paper, we instead cast inference-time scaling as a probabilistic inference\ntask and leverage sampling-based techniques to explore the typical set of the\nstate distribution of a state-space model with an approximate likelihood,\nrather than optimize for its mode directly. We propose a novel inference-time\nscaling approach by adapting particle-based Monte Carlo methods to this task.\nOur empirical evaluation demonstrates that our methods have a 4-16x better\nscaling rate over our deterministic search counterparts on various challenging\nmathematical reasoning tasks. Using our approach, we show that\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but\nalso connects the rich literature in probabilistic inference with\ninference-time scaling of LLMs to develop more robust algorithms in future\nwork. Code and further information is available at\nhttps://probabilistic-inference-scaling.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant performance gains via\nscaling up model sizes and/or data. However, recent evidence suggests\ndiminishing returns from such approaches, motivating scaling the computation\nspent at inference time. Existing inference-time scaling methods, usually with\nreward models, cast the task as a search problem, which tends to be vulnerable\nto reward hacking as a consequence of approximation errors in reward models. In\nthis paper, we instead cast inference-time scaling as a probabilistic inference\ntask and leverage sampling-based techniques to explore the typical set of the\nstate distribution of a state-space model with an approximate likelihood,\nrather than optimize for its mode directly. We propose a novel inference-time\nscaling approach by adapting particle-based Monte Carlo methods to this task.\nOur empirical evaluation demonstrates that our methods have a 4-16x better\nscaling rate over our deterministic search counterparts on various challenging\nmathematical reasoning tasks. Using our approach, we show that\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but\nalso connects the rich literature in probabilistic inference with\ninference-time scaling of LLMs to develop more robust algorithms in future\nwork. Code and further information is available at\nhttps://probabilistic-inference-scaling.github.io."
                },
                "authors": [
                    {
                        "name": "Isha Puri"
                    },
                    {
                        "name": "Shivchander Sudalairaj"
                    },
                    {
                        "name": "Guangxuan Xu"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19297v2",
                "updated": "2025-02-04T15:33:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    33,
                    51,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-31T16:55:17Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    17,
                    4,
                    31,
                    0
                ],
                "title": "Analysis of LLMs vs Human Experts in Requirements Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of LLMs vs Human Experts in Requirements Engineering"
                },
                "summary": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines."
                },
                "authors": [
                    {
                        "name": "Cory Hymel"
                    },
                    {
                        "name": "Hiroe Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Hiroe Johnson"
                },
                "author": "Hiroe Johnson",
                "arxiv_comment": "8 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02412v1",
                "updated": "2025-02-04T15:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    32,
                    34,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:32:34Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    32,
                    34,
                    1,
                    35,
                    0
                ],
                "title": "AI-Powered, But Power-Hungry? Energy Efficiency of LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Powered, But Power-Hungry? Energy Efficiency of LLM-Generated Code"
                },
                "summary": "Large language models (LLMs) are used in software development to assist in\nvarious tasks, e.g., code generation and code completion, but empirical\nevaluations of the quality of the results produced by these models focus on\ncorrectness and ignore other relevant aspects, such as their performance and\nenergy efficiency. Studying the performance of LLM-produced programs is\nessential to understand how well LLMs can support the construction of\nperformance- and energy-critical software, such as operating systems, servers,\nand mobile applications. This paper presents the first study analyzing the\nenergy efficiency and performance of LLM-generated code for three programming\nlanguages Python, Java, and C++, on two platforms, a Mac and a PC, leveraging\nthree frontier LLMs, Github Copilot, GPT-4o, and the recently-released OpenAI\no1-mini, and targeting ``hard'' programming problems from LeetCode. Our results\nshow that the models are much more successful in generating Python and Java\nthan C++ code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are used in software development to assist in\nvarious tasks, e.g., code generation and code completion, but empirical\nevaluations of the quality of the results produced by these models focus on\ncorrectness and ignore other relevant aspects, such as their performance and\nenergy efficiency. Studying the performance of LLM-produced programs is\nessential to understand how well LLMs can support the construction of\nperformance- and energy-critical software, such as operating systems, servers,\nand mobile applications. This paper presents the first study analyzing the\nenergy efficiency and performance of LLM-generated code for three programming\nlanguages Python, Java, and C++, on two platforms, a Mac and a PC, leveraging\nthree frontier LLMs, Github Copilot, GPT-4o, and the recently-released OpenAI\no1-mini, and targeting ``hard'' programming problems from LeetCode. Our results\nshow that the models are much more successful in generating Python and Java\nthan C++ code."
                },
                "authors": [
                    {
                        "name": "Lola Solovyeva"
                    },
                    {
                        "name": "Sophie Weidmann"
                    },
                    {
                        "name": "Fernando Castor"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Castor"
                },
                "author": "Fernando Castor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07668v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07668v3",
                "updated": "2025-02-04T15:29:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    29,
                    7,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-12T09:35:23Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    9,
                    35,
                    23,
                    1,
                    317,
                    0
                ],
                "title": "Towards Evaluation Guidelines for Empirical Studies involving LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluation Guidelines for Empirical Studies involving LLMs"
                },
                "summary": "In the short period since the release of ChatGPT, large language models\n(LLMs) have changed the software engineering research landscape. While there\nare numerous opportunities to use LLMs for supporting research or software\nengineering tasks, solid science needs rigorous empirical evaluations. However,\nso far, there are no specific guidelines for conducting and assessing studies\ninvolving LLMs in software engineering research. Our focus is on empirical\nstudies that either use LLMs as part of the research process or studies that\nevaluate existing or new tools that are based on LLMs. This paper contributes\nthe first set of holistic guidelines for such studies. Our goal is to start a\ndiscussion in the software engineering research community to reach a common\nunderstanding of our standards for high-quality empirical studies involving\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the short period since the release of ChatGPT, large language models\n(LLMs) have changed the software engineering research landscape. While there\nare numerous opportunities to use LLMs for supporting research or software\nengineering tasks, solid science needs rigorous empirical evaluations. However,\nso far, there are no specific guidelines for conducting and assessing studies\ninvolving LLMs in software engineering research. Our focus is on empirical\nstudies that either use LLMs as part of the research process or studies that\nevaluate existing or new tools that are based on LLMs. This paper contributes\nthe first set of holistic guidelines for such studies. Our goal is to start a\ndiscussion in the software engineering research community to reach a common\nunderstanding of our standards for high-quality empirical studies involving\nLLMs."
                },
                "authors": [
                    {
                        "name": "Stefan Wagner"
                    },
                    {
                        "name": "Marvin Muñoz Barón"
                    },
                    {
                        "name": "Davide Falessi"
                    },
                    {
                        "name": "Sebastian Baltes"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Baltes"
                },
                "author": "Sebastian Baltes",
                "arxiv_comment": "4 pages, 2nd IEEE/ACM International Workshop on Methodological Issues\n  with Empirical Studies in Software Engineering (WSESE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07668v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07668v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02407v1",
                "updated": "2025-02-04T15:25:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    25,
                    47,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:25:47Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    25,
                    47,
                    1,
                    35,
                    0
                ],
                "title": "Avoiding spurious sharpness minimization broadens applicability of SAM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoiding spurious sharpness minimization broadens applicability of SAM"
                },
                "summary": "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs)."
                },
                "authors": [
                    {
                        "name": "Sidak Pal Singh"
                    },
                    {
                        "name": "Hossein Mobahi"
                    },
                    {
                        "name": "Atish Agarwala"
                    },
                    {
                        "name": "Yann Dauphin"
                    }
                ],
                "author_detail": {
                    "name": "Yann Dauphin"
                },
                "author": "Yann Dauphin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02406v1",
                "updated": "2025-02-04T15:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    24,
                    16,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:24:16Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    24,
                    16,
                    1,
                    35,
                    0
                ],
                "title": "LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in\n  Multimodal Large Language Models"
                },
                "summary": "Cross-attention is commonly adopted in multimodal large language models\n(MLLMs) for integrating visual information into the language backbone. However,\nin applications with large visual inputs, such as video understanding,\nprocessing a large number of visual tokens in cross-attention layers leads to\nhigh memory demands and often necessitates distributed computation across\nmultiple GPUs. Existing distributed attention mechanisms face significant\ncommunication overheads, making cross-attention layers a critical bottleneck\nfor efficient training and inference of MLLMs. To address this, we propose\nLV-XAttn, a distributed, exact cross-attention mechanism with minimal\ncommunication overhead. We observe that in applications involving large visual\ninputs the size of the query block is typically much smaller than that of the\nkey-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally\non each GPU and exchange smaller query blocks across GPUs. We also introduce an\nefficient activation recomputation technique enabling support for longer visual\ncontext. We theoretically analyze the communication benefits of LV-XAttn and\nshow that it can achieve speedups for a wide range of models. Our evaluations\nwith mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to\n5.58$\\times$ end-to-end speedup compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-attention is commonly adopted in multimodal large language models\n(MLLMs) for integrating visual information into the language backbone. However,\nin applications with large visual inputs, such as video understanding,\nprocessing a large number of visual tokens in cross-attention layers leads to\nhigh memory demands and often necessitates distributed computation across\nmultiple GPUs. Existing distributed attention mechanisms face significant\ncommunication overheads, making cross-attention layers a critical bottleneck\nfor efficient training and inference of MLLMs. To address this, we propose\nLV-XAttn, a distributed, exact cross-attention mechanism with minimal\ncommunication overhead. We observe that in applications involving large visual\ninputs the size of the query block is typically much smaller than that of the\nkey-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally\non each GPU and exchange smaller query blocks across GPUs. We also introduce an\nefficient activation recomputation technique enabling support for longer visual\ncontext. We theoretically analyze the communication benefits of LV-XAttn and\nshow that it can achieve speedups for a wide range of models. Our evaluations\nwith mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to\n5.58$\\times$ end-to-end speedup compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Tzu-Tao Chang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02400v1",
                "updated": "2025-02-04T15:19:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    19,
                    24,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:19:24Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    19,
                    24,
                    1,
                    35,
                    0
                ],
                "title": "Inferring Ambient Cycles of Point Samples on Manifolds with Universal\n  Coverings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Ambient Cycles of Point Samples on Manifolds with Universal\n  Coverings"
                },
                "summary": "A central objective of topological data analysis is to identify topologically\nsignificant features in data represented as a finite point cloud. We consider\nthe setting where the ambient space of the point sample is a compact Riemannian\nmanifold. Given a simplicial complex constructed on the point set, we can\nrelate the first homology of the complex with that of the ambient manifold by\nmatching edges in the complex with minimising geodesics between points.\nProvided the universal covering of the manifold is known, we give a\nconstructive method for identifying whether a given edge loop (or\nrepresentative first homology cycle) on the complex corresponds to a\nnon-trivial loop (or first homology class) of the ambient manifold. We show\nthat metric data on the point cloud and its fibre in the covering suffices for\nthe construction, and formalise our approach in the framework of groupoids and\nmonodromy of coverings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central objective of topological data analysis is to identify topologically\nsignificant features in data represented as a finite point cloud. We consider\nthe setting where the ambient space of the point sample is a compact Riemannian\nmanifold. Given a simplicial complex constructed on the point set, we can\nrelate the first homology of the complex with that of the ambient manifold by\nmatching edges in the complex with minimising geodesics between points.\nProvided the universal covering of the manifold is known, we give a\nconstructive method for identifying whether a given edge loop (or\nrepresentative first homology cycle) on the complex corresponds to a\nnon-trivial loop (or first homology class) of the ambient manifold. We show\nthat metric data on the point cloud and its fibre in the covering suffices for\nthe construction, and formalise our approach in the framework of groupoids and\nmonodromy of coverings."
                },
                "authors": [
                    {
                        "name": "Ka Man Yim"
                    }
                ],
                "author_detail": {
                    "name": "Ka Man Yim"
                },
                "author": "Ka Man Yim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "55N31, 57M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02390v1",
                "updated": "2025-02-04T15:10:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    10,
                    33,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:10:33Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    10,
                    33,
                    1,
                    35,
                    0
                ],
                "title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large\n  Language Models Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large\n  Language Models Reasoning"
                },
                "summary": "Research on LLM technologies is rapidly emerging, with most of them employing\na 'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. To validate the\neffectiveness of our framework, we conducted extensive experiments across a\nrange of generative and reasoning tasks. These experiments demonstrated that\nour framework outperforms conventional inference processes on accuracy,\ncoherence, and diversity. The framework's ability to iteratively expand its\nsearch space while retaining contextually relevant information results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on LLM technologies is rapidly emerging, with most of them employing\na 'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. To validate the\neffectiveness of our framework, we conducted extensive experiments across a\nrange of generative and reasoning tasks. These experiments demonstrated that\nour framework outperforms conventional inference processes on accuracy,\ncoherence, and diversity. The framework's ability to iteratively expand its\nsearch space while retaining contextually relevant information results."
                },
                "authors": [
                    {
                        "name": "Jianfeng Pan"
                    },
                    {
                        "name": "Senyou Deng"
                    },
                    {
                        "name": "Shaomang Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shaomang Huang"
                },
                "author": "Shaomang Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01325v2",
                "updated": "2025-02-04T15:08:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    8,
                    42,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-03T12:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    59,
                    36,
                    0,
                    34,
                    0
                ],
                "title": "The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in\n  Parent-Child Homework Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in\n  Parent-Child Homework Interactions"
                },
                "summary": "Parental involvement in homework is a crucial aspect of family education, but\nit often leads to emotional strain and conflicts that can severely impact\nfamily well-being. This paper presents findings from a 4-week in situ study\ninvolving 78 families in China, where we collected and analyzed 602 valid audio\nrecordings (totalling 475 hours) and daily surveys. Leveraging large language\nmodels (LLMs) to analyze parent-child conversations, we gained a nuanced\nunderstanding of emotional and behavioural dynamics that overcomes the\nlimitations of traditional one-time surveys and interviews. Our findings reveal\nsignificant emotional shifts in parents before and after homework involvement\nand summarise a range of positive, neutral and negative parental behaviours. We\nalso catalogue seven common conflicts, with Knowledge Conflict being the most\nfrequent. Notably, we found that even well-intentioned parental behaviours,\nsuch as Unlabelled Praise, were significantly positively correlated with\nspecific conflict types. This work advances ubiquitous computing's research to\nsense and understand complex family dynamics, while offering evidence-based\ninsights for designing future ambient intelligent systems to support healthy\nfamily education environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parental involvement in homework is a crucial aspect of family education, but\nit often leads to emotional strain and conflicts that can severely impact\nfamily well-being. This paper presents findings from a 4-week in situ study\ninvolving 78 families in China, where we collected and analyzed 602 valid audio\nrecordings (totalling 475 hours) and daily surveys. Leveraging large language\nmodels (LLMs) to analyze parent-child conversations, we gained a nuanced\nunderstanding of emotional and behavioural dynamics that overcomes the\nlimitations of traditional one-time surveys and interviews. Our findings reveal\nsignificant emotional shifts in parents before and after homework involvement\nand summarise a range of positive, neutral and negative parental behaviours. We\nalso catalogue seven common conflicts, with Knowledge Conflict being the most\nfrequent. Notably, we found that even well-intentioned parental behaviours,\nsuch as Unlabelled Praise, were significantly positively correlated with\nspecific conflict types. This work advances ubiquitous computing's research to\nsense and understand complex family dynamics, while offering evidence-based\ninsights for designing future ambient intelligent systems to support healthy\nfamily education environments."
                },
                "authors": [
                    {
                        "name": "Nan Gao"
                    },
                    {
                        "name": "Yibin Liu"
                    },
                    {
                        "name": "Xin Tang"
                    },
                    {
                        "name": "Yanyan Liu"
                    },
                    {
                        "name": "Chun Yu"
                    },
                    {
                        "name": "Yun Huang"
                    },
                    {
                        "name": "Yuntao Wang"
                    },
                    {
                        "name": "Flora D. Salim"
                    },
                    {
                        "name": "Xuhai Orson Xu"
                    },
                    {
                        "name": "Jun Wei"
                    },
                    {
                        "name": "Yuanchun Shi"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Shi"
                },
                "author": "Yuanchun Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02384v1",
                "updated": "2025-02-04T15:02:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    2,
                    55,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:02:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    2,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAIR: Improving Safety Alignment with Introspective Reasoning"
                },
                "summary": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has\nbecome equally critical as their performance in applications. However, existing\nsafety alignment methods typically suffer from safety-performance trade-offs\nand the susceptibility to jailbreak attacks, primarily due to their reliance on\ndirect refusals for malicious queries. In this paper, we propose STAIR, a novel\nframework that integrates SafeTy Alignment with Itrospective Reasoning. We\nenable LLMs to identify safety risks through step-by-step analysis by\nself-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR\nfirst equips the model with a structured reasoning capability and then advances\nsafety alignment via iterative preference optimization on step-level reasoning\ndata generated using our newly proposed Safety-Informed Monte Carlo Tree Search\n(SI-MCTS). We further train a process reward model on this data to guide\ntest-time searches for improved responses. Extensive experiments show that\nSTAIR effectively mitigates harmful outputs while better preserving\nhelpfulness, compared to instinctive alignment strategies. With test-time\nscaling, STAIR achieves a safety performance comparable to Claude-3.5 against\npopular jailbreak attacks. Relevant resources in this work are available at\nhttps://github.com/thu-ml/STAIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has\nbecome equally critical as their performance in applications. However, existing\nsafety alignment methods typically suffer from safety-performance trade-offs\nand the susceptibility to jailbreak attacks, primarily due to their reliance on\ndirect refusals for malicious queries. In this paper, we propose STAIR, a novel\nframework that integrates SafeTy Alignment with Itrospective Reasoning. We\nenable LLMs to identify safety risks through step-by-step analysis by\nself-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR\nfirst equips the model with a structured reasoning capability and then advances\nsafety alignment via iterative preference optimization on step-level reasoning\ndata generated using our newly proposed Safety-Informed Monte Carlo Tree Search\n(SI-MCTS). We further train a process reward model on this data to guide\ntest-time searches for improved responses. Extensive experiments show that\nSTAIR effectively mitigates harmful outputs while better preserving\nhelpfulness, compared to instinctive alignment strategies. With test-time\nscaling, STAIR achieves a safety performance comparable to Claude-3.5 against\npopular jailbreak attacks. Relevant resources in this work are available at\nhttps://github.com/thu-ml/STAIR."
                },
                "authors": [
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Siyuan Zhang"
                    },
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Zeyu Xia"
                    },
                    {
                        "name": "Zhengwei Fang"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "22 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02368v1",
                "updated": "2025-02-04T14:50:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    50,
                    23,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:50:23Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    50,
                    23,
                    1,
                    35,
                    0
                ],
                "title": "Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in\n  Real-World Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in\n  Real-World Projects"
                },
                "summary": "Large Language Models (LLMs) have gained attention for addressing coding\nproblems, but their effectiveness in fixing code maintainability remains\nunclear. This study evaluates LLMs capability to resolve 127 maintainability\nissues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat\nand Llama 3.1, and few-shot prompting with Llama only. The LLM-generated\nsolutions are assessed for compilation errors, test failures, and new\nmaintainability problems. Llama with few-shot prompting successfully fixed\n44.9% of the methods, while Copilot Chat and Llama zero-shot fixed 32.29% and\n30%, respectively. However, most solutions introduced errors or new\nmaintainability issues. We also conducted a human study with 45 participants to\nevaluate the readability of 51 LLM-generated solutions. The human study showed\nthat 68.63% of participants observed improved readability. Overall, while LLMs\nshow potential for fixing maintainability issues, their introduction of errors\nhighlights their current limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained attention for addressing coding\nproblems, but their effectiveness in fixing code maintainability remains\nunclear. This study evaluates LLMs capability to resolve 127 maintainability\nissues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat\nand Llama 3.1, and few-shot prompting with Llama only. The LLM-generated\nsolutions are assessed for compilation errors, test failures, and new\nmaintainability problems. Llama with few-shot prompting successfully fixed\n44.9% of the methods, while Copilot Chat and Llama zero-shot fixed 32.29% and\n30%, respectively. However, most solutions introduced errors or new\nmaintainability issues. We also conducted a human study with 45 participants to\nevaluate the readability of 51 LLM-generated solutions. The human study showed\nthat 68.63% of participants observed improved readability. Overall, while LLMs\nshow potential for fixing maintainability issues, their introduction of errors\nhighlights their current limitations."
                },
                "authors": [
                    {
                        "name": "Henrique Nunes"
                    },
                    {
                        "name": "Eduardo Figueiredo"
                    },
                    {
                        "name": "Larissa Rocha"
                    },
                    {
                        "name": "Sarah Nadi"
                    },
                    {
                        "name": "Fischer Ferreira"
                    },
                    {
                        "name": "Geanderson Esteves"
                    }
                ],
                "author_detail": {
                    "name": "Geanderson Esteves"
                },
                "author": "Geanderson Esteves",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02364v1",
                "updated": "2025-02-04T14:47:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    47,
                    55,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:47:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    47,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "Variational inference for approximate reference priors using neural\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference for approximate reference priors using neural\n  networks"
                },
                "summary": "In Bayesian statistics, the choice of the prior can have an important\ninfluence on the posterior and the parameter estimation, especially when few\ndata samples are available. To limit the added subjectivity from a priori\ninformation, one can use the framework of reference priors. However, computing\nsuch priors is a difficult task in general. We develop in this paper a flexible\nalgorithm based on variational inference which computes approximations of\nreference priors from a set of parametric distributions using neural networks.\nWe also show that our algorithm can retrieve reference priors when constraints\nare specified in the optimization problem to ensure the solution is proper. We\npropose a simple method to recover a relevant approximation of the parametric\nposterior distribution using Markov Chain Monte Carlo (MCMC) methods even if\nthe density function of the parametric prior is not known in general. Numerical\nexperiments on several statistical models of increasing complexity are\npresented. We show the usefulness of this approach by recovering the target\ndistribution. The performance of the algorithm is evaluated on the prior\ndistributions as well as the posterior distributions, jointly using variational\ninference and MCMC sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Bayesian statistics, the choice of the prior can have an important\ninfluence on the posterior and the parameter estimation, especially when few\ndata samples are available. To limit the added subjectivity from a priori\ninformation, one can use the framework of reference priors. However, computing\nsuch priors is a difficult task in general. We develop in this paper a flexible\nalgorithm based on variational inference which computes approximations of\nreference priors from a set of parametric distributions using neural networks.\nWe also show that our algorithm can retrieve reference priors when constraints\nare specified in the optimization problem to ensure the solution is proper. We\npropose a simple method to recover a relevant approximation of the parametric\nposterior distribution using Markov Chain Monte Carlo (MCMC) methods even if\nthe density function of the parametric prior is not known in general. Numerical\nexperiments on several statistical models of increasing complexity are\npresented. We show the usefulness of this approach by recovering the target\ndistribution. The performance of the algorithm is evaluated on the prior\ndistributions as well as the posterior distributions, jointly using variational\ninference and MCMC sampling."
                },
                "authors": [
                    {
                        "name": "Nils Baillie"
                    },
                    {
                        "name": "Antoine Van Biesbroeck"
                    },
                    {
                        "name": "Clément Gauchy"
                    }
                ],
                "author_detail": {
                    "name": "Clément Gauchy"
                },
                "author": "Clément Gauchy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02363v1",
                "updated": "2025-02-04T14:46:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    46,
                    8,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:46:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    46,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "FAB-PPI: Frequentist, Assisted by Bayes, Prediction-Powered Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAB-PPI: Frequentist, Assisted by Bayes, Prediction-Powered Inference"
                },
                "summary": "Prediction-powered inference (PPI) enables valid statistical inference by\ncombining experimental data with machine learning predictions. When a\nsufficient number of high-quality predictions is available, PPI results in more\naccurate estimates and tighter confidence intervals than traditional methods.\nIn this paper, we propose to inform the PPI framework with prior knowledge on\nthe quality of the predictions. The resulting method, which we call\nfrequentist, assisted by Bayes, PPI (FAB-PPI), improves over PPI when the\nobserved prediction quality is likely under the prior, while maintaining its\nfrequentist guarantees. Furthermore, when using heavy-tailed priors, FAB-PPI\nadaptively reverts to standard PPI in low prior probability regions. We\ndemonstrate the benefits of FAB-PPI in real and synthetic examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction-powered inference (PPI) enables valid statistical inference by\ncombining experimental data with machine learning predictions. When a\nsufficient number of high-quality predictions is available, PPI results in more\naccurate estimates and tighter confidence intervals than traditional methods.\nIn this paper, we propose to inform the PPI framework with prior knowledge on\nthe quality of the predictions. The resulting method, which we call\nfrequentist, assisted by Bayes, PPI (FAB-PPI), improves over PPI when the\nobserved prediction quality is likely under the prior, while maintaining its\nfrequentist guarantees. Furthermore, when using heavy-tailed priors, FAB-PPI\nadaptively reverts to standard PPI in low prior probability regions. We\ndemonstrate the benefits of FAB-PPI in real and synthetic examples."
                },
                "authors": [
                    {
                        "name": "Stefano Cortinovis"
                    },
                    {
                        "name": "François Caron"
                    }
                ],
                "author_detail": {
                    "name": "François Caron"
                },
                "author": "François Caron",
                "arxiv_comment": "28 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02362v1",
                "updated": "2025-02-04T14:44:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    44,
                    58,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:44:58Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    44,
                    58,
                    1,
                    35,
                    0
                ],
                "title": "Premise-Augmented Reasoning Chains Improve Error Identification in Math\n  reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Premise-Augmented Reasoning Chains Improve Error Identification in Math\n  reasoning with LLMs"
                },
                "summary": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations."
                },
                "authors": [
                    {
                        "name": "Sagnik Mukherjee"
                    },
                    {
                        "name": "Abhinav Chinta"
                    },
                    {
                        "name": "Takyoung Kim"
                    },
                    {
                        "name": "Tarun Anoop Sharma"
                    },
                    {
                        "name": "Dilek Hakkani Tur"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani Tur"
                },
                "author": "Dilek Hakkani Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02358v1",
                "updated": "2025-02-04T14:43:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    43,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:43:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    43,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "MotionLab: Unified Human Motion Generation and Editing via the\n  Motion-Condition-Motion Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionLab: Unified Human Motion Generation and Editing via the\n  Motion-Condition-Motion Paradigm"
                },
                "summary": "Human motion generation and editing are key components of computer graphics\nand vision. However, current approaches in this field tend to offer isolated\nsolutions tailored to specific tasks, which can be inefficient and impractical\nfor real-world applications. While some efforts have aimed to unify\nmotion-related tasks, these methods simply use different modalities as\nconditions to guide motion generation. Consequently, they lack editing\ncapabilities, fine-grained control, and fail to facilitate knowledge sharing\nacross tasks. To address these limitations and provide a versatile, unified\nframework capable of handling both human motion generation and editing, we\nintroduce a novel paradigm: Motion-Condition-Motion, which enables the unified\nformulation of diverse tasks with three concepts: source motion, condition, and\ntarget motion.Based on this paradigm, we propose a unified framework,\nMotionLab, which incorporates rectified flows to learn the mapping from source\nmotion to target motion, guided by the specified conditions.In MotionLab, we\nintroduce the 1) MotionFlow Transformer to enhance conditional generation and\nediting without task-specific modules; 2) Aligned Rotational Position Encoding}\nto guarantee the time synchronization between source motion and target motion;\n3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for\neffective multi-task learning and knowledge sharing across tasks. Notably, our\nMotionLab demonstrates promising generalization capabilities and inference\nefficiency across multiple benchmarks for human motion. Our code and additional\nvideo results are available at: https://diouo.github.io/motionlab.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion generation and editing are key components of computer graphics\nand vision. However, current approaches in this field tend to offer isolated\nsolutions tailored to specific tasks, which can be inefficient and impractical\nfor real-world applications. While some efforts have aimed to unify\nmotion-related tasks, these methods simply use different modalities as\nconditions to guide motion generation. Consequently, they lack editing\ncapabilities, fine-grained control, and fail to facilitate knowledge sharing\nacross tasks. To address these limitations and provide a versatile, unified\nframework capable of handling both human motion generation and editing, we\nintroduce a novel paradigm: Motion-Condition-Motion, which enables the unified\nformulation of diverse tasks with three concepts: source motion, condition, and\ntarget motion.Based on this paradigm, we propose a unified framework,\nMotionLab, which incorporates rectified flows to learn the mapping from source\nmotion to target motion, guided by the specified conditions.In MotionLab, we\nintroduce the 1) MotionFlow Transformer to enhance conditional generation and\nediting without task-specific modules; 2) Aligned Rotational Position Encoding}\nto guarantee the time synchronization between source motion and target motion;\n3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for\neffective multi-task learning and knowledge sharing across tasks. Notably, our\nMotionLab demonstrates promising generalization capabilities and inference\nefficiency across multiple benchmarks for human motion. Our code and additional\nvideo results are available at: https://diouo.github.io/motionlab.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziyan Guo"
                    },
                    {
                        "name": "Zeyu Hu"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "De Wen Soh"
                    }
                ],
                "author_detail": {
                    "name": "De Wen Soh"
                },
                "author": "De Wen Soh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00830v2",
                "updated": "2025-02-04T14:37:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    37,
                    29,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-01T13:20:01Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    20,
                    1,
                    2,
                    1,
                    0
                ],
                "title": "LLM+AL: Bridging Large Language Models and Action Languages for Complex\n  Reasoning about Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM+AL: Bridging Large Language Models and Action Languages for Complex\n  Reasoning about Actions"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in various\nintelligent tasks but still struggle with complex action reasoning tasks that\nrequire systematic search. To address this limitation, we propose a method that\nbridges the natural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages. Our approach, termed\n\"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's proficiency in automated\nreasoning based on encoded knowledge. We compare LLM+AL against\nstate-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0,\nand o1-preview, using benchmarks for complex reasoning about actions. Our\nfindings indicate that, although all methods exhibit errors, LLM+AL, with\nrelatively minimal human corrections, consistently leads to correct answers,\nwhereas standalone LLMs fail to improve even with human feedback. LLM+AL also\ncontributes to automated generation of action languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in various\nintelligent tasks but still struggle with complex action reasoning tasks that\nrequire systematic search. To address this limitation, we propose a method that\nbridges the natural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages. Our approach, termed\n\"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's proficiency in automated\nreasoning based on encoded knowledge. We compare LLM+AL against\nstate-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0,\nand o1-preview, using benchmarks for complex reasoning about actions. Our\nfindings indicate that, although all methods exhibit errors, LLM+AL, with\nrelatively minimal human corrections, consistently leads to correct answers,\nwhereas standalone LLMs fail to improve even with human feedback. LLM+AL also\ncontributes to automated generation of action languages."
                },
                "authors": [
                    {
                        "name": "Adam Ishay"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20446v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20446v3",
                "updated": "2025-02-04T14:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    35,
                    38,
                    1,
                    35,
                    0
                ],
                "published": "2024-05-30T19:46:36Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    19,
                    46,
                    36,
                    3,
                    151,
                    0
                ],
                "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks\n  Against Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is My Data in Your Retrieval Database? Membership Inference Attacks\n  Against Retrieval Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) systems have shown great promise in\nnatural language processing. However, their reliance on data stored in a\nretrieval database, which may contain proprietary or sensitive information,\nintroduces new privacy concerns. Specifically, an attacker may be able to infer\nwhether a certain text passage appears in the retrieval database by observing\nthe outputs of the RAG system, an attack known as a Membership Inference Attack\n(MIA). Despite the significance of this threat, MIAs against RAG systems have\nyet remained under-explored. This study addresses this gap by introducing an\nefficient and easy-to-use method for conducting MIA against RAG systems. We\ndemonstrate the effectiveness of our attack using two benchmark datasets and\nmultiple generative models, showing that the membership of a document in the\nretrieval database can be efficiently determined through the creation of an\nappropriate prompt in both black-box and gray-box settings. Moreover, we\nintroduce an initial defense strategy based on adding instructions to the RAG\ntemplate, which shows high effectiveness for some datasets and models. Our\nfindings highlight the importance of implementing security countermeasures in\ndeployed RAG systems and developing more advanced defenses to protect the\nprivacy and security of retrieval databases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) systems have shown great promise in\nnatural language processing. However, their reliance on data stored in a\nretrieval database, which may contain proprietary or sensitive information,\nintroduces new privacy concerns. Specifically, an attacker may be able to infer\nwhether a certain text passage appears in the retrieval database by observing\nthe outputs of the RAG system, an attack known as a Membership Inference Attack\n(MIA). Despite the significance of this threat, MIAs against RAG systems have\nyet remained under-explored. This study addresses this gap by introducing an\nefficient and easy-to-use method for conducting MIA against RAG systems. We\ndemonstrate the effectiveness of our attack using two benchmark datasets and\nmultiple generative models, showing that the membership of a document in the\nretrieval database can be efficiently determined through the creation of an\nappropriate prompt in both black-box and gray-box settings. Moreover, we\nintroduce an initial defense strategy based on adding instructions to the RAG\ntemplate, which shows high effectiveness for some datasets and models. Our\nfindings highlight the importance of implementing security countermeasures in\ndeployed RAG systems and developing more advanced defenses to protect the\nprivacy and security of retrieval databases."
                },
                "authors": [
                    {
                        "name": "Maya Anderson"
                    },
                    {
                        "name": "Guy Amit"
                    },
                    {
                        "name": "Abigail Goldsteen"
                    }
                ],
                "author_detail": {
                    "name": "Abigail Goldsteen"
                },
                "author": "Abigail Goldsteen",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20446v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20446v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08062v2",
                "updated": "2025-02-04T14:35:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    35,
                    1,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-15T10:03:30Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    30,
                    3,
                    228,
                    0
                ],
                "title": "BINDy -- Bayesian identification of nonlinear dynamics with\n  reversible-jump Markov-chain Monte-Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BINDy -- Bayesian identification of nonlinear dynamics with\n  reversible-jump Markov-chain Monte-Carlo"
                },
                "summary": "Model parsimony is an important \\emph{cognitive bias} in data-driven\nmodelling that aids interpretability and helps to prevent over-fitting. Sparse\nidentification of nonlinear dynamics (SINDy) methods are able to learn sparse\nrepresentations of complex dynamics directly from data, given a basis of\nlibrary functions. In this work, a novel Bayesian treatment of dictionary\nlearning system identification, as an alternative to SINDy, is envisaged. The\nproposed method -- Bayesian identification of nonlinear dynamics (BINDy) -- is\ndistinct from previous approaches in that it targets the full joint posterior\ndistribution over both the terms in the library and their parameterisation in\nthe model. This formulation confers the advantage that an arbitrary prior may\nbe placed over the model structure to produce models that are sparse in the\nmodel space rather than in parameter space. Because this posterior is defined\nover parameter vectors that can change in dimension, the inference cannot be\nperformed by standard techniques. Instead, a Gibbs sampler based on\nreversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to compare\nfavourably to ensemble SINDy in three benchmark case-studies. In particular, it\nis seen that the proposed method is better able to assign high probability to\ncorrect model terms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model parsimony is an important \\emph{cognitive bias} in data-driven\nmodelling that aids interpretability and helps to prevent over-fitting. Sparse\nidentification of nonlinear dynamics (SINDy) methods are able to learn sparse\nrepresentations of complex dynamics directly from data, given a basis of\nlibrary functions. In this work, a novel Bayesian treatment of dictionary\nlearning system identification, as an alternative to SINDy, is envisaged. The\nproposed method -- Bayesian identification of nonlinear dynamics (BINDy) -- is\ndistinct from previous approaches in that it targets the full joint posterior\ndistribution over both the terms in the library and their parameterisation in\nthe model. This formulation confers the advantage that an arbitrary prior may\nbe placed over the model structure to produce models that are sparse in the\nmodel space rather than in parameter space. Because this posterior is defined\nover parameter vectors that can change in dimension, the inference cannot be\nperformed by standard techniques. Instead, a Gibbs sampler based on\nreversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to compare\nfavourably to ensemble SINDy in three benchmark case-studies. In particular, it\nis seen that the proposed method is better able to assign high probability to\ncorrect model terms."
                },
                "authors": [
                    {
                        "name": "Max D. Champneys"
                    },
                    {
                        "name": "Timothy J. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Rogers"
                },
                "author": "Timothy J. Rogers",
                "arxiv_doi": "10.1098/rspa.2024.0620",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1098/rspa.2024.0620",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01600v2",
                "updated": "2025-02-04T14:28:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    28,
                    50,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-03T18:35:42Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    35,
                    42,
                    0,
                    34,
                    0
                ],
                "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Long-Horizon Interactive LLM Agents"
                },
                "summary": "Interactive digital agents (IDAs) leverage APIs of stateful digital\nenvironments to perform tasks in response to user requests. While IDAs powered\nby instruction-tuned large language models (LLMs) can react to feedback from\ninterface invocations in multi-step exchanges, they have not been trained in\ntheir respective digital environments. Prior methods accomplish less than half\nof tasks in sophisticated benchmarks such as AppWorld. We present a\nreinforcement learning (RL) approach that trains IDAs directly in their target\nenvironments. We formalize this training as a partially observable Markov\ndecision process and derive LOOP, a data- and memory-efficient variant of\nproximal policy optimization. LOOP uses no value network and maintains exactly\none copy of the underlying LLM in memory, making its implementation\nstraightforward and as memory-efficient as fine-tuning a single LLM. A\n32-billion-parameter agent trained with LOOP in the AppWorld environment\noutperforms the much larger OpenAI o1 agent by 9 percentage points (15%\nrelative). To our knowledge, this is the first reported application of RL to\nIDAs that interact with a stateful, multi-domain, multi-app environment via\ndirect API calls. Our analysis sheds light on the effectiveness of RL in this\narea, showing that the agent learns to consult the API documentation, avoid\nunwarranted assumptions, minimize confabulation, and recover from setbacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive digital agents (IDAs) leverage APIs of stateful digital\nenvironments to perform tasks in response to user requests. While IDAs powered\nby instruction-tuned large language models (LLMs) can react to feedback from\ninterface invocations in multi-step exchanges, they have not been trained in\ntheir respective digital environments. Prior methods accomplish less than half\nof tasks in sophisticated benchmarks such as AppWorld. We present a\nreinforcement learning (RL) approach that trains IDAs directly in their target\nenvironments. We formalize this training as a partially observable Markov\ndecision process and derive LOOP, a data- and memory-efficient variant of\nproximal policy optimization. LOOP uses no value network and maintains exactly\none copy of the underlying LLM in memory, making its implementation\nstraightforward and as memory-efficient as fine-tuning a single LLM. A\n32-billion-parameter agent trained with LOOP in the AppWorld environment\noutperforms the much larger OpenAI o1 agent by 9 percentage points (15%\nrelative). To our knowledge, this is the first reported application of RL to\nIDAs that interact with a stateful, multi-domain, multi-app environment via\ndirect API calls. Our analysis sheds light on the effectiveness of RL in this\narea, showing that the agent learns to consult the API documentation, avoid\nunwarranted assumptions, minimize confabulation, and recover from setbacks."
                },
                "authors": [
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Marco Cusumano-Towner"
                    },
                    {
                        "name": "Brody Huval"
                    },
                    {
                        "name": "Aleksei Petrenko"
                    },
                    {
                        "name": "Jackson Hamburger"
                    },
                    {
                        "name": "Vladlen Koltun"
                    },
                    {
                        "name": "Philipp Krähenbühl"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Krähenbühl"
                },
                "author": "Philipp Krähenbühl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02345v1",
                "updated": "2025-02-04T14:27:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    27,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:27:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    27,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "Optimal Subspace Inference for the Laplace Approximation of Bayesian\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Subspace Inference for the Laplace Approximation of Bayesian\n  Neural Networks"
                },
                "summary": "Subspace inference for neural networks assumes that a subspace of their\nparameter space suffices to produce a reliable uncertainty quantification. In\nthis work, we mathematically derive the optimal subspace model to a Bayesian\ninference scenario based on the Laplace approximation. We demonstrate\nempirically that, in the optimal case, often a fraction of parameters less than\n1% is sufficient to obtain a reliable estimate of the full Laplace\napproximation. Since the optimal solution is derived, we can evaluate all other\nsubspace models against a baseline. In addition, we give an approximation of\nour method that is applicable to larger problem settings, in which the optimal\nsolution is not computable, and compare it to existing subspace models from the\nliterature. In general, our approximation scheme outperforms previous work.\nFurthermore, we present a metric to qualitatively compare different subspace\nmodels even if the exact Laplace approximation is unknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subspace inference for neural networks assumes that a subspace of their\nparameter space suffices to produce a reliable uncertainty quantification. In\nthis work, we mathematically derive the optimal subspace model to a Bayesian\ninference scenario based on the Laplace approximation. We demonstrate\nempirically that, in the optimal case, often a fraction of parameters less than\n1% is sufficient to obtain a reliable estimate of the full Laplace\napproximation. Since the optimal solution is derived, we can evaluate all other\nsubspace models against a baseline. In addition, we give an approximation of\nour method that is applicable to larger problem settings, in which the optimal\nsolution is not computable, and compare it to existing subspace models from the\nliterature. In general, our approximation scheme outperforms previous work.\nFurthermore, we present a metric to qualitatively compare different subspace\nmodels even if the exact Laplace approximation is unknown."
                },
                "authors": [
                    {
                        "name": "Josua Faller"
                    },
                    {
                        "name": "Jörg Martin"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Martin"
                },
                "author": "Jörg Martin",
                "arxiv_comment": "for associated code, see\n  https://github.com/josh3142/LowRankLaplaceApproximation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02342v1",
                "updated": "2025-02-04T14:20:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    20,
                    51,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:20:51Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    20,
                    51,
                    1,
                    35,
                    0
                ],
                "title": "SHIELD: APT Detection and Intelligent Explanation Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHIELD: APT Detection and Intelligent Explanation Using LLM"
                },
                "summary": "Advanced persistent threats (APTs) are sophisticated cyber attacks that can\nremain undetected for extended periods, making their mitigation particularly\nchallenging. Given their persistence, significant effort is required to detect\nthem and respond effectively. Existing provenance-based attack detection\nmethods often lack interpretability and suffer from high false positive rates,\nwhile investigation approaches are either supervised or limited to known\nattacks. To address these challenges, we introduce SHIELD, a novel approach\nthat combines statistical anomaly detection and graph-based analysis with the\ncontextual analysis capabilities of large language models (LLMs). SHIELD\nleverages the implicit knowledge of LLMs to uncover hidden attack patterns in\nprovenance data, while reducing false positives and providing clear,\ninterpretable attack descriptions. This reduces analysts' alert fatigue and\nmakes it easier for them to understand the threat landscape. Our extensive\nevaluation demonstrates SHIELD's effectiveness and computational efficiency in\nreal-world scenarios. SHIELD was shown to outperform state-of-the-art methods,\nachieving higher precision and recall. SHIELD's integration of anomaly\ndetection, LLM-driven contextual analysis, and advanced graph-based correlation\nestablishes a new benchmark for APT detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced persistent threats (APTs) are sophisticated cyber attacks that can\nremain undetected for extended periods, making their mitigation particularly\nchallenging. Given their persistence, significant effort is required to detect\nthem and respond effectively. Existing provenance-based attack detection\nmethods often lack interpretability and suffer from high false positive rates,\nwhile investigation approaches are either supervised or limited to known\nattacks. To address these challenges, we introduce SHIELD, a novel approach\nthat combines statistical anomaly detection and graph-based analysis with the\ncontextual analysis capabilities of large language models (LLMs). SHIELD\nleverages the implicit knowledge of LLMs to uncover hidden attack patterns in\nprovenance data, while reducing false positives and providing clear,\ninterpretable attack descriptions. This reduces analysts' alert fatigue and\nmakes it easier for them to understand the threat landscape. Our extensive\nevaluation demonstrates SHIELD's effectiveness and computational efficiency in\nreal-world scenarios. SHIELD was shown to outperform state-of-the-art methods,\nachieving higher precision and recall. SHIELD's integration of anomaly\ndetection, LLM-driven contextual analysis, and advanced graph-based correlation\nestablishes a new benchmark for APT detection."
                },
                "authors": [
                    {
                        "name": "Parth Atulbhai Gandhi"
                    },
                    {
                        "name": "Prasanna N. Wudali"
                    },
                    {
                        "name": "Yonatan Amaru"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02339v1",
                "updated": "2025-02-04T14:18:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    18,
                    29,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:18:29Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    18,
                    29,
                    1,
                    35,
                    0
                ],
                "title": "Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking"
                },
                "summary": "Multimodal large language models (MLLMs) exhibit impressive capabilities but\nstill face challenges in complex visual reasoning. While recent efforts attempt\nto enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking\nthrough explicit search structures or teacher-guided distillation, they often\nstruggle to balance performance and efficiency. A critical limitation is their\nheavy reliance on extensive data and search spaces, resulting in low-efficiency\nimplicit insight extraction and data utilization. To address this, we propose\nAStar, an Automated Structured thinking paradigm for multimodal reasoning via\nMonte Carlo Tree Search (MCTS). AStar automatically derives high-level\ncognitive reasoning patterns from limited data using MCTS-powered hierarchical\nstructures. Building on these explicit patterns, we design a unified reasoning\nframework that seamlessly integrates models' internal reasoning capabilities\nand external reasoning guidelines, enabling efficient inference with minimal\ntree iterations. This novel paradigm strikes a compelling balance between\nperformance and efficiency. Extensive experiments demonstrate AStar's\neffectiveness, achieving superior accuracy (54.0$\\%$) on the MathVerse\nbenchmark with a 7B backbone, surpassing GPT-4o (50.2$\\%$) while maintaining\nsubstantial data and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) exhibit impressive capabilities but\nstill face challenges in complex visual reasoning. While recent efforts attempt\nto enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking\nthrough explicit search structures or teacher-guided distillation, they often\nstruggle to balance performance and efficiency. A critical limitation is their\nheavy reliance on extensive data and search spaces, resulting in low-efficiency\nimplicit insight extraction and data utilization. To address this, we propose\nAStar, an Automated Structured thinking paradigm for multimodal reasoning via\nMonte Carlo Tree Search (MCTS). AStar automatically derives high-level\ncognitive reasoning patterns from limited data using MCTS-powered hierarchical\nstructures. Building on these explicit patterns, we design a unified reasoning\nframework that seamlessly integrates models' internal reasoning capabilities\nand external reasoning guidelines, enabling efficient inference with minimal\ntree iterations. This novel paradigm strikes a compelling balance between\nperformance and efficiency. Extensive experiments demonstrate AStar's\neffectiveness, achieving superior accuracy (54.0$\\%$) on the MathVerse\nbenchmark with a 7B backbone, surpassing GPT-4o (50.2$\\%$) while maintaining\nsubstantial data and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Ruihan Jin"
                    },
                    {
                        "name": "Feihu Che"
                    },
                    {
                        "name": "Zengqi Wen"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02338v1",
                "updated": "2025-02-04T14:17:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    17,
                    18,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:17:18Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    17,
                    18,
                    1,
                    35,
                    0
                ],
                "title": "Geometric Neural Process Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric Neural Process Fields"
                },
                "summary": "This paper addresses the challenge of Neural Field (NeF) generalization,\nwhere models must efficiently adapt to new signals given only a few\nobservations. To tackle this, we propose Geometric Neural Process Fields\n(G-NPF), a probabilistic framework for neural radiance fields that explicitly\ncaptures uncertainty. We formulate NeF generalization as a probabilistic\nproblem, enabling direct inference of NeF function distributions from limited\ncontext observations. To incorporate structural inductive biases, we introduce\na set of geometric bases that encode spatial structure and facilitate the\ninference of NeF function distributions. Building on these bases, we design a\nhierarchical latent variable model, allowing G-NPF to integrate structural\ninformation across multiple spatial levels and effectively parameterize INR\nfunctions. This hierarchical approach improves generalization to novel scenes\nand unseen signals. Experiments on novel-view synthesis for 3D scenes, as well\nas 2D image and 1D signal regression, demonstrate the effectiveness of our\nmethod in capturing uncertainty and leveraging structural information for\nimproved generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of Neural Field (NeF) generalization,\nwhere models must efficiently adapt to new signals given only a few\nobservations. To tackle this, we propose Geometric Neural Process Fields\n(G-NPF), a probabilistic framework for neural radiance fields that explicitly\ncaptures uncertainty. We formulate NeF generalization as a probabilistic\nproblem, enabling direct inference of NeF function distributions from limited\ncontext observations. To incorporate structural inductive biases, we introduce\na set of geometric bases that encode spatial structure and facilitate the\ninference of NeF function distributions. Building on these bases, we design a\nhierarchical latent variable model, allowing G-NPF to integrate structural\ninformation across multiple spatial levels and effectively parameterize INR\nfunctions. This hierarchical approach improves generalization to novel scenes\nand unseen signals. Experiments on novel-view synthesis for 3D scenes, as well\nas 2D image and 1D signal regression, demonstrate the effectiveness of our\nmethod in capturing uncertainty and leveraging structural information for\nimproved generalization."
                },
                "authors": [
                    {
                        "name": "Wenzhe Yin"
                    },
                    {
                        "name": "Zehao Xiao"
                    },
                    {
                        "name": "Jiayi Shen"
                    },
                    {
                        "name": "Yunlu Chen"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Jan-Jakob Sonke"
                    },
                    {
                        "name": "Efstratios Gavves"
                    }
                ],
                "author_detail": {
                    "name": "Efstratios Gavves"
                },
                "author": "Efstratios Gavves",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02337v1",
                "updated": "2025-02-04T14:16:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    16,
                    2,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:16:02Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    16,
                    2,
                    1,
                    35,
                    0
                ],
                "title": "Rule-ATT&CK Mapper (RAM): Mapping SIEM Rules to TTPs Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rule-ATT&CK Mapper (RAM): Mapping SIEM Rules to TTPs Using LLMs"
                },
                "summary": "The growing frequency of cyberattacks has heightened the demand for accurate\nand efficient threat detection systems. SIEM platforms are important for\nanalyzing log data and detecting adversarial activities through rule-based\nqueries, also known as SIEM rules. The efficiency of the threat analysis\nprocess relies heavily on mapping these SIEM rules to the relevant attack\ntechniques in the MITRE ATT&CK framework. Inaccurate annotation of SIEM rules\ncan result in the misinterpretation of attacks, increasing the likelihood that\nthreats will be overlooked. Existing solutions for annotating SIEM rules with\nMITRE ATT&CK technique labels have notable limitations: manual annotation of\nSIEM rules is both time-consuming and prone to errors, and ML-based approaches\nmainly focus on annotating unstructured free text sources rather than\nstructured data like SIEM rules. Structured data often contains limited\ninformation, further complicating the annotation process and making it a\nchallenging task. To address these challenges, we propose Rule-ATT&CK Mapper\n(RAM), a novel framework that leverages LLMs to automate the mapping of\nstructured SIEM rules to MITRE ATT&CK techniques. RAM's multi-stage pipeline,\nwhich was inspired by the prompt chaining technique, enhances mapping accuracy\nwithout requiring LLM pre-training or fine-tuning. Using the Splunk Security\nContent dataset, we evaluate RAM's performance using several LLMs, including\nGPT-4-Turbo, Qwen, IBM Granite, and Mistral. Our evaluation highlights\nGPT-4-Turbo's superior performance, which derives from its enriched knowledge\nbase, and an ablation study emphasizes the importance of external contextual\nknowledge in overcoming the limitations of LLMs' implicit knowledge for\ndomain-specific tasks. These findings demonstrate RAM's potential in automating\ncybersecurity workflows and provide valuable insights for future advancements\nin this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing frequency of cyberattacks has heightened the demand for accurate\nand efficient threat detection systems. SIEM platforms are important for\nanalyzing log data and detecting adversarial activities through rule-based\nqueries, also known as SIEM rules. The efficiency of the threat analysis\nprocess relies heavily on mapping these SIEM rules to the relevant attack\ntechniques in the MITRE ATT&CK framework. Inaccurate annotation of SIEM rules\ncan result in the misinterpretation of attacks, increasing the likelihood that\nthreats will be overlooked. Existing solutions for annotating SIEM rules with\nMITRE ATT&CK technique labels have notable limitations: manual annotation of\nSIEM rules is both time-consuming and prone to errors, and ML-based approaches\nmainly focus on annotating unstructured free text sources rather than\nstructured data like SIEM rules. Structured data often contains limited\ninformation, further complicating the annotation process and making it a\nchallenging task. To address these challenges, we propose Rule-ATT&CK Mapper\n(RAM), a novel framework that leverages LLMs to automate the mapping of\nstructured SIEM rules to MITRE ATT&CK techniques. RAM's multi-stage pipeline,\nwhich was inspired by the prompt chaining technique, enhances mapping accuracy\nwithout requiring LLM pre-training or fine-tuning. Using the Splunk Security\nContent dataset, we evaluate RAM's performance using several LLMs, including\nGPT-4-Turbo, Qwen, IBM Granite, and Mistral. Our evaluation highlights\nGPT-4-Turbo's superior performance, which derives from its enriched knowledge\nbase, and an ablation study emphasizes the importance of external contextual\nknowledge in overcoming the limitations of LLMs' implicit knowledge for\ndomain-specific tasks. These findings demonstrate RAM's potential in automating\ncybersecurity workflows and provide valuable insights for future advancements\nin this field."
                },
                "authors": [
                    {
                        "name": "Prasanna N. Wudali"
                    },
                    {
                        "name": "Moshe Kravchik"
                    },
                    {
                        "name": "Ehud Malul"
                    },
                    {
                        "name": "Parth A. Gandhi"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00029v2",
                "updated": "2025-02-04T14:15:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    15,
                    35,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-23T11:35:17Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    35,
                    17,
                    3,
                    23,
                    0
                ],
                "title": "AlphaSharpe: LLM-Driven Discovery of Robust Risk-Adjusted Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaSharpe: LLM-Driven Discovery of Robust Risk-Adjusted Metrics"
                },
                "summary": "Financial metrics like the Sharpe ratio are pivotal in evaluating investment\nperformance by balancing risk and return. However, traditional metrics often\nstruggle with robustness and generalization, particularly in dynamic and\nvolatile market conditions. This paper introduces AlphaSharpe, a novel\nframework leveraging large language models (LLMs) to iteratively evolve and\noptimize financial metrics to discover enhanced risk-return metrics that\noutperform traditional approaches in robustness and correlation with future\nperformance metrics by employing iterative crossover, mutation, and evaluation.\nKey contributions of this work include: (1) a novel use of LLMs to generate and\nrefine financial metrics with implicit domain-specific knowledge, (2) a scoring\nmechanism to ensure that evolved metrics generalize effectively to unseen data,\nand (3) an empirical demonstration of 3x predictive power for future\nrisk-returns, and 2x portfolio performance. Experimental results in a\nreal-world dataset highlight the superiority of discovered metrics, making them\nhighly relevant to portfolio managers and financial decision-makers. This\nframework not only addresses the limitations of existing metrics but also\nshowcases the potential of LLMs in advancing financial analytics, paving the\nway for informed and robust investment strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial metrics like the Sharpe ratio are pivotal in evaluating investment\nperformance by balancing risk and return. However, traditional metrics often\nstruggle with robustness and generalization, particularly in dynamic and\nvolatile market conditions. This paper introduces AlphaSharpe, a novel\nframework leveraging large language models (LLMs) to iteratively evolve and\noptimize financial metrics to discover enhanced risk-return metrics that\noutperform traditional approaches in robustness and correlation with future\nperformance metrics by employing iterative crossover, mutation, and evaluation.\nKey contributions of this work include: (1) a novel use of LLMs to generate and\nrefine financial metrics with implicit domain-specific knowledge, (2) a scoring\nmechanism to ensure that evolved metrics generalize effectively to unseen data,\nand (3) an empirical demonstration of 3x predictive power for future\nrisk-returns, and 2x portfolio performance. Experimental results in a\nreal-world dataset highlight the superiority of discovered metrics, making them\nhighly relevant to portfolio managers and financial decision-makers. This\nframework not only addresses the limitations of existing metrics but also\nshowcases the potential of LLMs in advancing financial analytics, paving the\nway for informed and robust investment strategies."
                },
                "authors": [
                    {
                        "name": "Kamer Ali Yuksel"
                    },
                    {
                        "name": "Hassan Sawaf"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sawaf"
                },
                "author": "Hassan Sawaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02329v1",
                "updated": "2025-02-04T14:00:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    0,
                    32,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:00:32Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    0,
                    32,
                    1,
                    35,
                    0
                ],
                "title": "ReSpark: Leveraging Previous Data Reports as References to Generate New\n  Reports with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSpark: Leveraging Previous Data Reports as References to Generate New\n  Reports with LLMs"
                },
                "summary": "Creating data reports is time-consuming, as it requires iterative exploration\nand understanding of data, followed by summarizing the insights. While large\nlanguage models (LLMs) are powerful tools for data processing and text\ngeneration, they often struggle to produce complete data reports that fully\nmeet user expectations. One significant challenge is effectively communicating\nthe entire analysis logic to LLMs. Moreover, determining a comprehensive\nanalysis logic can be mentally taxing for users. To address these challenges,\nwe propose ReSpark, an LLM-based method that leverages existing data reports as\nreferences for creating new ones. Given a data table, ReSpark searches for\nsimilar-topic reports, parses them into interdependent segments corresponding\nto analytical objectives, and executes them with new data. It identifies\ninconsistencies and customizes the objectives, data transformations, and\ntextual descriptions. ReSpark allows users to review real-time outputs, insert\nnew objectives, and modify report content. Its effectiveness was evaluated\nthrough comparative and user studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating data reports is time-consuming, as it requires iterative exploration\nand understanding of data, followed by summarizing the insights. While large\nlanguage models (LLMs) are powerful tools for data processing and text\ngeneration, they often struggle to produce complete data reports that fully\nmeet user expectations. One significant challenge is effectively communicating\nthe entire analysis logic to LLMs. Moreover, determining a comprehensive\nanalysis logic can be mentally taxing for users. To address these challenges,\nwe propose ReSpark, an LLM-based method that leverages existing data reports as\nreferences for creating new ones. Given a data table, ReSpark searches for\nsimilar-topic reports, parses them into interdependent segments corresponding\nto analytical objectives, and executes them with new data. It identifies\ninconsistencies and customizes the objectives, data transformations, and\ntextual descriptions. ReSpark allows users to review real-time outputs, insert\nnew objectives, and modify report content. Its effectiveness was evaluated\nthrough comparative and user studies."
                },
                "authors": [
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Chuhan Zhang"
                    },
                    {
                        "name": "Xiaotong Wang"
                    },
                    {
                        "name": "Sitong Pan"
                    },
                    {
                        "name": "Weiwei Cui"
                    },
                    {
                        "name": "Haidong Zhang"
                    },
                    {
                        "name": "Dazhen Deng"
                    },
                    {
                        "name": "Yingcai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yingcai Wu"
                },
                "author": "Yingcai Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14630v2",
                "updated": "2025-02-04T13:55:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    55,
                    19,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-24T16:49:08Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    49,
                    8,
                    4,
                    24,
                    0
                ],
                "title": "Extracting Problem Structure with LLMs for Optimized SAT Local Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Problem Structure with LLMs for Optimized SAT Local Search"
                },
                "summary": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems."
                },
                "authors": [
                    {
                        "name": "André Schidler"
                    },
                    {
                        "name": "Stefan Szeider"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Szeider"
                },
                "author": "Stefan Szeider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02316v1",
                "updated": "2025-02-04T13:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    37,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T13:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    37,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "DIME:Diffusion-Based Maximum Entropy Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIME:Diffusion-Based Maximum Entropy Reinforcement Learning"
                },
                "summary": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard\napproach to RL due to its beneficial exploration properties. Traditionally,\npolicies are parameterized using Gaussian distributions, which significantly\nlimits their representational capacity. Diffusion-based policies offer a more\nexpressive alternative, yet integrating them into MaxEnt-RL poses\nchallenges--primarily due to the intractability of computing their marginal\nentropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL\n(DIME). DIME leverages recent advances in approximate inference with diffusion\nmodels to derive a lower bound on the maximum entropy objective. Additionally,\nwe propose a policy iteration scheme that provably converges to the optimal\ndiffusion policy. Our method enables the use of expressive diffusion-based\npolicies while retaining the principled exploration benefits of MaxEnt-RL,\nsignificantly outperforming other diffusion-based methods on challenging\nhigh-dimensional control benchmarks. It is also competitive with\nstate-of-the-art non-diffusion based RL methods while requiring fewer\nalgorithmic design choices and smaller update-to-data ratios, reducing\ncomputational complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard\napproach to RL due to its beneficial exploration properties. Traditionally,\npolicies are parameterized using Gaussian distributions, which significantly\nlimits their representational capacity. Diffusion-based policies offer a more\nexpressive alternative, yet integrating them into MaxEnt-RL poses\nchallenges--primarily due to the intractability of computing their marginal\nentropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL\n(DIME). DIME leverages recent advances in approximate inference with diffusion\nmodels to derive a lower bound on the maximum entropy objective. Additionally,\nwe propose a policy iteration scheme that provably converges to the optimal\ndiffusion policy. Our method enables the use of expressive diffusion-based\npolicies while retaining the principled exploration benefits of MaxEnt-RL,\nsignificantly outperforming other diffusion-based methods on challenging\nhigh-dimensional control benchmarks. It is also competitive with\nstate-of-the-art non-diffusion based RL methods while requiring fewer\nalgorithmic design choices and smaller update-to-data ratios, reducing\ncomputational complexity."
                },
                "authors": [
                    {
                        "name": "Onur Celik"
                    },
                    {
                        "name": "Zechu Li"
                    },
                    {
                        "name": "Denis Blessing"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Daniel Palanicek"
                    },
                    {
                        "name": "Jan Peters"
                    },
                    {
                        "name": "Georgia Chalvatzaki"
                    },
                    {
                        "name": "Gerhard Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Neumann"
                },
                "author": "Gerhard Neumann",
                "arxiv_comment": "8 pages main text, 18 pages all included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02315v1",
                "updated": "2025-02-04T13:36:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    36,
                    54,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T13:36:54Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    36,
                    54,
                    1,
                    35,
                    0
                ],
                "title": "VaiBot: Shuttle Between the Instructions and Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VaiBot: Shuttle Between the Instructions and Parameters"
                },
                "summary": "How to interact with LLMs through \\emph{instructions} has been widely studied\nby researchers. However, previous studies have treated the emergence of\ninstructions and the training of LLMs on task data as separate processes,\noverlooking the inherent unity between the two. This paper proposes a neural\nnetwork framework, VaiBot, that integrates VAE and VIB, designed to uniformly\nmodel, learn, and infer both deduction and induction tasks under LLMs. Through\nexperiments, we demonstrate that VaiBot performs on par with existing baseline\nmethods in terms of deductive capabilities while significantly surpassing them\nin inductive capabilities. We also find that VaiBot can scale up using general\ninstruction-following data and exhibits excellent one-shot induction abilities.\nWe finally synergistically integrate the deductive and inductive processes of\nVaiBot. Through T-SNE dimensionality reduction, we observe that its\ninductive-deductive process significantly improves the distribution of training\nparameters, enabling it to outperform baseline methods in inductive reasoning\ntasks. The code and data for this paper can be found at\nhttps://anonymous.4open.science/r/VaiBot-021F.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to interact with LLMs through \\emph{instructions} has been widely studied\nby researchers. However, previous studies have treated the emergence of\ninstructions and the training of LLMs on task data as separate processes,\noverlooking the inherent unity between the two. This paper proposes a neural\nnetwork framework, VaiBot, that integrates VAE and VIB, designed to uniformly\nmodel, learn, and infer both deduction and induction tasks under LLMs. Through\nexperiments, we demonstrate that VaiBot performs on par with existing baseline\nmethods in terms of deductive capabilities while significantly surpassing them\nin inductive capabilities. We also find that VaiBot can scale up using general\ninstruction-following data and exhibits excellent one-shot induction abilities.\nWe finally synergistically integrate the deductive and inductive processes of\nVaiBot. Through T-SNE dimensionality reduction, we observe that its\ninductive-deductive process significantly improves the distribution of training\nparameters, enabling it to outperform baseline methods in inductive reasoning\ntasks. The code and data for this paper can be found at\nhttps://anonymous.4open.science/r/VaiBot-021F."
                },
                "authors": [
                    {
                        "name": "Wangtao Sun"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Xuanqing Yu"
                    },
                    {
                        "name": "Zhongtao Jiang"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02304v1",
                "updated": "2025-02-04T13:18:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    18,
                    51,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T13:18:51Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    18,
                    51,
                    1,
                    35,
                    0
                ],
                "title": "Comparative Analysis of FPGA and GPU Performance for Machine\n  Learning-Based Track Reconstruction at LHCb",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of FPGA and GPU Performance for Machine\n  Learning-Based Track Reconstruction at LHCb"
                },
                "summary": "In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power."
                },
                "authors": [
                    {
                        "name": "Fotis I. Giasemis"
                    },
                    {
                        "name": "Vladimir Lončar"
                    },
                    {
                        "name": "Bertrand Granado"
                    },
                    {
                        "name": "Vladimir Vava Gligorov"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Vava Gligorov"
                },
                "author": "Vladimir Vava Gligorov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02294v1",
                "updated": "2025-02-04T13:05:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    5,
                    39,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T13:05:39Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    5,
                    39,
                    1,
                    35,
                    0
                ],
                "title": "Differentiable Cosmological Hydrodynamics for Field-Level Inference and\n  High Dimensional Parameter Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Cosmological Hydrodynamics for Field-Level Inference and\n  High Dimensional Parameter Constraints"
                },
                "summary": "Hydrodynamical simulations are the most accurate way to model structure\nformation in the universe, but they often involve a large number of\nastrophysical parameters modeling subgrid physics, in addition to cosmological\nparameters. This results in a high-dimensional space that is difficult to\njointly constrain using traditional statistical methods due to prohibitive\ncomputational costs. To address this, we present a fully differentiable\napproach for cosmological hydrodynamical simulations and a proof-of-concept\nimplementation, diffhydro. By back-propagating through an upwind finite volume\nscheme for solving the Euler Equations jointly with a dark matter particle-mesh\nmethod for Poisson equation, we are able to efficiently evaluate derivatives of\nthe output baryonic fields with respect to input density and model parameters.\nImportantly, we demonstrate how to differentiate through stochastically sampled\ndiscrete random variables, which frequently appear in subgrid models. We use\nthis framework to rapidly sample sub-grid physics and cosmological parameters\nas well as perform field level inference of initial conditions using high\ndimensional optimization techniques. Our code is implemented in JAX (python),\nallowing easy code development and GPU acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydrodynamical simulations are the most accurate way to model structure\nformation in the universe, but they often involve a large number of\nastrophysical parameters modeling subgrid physics, in addition to cosmological\nparameters. This results in a high-dimensional space that is difficult to\njointly constrain using traditional statistical methods due to prohibitive\ncomputational costs. To address this, we present a fully differentiable\napproach for cosmological hydrodynamical simulations and a proof-of-concept\nimplementation, diffhydro. By back-propagating through an upwind finite volume\nscheme for solving the Euler Equations jointly with a dark matter particle-mesh\nmethod for Poisson equation, we are able to efficiently evaluate derivatives of\nthe output baryonic fields with respect to input density and model parameters.\nImportantly, we demonstrate how to differentiate through stochastically sampled\ndiscrete random variables, which frequently appear in subgrid models. We use\nthis framework to rapidly sample sub-grid physics and cosmological parameters\nas well as perform field level inference of initial conditions using high\ndimensional optimization techniques. Our code is implemented in JAX (python),\nallowing easy code development and GPU acceleration."
                },
                "authors": [
                    {
                        "name": "Benjamin Horowitz"
                    },
                    {
                        "name": "Zarija Lukic"
                    }
                ],
                "author_detail": {
                    "name": "Zarija Lukic"
                },
                "author": "Zarija Lukic",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02289v1",
                "updated": "2025-02-04T12:58:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    58,
                    19,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T12:58:19Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    58,
                    19,
                    1,
                    35,
                    0
                ],
                "title": "Evalita-LLM: Benchmarking Large Language Models on Italian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evalita-LLM: Benchmarking Large Language Models on Italian"
                },
                "summary": "We describe Evalita-LLM, a new benchmark designed to evaluate Large Language\nModels (LLMs) on Italian tasks. The distinguishing and innovative features of\nEvalita-LLM are the following: (i) all tasks are native Italian, avoiding\nissues of translating from Italian and potential cultural biases; (ii) in\naddition to well established multiple-choice tasks, the benchmark includes\ngenerative tasks, enabling more natural interaction with LLMs; (iii) all tasks\nare evaluated against multiple prompts, this way mitigating the model\nsensitivity to specific prompts and allowing a fairer and objective evaluation.\nWe propose an iterative methodology, where candidate tasks and candidate\nprompts are validated against a set of LLMs used for development. We report\nexperimental results from the benchmark's development phase, and provide\nperformance statistics for several state-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Evalita-LLM, a new benchmark designed to evaluate Large Language\nModels (LLMs) on Italian tasks. The distinguishing and innovative features of\nEvalita-LLM are the following: (i) all tasks are native Italian, avoiding\nissues of translating from Italian and potential cultural biases; (ii) in\naddition to well established multiple-choice tasks, the benchmark includes\ngenerative tasks, enabling more natural interaction with LLMs; (iii) all tasks\nare evaluated against multiple prompts, this way mitigating the model\nsensitivity to specific prompts and allowing a fairer and objective evaluation.\nWe propose an iterative methodology, where candidate tasks and candidate\nprompts are validated against a set of LLMs used for development. We report\nexperimental results from the benchmark's development phase, and provide\nperformance statistics for several state-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Bernardo Magnini"
                    },
                    {
                        "name": "Roberto Zanoli"
                    },
                    {
                        "name": "Michele Resta"
                    },
                    {
                        "name": "Martin Cimmino"
                    },
                    {
                        "name": "Paolo Albano"
                    },
                    {
                        "name": "Marco Madeddu"
                    },
                    {
                        "name": "Viviana Patti"
                    }
                ],
                "author_detail": {
                    "name": "Viviana Patti"
                },
                "author": "Viviana Patti",
                "arxiv_comment": "42 pages, 1 figure, 32 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02287v1",
                "updated": "2025-02-04T12:56:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    56,
                    59,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T12:56:59Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    56,
                    59,
                    1,
                    35,
                    0
                ],
                "title": "Adaptive Resource Allocation Optimization Using Large Language Models in\n  Dynamic Wireless Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Resource Allocation Optimization Using Large Language Models in\n  Dynamic Wireless Environments"
                },
                "summary": "Deep learning (DL) has made notable progress in addressing complex radio\naccess network control challenges that conventional analytic methods have\nstruggled to solve. However, DL has shown limitations in solving constrained\nNP-hard problems often encountered in network optimization, such as those\ninvolving quality of service (QoS) or discrete variables like user indices.\nCurrent solutions rely on domain-specific architectures or heuristic\ntechniques, and a general DL approach for constrained optimization remains\nundeveloped. Moreover, even minor changes in communication objectives demand\ntime-consuming retraining, limiting their adaptability to dynamic environments\nwhere task objectives, constraints, environmental factors, and communication\nscenarios frequently change. To address these challenges, we propose a large\nlanguage model for resource allocation optimizer (LLM-RAO), a novel approach\nthat harnesses the capabilities of LLMs to address the complex resource\nallocation problem while adhering to QoS constraints. By employing a\nprompt-based tuning strategy to flexibly convey ever-changing task descriptions\nand requirements to the LLM, LLM-RAO demonstrates robust performance and\nseamless adaptability in dynamic environments without requiring extensive\nretraining. Simulation results reveal that LLM-RAO achieves up to a 40%\nperformance enhancement compared to conventional DL methods and up to an $80$\\%\nimprovement over analytical approaches. Moreover, in scenarios with fluctuating\ncommunication objectives, LLM-RAO attains up to 2.9 times the performance of\ntraditional DL-based networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) has made notable progress in addressing complex radio\naccess network control challenges that conventional analytic methods have\nstruggled to solve. However, DL has shown limitations in solving constrained\nNP-hard problems often encountered in network optimization, such as those\ninvolving quality of service (QoS) or discrete variables like user indices.\nCurrent solutions rely on domain-specific architectures or heuristic\ntechniques, and a general DL approach for constrained optimization remains\nundeveloped. Moreover, even minor changes in communication objectives demand\ntime-consuming retraining, limiting their adaptability to dynamic environments\nwhere task objectives, constraints, environmental factors, and communication\nscenarios frequently change. To address these challenges, we propose a large\nlanguage model for resource allocation optimizer (LLM-RAO), a novel approach\nthat harnesses the capabilities of LLMs to address the complex resource\nallocation problem while adhering to QoS constraints. By employing a\nprompt-based tuning strategy to flexibly convey ever-changing task descriptions\nand requirements to the LLM, LLM-RAO demonstrates robust performance and\nseamless adaptability in dynamic environments without requiring extensive\nretraining. Simulation results reveal that LLM-RAO achieves up to a 40%\nperformance enhancement compared to conventional DL methods and up to an $80$\\%\nimprovement over analytical approaches. Moreover, in scenarios with fluctuating\ncommunication objectives, LLM-RAO attains up to 2.9 times the performance of\ntraditional DL-based networks."
                },
                "authors": [
                    {
                        "name": "Hyeonho Noh"
                    },
                    {
                        "name": "Byonghyo Shim"
                    },
                    {
                        "name": "Hyun Jong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Jong Yang"
                },
                "author": "Hyun Jong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02283v1",
                "updated": "2025-02-04T12:50:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    50,
                    16,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T12:50:16Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    50,
                    16,
                    1,
                    35,
                    0
                ],
                "title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting"
                },
                "summary": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds consistently compromises the scene reconstruction quality. To\naddress these limitations, this paper proposes a novel 3D reconstruction\nframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output\nGaussian Process model is developed to achieve adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. The densified point clouds\nprovide high-quality initial 3D Gaussians to enhance reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds consistently compromises the scene reconstruction quality. To\naddress these limitations, this paper proposes a novel 3D reconstruction\nframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output\nGaussian Process model is developed to achieve adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. The densified point clouds\nprovide high-quality initial 3D Gaussians to enhance reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework."
                },
                "authors": [
                    {
                        "name": "Zhihao Guo"
                    },
                    {
                        "name": "Jingxuan Su"
                    },
                    {
                        "name": "Shenglin Wang"
                    },
                    {
                        "name": "Jinlong Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Liangxiu Han"
                    },
                    {
                        "name": "Peng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Wang"
                },
                "author": "Peng Wang",
                "arxiv_comment": "14 pages,11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00846v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00846v3",
                "updated": "2025-02-04T12:18:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    18,
                    16,
                    1,
                    35,
                    0
                ],
                "published": "2024-02-29T14:22:16Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    14,
                    22,
                    16,
                    3,
                    60,
                    0
                ],
                "title": "Quantum Bayesian Inference with Renormalization for Gravitational Waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Bayesian Inference with Renormalization for Gravitational Waves"
                },
                "summary": "Advancements in gravitational-wave interferometers, particularly the next\ngeneration, are poised to profoundly impact gravitational wave astronomy and\nmultimessenger astrophysics. A hybrid quantum algorithm is proposed to carry\nout quantum inference of parameters from compact binary coalescences detected\nin gravitational-wave interferometers. It performs quantum Bayesian Inference\nwith Renormalization and Downsampling (qBIRD). We choose binary black hole\n(BBH) mergers from LIGO observatories as the first case to test the algorithm,\nbut its application can be extended to more general instances. The quantum\nalgorithm is able to generate corner plots of relevant parameters such as chirp\nmass, mass ratio, spins, etc. by inference of simulated gravitational waves\nwith known injected parameter values with zero noise, Gaussian noise and real\ndata, thus recovering an accuracy equivalent to that of classical Markov Chain\nMonte Carlo inferences. The simulations are performed with sets of 2 and 4\nparameters. These results enhance the possibilities to extend our capacity to\ntrack signals from coalescences over longer durations and at lower frequencies\nextending the accuracy and promptness of gravitational wave parameter\nestimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in gravitational-wave interferometers, particularly the next\ngeneration, are poised to profoundly impact gravitational wave astronomy and\nmultimessenger astrophysics. A hybrid quantum algorithm is proposed to carry\nout quantum inference of parameters from compact binary coalescences detected\nin gravitational-wave interferometers. It performs quantum Bayesian Inference\nwith Renormalization and Downsampling (qBIRD). We choose binary black hole\n(BBH) mergers from LIGO observatories as the first case to test the algorithm,\nbut its application can be extended to more general instances. The quantum\nalgorithm is able to generate corner plots of relevant parameters such as chirp\nmass, mass ratio, spins, etc. by inference of simulated gravitational waves\nwith known injected parameter values with zero noise, Gaussian noise and real\ndata, thus recovering an accuracy equivalent to that of classical Markov Chain\nMonte Carlo inferences. The simulations are performed with sets of 2 and 4\nparameters. These results enhance the possibilities to extend our capacity to\ntrack signals from coalescences over longer durations and at lower frequencies\nextending the accuracy and promptness of gravitational wave parameter\nestimation."
                },
                "authors": [
                    {
                        "name": "Gabriel Escrig"
                    },
                    {
                        "name": "Roberto Campos"
                    },
                    {
                        "name": "Hong Qi"
                    },
                    {
                        "name": "M. A. Martin-Delgado"
                    }
                ],
                "author_detail": {
                    "name": "M. A. Martin-Delgado"
                },
                "author": "M. A. Martin-Delgado",
                "arxiv_doi": "10.3847/2041-8213/ada6ae",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ada6ae",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.00846v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00846v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "(6 pages, 4 figures)",
                "arxiv_journal_ref": "The Astrophysical Journal Letters 979 2, 2025",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02260v1",
                "updated": "2025-02-04T12:17:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    17,
                    8,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T12:17:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    17,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "Adversarial ML Problems Are Getting Harder to Solve and to Evaluate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial ML Problems Are Getting Harder to Solve and to Evaluate"
                },
                "summary": "In the past decade, considerable research effort has been devoted to securing\nmachine learning (ML) models that operate in adversarial settings. Yet,\nprogress has been slow even for simple \"toy\" problems (e.g., robustness to\nsmall adversarial perturbations) and is often hindered by non-rigorous\nevaluations. Today, adversarial ML research has shifted towards studying\nlarger, general-purpose language models. In this position paper, we argue that\nthe situation is now even worse: in the era of LLMs, the field of adversarial\nML studies problems that are (1) less clearly defined, (2) harder to solve, and\n(3) even more challenging to evaluate. As a result, we caution that yet another\ndecade of work on adversarial ML may fail to produce meaningful progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past decade, considerable research effort has been devoted to securing\nmachine learning (ML) models that operate in adversarial settings. Yet,\nprogress has been slow even for simple \"toy\" problems (e.g., robustness to\nsmall adversarial perturbations) and is often hindered by non-rigorous\nevaluations. Today, adversarial ML research has shifted towards studying\nlarger, general-purpose language models. In this position paper, we argue that\nthe situation is now even worse: in the era of LLMs, the field of adversarial\nML studies problems that are (1) less clearly defined, (2) harder to solve, and\n(3) even more challenging to evaluate. As a result, we caution that yet another\ndecade of work on adversarial ML may fail to produce meaningful progress."
                },
                "authors": [
                    {
                        "name": "Javier Rando"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Florian Tramèr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramèr"
                },
                "author": "Florian Tramèr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02249v1",
                "updated": "2025-02-04T11:50:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    50,
                    40,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T11:50:40Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    50,
                    40,
                    1,
                    35,
                    0
                ],
                "title": "Conversation AI Dialog for Medicare powered by Finetuning and Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation AI Dialog for Medicare powered by Finetuning and Retrieval\n  Augmented Generation"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage processing tasks, including dialogue generation. This research aims to\nconduct a novel comparative analysis of two prominent techniques, fine-tuning\nwith LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG)\nframework, in the context of doctor-patient chat conversations with multiple\ndatasets of mixed medical domains. The analysis involves three state-of-the-art\nmodels: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient\ndialogues, we comprehensively evaluate the performance of models, assessing key\nmetrics such as language quality (perplexity, BLEU score), factual accuracy\n(fact-checking against medical knowledge bases), adherence to medical\nguidelines, and overall human judgments (coherence, empathy, safety). The\nfindings provide insights into the strengths and limitations of each approach,\nshedding light on their suitability for healthcare applications. Furthermore,\nthe research investigates the robustness of the models in handling diverse\npatient queries, ranging from general health inquiries to specific medical\nconditions. The impact of domain-specific knowledge integration is also\nexplored, highlighting the potential for enhancing LLM performance through\ntargeted data augmentation and retrieval strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage processing tasks, including dialogue generation. This research aims to\nconduct a novel comparative analysis of two prominent techniques, fine-tuning\nwith LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG)\nframework, in the context of doctor-patient chat conversations with multiple\ndatasets of mixed medical domains. The analysis involves three state-of-the-art\nmodels: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient\ndialogues, we comprehensively evaluate the performance of models, assessing key\nmetrics such as language quality (perplexity, BLEU score), factual accuracy\n(fact-checking against medical knowledge bases), adherence to medical\nguidelines, and overall human judgments (coherence, empathy, safety). The\nfindings provide insights into the strengths and limitations of each approach,\nshedding light on their suitability for healthcare applications. Furthermore,\nthe research investigates the robustness of the models in handling diverse\npatient queries, ranging from general health inquiries to specific medical\nconditions. The impact of domain-specific knowledge integration is also\nexplored, highlighting the potential for enhancing LLM performance through\ntargeted data augmentation and retrieval strategies."
                },
                "authors": [
                    {
                        "name": "Atharva Mangeshkumar Agrawal"
                    },
                    {
                        "name": "Rutika Pandurang Shinde"
                    },
                    {
                        "name": "Vasanth Kumar Bhukya"
                    },
                    {
                        "name": "Ashmita Chakraborty"
                    },
                    {
                        "name": "Sagar Bharat Shah"
                    },
                    {
                        "name": "Tanmay Shukla"
                    },
                    {
                        "name": "Sree Pradeep Kumar Relangi"
                    },
                    {
                        "name": "Nilesh Mutyam"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Mutyam"
                },
                "author": "Nilesh Mutyam",
                "arxiv_comment": "12 pages",
                "arxiv_journal_ref": "ResMilitaris 2022",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00791v2",
                "updated": "2025-02-04T11:45:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    45,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-02T13:10:06Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    13,
                    10,
                    6,
                    6,
                    33,
                    0
                ],
                "title": "Vision-centric Token Compression in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-centric Token Compression in Large Language Model"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nexcelling in handling longer sequences. However, the inefficiency and\nredundancy in processing extended in-context tokens remain a challenge. Many\nattempts to address this rely on compressing tokens with smaller text encoders,\nyet we question whether text encoders are truly indispensable. Our journey\nleads to an unexpected discovery-a much smaller vision encoder, applied\ndirectly to sequences of text tokens, can rival text encoders on text tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small text understanding benchmarks, VIST leads to comparable results with\n16% fewer FLOPs and 50% less memory usage. We further uncover significant token\nredundancy and devise a frequency-based masking strategy to guide the focus of\nthe visual encoder toward the most critical tokens. Interestingly, we observe\nthe trained visual encoder performs like a summarizer, selectively ignoring\nless important words such as prepositions and conjunctions. This approach\ndelivers remarkable results, outperforming traditional text encoder-based\nmethods by 5.7% on average over benchmarks like TriviaQA, NQ, PopQA, TREF,\nSST2, and SST5, setting a new standard for token efficiency in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nexcelling in handling longer sequences. However, the inefficiency and\nredundancy in processing extended in-context tokens remain a challenge. Many\nattempts to address this rely on compressing tokens with smaller text encoders,\nyet we question whether text encoders are truly indispensable. Our journey\nleads to an unexpected discovery-a much smaller vision encoder, applied\ndirectly to sequences of text tokens, can rival text encoders on text tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small text understanding benchmarks, VIST leads to comparable results with\n16% fewer FLOPs and 50% less memory usage. We further uncover significant token\nredundancy and devise a frequency-based masking strategy to guide the focus of\nthe visual encoder toward the most critical tokens. Interestingly, we observe\nthe trained visual encoder performs like a summarizer, selectively ignoring\nless important words such as prepositions and conjunctions. This approach\ndelivers remarkable results, outperforming traditional text encoder-based\nmethods by 5.7% on average over benchmarks like TriviaQA, NQ, PopQA, TREF,\nSST2, and SST5, setting a new standard for token efficiency in LLMs."
                },
                "authors": [
                    {
                        "name": "Ling Xing"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Jinhui Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jinhui Tang"
                },
                "author": "Jinhui Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12496v2",
                "updated": "2025-02-04T11:39:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    39,
                    49,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-17T02:56:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    56,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training"
                },
                "summary": "Vision Mamba (e.g., Vim) has successfully been integrated into computer\nvision, and token reduction has yielded promising outcomes in Vision\nTransformers (ViTs). However, token reduction performs less effectively on\nVision Mamba compared to ViTs. Pruning informative tokens in Mamba leads to a\nhigh loss of key knowledge and bad performance. This makes it not a good\nsolution for enhancing efficiency in Mamba. Token merging, which preserves more\ntoken information than pruning, has demonstrated commendable performance in\nViTs. Nevertheless, vanilla merging performance decreases as the reduction\nratio increases either, failing to maintain the key knowledge in Mamba.\nRe-training the token-reduced model enhances the performance of Mamba, by\neffectively rebuilding the key knowledge. Empirically, pruned Vims only drop up\nto 0.9% accuracy on ImageNet-1K, recovered by our proposed framework R-MeeTo in\nour main evaluation. We show how simple and effective the fast recovery can be\nachieved at minute-level, in particular, a 35.9% accuracy spike over 3 epochs\nof training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17\nminutes, and Vim-S only drop 1.3% with 1.2x (up to 1.5x) speed up in inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Mamba (e.g., Vim) has successfully been integrated into computer\nvision, and token reduction has yielded promising outcomes in Vision\nTransformers (ViTs). However, token reduction performs less effectively on\nVision Mamba compared to ViTs. Pruning informative tokens in Mamba leads to a\nhigh loss of key knowledge and bad performance. This makes it not a good\nsolution for enhancing efficiency in Mamba. Token merging, which preserves more\ntoken information than pruning, has demonstrated commendable performance in\nViTs. Nevertheless, vanilla merging performance decreases as the reduction\nratio increases either, failing to maintain the key knowledge in Mamba.\nRe-training the token-reduced model enhances the performance of Mamba, by\neffectively rebuilding the key knowledge. Empirically, pruned Vims only drop up\nto 0.9% accuracy on ImageNet-1K, recovered by our proposed framework R-MeeTo in\nour main evaluation. We show how simple and effective the fast recovery can be\nachieved at minute-level, in particular, a 35.9% accuracy spike over 3 epochs\nof training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17\nminutes, and Vim-S only drop 1.3% with 1.2x (up to 1.5x) speed up in inference."
                },
                "authors": [
                    {
                        "name": "Mingjia Shi"
                    },
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Ruiji Yu"
                    },
                    {
                        "name": "Zekai Li"
                    },
                    {
                        "name": "Zhiyuan Liang"
                    },
                    {
                        "name": "Xuanlei Zhao"
                    },
                    {
                        "name": "Xiaojiang Peng"
                    },
                    {
                        "name": "Shanmukha Ramakrishna Vedantam"
                    },
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02238v1",
                "updated": "2025-02-04T11:27:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    27,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T11:27:31Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    27,
                    31,
                    1,
                    35,
                    0
                ],
                "title": "Using ChatGPT to refine draft conceptual schemata in supply-driven\n  design of multidimensional cubes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using ChatGPT to refine draft conceptual schemata in supply-driven\n  design of multidimensional cubes"
                },
                "summary": "Refinement is a critical step in supply-driven conceptual design of\nmultidimensional cubes because it can hardly be automated. In fact, it includes\nsteps such as the labeling of attributes as descriptive and the removal of\nuninteresting attributes, thus relying on the end-users' requirements on the\none hand, and on the semantics of measures, dimensions, and attributes on the\nother. As a consequence, it is normally carried out manually by designers in\nclose collaboration with end-users. The goal of this work is to check whether\nLLMs can act as facilitators for the refinement task, so as to let it be\ncarried out entirely -- or mostly -- by end-users. The Dimensional Fact Model\nis the target formalism for our study; as a representative LLM, we use\nChatGPT's model GPT-4o. To achieve our goal, we formulate three research\nquestions aimed at (i) understanding the basic competences of ChatGPT in\nmultidimensional modeling; (ii) understanding the basic competences of ChatGPT\nin refinement; and (iii) investigating if the latter can be improved via prompt\nengineering. The results of our experiments show that, indeed, a careful prompt\nengineering can significantly improve the accuracy of refinement, and that the\nresidual errors can quickly be fixed via one additional prompt. However, we\nconclude that, at present, some involvement of designers in refinement is still\nnecessary to ensure the validity of the refined schemata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refinement is a critical step in supply-driven conceptual design of\nmultidimensional cubes because it can hardly be automated. In fact, it includes\nsteps such as the labeling of attributes as descriptive and the removal of\nuninteresting attributes, thus relying on the end-users' requirements on the\none hand, and on the semantics of measures, dimensions, and attributes on the\nother. As a consequence, it is normally carried out manually by designers in\nclose collaboration with end-users. The goal of this work is to check whether\nLLMs can act as facilitators for the refinement task, so as to let it be\ncarried out entirely -- or mostly -- by end-users. The Dimensional Fact Model\nis the target formalism for our study; as a representative LLM, we use\nChatGPT's model GPT-4o. To achieve our goal, we formulate three research\nquestions aimed at (i) understanding the basic competences of ChatGPT in\nmultidimensional modeling; (ii) understanding the basic competences of ChatGPT\nin refinement; and (iii) investigating if the latter can be improved via prompt\nengineering. The results of our experiments show that, indeed, a careful prompt\nengineering can significantly improve the accuracy of refinement, and that the\nresidual errors can quickly be fixed via one additional prompt. However, we\nconclude that, at present, some involvement of designers in refinement is still\nnecessary to ensure the validity of the refined schemata."
                },
                "authors": [
                    {
                        "name": "Stefano Rizzi"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Rizzi"
                },
                "author": "Stefano Rizzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04081v2",
                "updated": "2025-02-04T10:54:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    54,
                    7,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-05T08:27:53Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    27,
                    53,
                    5,
                    279,
                    0
                ],
                "title": "$ε$-VAE: Denoising as Visual Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$ε$-VAE: Denoising as Visual Decoding"
                },
                "summary": "In generative modeling, tokenization simplifies complex data into compact,\nstructured representations, creating a more efficient, learnable space. For\nhigh-dimensional visual data, it reduces redundancy and emphasizes key features\nfor high-quality generation. Current visual tokenization methods rely on a\ntraditional autoencoder framework, where the encoder compresses data into\nlatent representations, and the decoder reconstructs the original input. In\nthis work, we offer a new perspective by proposing denoising as decoding,\nshifting from single-step reconstruction to iterative refinement. Specifically,\nwe replace the decoder with a diffusion process that iteratively refines noise\nto recover the original image, guided by the latents provided by the encoder.\nWe evaluate our approach by assessing both reconstruction (rFID) and generation\nquality (FID), comparing it to state-of-the-art autoencoding approaches. By\nadopting iterative reconstruction through diffusion, our autoencoder, namely\n$\\epsilon$-VAE, achieves high reconstruction quality, which in turn enhances\ndownstream generation quality by 22% and provides 2.3$\\times$ inference\nspeedup. We hope this work offers new insights into integrating iterative\ngeneration and autoencoding for improved compression and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In generative modeling, tokenization simplifies complex data into compact,\nstructured representations, creating a more efficient, learnable space. For\nhigh-dimensional visual data, it reduces redundancy and emphasizes key features\nfor high-quality generation. Current visual tokenization methods rely on a\ntraditional autoencoder framework, where the encoder compresses data into\nlatent representations, and the decoder reconstructs the original input. In\nthis work, we offer a new perspective by proposing denoising as decoding,\nshifting from single-step reconstruction to iterative refinement. Specifically,\nwe replace the decoder with a diffusion process that iteratively refines noise\nto recover the original image, guided by the latents provided by the encoder.\nWe evaluate our approach by assessing both reconstruction (rFID) and generation\nquality (FID), comparing it to state-of-the-art autoencoding approaches. By\nadopting iterative reconstruction through diffusion, our autoencoder, namely\n$\\epsilon$-VAE, achieves high reconstruction quality, which in turn enhances\ndownstream generation quality by 22% and provides 2.3$\\times$ inference\nspeedup. We hope this work offers new insights into integrating iterative\ngeneration and autoencoding for improved compression and generation."
                },
                "authors": [
                    {
                        "name": "Long Zhao"
                    },
                    {
                        "name": "Sanghyun Woo"
                    },
                    {
                        "name": "Ziyu Wan"
                    },
                    {
                        "name": "Yandong Li"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Boqing Gong"
                    },
                    {
                        "name": "Hartwig Adam"
                    },
                    {
                        "name": "Xuhui Jia"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "Preprint. v2: added comparisons to SD-VAE and more visual results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11517v2",
                "updated": "2025-02-04T10:52:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    52,
                    2,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T07:51:09Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    51,
                    9,
                    0,
                    351,
                    0
                ],
                "title": "DART: An AIGT Detector using AMR of Rephrased Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: An AIGT Detector using AMR of Rephrased Text"
                },
                "summary": "As large language models (LLMs) generate more human-like texts, concerns\nabout the side effects of AI-generated texts (AIGT) have grown. So, researchers\nhave developed methods for detecting AIGT. However, two challenges remain.\nFirst, the performance of detecting black-box LLMs is low because existing\nmodels focus on probabilistic features. Second, most AIGT detectors have been\ntested on a single-candidate setting, which assumes that we know the origin of\nan AIGT and which may deviate from the real-world scenario. To resolve these\nchallenges, we propose DART, which consists of four steps: rephrasing, semantic\nparsing, scoring, and multiclass classification. We conducted three experiments\nto test the performance of DART. The experimental result shows that DART can\ndiscriminate multiple black-box LLMs without probabilistic features and the\norigin of AIGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) generate more human-like texts, concerns\nabout the side effects of AI-generated texts (AIGT) have grown. So, researchers\nhave developed methods for detecting AIGT. However, two challenges remain.\nFirst, the performance of detecting black-box LLMs is low because existing\nmodels focus on probabilistic features. Second, most AIGT detectors have been\ntested on a single-candidate setting, which assumes that we know the origin of\nan AIGT and which may deviate from the real-world scenario. To resolve these\nchallenges, we propose DART, which consists of four steps: rephrasing, semantic\nparsing, scoring, and multiclass classification. We conducted three experiments\nto test the performance of DART. The experimental result shows that DART can\ndiscriminate multiple black-box LLMs without probabilistic features and the\norigin of AIGT."
                },
                "authors": [
                    {
                        "name": "Hyeonchu Park"
                    },
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Presented in NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02215v1",
                "updated": "2025-02-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    51,
                    20,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    51,
                    20,
                    1,
                    35,
                    0
                ],
                "title": "InterLCM: Low-Quality Images as Intermediate States of Latent\n  Consistency Models for Effective Blind Face Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InterLCM: Low-Quality Images as Intermediate States of Latent\n  Consistency Models for Effective Blind Face Restoration"
                },
                "summary": "Diffusion priors have been used for blind face restoration (BFR) by\nfine-tuning diffusion models (DMs) on restoration datasets to recover\nlow-quality images. However, the naive application of DMs presents several key\nlimitations. (i) The diffusion prior has inferior semantic consistency (e.g.,\nID, structure and color.), increasing the difficulty of optimizing the BFR\nmodel; (ii) reliance on hundreds of denoising iterations, preventing the\neffective cooperation with perceptual losses, which is crucial for faithful\nrestoration. Observing that the latent consistency model (LCM) learns\nconsistency noise-to-data mappings on the ODE-trajectory and therefore shows\nmore semantic consistency in the subject identity, structural information and\ncolor preservation, we propose InterLCM to leverage the LCM for its superior\nsemantic consistency and efficiency to counter the above issues. Treating\nlow-quality images as the intermediate state of LCM, InterLCM achieves a\nbalance between fidelity and quality by starting from earlier LCM steps. LCM\nalso allows the integration of perceptual loss during training, leading to\nimproved restoration quality, particularly in real-world scenarios. To mitigate\nstructural and semantic uncertainties, InterLCM incorporates a Visual Module to\nextract visual features and a Spatial Encoder to capture spatial details,\nenhancing the fidelity of restored images. Extensive experiments demonstrate\nthat InterLCM outperforms existing approaches in both synthetic and real-world\ndatasets while also achieving faster inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion priors have been used for blind face restoration (BFR) by\nfine-tuning diffusion models (DMs) on restoration datasets to recover\nlow-quality images. However, the naive application of DMs presents several key\nlimitations. (i) The diffusion prior has inferior semantic consistency (e.g.,\nID, structure and color.), increasing the difficulty of optimizing the BFR\nmodel; (ii) reliance on hundreds of denoising iterations, preventing the\neffective cooperation with perceptual losses, which is crucial for faithful\nrestoration. Observing that the latent consistency model (LCM) learns\nconsistency noise-to-data mappings on the ODE-trajectory and therefore shows\nmore semantic consistency in the subject identity, structural information and\ncolor preservation, we propose InterLCM to leverage the LCM for its superior\nsemantic consistency and efficiency to counter the above issues. Treating\nlow-quality images as the intermediate state of LCM, InterLCM achieves a\nbalance between fidelity and quality by starting from earlier LCM steps. LCM\nalso allows the integration of perceptual loss during training, leading to\nimproved restoration quality, particularly in real-world scenarios. To mitigate\nstructural and semantic uncertainties, InterLCM incorporates a Visual Module to\nextract visual features and a Spatial Encoder to capture spatial details,\nenhancing the fidelity of restored images. Extensive experiments demonstrate\nthat InterLCM outperforms existing approaches in both synthetic and real-world\ndatasets while also achieving faster inference speed."
                },
                "authors": [
                    {
                        "name": "Senmao Li"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Joost van de Weijer"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Chun-Le Guo"
                    },
                    {
                        "name": "Shiqi Yang"
                    },
                    {
                        "name": "Yaxing Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Ming Cheng"
                },
                "author": "Ming-Ming Cheng",
                "arxiv_comment": "Accepted at ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02213v1",
                "updated": "2025-02-04T10:50:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    50,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:50:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    50,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "Sampling models for selective inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling models for selective inference"
                },
                "summary": "This paper explores the challenges of constructing suitable inferential\nmodels in scenarios where the parameter of interest is determined in light of\nthe data, such as regression after variable selection. Two compelling arguments\nfor conditioning converge in this context, whose interplay can introduce\nambiguity in the choice of conditioning strategy: the Conditionality Principle,\nfrom classical statistics, and the `condition on selection' paradigm, central\nto selective inference. We discuss two general principles that can be employed\nto resolve this ambiguity in some recurrent contexts. The first one refers to\nthe consideration of how information is processed at the selection stage. The\nsecond one concerns an exploration of ancillarity in the presence of selection.\nWe demonstrate that certain notions of ancillarity are preserved after\nconditioning on the selection event, supporting the application of the\nConditionality Principle. We illustrate these concepts through examples and\nprovide guidance on the adequate inferential approach in some common scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the challenges of constructing suitable inferential\nmodels in scenarios where the parameter of interest is determined in light of\nthe data, such as regression after variable selection. Two compelling arguments\nfor conditioning converge in this context, whose interplay can introduce\nambiguity in the choice of conditioning strategy: the Conditionality Principle,\nfrom classical statistics, and the `condition on selection' paradigm, central\nto selective inference. We discuss two general principles that can be employed\nto resolve this ambiguity in some recurrent contexts. The first one refers to\nthe consideration of how information is processed at the selection stage. The\nsecond one concerns an exploration of ancillarity in the presence of selection.\nWe demonstrate that certain notions of ancillarity are preserved after\nconditioning on the selection event, supporting the application of the\nConditionality Principle. We illustrate these concepts through examples and\nprovide guidance on the adequate inferential approach in some common scenarios."
                },
                "authors": [
                    {
                        "name": "Daniel García Rasines"
                    },
                    {
                        "name": "G. Alastair Young"
                    }
                ],
                "author_detail": {
                    "name": "G. Alastair Young"
                },
                "author": "G. Alastair Young",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02206v1",
                "updated": "2025-02-04T10:42:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    42,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:42:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    42,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "Target-aware Bayesian inference via generalized thermodynamic\n  integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Target-aware Bayesian inference via generalized thermodynamic\n  integration"
                },
                "summary": "In Bayesian inference, we are usually interested in the numerical\napproximation of integrals that are posterior expectations or marginal\nlikelihoods (a.k.a., Bayesian evidence). In this paper, we focus on the\ncomputation of the posterior expectation of a function $f(\\x)$. We consider a\n\\emph{target-aware} scenario where $f(\\x)$ is known in advance and can be\nexploited in order to improve the estimation of the posterior expectation. In\nthis scenario, this task can be reduced to perform several independent marginal\nlikelihood estimation tasks. The idea of using a path of tempered posterior\ndistributions has been widely applied in the literature for the computation of\nmarginal likelihoods. Thermodynamic integration, path sampling and annealing\nimportance sampling are well-known examples of algorithms belonging to this\nfamily of methods. In this work, we introduce a generalized thermodynamic\nintegration (GTI) scheme which is able to perform a target-aware Bayesian\ninference, i.e., GTI can approximate the posterior expectation of a given\nfunction. Several scenarios of application of GTI are discussed and different\nnumerical simulations are provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Bayesian inference, we are usually interested in the numerical\napproximation of integrals that are posterior expectations or marginal\nlikelihoods (a.k.a., Bayesian evidence). In this paper, we focus on the\ncomputation of the posterior expectation of a function $f(\\x)$. We consider a\n\\emph{target-aware} scenario where $f(\\x)$ is known in advance and can be\nexploited in order to improve the estimation of the posterior expectation. In\nthis scenario, this task can be reduced to perform several independent marginal\nlikelihood estimation tasks. The idea of using a path of tempered posterior\ndistributions has been widely applied in the literature for the computation of\nmarginal likelihoods. Thermodynamic integration, path sampling and annealing\nimportance sampling are well-known examples of algorithms belonging to this\nfamily of methods. In this work, we introduce a generalized thermodynamic\nintegration (GTI) scheme which is able to perform a target-aware Bayesian\ninference, i.e., GTI can approximate the posterior expectation of a given\nfunction. Several scenarios of application of GTI are discussed and different\nnumerical simulations are provided."
                },
                "authors": [
                    {
                        "name": "F. Llorente"
                    },
                    {
                        "name": "L. Martino"
                    },
                    {
                        "name": "D. Delgado"
                    }
                ],
                "author_detail": {
                    "name": "D. Delgado"
                },
                "author": "D. Delgado",
                "arxiv_doi": "10.1007/s00180-023-01358-0",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00180-023-01358-0",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.02206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Computational Statistics, Volume 38, Pages 2097-2119, year 2023",
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02205v1",
                "updated": "2025-02-04T10:42:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    42,
                    30,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:42:30Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    42,
                    30,
                    1,
                    35,
                    0
                ],
                "title": "From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for\n  Safe PDE Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for\n  Safe PDE Control"
                },
                "summary": "The application of deep learning for partial differential equation\n(PDE)-constrained control is gaining increasing attention. However, existing\nmethods rarely consider safety requirements crucial in real-world applications.\nTo address this limitation, we propose Safe Diffusion Models for PDE Control\n(SafeDiffCon), which introduce the uncertainty quantile as model uncertainty\nquantification to achieve optimal control under safety constraints through both\npost-training and inference phases. Firstly, our approach post-trains a\npre-trained diffusion model to generate control sequences that better satisfy\nsafety constraints while achieving improved control objectives via a reweighted\ndiffusion loss, which incorporates the uncertainty quantile estimated using\nconformal prediction. Secondly, during inference, the diffusion model\ndynamically adjusts both its generation process and parameters through\niterative guidance and fine-tuning, conditioned on control targets while\nsimultaneously integrating the estimated uncertainty quantile. We evaluate\nSafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible\nfluid, and controlled nuclear fusion problem. Results demonstrate that\nSafeDiffCon is the only method that satisfies all safety constraints, whereas\nother classical and deep learning baselines fail. Furthermore, while adhering\nto safety constraints, SafeDiffCon achieves the best control performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of deep learning for partial differential equation\n(PDE)-constrained control is gaining increasing attention. However, existing\nmethods rarely consider safety requirements crucial in real-world applications.\nTo address this limitation, we propose Safe Diffusion Models for PDE Control\n(SafeDiffCon), which introduce the uncertainty quantile as model uncertainty\nquantification to achieve optimal control under safety constraints through both\npost-training and inference phases. Firstly, our approach post-trains a\npre-trained diffusion model to generate control sequences that better satisfy\nsafety constraints while achieving improved control objectives via a reweighted\ndiffusion loss, which incorporates the uncertainty quantile estimated using\nconformal prediction. Secondly, during inference, the diffusion model\ndynamically adjusts both its generation process and parameters through\niterative guidance and fine-tuning, conditioned on control targets while\nsimultaneously integrating the estimated uncertainty quantile. We evaluate\nSafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible\nfluid, and controlled nuclear fusion problem. Results demonstrate that\nSafeDiffCon is the only method that satisfies all safety constraints, whereas\nother classical and deep learning baselines fail. Furthermore, while adhering\nto safety constraints, SafeDiffCon achieves the best control performance."
                },
                "authors": [
                    {
                        "name": "Peiyan Hu"
                    },
                    {
                        "name": "Xiaowei Qian"
                    },
                    {
                        "name": "Wenhao Deng"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Haodong Feng"
                    },
                    {
                        "name": "Ruiqi Feng"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Long Wei"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Zhi-Ming Ma"
                    },
                    {
                        "name": "Tailin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tailin Wu"
                },
                "author": "Tailin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02201v1",
                "updated": "2025-02-04T10:27:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    27,
                    40,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:27:40Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    27,
                    40,
                    1,
                    35,
                    0
                ],
                "title": "Can You Move These Over There? An LLM-based VR Mover for Supporting\n  Object Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Move These Over There? An LLM-based VR Mover for Supporting\n  Object Manipulation"
                },
                "summary": "In our daily lives, we can naturally convey instructions for the spatial\nmanipulation of objects using words and gestures. Transposing this form of\ninteraction into virtual reality (VR) object manipulation can be beneficial. We\npropose VR Mover, an LLM-empowered solution that can understand and interpret\nthe user's vocal instruction to support object manipulation. By simply pointing\nand speaking, the LLM can manipulate objects without structured input. Our user\nstudy demonstrates that VR Mover enhances user usability, overall experience\nand performance on multi-object manipulation, while also reducing workload and\narm fatigue. Users prefer the proposed natural interface for broad movements\nand may complementarily switch to gizmos or virtual hands for finer\nadjustments. These findings are believed to contribute to design implications\nfor future LLM-based object manipulation interfaces, highlighting the potential\nfor more intuitive and efficient user interactions in VR environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our daily lives, we can naturally convey instructions for the spatial\nmanipulation of objects using words and gestures. Transposing this form of\ninteraction into virtual reality (VR) object manipulation can be beneficial. We\npropose VR Mover, an LLM-empowered solution that can understand and interpret\nthe user's vocal instruction to support object manipulation. By simply pointing\nand speaking, the LLM can manipulate objects without structured input. Our user\nstudy demonstrates that VR Mover enhances user usability, overall experience\nand performance on multi-object manipulation, while also reducing workload and\narm fatigue. Users prefer the proposed natural interface for broad movements\nand may complementarily switch to gizmos or virtual hands for finer\nadjustments. These findings are believed to contribute to design implications\nfor future LLM-based object manipulation interfaces, highlighting the potential\nfor more intuitive and efficient user interactions in VR environments."
                },
                "authors": [
                    {
                        "name": "Xiangzhi Eric Wang"
                    },
                    {
                        "name": "Zackary P. T. Sin"
                    },
                    {
                        "name": "Ye Jia"
                    },
                    {
                        "name": "Daniel Archer"
                    },
                    {
                        "name": "Wynonna H. Y. Fong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Chen Li"
                    }
                ],
                "author_detail": {
                    "name": "Chen Li"
                },
                "author": "Chen Li",
                "arxiv_comment": "64 pages (30 in main text), 22 figures (19 in main text)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02199v1",
                "updated": "2025-02-04T10:23:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    23,
                    11,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:23:11Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    23,
                    11,
                    1,
                    35,
                    0
                ],
                "title": "When Dimensionality Hurts: The Role of LLM Embedding Compression for\n  Noisy Regression Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Dimensionality Hurts: The Role of LLM Embedding Compression for\n  Noisy Regression Tasks"
                },
                "summary": "Large language models (LLMs) have shown remarkable success in language\nmodelling due to scaling laws found in model size and the hidden dimension of\nthe model's text representation. Yet, we demonstrate that compressed\nrepresentations of text can yield better performance in LLM-based regression\ntasks. In this paper, we compare the relative performance of embedding\ncompression in three different signal-to-noise contexts: financial return\nprediction, writing quality assessment and review scoring. Our results show\nthat compressing embeddings, in a minimally supervised manner using an\nautoencoder's hidden representation, can mitigate overfitting and improve\nperformance on noisy tasks, such as financial return prediction; but that\ncompression reduces performance on tasks that have high causal dependencies\nbetween the input and target data. Our results suggest that the success of\ninterpretable compressed representations such as sentiment may be due to a\nregularising effect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable success in language\nmodelling due to scaling laws found in model size and the hidden dimension of\nthe model's text representation. Yet, we demonstrate that compressed\nrepresentations of text can yield better performance in LLM-based regression\ntasks. In this paper, we compare the relative performance of embedding\ncompression in three different signal-to-noise contexts: financial return\nprediction, writing quality assessment and review scoring. Our results show\nthat compressing embeddings, in a minimally supervised manner using an\nautoencoder's hidden representation, can mitigate overfitting and improve\nperformance on noisy tasks, such as financial return prediction; but that\ncompression reduces performance on tasks that have high causal dependencies\nbetween the input and target data. Our results suggest that the success of\ninterpretable compressed representations such as sentiment may be due to a\nregularising effect."
                },
                "authors": [
                    {
                        "name": "Felix Drinkall"
                    },
                    {
                        "name": "Janet B. Pierrehumbert"
                    },
                    {
                        "name": "Stefan Zohren"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Zohren"
                },
                "author": "Stefan Zohren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02191v1",
                "updated": "2025-02-04T10:13:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    13,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:13:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    13,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "Large language models in climate and sustainability policy: limits and\n  opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models in climate and sustainability policy: limits and\n  opportunities"
                },
                "summary": "As multiple crises threaten the sustainability of our societies and pose at\nrisk the planetary boundaries, complex challenges require timely, updated, and\nusable information. Natural-language processing (NLP) tools enhance and expand\ndata collection and processing and knowledge utilization capabilities to\nsupport the definition of an inclusive, sustainable future. In this work, we\napply different NLP techniques, tools and approaches to climate and\nsustainability documents to derive policy-relevant and actionable measures. We\nfocus on general and domain-specific large language models (LLMs) using a\ncombination of static and prompt-based methods. We find that the use of LLMs is\nsuccessful at processing, classifying and summarizing heterogeneous text-based\ndata. However, we also encounter challenges related to human intervention\nacross different workflow stages and knowledge utilization for policy\nprocesses. Our work presents a critical but empirically grounded application of\nLLMs to complex policy problems and suggests avenues to further expand\nArtificial Intelligence-powered computational social sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multiple crises threaten the sustainability of our societies and pose at\nrisk the planetary boundaries, complex challenges require timely, updated, and\nusable information. Natural-language processing (NLP) tools enhance and expand\ndata collection and processing and knowledge utilization capabilities to\nsupport the definition of an inclusive, sustainable future. In this work, we\napply different NLP techniques, tools and approaches to climate and\nsustainability documents to derive policy-relevant and actionable measures. We\nfocus on general and domain-specific large language models (LLMs) using a\ncombination of static and prompt-based methods. We find that the use of LLMs is\nsuccessful at processing, classifying and summarizing heterogeneous text-based\ndata. However, we also encounter challenges related to human intervention\nacross different workflow stages and knowledge utilization for policy\nprocesses. Our work presents a critical but empirically grounded application of\nLLMs to complex policy problems and suggests avenues to further expand\nArtificial Intelligence-powered computational social sciences."
                },
                "authors": [
                    {
                        "name": "Francesca Larosa"
                    },
                    {
                        "name": "Sergio Hoyas"
                    },
                    {
                        "name": "H. Alberto Conejero"
                    },
                    {
                        "name": "Javier Garcia-Martinez"
                    },
                    {
                        "name": "Francesco Fuso Nerini"
                    },
                    {
                        "name": "Ricardo Vinuesa"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Vinuesa"
                },
                "author": "Ricardo Vinuesa",
                "arxiv_comment": "15 pages; 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18712v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18712v3",
                "updated": "2025-02-04T09:56:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    56,
                    50,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-30T19:15:41Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    19,
                    15,
                    41,
                    3,
                    30,
                    0
                ],
                "title": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps"
                },
                "summary": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts."
                },
                "authors": [
                    {
                        "name": "Devansh Bhardwaj"
                    },
                    {
                        "name": "Naman Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Naman Mishra"
                },
                "author": "Naman Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18712v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18712v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16165v2",
                "updated": "2025-02-04T09:49:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    49,
                    41,
                    1,
                    35,
                    0
                ],
                "published": "2024-09-24T15:06:01Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    6,
                    1,
                    1,
                    268,
                    0
                ],
                "title": "Interactive Tools Substantially Assist LM Agents in Finding Security\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Tools Substantially Assist LM Agents in Finding Security\n  Vulnerabilities"
                },
                "summary": "Although language model (LM) agents have demonstrated increased performance\nin multiple domains, including coding and web-browsing, their success in\ncybersecurity has been limited. We present EnIGMA, an LM agent for autonomously\nsolving Capture The Flag (CTF) challenges. We introduce new tools and\ninterfaces to improve the agent's ability to find and exploit security\nvulnerabilities, focusing on interactive terminal programs. These novel\nInteractive Agent Tools enable LM agents, for the first time, to run\ninteractive utilities, such as a debugger and a server connection tool, which\nare essential for solving these challenges. Empirical analysis on 390 CTF\nchallenges across four benchmarks demonstrate that these new tools and\ninterfaces substantially improve our agent's performance, achieving\nstate-of-the-art results on NYU CTF, Intercode-CTF, and CyBench. Finally, we\nanalyze data leakage, developing new methods to quantify it and identifying a\nnew phenomenon we term soliloquizing, where the model self-generates\nhallucinated observations without interacting with the environment. Our code\nand development dataset are available at\nhttps://github.com/SWE-agent/SWE-agent/tree/v0.7 and\nhttps://github.com/NYU-LLM-CTF/NYU_CTF_Bench/tree/main/development\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although language model (LM) agents have demonstrated increased performance\nin multiple domains, including coding and web-browsing, their success in\ncybersecurity has been limited. We present EnIGMA, an LM agent for autonomously\nsolving Capture The Flag (CTF) challenges. We introduce new tools and\ninterfaces to improve the agent's ability to find and exploit security\nvulnerabilities, focusing on interactive terminal programs. These novel\nInteractive Agent Tools enable LM agents, for the first time, to run\ninteractive utilities, such as a debugger and a server connection tool, which\nare essential for solving these challenges. Empirical analysis on 390 CTF\nchallenges across four benchmarks demonstrate that these new tools and\ninterfaces substantially improve our agent's performance, achieving\nstate-of-the-art results on NYU CTF, Intercode-CTF, and CyBench. Finally, we\nanalyze data leakage, developing new methods to quantify it and identifying a\nnew phenomenon we term soliloquizing, where the model self-generates\nhallucinated observations without interacting with the environment. Our code\nand development dataset are available at\nhttps://github.com/SWE-agent/SWE-agent/tree/v0.7 and\nhttps://github.com/NYU-LLM-CTF/NYU_CTF_Bench/tree/main/development\nrespectively."
                },
                "authors": [
                    {
                        "name": "Talor Abramovich"
                    },
                    {
                        "name": "Meet Udeshi"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Kilian Lieret"
                    },
                    {
                        "name": "Haoran Xi"
                    },
                    {
                        "name": "Kimberly Milner"
                    },
                    {
                        "name": "Sofija Jancheska"
                    },
                    {
                        "name": "John Yang"
                    },
                    {
                        "name": "Carlos E. Jimenez"
                    },
                    {
                        "name": "Farshad Khorrami"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Brendan Dolan-Gavitt"
                    },
                    {
                        "name": "Muhammad Shafique"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Ofir Press"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Press"
                },
                "author": "Ofir Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02176v1",
                "updated": "2025-02-04T09:48:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    16,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:16Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    16,
                    1,
                    35,
                    0
                ],
                "title": "MAISTEP -- a new grid-based machine learning tool for inferring stellar\n  parameters I. Ages of giant-planet host stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAISTEP -- a new grid-based machine learning tool for inferring stellar\n  parameters I. Ages of giant-planet host stars"
                },
                "summary": "Our understanding of exoplanet demographics partly depends on their\ncorresponding host star parameters. With the majority of exoplanet-host stars\nhaving only atmospheric constraints available, robust inference of their\nparameters is susceptible to the approach used. The goal of this work is to\ndevelop a grid-based machine learning tool capable of determining the stellar\nradius, mass, and age using only atmospheric constraints and to analyse the age\ndistribution of stars hosting giant planets. Our machine learning approach\ninvolves combining four tree-based machine learning algorithms (Random Forest,\nExtra Trees, Extreme Gradient Boosting, and CatBoost) trained on a grid of\nstellar models to infer stellar radius, mass, and age using Teff, [Fe/H], and\nluminosities. We perform a detailed statistical analysis to compare the\ninferences of our tool with those based on seismic data from the APOKASC and\nLEGACY samples. Finally, we apply our tool to determine the ages of stars\nhosting giant planets. Comparing the stellar parameter inferences from our\nmachine learning tool with those from the APOKASC and LEGACY, we find a bias\n(and a scatter) of -0.5\\% (5\\%) and -0.2\\% (2\\%) in radius, 6\\% (5\\%) per cent\nand -2\\% (3\\%) in mass, and -9\\% (16\\%) and 7\\% (23\\%) in age, respectively.\nTherefore, our machine learning predictions are commensurate with seismic\ninferences. When applying our model to a sample of stars hosting Jupiter-mass\nplanets, we find the average age estimates for the hosts of Hot Jupiters, Warm\nJupiters, and Cold Jupiters to be 1.98, 2.98, and 3.51 Gyr, respectively. These\nstatistical ages of the host stars confirm previous predictions - based on\nstellar model ages for a relatively small number of hosts, as well as on the\naverage age-velocity dispersion relation - that stars hosting Hot Jupiters are\nstatistically younger than those hosting Warm and Cold Jupiters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our understanding of exoplanet demographics partly depends on their\ncorresponding host star parameters. With the majority of exoplanet-host stars\nhaving only atmospheric constraints available, robust inference of their\nparameters is susceptible to the approach used. The goal of this work is to\ndevelop a grid-based machine learning tool capable of determining the stellar\nradius, mass, and age using only atmospheric constraints and to analyse the age\ndistribution of stars hosting giant planets. Our machine learning approach\ninvolves combining four tree-based machine learning algorithms (Random Forest,\nExtra Trees, Extreme Gradient Boosting, and CatBoost) trained on a grid of\nstellar models to infer stellar radius, mass, and age using Teff, [Fe/H], and\nluminosities. We perform a detailed statistical analysis to compare the\ninferences of our tool with those based on seismic data from the APOKASC and\nLEGACY samples. Finally, we apply our tool to determine the ages of stars\nhosting giant planets. Comparing the stellar parameter inferences from our\nmachine learning tool with those from the APOKASC and LEGACY, we find a bias\n(and a scatter) of -0.5\\% (5\\%) and -0.2\\% (2\\%) in radius, 6\\% (5\\%) per cent\nand -2\\% (3\\%) in mass, and -9\\% (16\\%) and 7\\% (23\\%) in age, respectively.\nTherefore, our machine learning predictions are commensurate with seismic\ninferences. When applying our model to a sample of stars hosting Jupiter-mass\nplanets, we find the average age estimates for the hosts of Hot Jupiters, Warm\nJupiters, and Cold Jupiters to be 1.98, 2.98, and 3.51 Gyr, respectively. These\nstatistical ages of the host stars confirm previous predictions - based on\nstellar model ages for a relatively small number of hosts, as well as on the\naverage age-velocity dispersion relation - that stars hosting Hot Jupiters are\nstatistically younger than those hosting Warm and Cold Jupiters."
                },
                "authors": [
                    {
                        "name": "Juma Kamulali"
                    },
                    {
                        "name": "Benard Nsamba"
                    },
                    {
                        "name": "Vardan Adibekyan"
                    },
                    {
                        "name": "Achim Weiss"
                    },
                    {
                        "name": "Tiago L. Campante"
                    },
                    {
                        "name": "Nuno C. Santos"
                    }
                ],
                "author_detail": {
                    "name": "Nuno C. Santos"
                },
                "author": "Nuno C. Santos",
                "arxiv_comment": "12 pages, 10 figures, Accepted for publication in Astronomy &\n  Astrophysics Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02697v2",
                "updated": "2025-02-04T09:47:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    47,
                    30,
                    1,
                    35,
                    0
                ],
                "published": "2024-09-04T13:33:38Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    13,
                    33,
                    38,
                    2,
                    248,
                    0
                ],
                "title": "Decision Transformer for Enhancing Neural Local Search on the Job Shop\n  Scheduling Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision Transformer for Enhancing Neural Local Search on the Job Shop\n  Scheduling Problem"
                },
                "summary": "The job shop scheduling problem (JSSP) and its solution algorithms have been\nof enduring interest in both academia and industry for decades. In recent\nyears, machine learning (ML) is playing an increasingly important role in\nadvancing existing and building new heuristic solutions for the JSSP, aiming to\nfind better solutions in shorter computation times. In this paper we build on\ntop of a state-of-the-art deep reinforcement learning (DRL) agent, called\nNeural Local Search (NLS), which can efficiently and effectively control a\nlarge local neighborhood search on the JSSP. In particular, we develop a method\nfor training the decision transformer (DT) algorithm on search trajectories\ntaken by a trained NLS agent to further improve upon the learned\ndecision-making sequences. Our experiments show that the DT successfully learns\nlocal search strategies that are different and, in many cases, more effective\nthan those of the NLS agent itself. In terms of the tradeoff between solution\nquality and acceptable computational time needed for the search, the DT is\nparticularly superior in application scenarios where longer computational times\nare acceptable. In this case, it makes up for the longer inference times\nrequired per search step, which are caused by the larger neural network\narchitecture, through better quality decisions per step. Thereby, the DT\nachieves state-of-the-art results for solving the JSSP with ML-enhanced search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The job shop scheduling problem (JSSP) and its solution algorithms have been\nof enduring interest in both academia and industry for decades. In recent\nyears, machine learning (ML) is playing an increasingly important role in\nadvancing existing and building new heuristic solutions for the JSSP, aiming to\nfind better solutions in shorter computation times. In this paper we build on\ntop of a state-of-the-art deep reinforcement learning (DRL) agent, called\nNeural Local Search (NLS), which can efficiently and effectively control a\nlarge local neighborhood search on the JSSP. In particular, we develop a method\nfor training the decision transformer (DT) algorithm on search trajectories\ntaken by a trained NLS agent to further improve upon the learned\ndecision-making sequences. Our experiments show that the DT successfully learns\nlocal search strategies that are different and, in many cases, more effective\nthan those of the NLS agent itself. In terms of the tradeoff between solution\nquality and acceptable computational time needed for the search, the DT is\nparticularly superior in application scenarios where longer computational times\nare acceptable. In this case, it makes up for the longer inference times\nrequired per search step, which are caused by the larger neural network\narchitecture, through better quality decisions per step. Thereby, the DT\nachieves state-of-the-art results for solving the JSSP with ML-enhanced search."
                },
                "authors": [
                    {
                        "name": "Constantin Waubert de Puiseau"
                    },
                    {
                        "name": "Fabian Wolz"
                    },
                    {
                        "name": "Merlin Montag"
                    },
                    {
                        "name": "Jannik Peters"
                    },
                    {
                        "name": "Hasan Tercan"
                    },
                    {
                        "name": "Tobias Meisen"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Meisen"
                },
                "author": "Tobias Meisen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02172v1",
                "updated": "2025-02-04T09:45:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    45,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:45:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    45,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "EditIQ: Automated Cinematic Editing of Static Wide-Angle Videos via\n  Dialogue Interpretation and Saliency Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EditIQ: Automated Cinematic Editing of Static Wide-Angle Videos via\n  Dialogue Interpretation and Saliency Cues"
                },
                "summary": "We present EditIQ, a completely automated framework for cinematically editing\nscenes captured via a stationary, large field-of-view and high-resolution\ncamera. From the static camera feed, EditIQ initially generates multiple\nvirtual feeds, emulating a team of cameramen. These virtual camera shots termed\nrushes are subsequently assembled using an automated editing algorithm, whose\nobjective is to present the viewer with the most vivid scene content. To\nunderstand key scene elements and guide the editing process, we employ a\ntwo-pronged approach: (1) a large language model (LLM)-based dialogue\nunderstanding module to analyze conversational flow, coupled with (2) visual\nsaliency prediction to identify meaningful scene elements and camera shots\ntherefrom. We then formulate cinematic video editing as an energy minimization\nproblem over shot selection, where cinematic constraints determine shot\nchoices, transitions, and continuity. EditIQ synthesizes an aesthetically and\nvisually compelling representation of the original narrative while maintaining\ncinematic coherence and a smooth viewing experience. Efficacy of EditIQ against\ncompeting baselines is demonstrated via a psychophysical study involving twenty\nparticipants on the BBC Old School dataset plus eleven theatre performance\nvideos. Video samples from EditIQ can be found at\nhttps://editiq-ave.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present EditIQ, a completely automated framework for cinematically editing\nscenes captured via a stationary, large field-of-view and high-resolution\ncamera. From the static camera feed, EditIQ initially generates multiple\nvirtual feeds, emulating a team of cameramen. These virtual camera shots termed\nrushes are subsequently assembled using an automated editing algorithm, whose\nobjective is to present the viewer with the most vivid scene content. To\nunderstand key scene elements and guide the editing process, we employ a\ntwo-pronged approach: (1) a large language model (LLM)-based dialogue\nunderstanding module to analyze conversational flow, coupled with (2) visual\nsaliency prediction to identify meaningful scene elements and camera shots\ntherefrom. We then formulate cinematic video editing as an energy minimization\nproblem over shot selection, where cinematic constraints determine shot\nchoices, transitions, and continuity. EditIQ synthesizes an aesthetically and\nvisually compelling representation of the original narrative while maintaining\ncinematic coherence and a smooth viewing experience. Efficacy of EditIQ against\ncompeting baselines is demonstrated via a psychophysical study involving twenty\nparticipants on the BBC Old School dataset plus eleven theatre performance\nvideos. Video samples from EditIQ can be found at\nhttps://editiq-ave.github.io/."
                },
                "authors": [
                    {
                        "name": "Rohit Girmaji"
                    },
                    {
                        "name": "Bhav Beri"
                    },
                    {
                        "name": "Ramanathan Subramanian"
                    },
                    {
                        "name": "Vineet Gandhi"
                    }
                ],
                "author_detail": {
                    "name": "Vineet Gandhi"
                },
                "author": "Vineet Gandhi",
                "arxiv_doi": "10.1145/3708359.3712113",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712113",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.02172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at 30th International Conference on Intelligent User\n  Interfaces (IUI 25)",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.1; G.2; I.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02153v1",
                "updated": "2025-02-04T09:31:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    31,
                    54,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:31:54Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    31,
                    54,
                    1,
                    35,
                    0
                ],
                "title": "Vulnerability Mitigation for Safety-Aligned Language Models via\n  Debiasing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability Mitigation for Safety-Aligned Language Models via\n  Debiasing"
                },
                "summary": "Safety alignment is an essential research topic for real-world AI\napplications. Despite the multifaceted nature of safety and trustworthiness in\nAI, current safety alignment methods often focus on a comprehensive notion of\nsafety. By carefully assessing models from the existing safety-alignment\nmethods, we found that, while they generally improved overall safety\nperformance, they failed to ensure safety in specific categories. Our study\nfirst identified the difficulty of eliminating such vulnerabilities without\nsacrificing the model's helpfulness. We observed that, while smaller KL penalty\nparameters, increased training iterations, and dataset cleansing can enhance\nsafety, they do not necessarily improve the trade-off between safety and\nhelpfulness. We discovered that safety alignment could even induce undesired\neffects and result in a model that prefers generating negative tokens leading\nto rejective responses, regardless of the input context. To address this, we\nintroduced a learning-free method, Token-level Safety-Debiased Inference\n(TSDI), to estimate and correct this bias during the generation process using\nrandomly constructed prompts. Our experiments demonstrated that our method\ncould enhance the model's helpfulness while maintaining safety, thus improving\nthe trade-off Pareto-front.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is an essential research topic for real-world AI\napplications. Despite the multifaceted nature of safety and trustworthiness in\nAI, current safety alignment methods often focus on a comprehensive notion of\nsafety. By carefully assessing models from the existing safety-alignment\nmethods, we found that, while they generally improved overall safety\nperformance, they failed to ensure safety in specific categories. Our study\nfirst identified the difficulty of eliminating such vulnerabilities without\nsacrificing the model's helpfulness. We observed that, while smaller KL penalty\nparameters, increased training iterations, and dataset cleansing can enhance\nsafety, they do not necessarily improve the trade-off between safety and\nhelpfulness. We discovered that safety alignment could even induce undesired\neffects and result in a model that prefers generating negative tokens leading\nto rejective responses, regardless of the input context. To address this, we\nintroduced a learning-free method, Token-level Safety-Debiased Inference\n(TSDI), to estimate and correct this bias during the generation process using\nrandomly constructed prompts. Our experiments demonstrated that our method\ncould enhance the model's helpfulness while maintaining safety, thus improving\nthe trade-off Pareto-front."
                },
                "authors": [
                    {
                        "name": "Thien Q. Tran"
                    },
                    {
                        "name": "Akifumi Wachi"
                    },
                    {
                        "name": "Rei Sato"
                    },
                    {
                        "name": "Takumi Tanabe"
                    },
                    {
                        "name": "Youhei Akimoto"
                    }
                ],
                "author_detail": {
                    "name": "Youhei Akimoto"
                },
                "author": "Youhei Akimoto",
                "arxiv_comment": "37 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17621v2",
                "updated": "2025-02-04T09:24:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    24,
                    30,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-23T07:22:33Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    22,
                    33,
                    2,
                    297,
                    0
                ],
                "title": "Process Supervision-Guided Policy Optimization for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Supervision-Guided Policy Optimization for Code Generation"
                },
                "summary": "Reinforcement learning (RL) with unit test feedback has enhanced large\nlanguage models' (LLMs) code generation, but relies on sparse rewards provided\nonly after complete code evaluation, limiting learning efficiency and\nincremental improvements. When generated code fails all unit tests, no learning\nsignal is received, hindering progress on complex tasks. To address this, we\npropose a Process Reward Model (PRM) that delivers dense, line-level feedback\non code correctness during generation, mimicking human code refinement and\nproviding immediate guidance. We explore various strategies for training PRMs\nand integrating them into the RL framework, finding that using PRMs both as\ndense rewards and for value function initialization significantly boosts\nperformance. Our experimental results also highlight the effectiveness of PRMs\nin enhancing RL-driven code generation, especially for long-horizon scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with unit test feedback has enhanced large\nlanguage models' (LLMs) code generation, but relies on sparse rewards provided\nonly after complete code evaluation, limiting learning efficiency and\nincremental improvements. When generated code fails all unit tests, no learning\nsignal is received, hindering progress on complex tasks. To address this, we\npropose a Process Reward Model (PRM) that delivers dense, line-level feedback\non code correctness during generation, mimicking human code refinement and\nproviding immediate guidance. We explore various strategies for training PRMs\nand integrating them into the RL framework, finding that using PRMs both as\ndense rewards and for value function initialization significantly boosts\nperformance. Our experimental results also highlight the effectiveness of PRMs\nin enhancing RL-driven code generation, especially for long-horizon scenarios."
                },
                "authors": [
                    {
                        "name": "Ning Dai"
                    },
                    {
                        "name": "Zheng Wu"
                    },
                    {
                        "name": "Renjie Zheng"
                    },
                    {
                        "name": "Ziyun Wei"
                    },
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Xing Jin"
                    },
                    {
                        "name": "Guanlin Liu"
                    },
                    {
                        "name": "Chen Dun"
                    },
                    {
                        "name": "Liang Huang"
                    },
                    {
                        "name": "Lin Yan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yan"
                },
                "author": "Lin Yan",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02145v1",
                "updated": "2025-02-04T09:19:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    19,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:19:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    19,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "Risk-Aware Driving Scenario Analysis with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk-Aware Driving Scenario Analysis with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can capture nuanced contextual relationships,\nreasoning, and complex problem-solving. By leveraging their ability to process\nand interpret large-scale information, LLMs have shown potential to address\ndomain-specific challenges, including those in autonomous driving systems. This\npaper proposes a novel framework that leverages LLMs for risk-aware analysis of\ngenerated driving scenarios. We hypothesize that LLMs can effectively evaluate\nwhether driving scenarios generated by autonomous driving testing simulators\nare safety-critical. To validate this hypothesis, we conducted an empirical\nevaluation to assess the effectiveness of LLMs in performing this task. This\nframework will also provide feedback to generate the new safety-critical\nscenario by using adversarial method to modify existing non-critical scenarios\nand test their effectiveness in validating motion planning algorithms. Code and\nscenarios are available at:\nhttps://github.com/yuangao-tum/Riskaware-Scenario-analyse",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can capture nuanced contextual relationships,\nreasoning, and complex problem-solving. By leveraging their ability to process\nand interpret large-scale information, LLMs have shown potential to address\ndomain-specific challenges, including those in autonomous driving systems. This\npaper proposes a novel framework that leverages LLMs for risk-aware analysis of\ngenerated driving scenarios. We hypothesize that LLMs can effectively evaluate\nwhether driving scenarios generated by autonomous driving testing simulators\nare safety-critical. To validate this hypothesis, we conducted an empirical\nevaluation to assess the effectiveness of LLMs in performing this task. This\nframework will also provide feedback to generate the new safety-critical\nscenario by using adversarial method to modify existing non-critical scenarios\nand test their effectiveness in validating motion planning algorithms. Code and\nscenarios are available at:\nhttps://github.com/yuangao-tum/Riskaware-Scenario-analyse"
                },
                "authors": [
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Mattia Piccinini"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "arxiv_comment": "IEEE Intelligent Vehicles Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.02577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02577v1",
                "updated": "2025-02-04T18:53:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    53,
                    42,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:53:42Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    53,
                    42,
                    1,
                    35,
                    0
                ],
                "title": "A comparison of translation performance between DeepL and Supertext",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparison of translation performance between DeepL and Supertext"
                },
                "summary": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext."
                },
                "authors": [
                    {
                        "name": "Alex Flückiger"
                    },
                    {
                        "name": "Chantal Amrhein"
                    },
                    {
                        "name": "Tim Graf"
                    },
                    {
                        "name": "Philippe Schläpfer"
                    },
                    {
                        "name": "Florian Schottmann"
                    },
                    {
                        "name": "Samuel Läubli"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Läubli"
                },
                "author": "Samuel Läubli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14299v3",
                "updated": "2025-02-04T18:52:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    52,
                    39,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-21T16:50:11Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "title": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n  Harnessing AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n  Harnessing AI"
                },
                "summary": "Masala-CHAI is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masala-CHAI is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development."
                },
                "authors": [
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Yuheng He"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Hamed Rahmani"
                    },
                    {
                        "name": "Ramesh Karri"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Karri"
                },
                "author": "Ramesh Karri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02573v1",
                "updated": "2025-02-04T18:47:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    47,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:47:31Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    47,
                    31,
                    1,
                    35,
                    0
                ],
                "title": "Are Language Models Up to Sequential Optimization Problems? From\n  Evaluation to a Hegelian-Inspired Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Language Models Up to Sequential Optimization Problems? From\n  Evaluation to a Hegelian-Inspired Enhancement"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nnumerous fields, presenting an opportunity to revolutionize optimization\nproblem-solving, a crucial, ubiquitous, and complex domain. This paper explores\nthe proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We\nintroduce WorldGen, a dynamic framework for generating unseen SOPs with\ncontrollable complexities, to evaluate LLM performance. Our initial\nobservations reveal that while LLMs perform well on simple SOPs, their\nperformance significantly degrades with increased complexity. Motivated by\nthis, we revisit philosophical hypotheses on reasoning to enhance LLM\nperformance. Inspired by the influential framework of Hegelian Dialectics, we\npropose ACE, demonstrating how the performance of LLMs in SOP contexts can be\nsignificantly improved without any retraining or further fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nnumerous fields, presenting an opportunity to revolutionize optimization\nproblem-solving, a crucial, ubiquitous, and complex domain. This paper explores\nthe proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We\nintroduce WorldGen, a dynamic framework for generating unseen SOPs with\ncontrollable complexities, to evaluate LLM performance. Our initial\nobservations reveal that while LLMs perform well on simple SOPs, their\nperformance significantly degrades with increased complexity. Motivated by\nthis, we revisit philosophical hypotheses on reasoning to enhance LLM\nperformance. Inspired by the influential framework of Hegelian Dialectics, we\npropose ACE, demonstrating how the performance of LLMs in SOP contexts can be\nsignificantly improved without any retraining or further fine-tuning."
                },
                "authors": [
                    {
                        "name": "Soheil Abbasloo"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Abbasloo"
                },
                "author": "Soheil Abbasloo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02542v1",
                "updated": "2025-02-04T18:12:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    12,
                    41,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:12:41Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    12,
                    41,
                    1,
                    35,
                    0
                ],
                "title": "OVERTHINKING: Slowdown Attacks on Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVERTHINKING: Slowdown Attacks on Reasoning LLMs"
                },
                "summary": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 46x slowdown and high transferability of the\nattack across models. To protect applications, we discuss and implement\ndefenses leveraging LLM-based and system design approaches. Finally, we discuss\nsocietal, financial, and energy impacts of OVERTHINK attack which could amplify\nthe costs for third party applications operating reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 46x slowdown and high transferability of the\nattack across models. To protect applications, we discuss and implement\ndefenses leveraging LLM-based and system design approaches. Finally, we discuss\nsocietal, financial, and energy impacts of OVERTHINK attack which could amplify\nthe costs for third party applications operating reasoning models."
                },
                "authors": [
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Jaechul Roh"
                    },
                    {
                        "name": "Ali Naseh"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Mohit Iyyer"
                    },
                    {
                        "name": "Amir Houmansadr"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Bagdasarian"
                },
                "author": "Eugene Bagdasarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02539v1",
                "updated": "2025-02-04T18:06:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    6,
                    4,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:06:04Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    6,
                    4,
                    1,
                    35,
                    0
                ],
                "title": "LLMs for Generation of Architectural Components: An Exploratory\n  Empirical Study in the Serverless World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Generation of Architectural Components: An Exploratory\n  Empirical Study in the Serverless World"
                },
                "summary": "Recently, the exponential growth in capability and pervasiveness of Large\nLanguage Models (LLMs) has led to significant work done in the field of code\ngeneration. However, this generation has been limited to code snippets. Going\none step further, our desideratum is to automatically generate architectural\ncomponents. This would not only speed up development time, but would also\nenable us to eventually completely skip the development phase, moving directly\nfrom design decisions to deployment. To this end, we conduct an exploratory\nstudy on the capability of LLMs to generate architectural components for\nFunctions as a Service (FaaS), commonly known as serverless functions. The\nsmall size of their architectural components make this architectural style\namenable for generation using current LLMs compared to other styles like\nmonoliths and microservices. We perform the study by systematically selecting\nopen source serverless repositories, masking a serverless function and\nutilizing state of the art LLMs provided with varying levels of context\ninformation about the overall system to generate the masked function. We\nevaluate correctness through existing tests present in the repositories and use\nmetrics from the Software Engineering (SE) and Natural Language Processing\n(NLP) domains to evaluate code quality and the degree of similarity between\nhuman and LLM generated code respectively. Along with our findings, we also\npresent a discussion on the path forward for using GenAI in architectural\ncomponent generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the exponential growth in capability and pervasiveness of Large\nLanguage Models (LLMs) has led to significant work done in the field of code\ngeneration. However, this generation has been limited to code snippets. Going\none step further, our desideratum is to automatically generate architectural\ncomponents. This would not only speed up development time, but would also\nenable us to eventually completely skip the development phase, moving directly\nfrom design decisions to deployment. To this end, we conduct an exploratory\nstudy on the capability of LLMs to generate architectural components for\nFunctions as a Service (FaaS), commonly known as serverless functions. The\nsmall size of their architectural components make this architectural style\namenable for generation using current LLMs compared to other styles like\nmonoliths and microservices. We perform the study by systematically selecting\nopen source serverless repositories, masking a serverless function and\nutilizing state of the art LLMs provided with varying levels of context\ninformation about the overall system to generate the masked function. We\nevaluate correctness through existing tests present in the repositories and use\nmetrics from the Software Engineering (SE) and Natural Language Processing\n(NLP) domains to evaluate code quality and the degree of similarity between\nhuman and LLM generated code respectively. Along with our findings, we also\npresent a discussion on the path forward for using GenAI in architectural\ncomponent generation."
                },
                "authors": [
                    {
                        "name": "Shrikara Arun"
                    },
                    {
                        "name": "Meghana Tedla"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Vaidhyanathan"
                },
                "author": "Karthik Vaidhyanathan",
                "arxiv_comment": "Accepted to IEEE International Conference on Software Architecture\n  (ICSA) 2025 Main Track (https://conf.researchr.org/home/icsa-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02534v1",
                "updated": "2025-02-04T17:57:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    57,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:57:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    57,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Adaptive Self-improvement LLM Agentic System for ML Library Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Self-improvement LLM Agentic System for ML Library Development"
                },
                "summary": "ML libraries, often written in architecture-specific programming languages\n(ASPLs) that target domain-specific architectures, are key to efficient ML\nsystems. However, writing these high-performance ML libraries is challenging\nbecause it requires expert knowledge of ML algorithms and the ASPL. Large\nlanguage models (LLMs), on the other hand, have shown general coding\ncapabilities. However, challenges remain when using LLMs for generating ML\nlibraries using ASPLs because 1) this task is complicated even for experienced\nhuman programmers and 2) there are limited code examples because of the\nesoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning\nwith limited data in order to complete this task. To address these challenges,\nwe introduce an adaptive self-improvement agentic system. In order to evaluate\nthe effectiveness of our system, we construct a benchmark of a typical ML\nlibrary and generate ASPL code with both open and closed-source LLMs on this\nbenchmark. Our results show improvements of up to $3.9\\times$ over a baseline\nsingle LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML libraries, often written in architecture-specific programming languages\n(ASPLs) that target domain-specific architectures, are key to efficient ML\nsystems. However, writing these high-performance ML libraries is challenging\nbecause it requires expert knowledge of ML algorithms and the ASPL. Large\nlanguage models (LLMs), on the other hand, have shown general coding\ncapabilities. However, challenges remain when using LLMs for generating ML\nlibraries using ASPLs because 1) this task is complicated even for experienced\nhuman programmers and 2) there are limited code examples because of the\nesoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning\nwith limited data in order to complete this task. To address these challenges,\nwe introduce an adaptive self-improvement agentic system. In order to evaluate\nthe effectiveness of our system, we construct a benchmark of a typical ML\nlibrary and generate ASPL code with both open and closed-source LLMs on this\nbenchmark. Our results show improvements of up to $3.9\\times$ over a baseline\nsingle LLM."
                },
                "authors": [
                    {
                        "name": "Genghan Zhang"
                    },
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Olivia Hsu"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02508v1",
                "updated": "2025-02-04T17:26:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    26,
                    58,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:26:58Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    26,
                    58,
                    1,
                    35,
                    0
                ],
                "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models will be fully open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models will be fully open-sourced."
                },
                "authors": [
                    {
                        "name": "Maohao Shen"
                    },
                    {
                        "name": "Guangtao Zeng"
                    },
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Zhang-Wei Hong"
                    },
                    {
                        "name": "Zhenfang Chen"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Gregory Wornell"
                    },
                    {
                        "name": "Subhro Das"
                    },
                    {
                        "name": "David Cox"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03277v2",
                "updated": "2025-02-04T17:22:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    22,
                    34,
                    1,
                    35,
                    0
                ],
                "published": "2024-09-05T06:41:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    41,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding"
                },
                "summary": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark."
                },
                "authors": [
                    {
                        "name": "Zhengzhuo Xu"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Sinan Du"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02500v1",
                "updated": "2025-02-04T17:15:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    15,
                    36,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:15:36Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    15,
                    36,
                    1,
                    35,
                    0
                ],
                "title": "The Skin Game: Revolutionizing Standards for AI Dermatology Model\n  Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Skin Game: Revolutionizing Standards for AI Dermatology Model\n  Comparison"
                },
                "summary": "Deep Learning approaches in dermatological image classification have shown\npromising results, yet the field faces significant methodological challenges\nthat impede proper evaluation. This paper presents a dual contribution: first,\na systematic analysis of current methodological practices in skin disease\nclassification research, revealing substantial inconsistencies in data\npreparation, augmentation strategies, and performance reporting; second, a\ncomprehensive training and evaluation framework demonstrated through\nexperiments with the DINOv2-Large vision transformer across three benchmark\ndatasets (HAM10000, DermNet, ISIC Atlas). The analysis identifies concerning\npatterns, including pre-split data augmentation and validation-based reporting,\npotentially leading to overestimated metrics, while highlighting the lack of\nunified methodology standards. The experimental results demonstrate DINOv2's\nperformance in skin disease classification, achieving macro-averaged F1-scores\nof 0.85 (HAM10000), 0.71 (DermNet), and 0.84 (ISIC Atlas). Attention map\nanalysis reveals critical patterns in the model's decision-making, showing\nsophisticated feature recognition in typical presentations but significant\nvulnerabilities with atypical cases and composite images. Our findings\nhighlight the need for standardized evaluation protocols and careful\nimplementation strategies in clinical settings. We propose comprehensive\nmethodological recommendations for model development, evaluation, and clinical\ndeployment, emphasizing rigorous data preparation, systematic error analysis,\nand specialized protocols for different image types. To promote\nreproducibility, we provide our implementation code through GitHub. This work\nestablishes a foundation for rigorous evaluation standards in dermatological\nimage classification and provides insights for responsible AI implementation in\nclinical dermatology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning approaches in dermatological image classification have shown\npromising results, yet the field faces significant methodological challenges\nthat impede proper evaluation. This paper presents a dual contribution: first,\na systematic analysis of current methodological practices in skin disease\nclassification research, revealing substantial inconsistencies in data\npreparation, augmentation strategies, and performance reporting; second, a\ncomprehensive training and evaluation framework demonstrated through\nexperiments with the DINOv2-Large vision transformer across three benchmark\ndatasets (HAM10000, DermNet, ISIC Atlas). The analysis identifies concerning\npatterns, including pre-split data augmentation and validation-based reporting,\npotentially leading to overestimated metrics, while highlighting the lack of\nunified methodology standards. The experimental results demonstrate DINOv2's\nperformance in skin disease classification, achieving macro-averaged F1-scores\nof 0.85 (HAM10000), 0.71 (DermNet), and 0.84 (ISIC Atlas). Attention map\nanalysis reveals critical patterns in the model's decision-making, showing\nsophisticated feature recognition in typical presentations but significant\nvulnerabilities with atypical cases and composite images. Our findings\nhighlight the need for standardized evaluation protocols and careful\nimplementation strategies in clinical settings. We propose comprehensive\nmethodological recommendations for model development, evaluation, and clinical\ndeployment, emphasizing rigorous data preparation, systematic error analysis,\nand specialized protocols for different image types. To promote\nreproducibility, we provide our implementation code through GitHub. This work\nestablishes a foundation for rigorous evaluation standards in dermatological\nimage classification and provides insights for responsible AI implementation in\nclinical dermatology."
                },
                "authors": [
                    {
                        "name": "Łukasz Miętkiewicz"
                    },
                    {
                        "name": "Leon Ciechanowski"
                    },
                    {
                        "name": "Dariusz Jemielniak"
                    }
                ],
                "author_detail": {
                    "name": "Dariusz Jemielniak"
                },
                "author": "Dariusz Jemielniak",
                "arxiv_comment": "60 pages, 69 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9; I.5.4; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18626v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18626v3",
                "updated": "2025-02-04T17:09:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-27T12:48:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    48,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt\n  Adversarial Attacks on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt\n  Adversarial Attacks on LLMs"
                },
                "summary": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18626v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18626v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02481v1",
                "updated": "2025-02-04T16:57:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    57,
                    3,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:57:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    57,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study"
                },
                "summary": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo."
                },
                "authors": [
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Pengzhi Gao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "BinWang"
                    }
                ],
                "author_detail": {
                    "name": "BinWang"
                },
                "author": "BinWang",
                "arxiv_comment": "Accept to NAACL2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01042v2",
                "updated": "2025-02-04T16:47:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    47,
                    38,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-03T04:23:33Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    23,
                    33,
                    0,
                    34,
                    0
                ],
                "title": "Internal Activation as the Polar Star for Steering Unsafe LLM Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal Activation as the Polar Star for Steering Unsafe LLM Behavior"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross a wide range of tasks but also pose significant risks due to their\npotential to generate harmful content. Although existing safety mechanisms can\nimprove model safety, they often lead to overly cautious behavior and fail to\nfully utilize LLMs' internal cognitive processes. Drawing inspiration from\ncognitive science, where humans rely on reflective reasoning (System 2\nthinking) to regulate language and behavior, we empirically demonstrate that\nLLMs also possess a similar capacity for internal assessment and regulation,\nwhich can be actively detected.\n  Building on this insight, we introduce SafeSwitch, a framework that\ndynamically regulates unsafe outputs by monitoring and utilizing the model's\ninternal states. Our empirical results show that SafeSwitch reduces harmful\noutputs by over 80% on safety benchmarks while maintaining strong utility.\nCompared to traditional safety alignment methods, SafeSwitch delivers more\ninformative and context-aware refusals, demonstrates resilience to unseen\nqueries, and achieves these benefits while only tuning less than 6% of the\noriginal parameters. These features make SafeSwitch a promising approach for\nimplementing nuanced safety controls in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross a wide range of tasks but also pose significant risks due to their\npotential to generate harmful content. Although existing safety mechanisms can\nimprove model safety, they often lead to overly cautious behavior and fail to\nfully utilize LLMs' internal cognitive processes. Drawing inspiration from\ncognitive science, where humans rely on reflective reasoning (System 2\nthinking) to regulate language and behavior, we empirically demonstrate that\nLLMs also possess a similar capacity for internal assessment and regulation,\nwhich can be actively detected.\n  Building on this insight, we introduce SafeSwitch, a framework that\ndynamically regulates unsafe outputs by monitoring and utilizing the model's\ninternal states. Our empirical results show that SafeSwitch reduces harmful\noutputs by over 80% on safety benchmarks while maintaining strong utility.\nCompared to traditional safety alignment methods, SafeSwitch delivers more\ninformative and context-aware refusals, demonstrates resilience to unseen\nqueries, and achieves these benefits while only tuning less than 6% of the\noriginal parameters. These features make SafeSwitch a promising approach for\nimplementing nuanced safety controls in LLMs."
                },
                "authors": [
                    {
                        "name": "Peixuan Han"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Yuji Zhang"
                    },
                    {
                        "name": "Denghui Zhang"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02458v1",
                "updated": "2025-02-04T16:28:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    28,
                    53,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:28:53Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    28,
                    53,
                    1,
                    35,
                    0
                ],
                "title": "SAISA: Towards Multimodal Large Language Models with Both Training and\n  Inference Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAISA: Towards Multimodal Large Language Models with Both Training and\n  Inference Efficiency"
                },
                "summary": "Multimodal Large Language Models (MLLMs) mainly fall into two architectures,\neach involving a trade-off between training and inference efficiency: embedding\nspace alignment (e.g., LLaVA-1.5) is inefficient during inference, while\ncross-attention space alignment (e.g., Flamingo) is inefficient in training. In\nthis paper, we compare these two architectures and identify the key factors for\nbuilding efficient MLLMs. A primary difference between them lies in how\nattention is applied to visual tokens, particularly in their interactions with\neach other. To investigate whether attention among visual tokens is necessary,\nwe propose a new self-attention mechanism, NAAViT (\\textbf{N}o\n\\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens), which\neliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that\nattention among visual tokens is highly redundant. Based on these insights, we\nintroduce SAISA (\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace\n\\textbf{A}lignment), a novel architecture that enhance both training and\ninference efficiency. SAISA directly aligns visual features with the input\nspaces of NAAViT self-attention blocks, reducing computational overhead in both\nself-attention blocks and feed-forward networks (FFNs). Using the same\nconfiguration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\\% and training\nbudget by 26\\%, while achieving superior performance in terms of accuracy.\nComprehensive ablation studies further validate the effectiveness of SAISA\nacross various LLMs and visual encoders. The code and model will be publicly\navailable at https://github.com/icip-cas/SAISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) mainly fall into two architectures,\neach involving a trade-off between training and inference efficiency: embedding\nspace alignment (e.g., LLaVA-1.5) is inefficient during inference, while\ncross-attention space alignment (e.g., Flamingo) is inefficient in training. In\nthis paper, we compare these two architectures and identify the key factors for\nbuilding efficient MLLMs. A primary difference between them lies in how\nattention is applied to visual tokens, particularly in their interactions with\neach other. To investigate whether attention among visual tokens is necessary,\nwe propose a new self-attention mechanism, NAAViT (\\textbf{N}o\n\\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens), which\neliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that\nattention among visual tokens is highly redundant. Based on these insights, we\nintroduce SAISA (\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace\n\\textbf{A}lignment), a novel architecture that enhance both training and\ninference efficiency. SAISA directly aligns visual features with the input\nspaces of NAAViT self-attention blocks, reducing computational overhead in both\nself-attention blocks and feed-forward networks (FFNs). Using the same\nconfiguration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\\% and training\nbudget by 26\\%, while achieving superior performance in terms of accuracy.\nComprehensive ablation studies further validate the effectiveness of SAISA\nacross various LLMs and visual encoders. The code and model will be publicly\navailable at https://github.com/icip-cas/SAISA."
                },
                "authors": [
                    {
                        "name": "Qianhao Yuan"
                    },
                    {
                        "name": "Yanjiang Liu"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19358v2",
                "updated": "2025-02-04T16:22:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    22,
                    43,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-31T18:10:53Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    10,
                    53,
                    4,
                    31,
                    0
                ],
                "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking"
                },
                "summary": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\nEPPO in mitigating reward hacking and improving RLHF performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\nEPPO in mitigating reward hacking and improving RLHF performance."
                },
                "authors": [
                    {
                        "name": "Yuchun Miao"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yuqi Zhang"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "28 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02451v1",
                "updated": "2025-02-04T16:17:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    17,
                    1,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:17:01Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    17,
                    1,
                    1,
                    35,
                    0
                ],
                "title": "Beyond English: Evaluating Automated Measurement of Moral Foundations in\n  Non-English Discourse with a Chinese Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond English: Evaluating Automated Measurement of Moral Foundations in\n  Non-English Discourse with a Chinese Case Study"
                },
                "summary": "This study explores computational approaches for measuring moral foundations\n(MFs) in non-English corpora. Since most resources are developed primarily for\nEnglish, cross-linguistic applications of moral foundation theory remain\nlimited. Using Chinese as a case study, this paper evaluates the effectiveness\nof applying English resources to machine translated text, local language\nlexicons, multilingual language models, and large language models (LLMs) in\nmeasuring MFs in non-English texts. The results indicate that machine\ntranslation and local lexicon approaches are insufficient for complex moral\nassessments, frequently resulting in a substantial loss of cultural\ninformation. In contrast, multilingual models and LLMs demonstrate reliable\ncross-language performance with transfer learning, with LLMs excelling in terms\nof data efficiency. Importantly, this study also underscores the need for\nhuman-in-the-loop validation of automated MF assessment, as the most advanced\nmodels may overlook cultural nuances in cross-language measurements. The\nfindings highlight the potential of LLMs for cross-language MF measurements and\nother complex multilingual deductive coding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores computational approaches for measuring moral foundations\n(MFs) in non-English corpora. Since most resources are developed primarily for\nEnglish, cross-linguistic applications of moral foundation theory remain\nlimited. Using Chinese as a case study, this paper evaluates the effectiveness\nof applying English resources to machine translated text, local language\nlexicons, multilingual language models, and large language models (LLMs) in\nmeasuring MFs in non-English texts. The results indicate that machine\ntranslation and local lexicon approaches are insufficient for complex moral\nassessments, frequently resulting in a substantial loss of cultural\ninformation. In contrast, multilingual models and LLMs demonstrate reliable\ncross-language performance with transfer learning, with LLMs excelling in terms\nof data efficiency. Importantly, this study also underscores the need for\nhuman-in-the-loop validation of automated MF assessment, as the most advanced\nmodels may overlook cultural nuances in cross-language measurements. The\nfindings highlight the potential of LLMs for cross-language MF measurements and\nother complex multilingual deductive coding tasks."
                },
                "authors": [
                    {
                        "name": "Calvin Yixiang Cheng"
                    },
                    {
                        "name": "Scott A Hale"
                    }
                ],
                "author_detail": {
                    "name": "Scott A Hale"
                },
                "author": "Scott A Hale",
                "arxiv_comment": "12 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00903v2",
                "updated": "2025-02-04T16:15:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    15,
                    45,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-02T20:29:10Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    20,
                    29,
                    10,
                    6,
                    33,
                    0
                ],
                "title": "Embracing Dialectic Intersubjectivity: Coordination of Different\n  Perspectives in Content Analysis with LLM Persona Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embracing Dialectic Intersubjectivity: Coordination of Different\n  Perspectives in Content Analysis with LLM Persona Simulation"
                },
                "summary": "This study attempts to advancing content analysis methodology from\nconsensus-oriented to coordination-oriented practices, thereby embracing\ndiverse coding outputs and exploring the dynamics among differential\nperspectives. As an exploratory investigation of this approach, we evaluate six\nGPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on\nBiden and Trump during the 2020 U.S. presidential campaign, examining patterns\nacross these models. By assessing each model's alignment with ideological\nperspectives, we explore how partisan selective processing could be identified\nin LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona\nLLMs exhibit stronger ideological biases when processing politically congruent\ncontent. Additionally, intercoder reliability is higher among same-partisan\npersonas compared to cross-partisan pairs. This approach enhances the nuanced\nunderstanding of LLM outputs and advances the integrity of AI-driven social\nscience research, enabling simulations of real-world implications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study attempts to advancing content analysis methodology from\nconsensus-oriented to coordination-oriented practices, thereby embracing\ndiverse coding outputs and exploring the dynamics among differential\nperspectives. As an exploratory investigation of this approach, we evaluate six\nGPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on\nBiden and Trump during the 2020 U.S. presidential campaign, examining patterns\nacross these models. By assessing each model's alignment with ideological\nperspectives, we explore how partisan selective processing could be identified\nin LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona\nLLMs exhibit stronger ideological biases when processing politically congruent\ncontent. Additionally, intercoder reliability is higher among same-partisan\npersonas compared to cross-partisan pairs. This approach enhances the nuanced\nunderstanding of LLM outputs and advances the integrity of AI-driven social\nscience research, enabling simulations of real-world implications."
                },
                "authors": [
                    {
                        "name": "Taewoo Kang"
                    },
                    {
                        "name": "Kjerstin Thorson"
                    },
                    {
                        "name": "Tai-Quan Peng"
                    },
                    {
                        "name": "Dan Hiaeshutter-Rice"
                    },
                    {
                        "name": "Sanguk Lee"
                    },
                    {
                        "name": "Stuart Soroka"
                    }
                ],
                "author_detail": {
                    "name": "Stuart Soroka"
                },
                "author": "Stuart Soroka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02444v1",
                "updated": "2025-02-04T16:10:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    10,
                    55,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:10:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    10,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models"
                },
                "summary": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Tianze Zhang"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Liyuan Zhang"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02441v1",
                "updated": "2025-02-04T16:08:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    8,
                    48,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:08:48Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    8,
                    48,
                    1,
                    35,
                    0
                ],
                "title": "LLMER: Crafting Interactive Extended Reality Worlds with JSON Data\n  Generated by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMER: Crafting Interactive Extended Reality Worlds with JSON Data\n  Generated by Large Language Models"
                },
                "summary": "The integration of Large Language Models (LLMs) like GPT-4 with Extended\nReality (XR) technologies offers the potential to build truly immersive XR\nenvironments that interact with human users through natural language, e.g.,\ngenerating and animating 3D scenes from audio inputs. However, the complexity\nof XR environments makes it difficult to accurately extract relevant contextual\ndata and scene/object parameters from an overwhelming volume of XR artifacts.\nIt leads to not only increased costs with pay-per-use models, but also elevated\nlevels of generation errors. Moreover, existing approaches focusing on coding\nscript generation are often prone to generation errors, resulting in flawed or\ninvalid scripts, application crashes, and ultimately a degraded user\nexperience. To overcome these challenges, we introduce LLMER, a novel framework\nthat creates interactive XR worlds using JSON data generated by LLMs. Unlike\nprior approaches focusing on coding script generation, LLMER translates natural\nlanguage inputs into JSON data, significantly reducing the likelihood of\napplication crashes and processing latency. It employs a multi-stage strategy\nto supply only the essential contextual information adapted to the user's\nrequest and features multiple modules designed for various XR tasks. Our\npreliminary user study reveals the effectiveness of the proposed system, with\nover 80% reduction in consumed tokens and around 60% reduction in task\ncompletion time compared to state-of-the-art approaches. The analysis of users'\nfeedback also illuminates a series of directions for further optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) like GPT-4 with Extended\nReality (XR) technologies offers the potential to build truly immersive XR\nenvironments that interact with human users through natural language, e.g.,\ngenerating and animating 3D scenes from audio inputs. However, the complexity\nof XR environments makes it difficult to accurately extract relevant contextual\ndata and scene/object parameters from an overwhelming volume of XR artifacts.\nIt leads to not only increased costs with pay-per-use models, but also elevated\nlevels of generation errors. Moreover, existing approaches focusing on coding\nscript generation are often prone to generation errors, resulting in flawed or\ninvalid scripts, application crashes, and ultimately a degraded user\nexperience. To overcome these challenges, we introduce LLMER, a novel framework\nthat creates interactive XR worlds using JSON data generated by LLMs. Unlike\nprior approaches focusing on coding script generation, LLMER translates natural\nlanguage inputs into JSON data, significantly reducing the likelihood of\napplication crashes and processing latency. It employs a multi-stage strategy\nto supply only the essential contextual information adapted to the user's\nrequest and features multiple modules designed for various XR tasks. Our\npreliminary user study reveals the effectiveness of the proposed system, with\nover 80% reduction in consumed tokens and around 60% reduction in task\ncompletion time compared to state-of-the-art approaches. The analysis of users'\nfeedback also illuminates a series of directions for further optimization."
                },
                "authors": [
                    {
                        "name": "Jiangong Chen"
                    },
                    {
                        "name": "Xiaoyi Wu"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04032v2",
                "updated": "2025-02-04T16:05:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    5,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-06T16:31:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    31,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Beemo: Benchmark of Expert-edited Machine-generated Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beemo: Benchmark of Expert-edited Machine-generated Outputs"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Jason Lucas"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Jooyoung Lee"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Adaku Uchendu"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    }
                ],
                "author_detail": {
                    "name": "Vladislav Mikhailov"
                },
                "author": "Vladislav Mikhailov",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09321v2",
                "updated": "2025-02-04T16:04:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    4,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-06-13T16:59:43Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    59,
                    43,
                    3,
                    165,
                    0
                ],
                "title": "JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts\n  Against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts\n  Against Large Language Models"
                },
                "summary": "Jailbreak attacks induce Large Language Models (LLMs) to generate harmful\nresponses, posing severe misuse threats. Though research on jailbreak attacks\nand defenses is emerging, there is no consensus on evaluating jailbreaks, i.e.,\nthe methods to assess the harmfulness of an LLM's response are varied. Each\napproach has its own set of strengths and weaknesses, impacting their alignment\nwith human values, as well as the time and financial cost. This diversity\nchallenges researchers in choosing suitable evaluation methods and comparing\ndifferent attacks and defenses. In this paper, we conduct a comprehensive\nanalysis of jailbreak evaluation methodologies, drawing from nearly 90\njailbreak research published between May 2023 and April 2024. Our study\nintroduces a systematic taxonomy of jailbreak evaluators, offering indepth\ninsights into their strengths and weaknesses, along with the current status of\ntheir adaptation. To aid further research, we propose JailbreakEval, a toolkit\nfor evaluating jailbreak attempts. JailbreakEval includes various evaluators\nout-of-the-box, enabling users to obtain results with a single command or\ncustomized evaluation workflows. In summary, we regard JailbreakEval to be a\ncatalyst that simplifies the evaluation process in jailbreak research and\nfosters an inclusive standard for jailbreak evaluation within the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks induce Large Language Models (LLMs) to generate harmful\nresponses, posing severe misuse threats. Though research on jailbreak attacks\nand defenses is emerging, there is no consensus on evaluating jailbreaks, i.e.,\nthe methods to assess the harmfulness of an LLM's response are varied. Each\napproach has its own set of strengths and weaknesses, impacting their alignment\nwith human values, as well as the time and financial cost. This diversity\nchallenges researchers in choosing suitable evaluation methods and comparing\ndifferent attacks and defenses. In this paper, we conduct a comprehensive\nanalysis of jailbreak evaluation methodologies, drawing from nearly 90\njailbreak research published between May 2023 and April 2024. Our study\nintroduces a systematic taxonomy of jailbreak evaluators, offering indepth\ninsights into their strengths and weaknesses, along with the current status of\ntheir adaptation. To aid further research, we propose JailbreakEval, a toolkit\nfor evaluating jailbreak attempts. JailbreakEval includes various evaluators\nout-of-the-box, enabling users to obtain results with a single command or\ncustomized evaluation workflows. In summary, we regard JailbreakEval to be a\ncatalyst that simplifies the evaluation process in jailbreak research and\nfosters an inclusive standard for jailbreak evaluation within the community."
                },
                "authors": [
                    {
                        "name": "Delong Ran"
                    },
                    {
                        "name": "Jinyuan Liu"
                    },
                    {
                        "name": "Yichen Gong"
                    },
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Anyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anyu Wang"
                },
                "author": "Anyu Wang",
                "arxiv_comment": "This is the Extended Version for the Poster at NDSS Symposium 2025,\n  Feb 24-28, 2025. Our code is available at\n  https://github.com/ThuCCSLab/JailbreakEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00883v2",
                "updated": "2025-02-04T16:02:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    2,
                    53,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-02T19:25:41Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    25,
                    41,
                    6,
                    33,
                    0
                ],
                "title": "SimPER: A Minimalist Approach to Preference Alignment without\n  Hyperparameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimPER: A Minimalist Approach to Preference Alignment without\n  Hyperparameters"
                },
                "summary": "Existing preference optimization objectives for language model alignment\nrequire additional hyperparameters that must be extensively tuned to achieve\noptimal performance, increasing both the complexity and time required for\nfine-tuning large language models. In this paper, we propose a simple yet\neffective hyperparameter-free preference optimization algorithm for alignment.\nWe observe that promising performance can be achieved simply by optimizing\ninverse perplexity, which is calculated as the inverse of the exponentiated\naverage log-likelihood of the chosen and rejected responses in the preference\ndataset. The resulting simple learning objective, SimPER, is easy to implement\nand eliminates the need for expensive hyperparameter tuning and a reference\nmodel, making it both computationally and memory efficient. Extensive\nexperiments on widely used real-world benchmarks, including MT-Bench,\nAlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base\nmodels, demonstrate that SimPER consistently and significantly outperforms\nexisting approaches-even without any hyperparameters or a reference model . For\nexample, despite its simplicity, SimPER outperforms state-of-the-art methods by\nup to 5.7 points on AlpacaEval 2 and achieves the highest average ranking\nacross 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is\npublicly available at: https://github.com/tengxiao1/SimPER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing preference optimization objectives for language model alignment\nrequire additional hyperparameters that must be extensively tuned to achieve\noptimal performance, increasing both the complexity and time required for\nfine-tuning large language models. In this paper, we propose a simple yet\neffective hyperparameter-free preference optimization algorithm for alignment.\nWe observe that promising performance can be achieved simply by optimizing\ninverse perplexity, which is calculated as the inverse of the exponentiated\naverage log-likelihood of the chosen and rejected responses in the preference\ndataset. The resulting simple learning objective, SimPER, is easy to implement\nand eliminates the need for expensive hyperparameter tuning and a reference\nmodel, making it both computationally and memory efficient. Extensive\nexperiments on widely used real-world benchmarks, including MT-Bench,\nAlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base\nmodels, demonstrate that SimPER consistently and significantly outperforms\nexisting approaches-even without any hyperparameters or a reference model . For\nexample, despite its simplicity, SimPER outperforms state-of-the-art methods by\nup to 5.7 points on AlpacaEval 2 and achieves the highest average ranking\nacross 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is\npublicly available at: https://github.com/tengxiao1/SimPER."
                },
                "authors": [
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Yige Yuan"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Mingxiao Li"
                    },
                    {
                        "name": "Shangsong Liang"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Vasant G Honavar"
                    }
                ],
                "author_detail": {
                    "name": "Vasant G Honavar"
                },
                "author": "Vasant G Honavar",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02435v1",
                "updated": "2025-02-04T15:59:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    59,
                    59,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:59:59Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    59,
                    59,
                    1,
                    35,
                    0
                ],
                "title": "Characterization of CMOS sensor using X-ray irradiation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of CMOS sensor using X-ray irradiation"
                },
                "summary": "Recent advancements in particle physics demand pixel detectors that can\nwithstand increased luminosity in the future collider experiments. In response,\nMALTA, a novel monolithic active pixel detector, has been developed with a\ncutting-edge readout architecture. This new class of monolithic pixel detectors\nis found to have exceptional radiation tolerance, superior hit rates, higher\nresolution and precise timing resolution, making them ideally suited for\nexperiments at the LHC. To optimize the performance of these sensors before\ntheir deployment in actual detectors, comprehensive electrical characterization\nhas been conducted. This study also includes comparative DAC analyses among\nsensors of varying thicknesses, providing crucial insights for performance\nenhancement. For the further understanding of the effect of radiation, the\nsensors are being exposed to different fluence using high intensity X-ray\nsource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in particle physics demand pixel detectors that can\nwithstand increased luminosity in the future collider experiments. In response,\nMALTA, a novel monolithic active pixel detector, has been developed with a\ncutting-edge readout architecture. This new class of monolithic pixel detectors\nis found to have exceptional radiation tolerance, superior hit rates, higher\nresolution and precise timing resolution, making them ideally suited for\nexperiments at the LHC. To optimize the performance of these sensors before\ntheir deployment in actual detectors, comprehensive electrical characterization\nhas been conducted. This study also includes comparative DAC analyses among\nsensors of varying thicknesses, providing crucial insights for performance\nenhancement. For the further understanding of the effect of radiation, the\nsensors are being exposed to different fluence using high intensity X-ray\nsource."
                },
                "authors": [
                    {
                        "name": "Anusree Vijay"
                    },
                    {
                        "name": "Prafulla Kumar Behera"
                    },
                    {
                        "name": "Theertha Chembakan"
                    },
                    {
                        "name": "Ganapati Dash"
                    }
                ],
                "author_detail": {
                    "name": "Ganapati Dash"
                },
                "author": "Ganapati Dash",
                "arxiv_comment": "5 pages, 6 figures, 11th International Workshop on Semiconductor\n  Pixel Detectors for Particles and Imaging",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12091v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12091v3",
                "updated": "2025-02-04T15:57:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    57,
                    59,
                    1,
                    35,
                    0
                ],
                "published": "2024-06-17T21:06:00Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    6,
                    0,
                    0,
                    169,
                    0
                ],
                "title": "Is poisoning a real threat to LLM alignment? Maybe more so than you\n  think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is poisoning a real threat to LLM alignment? Maybe more so than you\n  think"
                },
                "summary": "Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have\nsignificantly impacted the alignment of Large Language Models (LLMs). The\nsensitivity of reinforcement learning algorithms such as Proximal Policy\nOptimization (PPO) has led to new line work on Direct Policy Optimization\n(DPO), which treats RLHF in a supervised learning framework. The increased\npractical use of these RLHF methods warrants an analysis of their\nvulnerabilities. In this work, we investigate the vulnerabilities of DPO to\npoisoning attacks under different scenarios and compare the effectiveness of\npreference poisoning, a first of its kind. We comprehensively analyze DPO's\nvulnerabilities under different types of attacks, i.e., backdoor and\nnon-backdoor attacks, and different poisoning methods across a wide array of\nlanguage models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike\nPPO-based methods, which, when it comes to backdoor attacks, require at least\n4\\% of the data to be poisoned to elicit harmful behavior, we exploit the true\nvulnerabilities of DPO more simply so we can poison the model with only as much\nas 0.5\\% of the data. We further investigate the potential reasons behind the\nvulnerability and how well this vulnerability translates into backdoor vs\nnon-backdoor attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have\nsignificantly impacted the alignment of Large Language Models (LLMs). The\nsensitivity of reinforcement learning algorithms such as Proximal Policy\nOptimization (PPO) has led to new line work on Direct Policy Optimization\n(DPO), which treats RLHF in a supervised learning framework. The increased\npractical use of these RLHF methods warrants an analysis of their\nvulnerabilities. In this work, we investigate the vulnerabilities of DPO to\npoisoning attacks under different scenarios and compare the effectiveness of\npreference poisoning, a first of its kind. We comprehensively analyze DPO's\nvulnerabilities under different types of attacks, i.e., backdoor and\nnon-backdoor attacks, and different poisoning methods across a wide array of\nlanguage models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike\nPPO-based methods, which, when it comes to backdoor attacks, require at least\n4\\% of the data to be poisoned to elicit harmful behavior, we exploit the true\nvulnerabilities of DPO more simply so we can poison the model with only as much\nas 0.5\\% of the data. We further investigate the potential reasons behind the\nvulnerability and how well this vulnerability translates into backdoor vs\nnon-backdoor attacks."
                },
                "authors": [
                    {
                        "name": "Pankayaraj Pathmanathan"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Yongyuan Liang"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_journal_ref": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12091v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12091v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02421v1",
                "updated": "2025-02-04T15:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    42,
                    3,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    42,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Activation-Informed Merging of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-Informed Merging of Large Language Models"
                },
                "summary": "Model merging, a method that combines the parameters and embeddings of\nmultiple fine-tuned large language models (LLMs), offers a promising approach\nto enhance model performance across various tasks while maintaining\ncomputational efficiency. This paper introduces Activation-Informed Merging\n(AIM), a technique that integrates the information from the activation space of\nLLMs into the merging process to improve performance and robustness. AIM is\ndesigned as a flexible, complementary solution that is applicable to any\nexisting merging method. It aims to preserve critical weights from the base\nmodel, drawing on principles from continual learning~(CL) and model\ncompression. Utilizing a task-agnostic calibration set, AIM selectively\nprioritizes essential weights during merging. We empirically demonstrate that\nAIM significantly enhances the performance of merged models across multiple\nbenchmarks. Our findings suggest that considering the activation-space\ninformation can provide substantial advancements in the model merging\nstrategies for LLMs with up to 40\\% increase in benchmark performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging, a method that combines the parameters and embeddings of\nmultiple fine-tuned large language models (LLMs), offers a promising approach\nto enhance model performance across various tasks while maintaining\ncomputational efficiency. This paper introduces Activation-Informed Merging\n(AIM), a technique that integrates the information from the activation space of\nLLMs into the merging process to improve performance and robustness. AIM is\ndesigned as a flexible, complementary solution that is applicable to any\nexisting merging method. It aims to preserve critical weights from the base\nmodel, drawing on principles from continual learning~(CL) and model\ncompression. Utilizing a task-agnostic calibration set, AIM selectively\nprioritizes essential weights during merging. We empirically demonstrate that\nAIM significantly enhances the performance of merged models across multiple\nbenchmarks. Our findings suggest that considering the activation-space\ninformation can provide substantial advancements in the model merging\nstrategies for LLMs with up to 40\\% increase in benchmark performance."
                },
                "authors": [
                    {
                        "name": "Amin Heyrani Nobari"
                    },
                    {
                        "name": "Kaveh Alimohammadi"
                    },
                    {
                        "name": "Ali ArjomandBigdeli"
                    },
                    {
                        "name": "Akash Srivastava"
                    },
                    {
                        "name": "Faez Ahmed"
                    },
                    {
                        "name": "Navid Azizan"
                    }
                ],
                "author_detail": {
                    "name": "Navid Azizan"
                },
                "author": "Navid Azizan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02538v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02538v3",
                "updated": "2025-02-04T15:41:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    41,
                    27,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-04T19:17:17Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    19,
                    17,
                    17,
                    0,
                    309,
                    0
                ],
                "title": "MILU: A Multi-task Indic Language Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILU: A Multi-task Indic Language Understanding Benchmark"
                },
                "summary": "Evaluating Large Language Models (LLMs) in low-resource and linguistically\ndiverse languages remains a significant challenge in NLP, particularly for\nlanguages using non-Latin scripts like those spoken in India. Existing\nbenchmarks predominantly focus on English, leaving substantial gaps in\nassessing LLM capabilities in these languages. We introduce MILU, a Multi task\nIndic Language Understanding Benchmark, a comprehensive evaluation benchmark\ndesigned to address this gap. MILU spans 8 domains and 41 subjects across 11\nIndic languages, reflecting both general and culturally specific knowledge.\nWith an India-centric design, incorporates material from regional and\nstate-level examinations, covering topics such as local history, arts,\nfestivals, and laws, alongside standard subjects like science and mathematics.\nWe evaluate over 42 LLMs, and find that current LLMs struggle with MILU, with\nGPT-4o achieving the highest average accuracy at 74 percent. Open multilingual\nmodels outperform language-specific fine-tuned models, which perform only\nslightly better than random baselines. Models also perform better in high\nresource languages as compared to low resource ones. Domain-wise analysis\nindicates that models perform poorly in culturally relevant areas like Arts and\nHumanities, Law and Governance compared to general fields like STEM. To the\nbest of our knowledge, MILU is the first of its kind benchmark focused on Indic\nlanguages, serving as a crucial step towards comprehensive cultural evaluation.\nAll code, benchmarks, and artifacts are publicly available to foster open\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) in low-resource and linguistically\ndiverse languages remains a significant challenge in NLP, particularly for\nlanguages using non-Latin scripts like those spoken in India. Existing\nbenchmarks predominantly focus on English, leaving substantial gaps in\nassessing LLM capabilities in these languages. We introduce MILU, a Multi task\nIndic Language Understanding Benchmark, a comprehensive evaluation benchmark\ndesigned to address this gap. MILU spans 8 domains and 41 subjects across 11\nIndic languages, reflecting both general and culturally specific knowledge.\nWith an India-centric design, incorporates material from regional and\nstate-level examinations, covering topics such as local history, arts,\nfestivals, and laws, alongside standard subjects like science and mathematics.\nWe evaluate over 42 LLMs, and find that current LLMs struggle with MILU, with\nGPT-4o achieving the highest average accuracy at 74 percent. Open multilingual\nmodels outperform language-specific fine-tuned models, which perform only\nslightly better than random baselines. Models also perform better in high\nresource languages as compared to low resource ones. Domain-wise analysis\nindicates that models perform poorly in culturally relevant areas like Arts and\nHumanities, Law and Governance compared to general fields like STEM. To the\nbest of our knowledge, MILU is the first of its kind benchmark focused on Indic\nlanguages, serving as a crucial step towards comprehensive cultural evaluation.\nAll code, benchmarks, and artifacts are publicly available to foster open\nresearch."
                },
                "authors": [
                    {
                        "name": "Sshubam Verma"
                    },
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Vishwajeet Kumar"
                    },
                    {
                        "name": "Rudra Murthy"
                    },
                    {
                        "name": "Jaydeep Sen"
                    }
                ],
                "author_detail": {
                    "name": "Jaydeep Sen"
                },
                "author": "Jaydeep Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02538v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02538v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01618v2",
                "updated": "2025-02-04T15:39:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    39,
                    36,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-03T18:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    50,
                    50,
                    0,
                    34,
                    0
                ],
                "title": "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs\n  using Particle-Based Monte Carlo Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs\n  using Particle-Based Monte Carlo Methods"
                },
                "summary": "Large language models (LLMs) have achieved significant performance gains via\nscaling up model sizes and/or data. However, recent evidence suggests\ndiminishing returns from such approaches, motivating scaling the computation\nspent at inference time. Existing inference-time scaling methods, usually with\nreward models, cast the task as a search problem, which tends to be vulnerable\nto reward hacking as a consequence of approximation errors in reward models. In\nthis paper, we instead cast inference-time scaling as a probabilistic inference\ntask and leverage sampling-based techniques to explore the typical set of the\nstate distribution of a state-space model with an approximate likelihood,\nrather than optimize for its mode directly. We propose a novel inference-time\nscaling approach by adapting particle-based Monte Carlo methods to this task.\nOur empirical evaluation demonstrates that our methods have a 4-16x better\nscaling rate over our deterministic search counterparts on various challenging\nmathematical reasoning tasks. Using our approach, we show that\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but\nalso connects the rich literature in probabilistic inference with\ninference-time scaling of LLMs to develop more robust algorithms in future\nwork. Code and further information is available at\nhttps://probabilistic-inference-scaling.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant performance gains via\nscaling up model sizes and/or data. However, recent evidence suggests\ndiminishing returns from such approaches, motivating scaling the computation\nspent at inference time. Existing inference-time scaling methods, usually with\nreward models, cast the task as a search problem, which tends to be vulnerable\nto reward hacking as a consequence of approximation errors in reward models. In\nthis paper, we instead cast inference-time scaling as a probabilistic inference\ntask and leverage sampling-based techniques to explore the typical set of the\nstate distribution of a state-space model with an approximate likelihood,\nrather than optimize for its mode directly. We propose a novel inference-time\nscaling approach by adapting particle-based Monte Carlo methods to this task.\nOur empirical evaluation demonstrates that our methods have a 4-16x better\nscaling rate over our deterministic search counterparts on various challenging\nmathematical reasoning tasks. Using our approach, we show that\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but\nalso connects the rich literature in probabilistic inference with\ninference-time scaling of LLMs to develop more robust algorithms in future\nwork. Code and further information is available at\nhttps://probabilistic-inference-scaling.github.io."
                },
                "authors": [
                    {
                        "name": "Isha Puri"
                    },
                    {
                        "name": "Shivchander Sudalairaj"
                    },
                    {
                        "name": "Guangxuan Xu"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19297v2",
                "updated": "2025-02-04T15:33:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    33,
                    51,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-31T16:55:17Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    55,
                    17,
                    4,
                    31,
                    0
                ],
                "title": "Analysis of LLMs vs Human Experts in Requirements Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of LLMs vs Human Experts in Requirements Engineering"
                },
                "summary": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines."
                },
                "authors": [
                    {
                        "name": "Cory Hymel"
                    },
                    {
                        "name": "Hiroe Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Hiroe Johnson"
                },
                "author": "Hiroe Johnson",
                "arxiv_comment": "8 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02412v1",
                "updated": "2025-02-04T15:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    32,
                    34,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:32:34Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    32,
                    34,
                    1,
                    35,
                    0
                ],
                "title": "AI-Powered, But Power-Hungry? Energy Efficiency of LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Powered, But Power-Hungry? Energy Efficiency of LLM-Generated Code"
                },
                "summary": "Large language models (LLMs) are used in software development to assist in\nvarious tasks, e.g., code generation and code completion, but empirical\nevaluations of the quality of the results produced by these models focus on\ncorrectness and ignore other relevant aspects, such as their performance and\nenergy efficiency. Studying the performance of LLM-produced programs is\nessential to understand how well LLMs can support the construction of\nperformance- and energy-critical software, such as operating systems, servers,\nand mobile applications. This paper presents the first study analyzing the\nenergy efficiency and performance of LLM-generated code for three programming\nlanguages Python, Java, and C++, on two platforms, a Mac and a PC, leveraging\nthree frontier LLMs, Github Copilot, GPT-4o, and the recently-released OpenAI\no1-mini, and targeting ``hard'' programming problems from LeetCode. Our results\nshow that the models are much more successful in generating Python and Java\nthan C++ code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are used in software development to assist in\nvarious tasks, e.g., code generation and code completion, but empirical\nevaluations of the quality of the results produced by these models focus on\ncorrectness and ignore other relevant aspects, such as their performance and\nenergy efficiency. Studying the performance of LLM-produced programs is\nessential to understand how well LLMs can support the construction of\nperformance- and energy-critical software, such as operating systems, servers,\nand mobile applications. This paper presents the first study analyzing the\nenergy efficiency and performance of LLM-generated code for three programming\nlanguages Python, Java, and C++, on two platforms, a Mac and a PC, leveraging\nthree frontier LLMs, Github Copilot, GPT-4o, and the recently-released OpenAI\no1-mini, and targeting ``hard'' programming problems from LeetCode. Our results\nshow that the models are much more successful in generating Python and Java\nthan C++ code."
                },
                "authors": [
                    {
                        "name": "Lola Solovyeva"
                    },
                    {
                        "name": "Sophie Weidmann"
                    },
                    {
                        "name": "Fernando Castor"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Castor"
                },
                "author": "Fernando Castor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07668v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07668v3",
                "updated": "2025-02-04T15:29:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    29,
                    7,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-12T09:35:23Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    9,
                    35,
                    23,
                    1,
                    317,
                    0
                ],
                "title": "Towards Evaluation Guidelines for Empirical Studies involving LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluation Guidelines for Empirical Studies involving LLMs"
                },
                "summary": "In the short period since the release of ChatGPT, large language models\n(LLMs) have changed the software engineering research landscape. While there\nare numerous opportunities to use LLMs for supporting research or software\nengineering tasks, solid science needs rigorous empirical evaluations. However,\nso far, there are no specific guidelines for conducting and assessing studies\ninvolving LLMs in software engineering research. Our focus is on empirical\nstudies that either use LLMs as part of the research process or studies that\nevaluate existing or new tools that are based on LLMs. This paper contributes\nthe first set of holistic guidelines for such studies. Our goal is to start a\ndiscussion in the software engineering research community to reach a common\nunderstanding of our standards for high-quality empirical studies involving\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the short period since the release of ChatGPT, large language models\n(LLMs) have changed the software engineering research landscape. While there\nare numerous opportunities to use LLMs for supporting research or software\nengineering tasks, solid science needs rigorous empirical evaluations. However,\nso far, there are no specific guidelines for conducting and assessing studies\ninvolving LLMs in software engineering research. Our focus is on empirical\nstudies that either use LLMs as part of the research process or studies that\nevaluate existing or new tools that are based on LLMs. This paper contributes\nthe first set of holistic guidelines for such studies. Our goal is to start a\ndiscussion in the software engineering research community to reach a common\nunderstanding of our standards for high-quality empirical studies involving\nLLMs."
                },
                "authors": [
                    {
                        "name": "Stefan Wagner"
                    },
                    {
                        "name": "Marvin Muñoz Barón"
                    },
                    {
                        "name": "Davide Falessi"
                    },
                    {
                        "name": "Sebastian Baltes"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Baltes"
                },
                "author": "Sebastian Baltes",
                "arxiv_comment": "4 pages, 2nd IEEE/ACM International Workshop on Methodological Issues\n  with Empirical Studies in Software Engineering (WSESE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07668v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07668v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02407v1",
                "updated": "2025-02-04T15:25:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    25,
                    47,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:25:47Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    25,
                    47,
                    1,
                    35,
                    0
                ],
                "title": "Avoiding spurious sharpness minimization broadens applicability of SAM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoiding spurious sharpness minimization broadens applicability of SAM"
                },
                "summary": "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs)."
                },
                "authors": [
                    {
                        "name": "Sidak Pal Singh"
                    },
                    {
                        "name": "Hossein Mobahi"
                    },
                    {
                        "name": "Atish Agarwala"
                    },
                    {
                        "name": "Yann Dauphin"
                    }
                ],
                "author_detail": {
                    "name": "Yann Dauphin"
                },
                "author": "Yann Dauphin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02390v1",
                "updated": "2025-02-04T15:10:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    10,
                    33,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:10:33Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    10,
                    33,
                    1,
                    35,
                    0
                ],
                "title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large\n  Language Models Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large\n  Language Models Reasoning"
                },
                "summary": "Research on LLM technologies is rapidly emerging, with most of them employing\na 'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. To validate the\neffectiveness of our framework, we conducted extensive experiments across a\nrange of generative and reasoning tasks. These experiments demonstrated that\nour framework outperforms conventional inference processes on accuracy,\ncoherence, and diversity. The framework's ability to iteratively expand its\nsearch space while retaining contextually relevant information results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on LLM technologies is rapidly emerging, with most of them employing\na 'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. To validate the\neffectiveness of our framework, we conducted extensive experiments across a\nrange of generative and reasoning tasks. These experiments demonstrated that\nour framework outperforms conventional inference processes on accuracy,\ncoherence, and diversity. The framework's ability to iteratively expand its\nsearch space while retaining contextually relevant information results."
                },
                "authors": [
                    {
                        "name": "Jianfeng Pan"
                    },
                    {
                        "name": "Senyou Deng"
                    },
                    {
                        "name": "Shaomang Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shaomang Huang"
                },
                "author": "Shaomang Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01325v2",
                "updated": "2025-02-04T15:08:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    8,
                    42,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-03T12:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    59,
                    36,
                    0,
                    34,
                    0
                ],
                "title": "The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in\n  Parent-Child Homework Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in\n  Parent-Child Homework Interactions"
                },
                "summary": "Parental involvement in homework is a crucial aspect of family education, but\nit often leads to emotional strain and conflicts that can severely impact\nfamily well-being. This paper presents findings from a 4-week in situ study\ninvolving 78 families in China, where we collected and analyzed 602 valid audio\nrecordings (totalling 475 hours) and daily surveys. Leveraging large language\nmodels (LLMs) to analyze parent-child conversations, we gained a nuanced\nunderstanding of emotional and behavioural dynamics that overcomes the\nlimitations of traditional one-time surveys and interviews. Our findings reveal\nsignificant emotional shifts in parents before and after homework involvement\nand summarise a range of positive, neutral and negative parental behaviours. We\nalso catalogue seven common conflicts, with Knowledge Conflict being the most\nfrequent. Notably, we found that even well-intentioned parental behaviours,\nsuch as Unlabelled Praise, were significantly positively correlated with\nspecific conflict types. This work advances ubiquitous computing's research to\nsense and understand complex family dynamics, while offering evidence-based\ninsights for designing future ambient intelligent systems to support healthy\nfamily education environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parental involvement in homework is a crucial aspect of family education, but\nit often leads to emotional strain and conflicts that can severely impact\nfamily well-being. This paper presents findings from a 4-week in situ study\ninvolving 78 families in China, where we collected and analyzed 602 valid audio\nrecordings (totalling 475 hours) and daily surveys. Leveraging large language\nmodels (LLMs) to analyze parent-child conversations, we gained a nuanced\nunderstanding of emotional and behavioural dynamics that overcomes the\nlimitations of traditional one-time surveys and interviews. Our findings reveal\nsignificant emotional shifts in parents before and after homework involvement\nand summarise a range of positive, neutral and negative parental behaviours. We\nalso catalogue seven common conflicts, with Knowledge Conflict being the most\nfrequent. Notably, we found that even well-intentioned parental behaviours,\nsuch as Unlabelled Praise, were significantly positively correlated with\nspecific conflict types. This work advances ubiquitous computing's research to\nsense and understand complex family dynamics, while offering evidence-based\ninsights for designing future ambient intelligent systems to support healthy\nfamily education environments."
                },
                "authors": [
                    {
                        "name": "Nan Gao"
                    },
                    {
                        "name": "Yibin Liu"
                    },
                    {
                        "name": "Xin Tang"
                    },
                    {
                        "name": "Yanyan Liu"
                    },
                    {
                        "name": "Chun Yu"
                    },
                    {
                        "name": "Yun Huang"
                    },
                    {
                        "name": "Yuntao Wang"
                    },
                    {
                        "name": "Flora D. Salim"
                    },
                    {
                        "name": "Xuhai Orson Xu"
                    },
                    {
                        "name": "Jun Wei"
                    },
                    {
                        "name": "Yuanchun Shi"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Shi"
                },
                "author": "Yuanchun Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07288v3",
                "updated": "2025-02-04T15:05:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    5,
                    27,
                    1,
                    35,
                    0
                ],
                "published": "2023-12-12T14:05:19Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    14,
                    5,
                    19,
                    1,
                    346,
                    0
                ],
                "title": "Reconfigurable Intelligent Surfaces in 6G Radio Localization: A Survey\n  of Recent Developments, Opportunities, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surfaces in 6G Radio Localization: A Survey\n  of Recent Developments, Opportunities, and Challenges"
                },
                "summary": "In this survey paper, we present an extensive review of the use of RIS in 6G\nradio localization, highlighting their pivotal role as a low-cost,\nenergy-efficient technology that reshapes wireless communication and\nlocalization landscapes. Investigating the versatile capabilities of RIS, we\nexplore their dynamic control over electromagnetic wave manipulation, including\nreflection, refraction, and transmission, which opens new horizons in diverse\napplications ranging from IOT connectivity to advanced mobile communication,\nand various innovative applications in Industry 4.0. Our comprehensive review\nprovides an overview of RIS use in 6G radio localization, highlighting recent\nprogress in RIS technology assisted localization. It focuses on key aspects,\nincluding network scenarios, transmission bands, deployment environments, and\nnear-field operations. We discuss studies to examine the state-of-the-art\nRIS-assisted localization and optimization techniques and their performance\nevaluation matrices. In addition, we present a detailed taxonomy of\nRIS-assisted radio localization, emphasizing the rapid evolution and potential\nof RIS technology in non-line-of-sight scenarios as an alternative to\ntraditional base stations. Based on the careful investigation of the reviewed\nstudies, the survey also sheds light on future research directions, technical\nchallenges, and limitations, offering a clear perspective on the integration\nand optimization of RIS in 6G networks for enhanced localization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this survey paper, we present an extensive review of the use of RIS in 6G\nradio localization, highlighting their pivotal role as a low-cost,\nenergy-efficient technology that reshapes wireless communication and\nlocalization landscapes. Investigating the versatile capabilities of RIS, we\nexplore their dynamic control over electromagnetic wave manipulation, including\nreflection, refraction, and transmission, which opens new horizons in diverse\napplications ranging from IOT connectivity to advanced mobile communication,\nand various innovative applications in Industry 4.0. Our comprehensive review\nprovides an overview of RIS use in 6G radio localization, highlighting recent\nprogress in RIS technology assisted localization. It focuses on key aspects,\nincluding network scenarios, transmission bands, deployment environments, and\nnear-field operations. We discuss studies to examine the state-of-the-art\nRIS-assisted localization and optimization techniques and their performance\nevaluation matrices. In addition, we present a detailed taxonomy of\nRIS-assisted radio localization, emphasizing the rapid evolution and potential\nof RIS technology in non-line-of-sight scenarios as an alternative to\ntraditional base stations. Based on the careful investigation of the reviewed\nstudies, the survey also sheds light on future research directions, technical\nchallenges, and limitations, offering a clear perspective on the integration\nand optimization of RIS in 6G networks for enhanced localization capabilities."
                },
                "authors": [
                    {
                        "name": "Anum Umer"
                    },
                    {
                        "name": "Ivo Müürsepp"
                    },
                    {
                        "name": "Muhammad Mahtab Alam"
                    },
                    {
                        "name": "Henk Wymeersch"
                    }
                ],
                "author_detail": {
                    "name": "Henk Wymeersch"
                },
                "author": "Henk Wymeersch",
                "arxiv_doi": "10.1109/COMST.2025.3536517",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/COMST.2025.3536517",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.07288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "https://ieeexplore.ieee.org/document/10858311",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02384v1",
                "updated": "2025-02-04T15:02:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    2,
                    55,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:02:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    2,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAIR: Improving Safety Alignment with Introspective Reasoning"
                },
                "summary": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has\nbecome equally critical as their performance in applications. However, existing\nsafety alignment methods typically suffer from safety-performance trade-offs\nand the susceptibility to jailbreak attacks, primarily due to their reliance on\ndirect refusals for malicious queries. In this paper, we propose STAIR, a novel\nframework that integrates SafeTy Alignment with Itrospective Reasoning. We\nenable LLMs to identify safety risks through step-by-step analysis by\nself-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR\nfirst equips the model with a structured reasoning capability and then advances\nsafety alignment via iterative preference optimization on step-level reasoning\ndata generated using our newly proposed Safety-Informed Monte Carlo Tree Search\n(SI-MCTS). We further train a process reward model on this data to guide\ntest-time searches for improved responses. Extensive experiments show that\nSTAIR effectively mitigates harmful outputs while better preserving\nhelpfulness, compared to instinctive alignment strategies. With test-time\nscaling, STAIR achieves a safety performance comparable to Claude-3.5 against\npopular jailbreak attacks. Relevant resources in this work are available at\nhttps://github.com/thu-ml/STAIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has\nbecome equally critical as their performance in applications. However, existing\nsafety alignment methods typically suffer from safety-performance trade-offs\nand the susceptibility to jailbreak attacks, primarily due to their reliance on\ndirect refusals for malicious queries. In this paper, we propose STAIR, a novel\nframework that integrates SafeTy Alignment with Itrospective Reasoning. We\nenable LLMs to identify safety risks through step-by-step analysis by\nself-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR\nfirst equips the model with a structured reasoning capability and then advances\nsafety alignment via iterative preference optimization on step-level reasoning\ndata generated using our newly proposed Safety-Informed Monte Carlo Tree Search\n(SI-MCTS). We further train a process reward model on this data to guide\ntest-time searches for improved responses. Extensive experiments show that\nSTAIR effectively mitigates harmful outputs while better preserving\nhelpfulness, compared to instinctive alignment strategies. With test-time\nscaling, STAIR achieves a safety performance comparable to Claude-3.5 against\npopular jailbreak attacks. Relevant resources in this work are available at\nhttps://github.com/thu-ml/STAIR."
                },
                "authors": [
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Siyuan Zhang"
                    },
                    {
                        "name": "Yao Huang"
                    },
                    {
                        "name": "Zeyu Xia"
                    },
                    {
                        "name": "Zhengwei Fang"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "22 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02377v1",
                "updated": "2025-02-04T14:57:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    57,
                    54,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:57:54Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    57,
                    54,
                    1,
                    35,
                    0
                ],
                "title": "A Minimax Approach to Ad Hoc Teamwork",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimax Approach to Ad Hoc Teamwork"
                },
                "summary": "We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes\npolicies against an adversarial prior over partners, explicitly accounting for\nuncertainty about partners at time of deployment. Unlike existing methods that\nassume a specific distribution over partners, our approach improves worst-case\nperformance guarantees. Extensive experiments, including evaluations on\ncoordinated cooking tasks from the Melting Pot suite, show our method's\nsuperior robustness compared to self-play, fictitious play, and best response\nlearning. Our work highlights the importance of selecting an appropriate\ntraining distribution over teammates to achieve robustness in AHT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes\npolicies against an adversarial prior over partners, explicitly accounting for\nuncertainty about partners at time of deployment. Unlike existing methods that\nassume a specific distribution over partners, our approach improves worst-case\nperformance guarantees. Extensive experiments, including evaluations on\ncoordinated cooking tasks from the Melting Pot suite, show our method's\nsuperior robustness compared to self-play, fictitious play, and best response\nlearning. Our work highlights the importance of selecting an appropriate\ntraining distribution over teammates to achieve robustness in AHT."
                },
                "authors": [
                    {
                        "name": "Victor Villin"
                    },
                    {
                        "name": "Thomas Kleine Buening"
                    },
                    {
                        "name": "Christos Dimitrakakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Dimitrakakis"
                },
                "author": "Christos Dimitrakakis",
                "arxiv_comment": "Accepted at AAMAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02368v1",
                "updated": "2025-02-04T14:50:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    50,
                    23,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:50:23Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    50,
                    23,
                    1,
                    35,
                    0
                ],
                "title": "Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in\n  Real-World Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in\n  Real-World Projects"
                },
                "summary": "Large Language Models (LLMs) have gained attention for addressing coding\nproblems, but their effectiveness in fixing code maintainability remains\nunclear. This study evaluates LLMs capability to resolve 127 maintainability\nissues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat\nand Llama 3.1, and few-shot prompting with Llama only. The LLM-generated\nsolutions are assessed for compilation errors, test failures, and new\nmaintainability problems. Llama with few-shot prompting successfully fixed\n44.9% of the methods, while Copilot Chat and Llama zero-shot fixed 32.29% and\n30%, respectively. However, most solutions introduced errors or new\nmaintainability issues. We also conducted a human study with 45 participants to\nevaluate the readability of 51 LLM-generated solutions. The human study showed\nthat 68.63% of participants observed improved readability. Overall, while LLMs\nshow potential for fixing maintainability issues, their introduction of errors\nhighlights their current limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained attention for addressing coding\nproblems, but their effectiveness in fixing code maintainability remains\nunclear. This study evaluates LLMs capability to resolve 127 maintainability\nissues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat\nand Llama 3.1, and few-shot prompting with Llama only. The LLM-generated\nsolutions are assessed for compilation errors, test failures, and new\nmaintainability problems. Llama with few-shot prompting successfully fixed\n44.9% of the methods, while Copilot Chat and Llama zero-shot fixed 32.29% and\n30%, respectively. However, most solutions introduced errors or new\nmaintainability issues. We also conducted a human study with 45 participants to\nevaluate the readability of 51 LLM-generated solutions. The human study showed\nthat 68.63% of participants observed improved readability. Overall, while LLMs\nshow potential for fixing maintainability issues, their introduction of errors\nhighlights their current limitations."
                },
                "authors": [
                    {
                        "name": "Henrique Nunes"
                    },
                    {
                        "name": "Eduardo Figueiredo"
                    },
                    {
                        "name": "Larissa Rocha"
                    },
                    {
                        "name": "Sarah Nadi"
                    },
                    {
                        "name": "Fischer Ferreira"
                    },
                    {
                        "name": "Geanderson Esteves"
                    }
                ],
                "author_detail": {
                    "name": "Geanderson Esteves"
                },
                "author": "Geanderson Esteves",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02362v1",
                "updated": "2025-02-04T14:44:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    44,
                    58,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:44:58Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    44,
                    58,
                    1,
                    35,
                    0
                ],
                "title": "Premise-Augmented Reasoning Chains Improve Error Identification in Math\n  reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Premise-Augmented Reasoning Chains Improve Error Identification in Math\n  reasoning with LLMs"
                },
                "summary": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations."
                },
                "authors": [
                    {
                        "name": "Sagnik Mukherjee"
                    },
                    {
                        "name": "Abhinav Chinta"
                    },
                    {
                        "name": "Takyoung Kim"
                    },
                    {
                        "name": "Tarun Anoop Sharma"
                    },
                    {
                        "name": "Dilek Hakkani Tur"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani Tur"
                },
                "author": "Dilek Hakkani Tur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00830v2",
                "updated": "2025-02-04T14:37:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    37,
                    29,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-01T13:20:01Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    20,
                    1,
                    2,
                    1,
                    0
                ],
                "title": "LLM+AL: Bridging Large Language Models and Action Languages for Complex\n  Reasoning about Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM+AL: Bridging Large Language Models and Action Languages for Complex\n  Reasoning about Actions"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in various\nintelligent tasks but still struggle with complex action reasoning tasks that\nrequire systematic search. To address this limitation, we propose a method that\nbridges the natural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages. Our approach, termed\n\"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's proficiency in automated\nreasoning based on encoded knowledge. We compare LLM+AL against\nstate-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0,\nand o1-preview, using benchmarks for complex reasoning about actions. Our\nfindings indicate that, although all methods exhibit errors, LLM+AL, with\nrelatively minimal human corrections, consistently leads to correct answers,\nwhereas standalone LLMs fail to improve even with human feedback. LLM+AL also\ncontributes to automated generation of action languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in various\nintelligent tasks but still struggle with complex action reasoning tasks that\nrequire systematic search. To address this limitation, we propose a method that\nbridges the natural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages. Our approach, termed\n\"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's proficiency in automated\nreasoning based on encoded knowledge. We compare LLM+AL against\nstate-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0,\nand o1-preview, using benchmarks for complex reasoning about actions. Our\nfindings indicate that, although all methods exhibit errors, LLM+AL, with\nrelatively minimal human corrections, consistently leads to correct answers,\nwhereas standalone LLMs fail to improve even with human feedback. LLM+AL also\ncontributes to automated generation of action languages."
                },
                "authors": [
                    {
                        "name": "Adam Ishay"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01600v2",
                "updated": "2025-02-04T14:28:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    28,
                    50,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-03T18:35:42Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    35,
                    42,
                    0,
                    34,
                    0
                ],
                "title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Long-Horizon Interactive LLM Agents"
                },
                "summary": "Interactive digital agents (IDAs) leverage APIs of stateful digital\nenvironments to perform tasks in response to user requests. While IDAs powered\nby instruction-tuned large language models (LLMs) can react to feedback from\ninterface invocations in multi-step exchanges, they have not been trained in\ntheir respective digital environments. Prior methods accomplish less than half\nof tasks in sophisticated benchmarks such as AppWorld. We present a\nreinforcement learning (RL) approach that trains IDAs directly in their target\nenvironments. We formalize this training as a partially observable Markov\ndecision process and derive LOOP, a data- and memory-efficient variant of\nproximal policy optimization. LOOP uses no value network and maintains exactly\none copy of the underlying LLM in memory, making its implementation\nstraightforward and as memory-efficient as fine-tuning a single LLM. A\n32-billion-parameter agent trained with LOOP in the AppWorld environment\noutperforms the much larger OpenAI o1 agent by 9 percentage points (15%\nrelative). To our knowledge, this is the first reported application of RL to\nIDAs that interact with a stateful, multi-domain, multi-app environment via\ndirect API calls. Our analysis sheds light on the effectiveness of RL in this\narea, showing that the agent learns to consult the API documentation, avoid\nunwarranted assumptions, minimize confabulation, and recover from setbacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive digital agents (IDAs) leverage APIs of stateful digital\nenvironments to perform tasks in response to user requests. While IDAs powered\nby instruction-tuned large language models (LLMs) can react to feedback from\ninterface invocations in multi-step exchanges, they have not been trained in\ntheir respective digital environments. Prior methods accomplish less than half\nof tasks in sophisticated benchmarks such as AppWorld. We present a\nreinforcement learning (RL) approach that trains IDAs directly in their target\nenvironments. We formalize this training as a partially observable Markov\ndecision process and derive LOOP, a data- and memory-efficient variant of\nproximal policy optimization. LOOP uses no value network and maintains exactly\none copy of the underlying LLM in memory, making its implementation\nstraightforward and as memory-efficient as fine-tuning a single LLM. A\n32-billion-parameter agent trained with LOOP in the AppWorld environment\noutperforms the much larger OpenAI o1 agent by 9 percentage points (15%\nrelative). To our knowledge, this is the first reported application of RL to\nIDAs that interact with a stateful, multi-domain, multi-app environment via\ndirect API calls. Our analysis sheds light on the effectiveness of RL in this\narea, showing that the agent learns to consult the API documentation, avoid\nunwarranted assumptions, minimize confabulation, and recover from setbacks."
                },
                "authors": [
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Marco Cusumano-Towner"
                    },
                    {
                        "name": "Brody Huval"
                    },
                    {
                        "name": "Aleksei Petrenko"
                    },
                    {
                        "name": "Jackson Hamburger"
                    },
                    {
                        "name": "Vladlen Koltun"
                    },
                    {
                        "name": "Philipp Krähenbühl"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Krähenbühl"
                },
                "author": "Philipp Krähenbühl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02342v1",
                "updated": "2025-02-04T14:20:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    20,
                    51,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:20:51Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    20,
                    51,
                    1,
                    35,
                    0
                ],
                "title": "SHIELD: APT Detection and Intelligent Explanation Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHIELD: APT Detection and Intelligent Explanation Using LLM"
                },
                "summary": "Advanced persistent threats (APTs) are sophisticated cyber attacks that can\nremain undetected for extended periods, making their mitigation particularly\nchallenging. Given their persistence, significant effort is required to detect\nthem and respond effectively. Existing provenance-based attack detection\nmethods often lack interpretability and suffer from high false positive rates,\nwhile investigation approaches are either supervised or limited to known\nattacks. To address these challenges, we introduce SHIELD, a novel approach\nthat combines statistical anomaly detection and graph-based analysis with the\ncontextual analysis capabilities of large language models (LLMs). SHIELD\nleverages the implicit knowledge of LLMs to uncover hidden attack patterns in\nprovenance data, while reducing false positives and providing clear,\ninterpretable attack descriptions. This reduces analysts' alert fatigue and\nmakes it easier for them to understand the threat landscape. Our extensive\nevaluation demonstrates SHIELD's effectiveness and computational efficiency in\nreal-world scenarios. SHIELD was shown to outperform state-of-the-art methods,\nachieving higher precision and recall. SHIELD's integration of anomaly\ndetection, LLM-driven contextual analysis, and advanced graph-based correlation\nestablishes a new benchmark for APT detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced persistent threats (APTs) are sophisticated cyber attacks that can\nremain undetected for extended periods, making their mitigation particularly\nchallenging. Given their persistence, significant effort is required to detect\nthem and respond effectively. Existing provenance-based attack detection\nmethods often lack interpretability and suffer from high false positive rates,\nwhile investigation approaches are either supervised or limited to known\nattacks. To address these challenges, we introduce SHIELD, a novel approach\nthat combines statistical anomaly detection and graph-based analysis with the\ncontextual analysis capabilities of large language models (LLMs). SHIELD\nleverages the implicit knowledge of LLMs to uncover hidden attack patterns in\nprovenance data, while reducing false positives and providing clear,\ninterpretable attack descriptions. This reduces analysts' alert fatigue and\nmakes it easier for them to understand the threat landscape. Our extensive\nevaluation demonstrates SHIELD's effectiveness and computational efficiency in\nreal-world scenarios. SHIELD was shown to outperform state-of-the-art methods,\nachieving higher precision and recall. SHIELD's integration of anomaly\ndetection, LLM-driven contextual analysis, and advanced graph-based correlation\nestablishes a new benchmark for APT detection."
                },
                "authors": [
                    {
                        "name": "Parth Atulbhai Gandhi"
                    },
                    {
                        "name": "Prasanna N. Wudali"
                    },
                    {
                        "name": "Yonatan Amaru"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02337v1",
                "updated": "2025-02-04T14:16:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    16,
                    2,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:16:02Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    16,
                    2,
                    1,
                    35,
                    0
                ],
                "title": "Rule-ATT&CK Mapper (RAM): Mapping SIEM Rules to TTPs Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rule-ATT&CK Mapper (RAM): Mapping SIEM Rules to TTPs Using LLMs"
                },
                "summary": "The growing frequency of cyberattacks has heightened the demand for accurate\nand efficient threat detection systems. SIEM platforms are important for\nanalyzing log data and detecting adversarial activities through rule-based\nqueries, also known as SIEM rules. The efficiency of the threat analysis\nprocess relies heavily on mapping these SIEM rules to the relevant attack\ntechniques in the MITRE ATT&CK framework. Inaccurate annotation of SIEM rules\ncan result in the misinterpretation of attacks, increasing the likelihood that\nthreats will be overlooked. Existing solutions for annotating SIEM rules with\nMITRE ATT&CK technique labels have notable limitations: manual annotation of\nSIEM rules is both time-consuming and prone to errors, and ML-based approaches\nmainly focus on annotating unstructured free text sources rather than\nstructured data like SIEM rules. Structured data often contains limited\ninformation, further complicating the annotation process and making it a\nchallenging task. To address these challenges, we propose Rule-ATT&CK Mapper\n(RAM), a novel framework that leverages LLMs to automate the mapping of\nstructured SIEM rules to MITRE ATT&CK techniques. RAM's multi-stage pipeline,\nwhich was inspired by the prompt chaining technique, enhances mapping accuracy\nwithout requiring LLM pre-training or fine-tuning. Using the Splunk Security\nContent dataset, we evaluate RAM's performance using several LLMs, including\nGPT-4-Turbo, Qwen, IBM Granite, and Mistral. Our evaluation highlights\nGPT-4-Turbo's superior performance, which derives from its enriched knowledge\nbase, and an ablation study emphasizes the importance of external contextual\nknowledge in overcoming the limitations of LLMs' implicit knowledge for\ndomain-specific tasks. These findings demonstrate RAM's potential in automating\ncybersecurity workflows and provide valuable insights for future advancements\nin this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing frequency of cyberattacks has heightened the demand for accurate\nand efficient threat detection systems. SIEM platforms are important for\nanalyzing log data and detecting adversarial activities through rule-based\nqueries, also known as SIEM rules. The efficiency of the threat analysis\nprocess relies heavily on mapping these SIEM rules to the relevant attack\ntechniques in the MITRE ATT&CK framework. Inaccurate annotation of SIEM rules\ncan result in the misinterpretation of attacks, increasing the likelihood that\nthreats will be overlooked. Existing solutions for annotating SIEM rules with\nMITRE ATT&CK technique labels have notable limitations: manual annotation of\nSIEM rules is both time-consuming and prone to errors, and ML-based approaches\nmainly focus on annotating unstructured free text sources rather than\nstructured data like SIEM rules. Structured data often contains limited\ninformation, further complicating the annotation process and making it a\nchallenging task. To address these challenges, we propose Rule-ATT&CK Mapper\n(RAM), a novel framework that leverages LLMs to automate the mapping of\nstructured SIEM rules to MITRE ATT&CK techniques. RAM's multi-stage pipeline,\nwhich was inspired by the prompt chaining technique, enhances mapping accuracy\nwithout requiring LLM pre-training or fine-tuning. Using the Splunk Security\nContent dataset, we evaluate RAM's performance using several LLMs, including\nGPT-4-Turbo, Qwen, IBM Granite, and Mistral. Our evaluation highlights\nGPT-4-Turbo's superior performance, which derives from its enriched knowledge\nbase, and an ablation study emphasizes the importance of external contextual\nknowledge in overcoming the limitations of LLMs' implicit knowledge for\ndomain-specific tasks. These findings demonstrate RAM's potential in automating\ncybersecurity workflows and provide valuable insights for future advancements\nin this field."
                },
                "authors": [
                    {
                        "name": "Prasanna N. Wudali"
                    },
                    {
                        "name": "Moshe Kravchik"
                    },
                    {
                        "name": "Ehud Malul"
                    },
                    {
                        "name": "Parth A. Gandhi"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00029v2",
                "updated": "2025-02-04T14:15:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    15,
                    35,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-23T11:35:17Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    35,
                    17,
                    3,
                    23,
                    0
                ],
                "title": "AlphaSharpe: LLM-Driven Discovery of Robust Risk-Adjusted Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaSharpe: LLM-Driven Discovery of Robust Risk-Adjusted Metrics"
                },
                "summary": "Financial metrics like the Sharpe ratio are pivotal in evaluating investment\nperformance by balancing risk and return. However, traditional metrics often\nstruggle with robustness and generalization, particularly in dynamic and\nvolatile market conditions. This paper introduces AlphaSharpe, a novel\nframework leveraging large language models (LLMs) to iteratively evolve and\noptimize financial metrics to discover enhanced risk-return metrics that\noutperform traditional approaches in robustness and correlation with future\nperformance metrics by employing iterative crossover, mutation, and evaluation.\nKey contributions of this work include: (1) a novel use of LLMs to generate and\nrefine financial metrics with implicit domain-specific knowledge, (2) a scoring\nmechanism to ensure that evolved metrics generalize effectively to unseen data,\nand (3) an empirical demonstration of 3x predictive power for future\nrisk-returns, and 2x portfolio performance. Experimental results in a\nreal-world dataset highlight the superiority of discovered metrics, making them\nhighly relevant to portfolio managers and financial decision-makers. This\nframework not only addresses the limitations of existing metrics but also\nshowcases the potential of LLMs in advancing financial analytics, paving the\nway for informed and robust investment strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial metrics like the Sharpe ratio are pivotal in evaluating investment\nperformance by balancing risk and return. However, traditional metrics often\nstruggle with robustness and generalization, particularly in dynamic and\nvolatile market conditions. This paper introduces AlphaSharpe, a novel\nframework leveraging large language models (LLMs) to iteratively evolve and\noptimize financial metrics to discover enhanced risk-return metrics that\noutperform traditional approaches in robustness and correlation with future\nperformance metrics by employing iterative crossover, mutation, and evaluation.\nKey contributions of this work include: (1) a novel use of LLMs to generate and\nrefine financial metrics with implicit domain-specific knowledge, (2) a scoring\nmechanism to ensure that evolved metrics generalize effectively to unseen data,\nand (3) an empirical demonstration of 3x predictive power for future\nrisk-returns, and 2x portfolio performance. Experimental results in a\nreal-world dataset highlight the superiority of discovered metrics, making them\nhighly relevant to portfolio managers and financial decision-makers. This\nframework not only addresses the limitations of existing metrics but also\nshowcases the potential of LLMs in advancing financial analytics, paving the\nway for informed and robust investment strategies."
                },
                "authors": [
                    {
                        "name": "Kamer Ali Yuksel"
                    },
                    {
                        "name": "Hassan Sawaf"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sawaf"
                },
                "author": "Hassan Sawaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15127v2",
                "updated": "2025-02-04T14:01:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    1,
                    2,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-19T15:03:26Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    15,
                    3,
                    26,
                    5,
                    293,
                    0
                ],
                "title": "Reinfier and Reintrainer: Verification and Interpretation-Driven Safe\n  Deep Reinforcement Learning Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinfier and Reintrainer: Verification and Interpretation-Driven Safe\n  Deep Reinforcement Learning Frameworks"
                },
                "summary": "Ensuring verifiable and interpretable safety of deep reinforcement learning\n(DRL) is crucial for its deployment in real-world applications. Existing\napproaches like verification-in-the-loop training, however, face challenges\nsuch as difficulty in deployment, inefficient training, lack of\ninterpretability, and suboptimal performance in property satisfaction and\nreward performance. In this work, we propose a novel verification-driven\ninterpretation-in-the-loop framework Reintrainer to develop trustworthy DRL\nmodels, which are guaranteed to meet the expected constraint properties.\nSpecifically, in each iteration, this framework measures the gap between the\non-training model and predefined properties using formal verification,\ninterprets the contribution of each input feature to the model's output, and\nthen generates the training strategy derived from the on-the-fly measure\nresults, until all predefined properties are proven. Additionally, the low\nreusability of existing verifiers and interpreters motivates us to develop\nReinfier, a general and fundamental tool within Reintrainer for DRL\nverification and interpretation. Reinfier features breakpoints searching and\nverification-driven interpretation, associated with a concise\nconstraint-encoding language DRLP. Evaluations demonstrate that Reintrainer\noutperforms the state-of-the-art on six public benchmarks in both performance\nand property guarantees. Our framework can be accessed at\nhttps://github.com/Kurayuri/Reinfier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring verifiable and interpretable safety of deep reinforcement learning\n(DRL) is crucial for its deployment in real-world applications. Existing\napproaches like verification-in-the-loop training, however, face challenges\nsuch as difficulty in deployment, inefficient training, lack of\ninterpretability, and suboptimal performance in property satisfaction and\nreward performance. In this work, we propose a novel verification-driven\ninterpretation-in-the-loop framework Reintrainer to develop trustworthy DRL\nmodels, which are guaranteed to meet the expected constraint properties.\nSpecifically, in each iteration, this framework measures the gap between the\non-training model and predefined properties using formal verification,\ninterprets the contribution of each input feature to the model's output, and\nthen generates the training strategy derived from the on-the-fly measure\nresults, until all predefined properties are proven. Additionally, the low\nreusability of existing verifiers and interpreters motivates us to develop\nReinfier, a general and fundamental tool within Reintrainer for DRL\nverification and interpretation. Reinfier features breakpoints searching and\nverification-driven interpretation, associated with a concise\nconstraint-encoding language DRLP. Evaluations demonstrate that Reintrainer\noutperforms the state-of-the-art on six public benchmarks in both performance\nand property guarantees. Our framework can be accessed at\nhttps://github.com/Kurayuri/Reinfier."
                },
                "authors": [
                    {
                        "name": "Zixuan Yang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02329v1",
                "updated": "2025-02-04T14:00:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    0,
                    32,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:00:32Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    0,
                    32,
                    1,
                    35,
                    0
                ],
                "title": "ReSpark: Leveraging Previous Data Reports as References to Generate New\n  Reports with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSpark: Leveraging Previous Data Reports as References to Generate New\n  Reports with LLMs"
                },
                "summary": "Creating data reports is time-consuming, as it requires iterative exploration\nand understanding of data, followed by summarizing the insights. While large\nlanguage models (LLMs) are powerful tools for data processing and text\ngeneration, they often struggle to produce complete data reports that fully\nmeet user expectations. One significant challenge is effectively communicating\nthe entire analysis logic to LLMs. Moreover, determining a comprehensive\nanalysis logic can be mentally taxing for users. To address these challenges,\nwe propose ReSpark, an LLM-based method that leverages existing data reports as\nreferences for creating new ones. Given a data table, ReSpark searches for\nsimilar-topic reports, parses them into interdependent segments corresponding\nto analytical objectives, and executes them with new data. It identifies\ninconsistencies and customizes the objectives, data transformations, and\ntextual descriptions. ReSpark allows users to review real-time outputs, insert\nnew objectives, and modify report content. Its effectiveness was evaluated\nthrough comparative and user studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating data reports is time-consuming, as it requires iterative exploration\nand understanding of data, followed by summarizing the insights. While large\nlanguage models (LLMs) are powerful tools for data processing and text\ngeneration, they often struggle to produce complete data reports that fully\nmeet user expectations. One significant challenge is effectively communicating\nthe entire analysis logic to LLMs. Moreover, determining a comprehensive\nanalysis logic can be mentally taxing for users. To address these challenges,\nwe propose ReSpark, an LLM-based method that leverages existing data reports as\nreferences for creating new ones. Given a data table, ReSpark searches for\nsimilar-topic reports, parses them into interdependent segments corresponding\nto analytical objectives, and executes them with new data. It identifies\ninconsistencies and customizes the objectives, data transformations, and\ntextual descriptions. ReSpark allows users to review real-time outputs, insert\nnew objectives, and modify report content. Its effectiveness was evaluated\nthrough comparative and user studies."
                },
                "authors": [
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Chuhan Zhang"
                    },
                    {
                        "name": "Xiaotong Wang"
                    },
                    {
                        "name": "Sitong Pan"
                    },
                    {
                        "name": "Weiwei Cui"
                    },
                    {
                        "name": "Haidong Zhang"
                    },
                    {
                        "name": "Dazhen Deng"
                    },
                    {
                        "name": "Yingcai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yingcai Wu"
                },
                "author": "Yingcai Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14630v2",
                "updated": "2025-02-04T13:55:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    55,
                    19,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-24T16:49:08Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    49,
                    8,
                    4,
                    24,
                    0
                ],
                "title": "Extracting Problem Structure with LLMs for Optimized SAT Local Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Problem Structure with LLMs for Optimized SAT Local Search"
                },
                "summary": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems."
                },
                "authors": [
                    {
                        "name": "André Schidler"
                    },
                    {
                        "name": "Stefan Szeider"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Szeider"
                },
                "author": "Stefan Szeider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02315v1",
                "updated": "2025-02-04T13:36:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    36,
                    54,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T13:36:54Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    36,
                    54,
                    1,
                    35,
                    0
                ],
                "title": "VaiBot: Shuttle Between the Instructions and Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VaiBot: Shuttle Between the Instructions and Parameters"
                },
                "summary": "How to interact with LLMs through \\emph{instructions} has been widely studied\nby researchers. However, previous studies have treated the emergence of\ninstructions and the training of LLMs on task data as separate processes,\noverlooking the inherent unity between the two. This paper proposes a neural\nnetwork framework, VaiBot, that integrates VAE and VIB, designed to uniformly\nmodel, learn, and infer both deduction and induction tasks under LLMs. Through\nexperiments, we demonstrate that VaiBot performs on par with existing baseline\nmethods in terms of deductive capabilities while significantly surpassing them\nin inductive capabilities. We also find that VaiBot can scale up using general\ninstruction-following data and exhibits excellent one-shot induction abilities.\nWe finally synergistically integrate the deductive and inductive processes of\nVaiBot. Through T-SNE dimensionality reduction, we observe that its\ninductive-deductive process significantly improves the distribution of training\nparameters, enabling it to outperform baseline methods in inductive reasoning\ntasks. The code and data for this paper can be found at\nhttps://anonymous.4open.science/r/VaiBot-021F.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to interact with LLMs through \\emph{instructions} has been widely studied\nby researchers. However, previous studies have treated the emergence of\ninstructions and the training of LLMs on task data as separate processes,\noverlooking the inherent unity between the two. This paper proposes a neural\nnetwork framework, VaiBot, that integrates VAE and VIB, designed to uniformly\nmodel, learn, and infer both deduction and induction tasks under LLMs. Through\nexperiments, we demonstrate that VaiBot performs on par with existing baseline\nmethods in terms of deductive capabilities while significantly surpassing them\nin inductive capabilities. We also find that VaiBot can scale up using general\ninstruction-following data and exhibits excellent one-shot induction abilities.\nWe finally synergistically integrate the deductive and inductive processes of\nVaiBot. Through T-SNE dimensionality reduction, we observe that its\ninductive-deductive process significantly improves the distribution of training\nparameters, enabling it to outperform baseline methods in inductive reasoning\ntasks. The code and data for this paper can be found at\nhttps://anonymous.4open.science/r/VaiBot-021F."
                },
                "authors": [
                    {
                        "name": "Wangtao Sun"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Xuanqing Yu"
                    },
                    {
                        "name": "Zhongtao Jiang"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02304v1",
                "updated": "2025-02-04T13:18:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    18,
                    51,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T13:18:51Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    18,
                    51,
                    1,
                    35,
                    0
                ],
                "title": "Comparative Analysis of FPGA and GPU Performance for Machine\n  Learning-Based Track Reconstruction at LHCb",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of FPGA and GPU Performance for Machine\n  Learning-Based Track Reconstruction at LHCb"
                },
                "summary": "In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-energy physics, the increasing luminosity and detector granularity at\nthe Large Hadron Collider are driving the need for more efficient data\nprocessing solutions. Machine Learning has emerged as a promising tool for\nreconstructing charged particle tracks, due to its potentially linear\ncomputational scaling with detector hits. The recent implementation of a graph\nneural network-based track reconstruction pipeline in the first level trigger\nof the LHCb experiment on GPUs serves as a platform for comparative studies\nbetween computational architectures in the context of high-energy physics. This\npaper presents a novel comparison of the throughput of ML model inference\nbetween FPGAs and GPUs, focusing on the first step of the track reconstruction\npipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using\nHLS4ML for FPGA deployment, we benchmark its performance against the GPU\nimplementation and demonstrate the potential of FPGAs for high-throughput,\nlow-latency inference without the need for an expertise in FPGA development and\nwhile consuming significantly less power."
                },
                "authors": [
                    {
                        "name": "Fotis I. Giasemis"
                    },
                    {
                        "name": "Vladimir Lončar"
                    },
                    {
                        "name": "Bertrand Granado"
                    },
                    {
                        "name": "Vladimir Vava Gligorov"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Vava Gligorov"
                },
                "author": "Vladimir Vava Gligorov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02289v1",
                "updated": "2025-02-04T12:58:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    58,
                    19,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T12:58:19Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    58,
                    19,
                    1,
                    35,
                    0
                ],
                "title": "Evalita-LLM: Benchmarking Large Language Models on Italian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evalita-LLM: Benchmarking Large Language Models on Italian"
                },
                "summary": "We describe Evalita-LLM, a new benchmark designed to evaluate Large Language\nModels (LLMs) on Italian tasks. The distinguishing and innovative features of\nEvalita-LLM are the following: (i) all tasks are native Italian, avoiding\nissues of translating from Italian and potential cultural biases; (ii) in\naddition to well established multiple-choice tasks, the benchmark includes\ngenerative tasks, enabling more natural interaction with LLMs; (iii) all tasks\nare evaluated against multiple prompts, this way mitigating the model\nsensitivity to specific prompts and allowing a fairer and objective evaluation.\nWe propose an iterative methodology, where candidate tasks and candidate\nprompts are validated against a set of LLMs used for development. We report\nexperimental results from the benchmark's development phase, and provide\nperformance statistics for several state-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Evalita-LLM, a new benchmark designed to evaluate Large Language\nModels (LLMs) on Italian tasks. The distinguishing and innovative features of\nEvalita-LLM are the following: (i) all tasks are native Italian, avoiding\nissues of translating from Italian and potential cultural biases; (ii) in\naddition to well established multiple-choice tasks, the benchmark includes\ngenerative tasks, enabling more natural interaction with LLMs; (iii) all tasks\nare evaluated against multiple prompts, this way mitigating the model\nsensitivity to specific prompts and allowing a fairer and objective evaluation.\nWe propose an iterative methodology, where candidate tasks and candidate\nprompts are validated against a set of LLMs used for development. We report\nexperimental results from the benchmark's development phase, and provide\nperformance statistics for several state-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Bernardo Magnini"
                    },
                    {
                        "name": "Roberto Zanoli"
                    },
                    {
                        "name": "Michele Resta"
                    },
                    {
                        "name": "Martin Cimmino"
                    },
                    {
                        "name": "Paolo Albano"
                    },
                    {
                        "name": "Marco Madeddu"
                    },
                    {
                        "name": "Viviana Patti"
                    }
                ],
                "author_detail": {
                    "name": "Viviana Patti"
                },
                "author": "Viviana Patti",
                "arxiv_comment": "42 pages, 1 figure, 32 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02287v1",
                "updated": "2025-02-04T12:56:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    56,
                    59,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T12:56:59Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    56,
                    59,
                    1,
                    35,
                    0
                ],
                "title": "Adaptive Resource Allocation Optimization Using Large Language Models in\n  Dynamic Wireless Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Resource Allocation Optimization Using Large Language Models in\n  Dynamic Wireless Environments"
                },
                "summary": "Deep learning (DL) has made notable progress in addressing complex radio\naccess network control challenges that conventional analytic methods have\nstruggled to solve. However, DL has shown limitations in solving constrained\nNP-hard problems often encountered in network optimization, such as those\ninvolving quality of service (QoS) or discrete variables like user indices.\nCurrent solutions rely on domain-specific architectures or heuristic\ntechniques, and a general DL approach for constrained optimization remains\nundeveloped. Moreover, even minor changes in communication objectives demand\ntime-consuming retraining, limiting their adaptability to dynamic environments\nwhere task objectives, constraints, environmental factors, and communication\nscenarios frequently change. To address these challenges, we propose a large\nlanguage model for resource allocation optimizer (LLM-RAO), a novel approach\nthat harnesses the capabilities of LLMs to address the complex resource\nallocation problem while adhering to QoS constraints. By employing a\nprompt-based tuning strategy to flexibly convey ever-changing task descriptions\nand requirements to the LLM, LLM-RAO demonstrates robust performance and\nseamless adaptability in dynamic environments without requiring extensive\nretraining. Simulation results reveal that LLM-RAO achieves up to a 40%\nperformance enhancement compared to conventional DL methods and up to an $80$\\%\nimprovement over analytical approaches. Moreover, in scenarios with fluctuating\ncommunication objectives, LLM-RAO attains up to 2.9 times the performance of\ntraditional DL-based networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) has made notable progress in addressing complex radio\naccess network control challenges that conventional analytic methods have\nstruggled to solve. However, DL has shown limitations in solving constrained\nNP-hard problems often encountered in network optimization, such as those\ninvolving quality of service (QoS) or discrete variables like user indices.\nCurrent solutions rely on domain-specific architectures or heuristic\ntechniques, and a general DL approach for constrained optimization remains\nundeveloped. Moreover, even minor changes in communication objectives demand\ntime-consuming retraining, limiting their adaptability to dynamic environments\nwhere task objectives, constraints, environmental factors, and communication\nscenarios frequently change. To address these challenges, we propose a large\nlanguage model for resource allocation optimizer (LLM-RAO), a novel approach\nthat harnesses the capabilities of LLMs to address the complex resource\nallocation problem while adhering to QoS constraints. By employing a\nprompt-based tuning strategy to flexibly convey ever-changing task descriptions\nand requirements to the LLM, LLM-RAO demonstrates robust performance and\nseamless adaptability in dynamic environments without requiring extensive\nretraining. Simulation results reveal that LLM-RAO achieves up to a 40%\nperformance enhancement compared to conventional DL methods and up to an $80$\\%\nimprovement over analytical approaches. Moreover, in scenarios with fluctuating\ncommunication objectives, LLM-RAO attains up to 2.9 times the performance of\ntraditional DL-based networks."
                },
                "authors": [
                    {
                        "name": "Hyeonho Noh"
                    },
                    {
                        "name": "Byonghyo Shim"
                    },
                    {
                        "name": "Hyun Jong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Jong Yang"
                },
                "author": "Hyun Jong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02269v1",
                "updated": "2025-02-04T12:29:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    29,
                    29,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T12:29:29Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    29,
                    29,
                    1,
                    35,
                    0
                ],
                "title": "Survey of Quantization Techniques for On-Device Vision-based Crack\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey of Quantization Techniques for On-Device Vision-based Crack\n  Detection"
                },
                "summary": "Structural Health Monitoring (SHM) ensures the safety and longevity of\ninfrastructure by enabling timely damage detection. Vision-based crack\ndetection, combined with UAVs, addresses the limitations of traditional\nsensor-based SHM methods but requires the deployment of efficient deep learning\nmodels on resource-constrained devices. This study evaluates two lightweight\nconvolutional neural network models, MobileNetV1x0.25 and MobileNetV2x0.5,\nacross TensorFlow, PyTorch, and Open Neural Network Exchange platforms using\nthree quantization techniques: dynamic quantization, post-training quantization\n(PTQ), and quantization-aware training (QAT). Results show that QAT\nconsistently achieves near-floating-point accuracy, such as an F1-score of\n0.8376 for MBNV2x0.5 with Torch-QAT, while maintaining efficient resource\nusage. PTQ significantly reduces memory and energy consumption but suffers from\naccuracy loss, particularly in TensorFlow. Dynamic quantization preserves\naccuracy but faces deployment challenges on PyTorch. By leveraging QAT, this\nwork enables real-time, low-power crack detection on UAVs, enhancing safety,\nscalability, and cost-efficiency in SHM applications, while providing insights\ninto balancing accuracy and efficiency across different platforms for\nautonomous inspections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural Health Monitoring (SHM) ensures the safety and longevity of\ninfrastructure by enabling timely damage detection. Vision-based crack\ndetection, combined with UAVs, addresses the limitations of traditional\nsensor-based SHM methods but requires the deployment of efficient deep learning\nmodels on resource-constrained devices. This study evaluates two lightweight\nconvolutional neural network models, MobileNetV1x0.25 and MobileNetV2x0.5,\nacross TensorFlow, PyTorch, and Open Neural Network Exchange platforms using\nthree quantization techniques: dynamic quantization, post-training quantization\n(PTQ), and quantization-aware training (QAT). Results show that QAT\nconsistently achieves near-floating-point accuracy, such as an F1-score of\n0.8376 for MBNV2x0.5 with Torch-QAT, while maintaining efficient resource\nusage. PTQ significantly reduces memory and energy consumption but suffers from\naccuracy loss, particularly in TensorFlow. Dynamic quantization preserves\naccuracy but faces deployment challenges on PyTorch. By leveraging QAT, this\nwork enables real-time, low-power crack detection on UAVs, enhancing safety,\nscalability, and cost-efficiency in SHM applications, while providing insights\ninto balancing accuracy and efficiency across different platforms for\nautonomous inspections."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Luciano Sebastian Martinez-Rau"
                    },
                    {
                        "name": "Quynh Nguyen Phuong Vu"
                    },
                    {
                        "name": "Bengt Oelmann"
                    },
                    {
                        "name": "Sebastian Bader"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Bader"
                },
                "author": "Sebastian Bader",
                "arxiv_comment": "Accepted by IEEE International Instrumentation and Measurement\n  Technology Conference (I2MTC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19871v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19871v5",
                "updated": "2025-02-04T12:25:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    25,
                    48,
                    1,
                    35,
                    0
                ],
                "published": "2024-03-28T22:45:38Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    22,
                    45,
                    38,
                    3,
                    88,
                    0
                ],
                "title": "Towards Stable Machine Learning Model Retraining via Slowly Varying\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stable Machine Learning Model Retraining via Slowly Varying\n  Sequences"
                },
                "summary": "We consider the problem of retraining machine learning (ML) models when new\nbatches of data become available. Existing approaches greedily optimize for\npredictive power independently at each batch, without considering the stability\nof the model's structure or analytical insights across retraining iterations.\nWe propose a model-agnostic framework for finding sequences of models that are\nstable across retraining iterations. We develop a mixed-integer optimization\nformulation that is guaranteed to recover Pareto optimal models (in terms of\nthe predictive power-stability trade-off) with good generalization properties,\nas well as an efficient polynomial-time algorithm that performs well in\npractice. We focus on retaining consistent analytical insights-which is\nimportant to model interpretability, ease of implementation, and fostering\ntrust with users-by using custom-defined distance metrics that can be directly\nincorporated into the optimization problem. We evaluate our framework across\nmodels (regression, decision trees, boosted trees, and neural networks) and\napplication domains (healthcare, vision, and language), including deployment in\na production pipeline at a major US hospital. We find that, on average, a 2%\nreduction in predictive power leads to a 30% improvement in stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of retraining machine learning (ML) models when new\nbatches of data become available. Existing approaches greedily optimize for\npredictive power independently at each batch, without considering the stability\nof the model's structure or analytical insights across retraining iterations.\nWe propose a model-agnostic framework for finding sequences of models that are\nstable across retraining iterations. We develop a mixed-integer optimization\nformulation that is guaranteed to recover Pareto optimal models (in terms of\nthe predictive power-stability trade-off) with good generalization properties,\nas well as an efficient polynomial-time algorithm that performs well in\npractice. We focus on retaining consistent analytical insights-which is\nimportant to model interpretability, ease of implementation, and fostering\ntrust with users-by using custom-defined distance metrics that can be directly\nincorporated into the optimization problem. We evaluate our framework across\nmodels (regression, decision trees, boosted trees, and neural networks) and\napplication domains (healthcare, vision, and language), including deployment in\na production pipeline at a major US hospital. We find that, on average, a 2%\nreduction in predictive power leads to a 30% improvement in stability."
                },
                "authors": [
                    {
                        "name": "Dimitris Bertsimas"
                    },
                    {
                        "name": "Vassilis Digalakis Jr"
                    },
                    {
                        "name": "Yu Ma"
                    },
                    {
                        "name": "Phevos Paschalidis"
                    }
                ],
                "author_detail": {
                    "name": "Phevos Paschalidis"
                },
                "author": "Phevos Paschalidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19871v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19871v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02260v1",
                "updated": "2025-02-04T12:17:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    17,
                    8,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T12:17:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    17,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "Adversarial ML Problems Are Getting Harder to Solve and to Evaluate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial ML Problems Are Getting Harder to Solve and to Evaluate"
                },
                "summary": "In the past decade, considerable research effort has been devoted to securing\nmachine learning (ML) models that operate in adversarial settings. Yet,\nprogress has been slow even for simple \"toy\" problems (e.g., robustness to\nsmall adversarial perturbations) and is often hindered by non-rigorous\nevaluations. Today, adversarial ML research has shifted towards studying\nlarger, general-purpose language models. In this position paper, we argue that\nthe situation is now even worse: in the era of LLMs, the field of adversarial\nML studies problems that are (1) less clearly defined, (2) harder to solve, and\n(3) even more challenging to evaluate. As a result, we caution that yet another\ndecade of work on adversarial ML may fail to produce meaningful progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past decade, considerable research effort has been devoted to securing\nmachine learning (ML) models that operate in adversarial settings. Yet,\nprogress has been slow even for simple \"toy\" problems (e.g., robustness to\nsmall adversarial perturbations) and is often hindered by non-rigorous\nevaluations. Today, adversarial ML research has shifted towards studying\nlarger, general-purpose language models. In this position paper, we argue that\nthe situation is now even worse: in the era of LLMs, the field of adversarial\nML studies problems that are (1) less clearly defined, (2) harder to solve, and\n(3) even more challenging to evaluate. As a result, we caution that yet another\ndecade of work on adversarial ML may fail to produce meaningful progress."
                },
                "authors": [
                    {
                        "name": "Javier Rando"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Florian Tramèr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramèr"
                },
                "author": "Florian Tramèr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02253v1",
                "updated": "2025-02-04T11:59:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    59,
                    58,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T11:59:58Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    59,
                    58,
                    1,
                    35,
                    0
                ],
                "title": "Network Digital Twin for 5G-Enabled Mobile Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network Digital Twin for 5G-Enabled Mobile Robots"
                },
                "summary": "The maturity and commercial roll-out of 5G networks and its deployment for\nprivate networks makes 5G a key enabler for various vertical industries and\napplications, including robotics. Providing ultra-low latency, high data rates,\nand ubiquitous coverage and wireless connectivity, 5G fully unlocks the\npotential of robot autonomy and boosts emerging robotic applications,\nparticularly in the domain of autonomous mobile robots. Ensuring seamless,\nefficient, and reliable navigation and operation of robots within a 5G network\nrequires a clear understanding of the expected network quality in the\ndeployment environment. However, obtaining real-time insights into network\nconditions, particularly in highly dynamic environments, presents a significant\nand practical challenge. In this paper, we present a novel framework for\nbuilding a Network Digital Twin (NDT) using real-time data collected by robots.\nThis framework provides a comprehensive solution for monitoring, controlling,\nand optimizing robotic operations in dynamic network environments. We develop a\npipeline integrating robotic data into the NDT, demonstrating its evolution\nwith real-world robotic traces. We evaluate its performances in radio-aware\nnavigation use case, highlighting its potential to enhance energy efficiency\nand reliability for 5Genabled robotic operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maturity and commercial roll-out of 5G networks and its deployment for\nprivate networks makes 5G a key enabler for various vertical industries and\napplications, including robotics. Providing ultra-low latency, high data rates,\nand ubiquitous coverage and wireless connectivity, 5G fully unlocks the\npotential of robot autonomy and boosts emerging robotic applications,\nparticularly in the domain of autonomous mobile robots. Ensuring seamless,\nefficient, and reliable navigation and operation of robots within a 5G network\nrequires a clear understanding of the expected network quality in the\ndeployment environment. However, obtaining real-time insights into network\nconditions, particularly in highly dynamic environments, presents a significant\nand practical challenge. In this paper, we present a novel framework for\nbuilding a Network Digital Twin (NDT) using real-time data collected by robots.\nThis framework provides a comprehensive solution for monitoring, controlling,\nand optimizing robotic operations in dynamic network environments. We develop a\npipeline integrating robotic data into the NDT, demonstrating its evolution\nwith real-world robotic traces. We evaluate its performances in radio-aware\nnavigation use case, highlighting its potential to enhance energy efficiency\nand reliability for 5Genabled robotic operations."
                },
                "authors": [
                    {
                        "name": "Luis Roda Sanchez"
                    },
                    {
                        "name": "Lanfranco Zanzi"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Guillem Gari"
                    },
                    {
                        "name": "Xavier Costa Perez"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Costa Perez"
                },
                "author": "Xavier Costa Perez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02249v1",
                "updated": "2025-02-04T11:50:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    50,
                    40,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T11:50:40Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    50,
                    40,
                    1,
                    35,
                    0
                ],
                "title": "Conversation AI Dialog for Medicare powered by Finetuning and Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation AI Dialog for Medicare powered by Finetuning and Retrieval\n  Augmented Generation"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage processing tasks, including dialogue generation. This research aims to\nconduct a novel comparative analysis of two prominent techniques, fine-tuning\nwith LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG)\nframework, in the context of doctor-patient chat conversations with multiple\ndatasets of mixed medical domains. The analysis involves three state-of-the-art\nmodels: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient\ndialogues, we comprehensively evaluate the performance of models, assessing key\nmetrics such as language quality (perplexity, BLEU score), factual accuracy\n(fact-checking against medical knowledge bases), adherence to medical\nguidelines, and overall human judgments (coherence, empathy, safety). The\nfindings provide insights into the strengths and limitations of each approach,\nshedding light on their suitability for healthcare applications. Furthermore,\nthe research investigates the robustness of the models in handling diverse\npatient queries, ranging from general health inquiries to specific medical\nconditions. The impact of domain-specific knowledge integration is also\nexplored, highlighting the potential for enhancing LLM performance through\ntargeted data augmentation and retrieval strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage processing tasks, including dialogue generation. This research aims to\nconduct a novel comparative analysis of two prominent techniques, fine-tuning\nwith LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG)\nframework, in the context of doctor-patient chat conversations with multiple\ndatasets of mixed medical domains. The analysis involves three state-of-the-art\nmodels: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient\ndialogues, we comprehensively evaluate the performance of models, assessing key\nmetrics such as language quality (perplexity, BLEU score), factual accuracy\n(fact-checking against medical knowledge bases), adherence to medical\nguidelines, and overall human judgments (coherence, empathy, safety). The\nfindings provide insights into the strengths and limitations of each approach,\nshedding light on their suitability for healthcare applications. Furthermore,\nthe research investigates the robustness of the models in handling diverse\npatient queries, ranging from general health inquiries to specific medical\nconditions. The impact of domain-specific knowledge integration is also\nexplored, highlighting the potential for enhancing LLM performance through\ntargeted data augmentation and retrieval strategies."
                },
                "authors": [
                    {
                        "name": "Atharva Mangeshkumar Agrawal"
                    },
                    {
                        "name": "Rutika Pandurang Shinde"
                    },
                    {
                        "name": "Vasanth Kumar Bhukya"
                    },
                    {
                        "name": "Ashmita Chakraborty"
                    },
                    {
                        "name": "Sagar Bharat Shah"
                    },
                    {
                        "name": "Tanmay Shukla"
                    },
                    {
                        "name": "Sree Pradeep Kumar Relangi"
                    },
                    {
                        "name": "Nilesh Mutyam"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Mutyam"
                },
                "author": "Nilesh Mutyam",
                "arxiv_comment": "12 pages",
                "arxiv_journal_ref": "ResMilitaris 2022",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00791v2",
                "updated": "2025-02-04T11:45:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    45,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-02T13:10:06Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    13,
                    10,
                    6,
                    6,
                    33,
                    0
                ],
                "title": "Vision-centric Token Compression in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-centric Token Compression in Large Language Model"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nexcelling in handling longer sequences. However, the inefficiency and\nredundancy in processing extended in-context tokens remain a challenge. Many\nattempts to address this rely on compressing tokens with smaller text encoders,\nyet we question whether text encoders are truly indispensable. Our journey\nleads to an unexpected discovery-a much smaller vision encoder, applied\ndirectly to sequences of text tokens, can rival text encoders on text tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small text understanding benchmarks, VIST leads to comparable results with\n16% fewer FLOPs and 50% less memory usage. We further uncover significant token\nredundancy and devise a frequency-based masking strategy to guide the focus of\nthe visual encoder toward the most critical tokens. Interestingly, we observe\nthe trained visual encoder performs like a summarizer, selectively ignoring\nless important words such as prepositions and conjunctions. This approach\ndelivers remarkable results, outperforming traditional text encoder-based\nmethods by 5.7% on average over benchmarks like TriviaQA, NQ, PopQA, TREF,\nSST2, and SST5, setting a new standard for token efficiency in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nexcelling in handling longer sequences. However, the inefficiency and\nredundancy in processing extended in-context tokens remain a challenge. Many\nattempts to address this rely on compressing tokens with smaller text encoders,\nyet we question whether text encoders are truly indispensable. Our journey\nleads to an unexpected discovery-a much smaller vision encoder, applied\ndirectly to sequences of text tokens, can rival text encoders on text tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small text understanding benchmarks, VIST leads to comparable results with\n16% fewer FLOPs and 50% less memory usage. We further uncover significant token\nredundancy and devise a frequency-based masking strategy to guide the focus of\nthe visual encoder toward the most critical tokens. Interestingly, we observe\nthe trained visual encoder performs like a summarizer, selectively ignoring\nless important words such as prepositions and conjunctions. This approach\ndelivers remarkable results, outperforming traditional text encoder-based\nmethods by 5.7% on average over benchmarks like TriviaQA, NQ, PopQA, TREF,\nSST2, and SST5, setting a new standard for token efficiency in LLMs."
                },
                "authors": [
                    {
                        "name": "Ling Xing"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Jinhui Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jinhui Tang"
                },
                "author": "Jinhui Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02238v1",
                "updated": "2025-02-04T11:27:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    27,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T11:27:31Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    11,
                    27,
                    31,
                    1,
                    35,
                    0
                ],
                "title": "Using ChatGPT to refine draft conceptual schemata in supply-driven\n  design of multidimensional cubes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using ChatGPT to refine draft conceptual schemata in supply-driven\n  design of multidimensional cubes"
                },
                "summary": "Refinement is a critical step in supply-driven conceptual design of\nmultidimensional cubes because it can hardly be automated. In fact, it includes\nsteps such as the labeling of attributes as descriptive and the removal of\nuninteresting attributes, thus relying on the end-users' requirements on the\none hand, and on the semantics of measures, dimensions, and attributes on the\nother. As a consequence, it is normally carried out manually by designers in\nclose collaboration with end-users. The goal of this work is to check whether\nLLMs can act as facilitators for the refinement task, so as to let it be\ncarried out entirely -- or mostly -- by end-users. The Dimensional Fact Model\nis the target formalism for our study; as a representative LLM, we use\nChatGPT's model GPT-4o. To achieve our goal, we formulate three research\nquestions aimed at (i) understanding the basic competences of ChatGPT in\nmultidimensional modeling; (ii) understanding the basic competences of ChatGPT\nin refinement; and (iii) investigating if the latter can be improved via prompt\nengineering. The results of our experiments show that, indeed, a careful prompt\nengineering can significantly improve the accuracy of refinement, and that the\nresidual errors can quickly be fixed via one additional prompt. However, we\nconclude that, at present, some involvement of designers in refinement is still\nnecessary to ensure the validity of the refined schemata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refinement is a critical step in supply-driven conceptual design of\nmultidimensional cubes because it can hardly be automated. In fact, it includes\nsteps such as the labeling of attributes as descriptive and the removal of\nuninteresting attributes, thus relying on the end-users' requirements on the\none hand, and on the semantics of measures, dimensions, and attributes on the\nother. As a consequence, it is normally carried out manually by designers in\nclose collaboration with end-users. The goal of this work is to check whether\nLLMs can act as facilitators for the refinement task, so as to let it be\ncarried out entirely -- or mostly -- by end-users. The Dimensional Fact Model\nis the target formalism for our study; as a representative LLM, we use\nChatGPT's model GPT-4o. To achieve our goal, we formulate three research\nquestions aimed at (i) understanding the basic competences of ChatGPT in\nmultidimensional modeling; (ii) understanding the basic competences of ChatGPT\nin refinement; and (iii) investigating if the latter can be improved via prompt\nengineering. The results of our experiments show that, indeed, a careful prompt\nengineering can significantly improve the accuracy of refinement, and that the\nresidual errors can quickly be fixed via one additional prompt. However, we\nconclude that, at present, some involvement of designers in refinement is still\nnecessary to ensure the validity of the refined schemata."
                },
                "authors": [
                    {
                        "name": "Stefano Rizzi"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Rizzi"
                },
                "author": "Stefano Rizzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11517v2",
                "updated": "2025-02-04T10:52:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    52,
                    2,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T07:51:09Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    51,
                    9,
                    0,
                    351,
                    0
                ],
                "title": "DART: An AIGT Detector using AMR of Rephrased Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: An AIGT Detector using AMR of Rephrased Text"
                },
                "summary": "As large language models (LLMs) generate more human-like texts, concerns\nabout the side effects of AI-generated texts (AIGT) have grown. So, researchers\nhave developed methods for detecting AIGT. However, two challenges remain.\nFirst, the performance of detecting black-box LLMs is low because existing\nmodels focus on probabilistic features. Second, most AIGT detectors have been\ntested on a single-candidate setting, which assumes that we know the origin of\nan AIGT and which may deviate from the real-world scenario. To resolve these\nchallenges, we propose DART, which consists of four steps: rephrasing, semantic\nparsing, scoring, and multiclass classification. We conducted three experiments\nto test the performance of DART. The experimental result shows that DART can\ndiscriminate multiple black-box LLMs without probabilistic features and the\norigin of AIGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) generate more human-like texts, concerns\nabout the side effects of AI-generated texts (AIGT) have grown. So, researchers\nhave developed methods for detecting AIGT. However, two challenges remain.\nFirst, the performance of detecting black-box LLMs is low because existing\nmodels focus on probabilistic features. Second, most AIGT detectors have been\ntested on a single-candidate setting, which assumes that we know the origin of\nan AIGT and which may deviate from the real-world scenario. To resolve these\nchallenges, we propose DART, which consists of four steps: rephrasing, semantic\nparsing, scoring, and multiclass classification. We conducted three experiments\nto test the performance of DART. The experimental result shows that DART can\ndiscriminate multiple black-box LLMs without probabilistic features and the\norigin of AIGT."
                },
                "authors": [
                    {
                        "name": "Hyeonchu Park"
                    },
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Presented in NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02211v1",
                "updated": "2025-02-04T10:49:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    49,
                    23,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:49:23Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    49,
                    23,
                    1,
                    35,
                    0
                ],
                "title": "Tadah! A Swiss Army Knife for Developing and Deployment of Machine\n  Learning Interatomic Potentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tadah! A Swiss Army Knife for Developing and Deployment of Machine\n  Learning Interatomic Potentials"
                },
                "summary": "The Tadah! code provides a versatile platform for developing and optimizing\nMachine Learning Interatomic Potentials (MLIPs). By integrating composite\ndescriptors, it allows for a nuanced representation of system interactions,\ncustomized with unique cutoff functions and interaction distances. Tadah!\nsupports Bayesian Linear Regression (BLR) and Kernel Ridge Regression (KRR) to\nenhance model accuracy and uncertainty management. A key feature is its\nhyperparameter optimization cycle, iteratively refining model architecture to\nimprove transferability. This approach incorporates performance constraints,\naligning predictions with experimental and theoretical data. Tadah! provides an\ninterface for LAMMPS, enabling the deployment of MLIPs in molecular dynamics\nsimulations. It is designed for broad accessibility, supporting parallel\ncomputations on desktop and HPC systems. Tadah! leverages a modular C++\ncodebase, utilizing both compile-time and runtime polymorphism for flexibility\nand efficiency. Neural network support and predefined bonding schemes are\npotential future developments, and Tadah! remains open to community-driven\nfeature expansion. Comprehensive documentation and command-line tools further\nstreamline the development and application of MLIPs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Tadah! code provides a versatile platform for developing and optimizing\nMachine Learning Interatomic Potentials (MLIPs). By integrating composite\ndescriptors, it allows for a nuanced representation of system interactions,\ncustomized with unique cutoff functions and interaction distances. Tadah!\nsupports Bayesian Linear Regression (BLR) and Kernel Ridge Regression (KRR) to\nenhance model accuracy and uncertainty management. A key feature is its\nhyperparameter optimization cycle, iteratively refining model architecture to\nimprove transferability. This approach incorporates performance constraints,\naligning predictions with experimental and theoretical data. Tadah! provides an\ninterface for LAMMPS, enabling the deployment of MLIPs in molecular dynamics\nsimulations. It is designed for broad accessibility, supporting parallel\ncomputations on desktop and HPC systems. Tadah! leverages a modular C++\ncodebase, utilizing both compile-time and runtime polymorphism for flexibility\nand efficiency. Neural network support and predefined bonding schemes are\npotential future developments, and Tadah! remains open to community-driven\nfeature expansion. Comprehensive documentation and command-line tools further\nstreamline the development and application of MLIPs."
                },
                "authors": [
                    {
                        "name": "M. Kirsz"
                    },
                    {
                        "name": "A. Daramola"
                    },
                    {
                        "name": "A. Hermann"
                    },
                    {
                        "name": "H. Zong"
                    },
                    {
                        "name": "G. J. Ackland"
                    }
                ],
                "author_detail": {
                    "name": "G. J. Ackland"
                },
                "author": "G. J. Ackland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02201v1",
                "updated": "2025-02-04T10:27:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    27,
                    40,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:27:40Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    27,
                    40,
                    1,
                    35,
                    0
                ],
                "title": "Can You Move These Over There? An LLM-based VR Mover for Supporting\n  Object Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Move These Over There? An LLM-based VR Mover for Supporting\n  Object Manipulation"
                },
                "summary": "In our daily lives, we can naturally convey instructions for the spatial\nmanipulation of objects using words and gestures. Transposing this form of\ninteraction into virtual reality (VR) object manipulation can be beneficial. We\npropose VR Mover, an LLM-empowered solution that can understand and interpret\nthe user's vocal instruction to support object manipulation. By simply pointing\nand speaking, the LLM can manipulate objects without structured input. Our user\nstudy demonstrates that VR Mover enhances user usability, overall experience\nand performance on multi-object manipulation, while also reducing workload and\narm fatigue. Users prefer the proposed natural interface for broad movements\nand may complementarily switch to gizmos or virtual hands for finer\nadjustments. These findings are believed to contribute to design implications\nfor future LLM-based object manipulation interfaces, highlighting the potential\nfor more intuitive and efficient user interactions in VR environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our daily lives, we can naturally convey instructions for the spatial\nmanipulation of objects using words and gestures. Transposing this form of\ninteraction into virtual reality (VR) object manipulation can be beneficial. We\npropose VR Mover, an LLM-empowered solution that can understand and interpret\nthe user's vocal instruction to support object manipulation. By simply pointing\nand speaking, the LLM can manipulate objects without structured input. Our user\nstudy demonstrates that VR Mover enhances user usability, overall experience\nand performance on multi-object manipulation, while also reducing workload and\narm fatigue. Users prefer the proposed natural interface for broad movements\nand may complementarily switch to gizmos or virtual hands for finer\nadjustments. These findings are believed to contribute to design implications\nfor future LLM-based object manipulation interfaces, highlighting the potential\nfor more intuitive and efficient user interactions in VR environments."
                },
                "authors": [
                    {
                        "name": "Xiangzhi Eric Wang"
                    },
                    {
                        "name": "Zackary P. T. Sin"
                    },
                    {
                        "name": "Ye Jia"
                    },
                    {
                        "name": "Daniel Archer"
                    },
                    {
                        "name": "Wynonna H. Y. Fong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Chen Li"
                    }
                ],
                "author_detail": {
                    "name": "Chen Li"
                },
                "author": "Chen Li",
                "arxiv_comment": "64 pages (30 in main text), 22 figures (19 in main text)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02199v1",
                "updated": "2025-02-04T10:23:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    23,
                    11,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:23:11Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    23,
                    11,
                    1,
                    35,
                    0
                ],
                "title": "When Dimensionality Hurts: The Role of LLM Embedding Compression for\n  Noisy Regression Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Dimensionality Hurts: The Role of LLM Embedding Compression for\n  Noisy Regression Tasks"
                },
                "summary": "Large language models (LLMs) have shown remarkable success in language\nmodelling due to scaling laws found in model size and the hidden dimension of\nthe model's text representation. Yet, we demonstrate that compressed\nrepresentations of text can yield better performance in LLM-based regression\ntasks. In this paper, we compare the relative performance of embedding\ncompression in three different signal-to-noise contexts: financial return\nprediction, writing quality assessment and review scoring. Our results show\nthat compressing embeddings, in a minimally supervised manner using an\nautoencoder's hidden representation, can mitigate overfitting and improve\nperformance on noisy tasks, such as financial return prediction; but that\ncompression reduces performance on tasks that have high causal dependencies\nbetween the input and target data. Our results suggest that the success of\ninterpretable compressed representations such as sentiment may be due to a\nregularising effect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable success in language\nmodelling due to scaling laws found in model size and the hidden dimension of\nthe model's text representation. Yet, we demonstrate that compressed\nrepresentations of text can yield better performance in LLM-based regression\ntasks. In this paper, we compare the relative performance of embedding\ncompression in three different signal-to-noise contexts: financial return\nprediction, writing quality assessment and review scoring. Our results show\nthat compressing embeddings, in a minimally supervised manner using an\nautoencoder's hidden representation, can mitigate overfitting and improve\nperformance on noisy tasks, such as financial return prediction; but that\ncompression reduces performance on tasks that have high causal dependencies\nbetween the input and target data. Our results suggest that the success of\ninterpretable compressed representations such as sentiment may be due to a\nregularising effect."
                },
                "authors": [
                    {
                        "name": "Felix Drinkall"
                    },
                    {
                        "name": "Janet B. Pierrehumbert"
                    },
                    {
                        "name": "Stefan Zohren"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Zohren"
                },
                "author": "Stefan Zohren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19689v2",
                "updated": "2025-02-04T10:23:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    23,
                    5,
                    1,
                    35,
                    0
                ],
                "published": "2024-09-29T12:35:47Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    35,
                    47,
                    6,
                    273,
                    0
                ],
                "title": "InfantCryNet: A Data-driven Framework for Intelligent Analysis of Infant\n  Cries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfantCryNet: A Data-driven Framework for Intelligent Analysis of Infant\n  Cries"
                },
                "summary": "Understanding the meaning of infant cries is a significant challenge for\nyoung parents in caring for their newborns. The presence of background noise\nand the lack of labeled data present practical challenges in developing systems\nthat can detect crying and analyze its underlying reasons. In this paper, we\npresent a novel data-driven framework, \"InfantCryNet,\" for accomplishing these\ntasks. To address the issue of data scarcity, we employ pre-trained audio\nmodels to incorporate prior knowledge into our model. We propose the use of\nstatistical pooling and multi-head attention pooling techniques to extract\nfeatures more effectively. Additionally, knowledge distillation and model\nquantization are applied to enhance model efficiency and reduce the model size,\nbetter supporting industrial deployment in mobile devices. Experiments on\nreal-life datasets demonstrate the superior performance of the proposed\nframework, outperforming state-of-the-art baselines by 4.4% in classification\naccuracy. The model compression effectively reduces the model size by 7%\nwithout compromising performance and by up to 28% with only an 8% decrease in\naccuracy, offering practical insights for model selection and system design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the meaning of infant cries is a significant challenge for\nyoung parents in caring for their newborns. The presence of background noise\nand the lack of labeled data present practical challenges in developing systems\nthat can detect crying and analyze its underlying reasons. In this paper, we\npresent a novel data-driven framework, \"InfantCryNet,\" for accomplishing these\ntasks. To address the issue of data scarcity, we employ pre-trained audio\nmodels to incorporate prior knowledge into our model. We propose the use of\nstatistical pooling and multi-head attention pooling techniques to extract\nfeatures more effectively. Additionally, knowledge distillation and model\nquantization are applied to enhance model efficiency and reduce the model size,\nbetter supporting industrial deployment in mobile devices. Experiments on\nreal-life datasets demonstrate the superior performance of the proposed\nframework, outperforming state-of-the-art baselines by 4.4% in classification\naccuracy. The model compression effectively reduces the model size by 7%\nwithout compromising performance and by up to 28% with only an 8% decrease in\naccuracy, offering practical insights for model selection and system design."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lingxiao Yang"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Di Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Di Jiang"
                },
                "author": "Di Jiang",
                "arxiv_comment": "Accepted by the 16th Asian Conference on Machine Learning (ACML 2024)",
                "arxiv_journal_ref": "PMLR 260:845-857, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02191v1",
                "updated": "2025-02-04T10:13:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    13,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T10:13:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    13,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "Large language models in climate and sustainability policy: limits and\n  opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models in climate and sustainability policy: limits and\n  opportunities"
                },
                "summary": "As multiple crises threaten the sustainability of our societies and pose at\nrisk the planetary boundaries, complex challenges require timely, updated, and\nusable information. Natural-language processing (NLP) tools enhance and expand\ndata collection and processing and knowledge utilization capabilities to\nsupport the definition of an inclusive, sustainable future. In this work, we\napply different NLP techniques, tools and approaches to climate and\nsustainability documents to derive policy-relevant and actionable measures. We\nfocus on general and domain-specific large language models (LLMs) using a\ncombination of static and prompt-based methods. We find that the use of LLMs is\nsuccessful at processing, classifying and summarizing heterogeneous text-based\ndata. However, we also encounter challenges related to human intervention\nacross different workflow stages and knowledge utilization for policy\nprocesses. Our work presents a critical but empirically grounded application of\nLLMs to complex policy problems and suggests avenues to further expand\nArtificial Intelligence-powered computational social sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multiple crises threaten the sustainability of our societies and pose at\nrisk the planetary boundaries, complex challenges require timely, updated, and\nusable information. Natural-language processing (NLP) tools enhance and expand\ndata collection and processing and knowledge utilization capabilities to\nsupport the definition of an inclusive, sustainable future. In this work, we\napply different NLP techniques, tools and approaches to climate and\nsustainability documents to derive policy-relevant and actionable measures. We\nfocus on general and domain-specific large language models (LLMs) using a\ncombination of static and prompt-based methods. We find that the use of LLMs is\nsuccessful at processing, classifying and summarizing heterogeneous text-based\ndata. However, we also encounter challenges related to human intervention\nacross different workflow stages and knowledge utilization for policy\nprocesses. Our work presents a critical but empirically grounded application of\nLLMs to complex policy problems and suggests avenues to further expand\nArtificial Intelligence-powered computational social sciences."
                },
                "authors": [
                    {
                        "name": "Francesca Larosa"
                    },
                    {
                        "name": "Sergio Hoyas"
                    },
                    {
                        "name": "H. Alberto Conejero"
                    },
                    {
                        "name": "Javier Garcia-Martinez"
                    },
                    {
                        "name": "Francesco Fuso Nerini"
                    },
                    {
                        "name": "Ricardo Vinuesa"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Vinuesa"
                },
                "author": "Ricardo Vinuesa",
                "arxiv_comment": "15 pages; 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18712v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18712v3",
                "updated": "2025-02-04T09:56:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    56,
                    50,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-30T19:15:41Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    19,
                    15,
                    41,
                    3,
                    30,
                    0
                ],
                "title": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps"
                },
                "summary": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts."
                },
                "authors": [
                    {
                        "name": "Devansh Bhardwaj"
                    },
                    {
                        "name": "Naman Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Naman Mishra"
                },
                "author": "Naman Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18712v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18712v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16165v2",
                "updated": "2025-02-04T09:49:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    49,
                    41,
                    1,
                    35,
                    0
                ],
                "published": "2024-09-24T15:06:01Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    6,
                    1,
                    1,
                    268,
                    0
                ],
                "title": "Interactive Tools Substantially Assist LM Agents in Finding Security\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Tools Substantially Assist LM Agents in Finding Security\n  Vulnerabilities"
                },
                "summary": "Although language model (LM) agents have demonstrated increased performance\nin multiple domains, including coding and web-browsing, their success in\ncybersecurity has been limited. We present EnIGMA, an LM agent for autonomously\nsolving Capture The Flag (CTF) challenges. We introduce new tools and\ninterfaces to improve the agent's ability to find and exploit security\nvulnerabilities, focusing on interactive terminal programs. These novel\nInteractive Agent Tools enable LM agents, for the first time, to run\ninteractive utilities, such as a debugger and a server connection tool, which\nare essential for solving these challenges. Empirical analysis on 390 CTF\nchallenges across four benchmarks demonstrate that these new tools and\ninterfaces substantially improve our agent's performance, achieving\nstate-of-the-art results on NYU CTF, Intercode-CTF, and CyBench. Finally, we\nanalyze data leakage, developing new methods to quantify it and identifying a\nnew phenomenon we term soliloquizing, where the model self-generates\nhallucinated observations without interacting with the environment. Our code\nand development dataset are available at\nhttps://github.com/SWE-agent/SWE-agent/tree/v0.7 and\nhttps://github.com/NYU-LLM-CTF/NYU_CTF_Bench/tree/main/development\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although language model (LM) agents have demonstrated increased performance\nin multiple domains, including coding and web-browsing, their success in\ncybersecurity has been limited. We present EnIGMA, an LM agent for autonomously\nsolving Capture The Flag (CTF) challenges. We introduce new tools and\ninterfaces to improve the agent's ability to find and exploit security\nvulnerabilities, focusing on interactive terminal programs. These novel\nInteractive Agent Tools enable LM agents, for the first time, to run\ninteractive utilities, such as a debugger and a server connection tool, which\nare essential for solving these challenges. Empirical analysis on 390 CTF\nchallenges across four benchmarks demonstrate that these new tools and\ninterfaces substantially improve our agent's performance, achieving\nstate-of-the-art results on NYU CTF, Intercode-CTF, and CyBench. Finally, we\nanalyze data leakage, developing new methods to quantify it and identifying a\nnew phenomenon we term soliloquizing, where the model self-generates\nhallucinated observations without interacting with the environment. Our code\nand development dataset are available at\nhttps://github.com/SWE-agent/SWE-agent/tree/v0.7 and\nhttps://github.com/NYU-LLM-CTF/NYU_CTF_Bench/tree/main/development\nrespectively."
                },
                "authors": [
                    {
                        "name": "Talor Abramovich"
                    },
                    {
                        "name": "Meet Udeshi"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Kilian Lieret"
                    },
                    {
                        "name": "Haoran Xi"
                    },
                    {
                        "name": "Kimberly Milner"
                    },
                    {
                        "name": "Sofija Jancheska"
                    },
                    {
                        "name": "John Yang"
                    },
                    {
                        "name": "Carlos E. Jimenez"
                    },
                    {
                        "name": "Farshad Khorrami"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Brendan Dolan-Gavitt"
                    },
                    {
                        "name": "Muhammad Shafique"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Ofir Press"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Press"
                },
                "author": "Ofir Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02172v1",
                "updated": "2025-02-04T09:45:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    45,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:45:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    45,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "EditIQ: Automated Cinematic Editing of Static Wide-Angle Videos via\n  Dialogue Interpretation and Saliency Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EditIQ: Automated Cinematic Editing of Static Wide-Angle Videos via\n  Dialogue Interpretation and Saliency Cues"
                },
                "summary": "We present EditIQ, a completely automated framework for cinematically editing\nscenes captured via a stationary, large field-of-view and high-resolution\ncamera. From the static camera feed, EditIQ initially generates multiple\nvirtual feeds, emulating a team of cameramen. These virtual camera shots termed\nrushes are subsequently assembled using an automated editing algorithm, whose\nobjective is to present the viewer with the most vivid scene content. To\nunderstand key scene elements and guide the editing process, we employ a\ntwo-pronged approach: (1) a large language model (LLM)-based dialogue\nunderstanding module to analyze conversational flow, coupled with (2) visual\nsaliency prediction to identify meaningful scene elements and camera shots\ntherefrom. We then formulate cinematic video editing as an energy minimization\nproblem over shot selection, where cinematic constraints determine shot\nchoices, transitions, and continuity. EditIQ synthesizes an aesthetically and\nvisually compelling representation of the original narrative while maintaining\ncinematic coherence and a smooth viewing experience. Efficacy of EditIQ against\ncompeting baselines is demonstrated via a psychophysical study involving twenty\nparticipants on the BBC Old School dataset plus eleven theatre performance\nvideos. Video samples from EditIQ can be found at\nhttps://editiq-ave.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present EditIQ, a completely automated framework for cinematically editing\nscenes captured via a stationary, large field-of-view and high-resolution\ncamera. From the static camera feed, EditIQ initially generates multiple\nvirtual feeds, emulating a team of cameramen. These virtual camera shots termed\nrushes are subsequently assembled using an automated editing algorithm, whose\nobjective is to present the viewer with the most vivid scene content. To\nunderstand key scene elements and guide the editing process, we employ a\ntwo-pronged approach: (1) a large language model (LLM)-based dialogue\nunderstanding module to analyze conversational flow, coupled with (2) visual\nsaliency prediction to identify meaningful scene elements and camera shots\ntherefrom. We then formulate cinematic video editing as an energy minimization\nproblem over shot selection, where cinematic constraints determine shot\nchoices, transitions, and continuity. EditIQ synthesizes an aesthetically and\nvisually compelling representation of the original narrative while maintaining\ncinematic coherence and a smooth viewing experience. Efficacy of EditIQ against\ncompeting baselines is demonstrated via a psychophysical study involving twenty\nparticipants on the BBC Old School dataset plus eleven theatre performance\nvideos. Video samples from EditIQ can be found at\nhttps://editiq-ave.github.io/."
                },
                "authors": [
                    {
                        "name": "Rohit Girmaji"
                    },
                    {
                        "name": "Bhav Beri"
                    },
                    {
                        "name": "Ramanathan Subramanian"
                    },
                    {
                        "name": "Vineet Gandhi"
                    }
                ],
                "author_detail": {
                    "name": "Vineet Gandhi"
                },
                "author": "Vineet Gandhi",
                "arxiv_doi": "10.1145/3708359.3712113",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712113",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.02172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at 30th International Conference on Intelligent User\n  Interfaces (IUI 25)",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.1; G.2; I.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02161v1",
                "updated": "2025-02-04T09:37:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    37,
                    20,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:37:20Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    37,
                    20,
                    1,
                    35,
                    0
                ],
                "title": "A plug-and-play solution for characterizing two-way optical frequency\n  transfer over free-space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A plug-and-play solution for characterizing two-way optical frequency\n  transfer over free-space"
                },
                "summary": "Optical clock networks connected by phase-coherent links offer significant\npotential for advancing fundamental research and diverse scientific\napplications. Free-space optical frequency transfer extends fiber-based\nconnectivity to remote areas and holds the potential for global coverage via\nsatellite links. Here we present a compact and robust portable, rack-integrated\ntwo-way free-space link characterization system. Equipped with plug-and-play\ncapabilities, the system enables straightforward interfacing with various\noptical systems and facilitates quick deployment for field experiments. In this\nwork, we achieve a fractional frequency instability of $2.0 \\times 10^{-19}$\nfor an averaging time of 10 s over a 3.4 km horizontal fully folded intra-city\nfree-space link. Moreover, the system maintains an uptime of $94\\%$ over 15\nhours, illustrating its reliability and effectiveness for high-precision\noptical frequency comparisons over free-space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical clock networks connected by phase-coherent links offer significant\npotential for advancing fundamental research and diverse scientific\napplications. Free-space optical frequency transfer extends fiber-based\nconnectivity to remote areas and holds the potential for global coverage via\nsatellite links. Here we present a compact and robust portable, rack-integrated\ntwo-way free-space link characterization system. Equipped with plug-and-play\ncapabilities, the system enables straightforward interfacing with various\noptical systems and facilitates quick deployment for field experiments. In this\nwork, we achieve a fractional frequency instability of $2.0 \\times 10^{-19}$\nfor an averaging time of 10 s over a 3.4 km horizontal fully folded intra-city\nfree-space link. Moreover, the system maintains an uptime of $94\\%$ over 15\nhours, illustrating its reliability and effectiveness for high-precision\noptical frequency comparisons over free-space."
                },
                "authors": [
                    {
                        "name": "Jingxian Ji"
                    },
                    {
                        "name": "Shambo Mukherjee"
                    },
                    {
                        "name": "Alexander Kuhl"
                    },
                    {
                        "name": "Sebastian Koke"
                    },
                    {
                        "name": "Markus Leipe"
                    },
                    {
                        "name": "Markus Rothe"
                    },
                    {
                        "name": "Fabian Steinlechner"
                    },
                    {
                        "name": "Jochen Kronjäger"
                    }
                ],
                "author_detail": {
                    "name": "Jochen Kronjäger"
                },
                "author": "Jochen Kronjäger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17621v2",
                "updated": "2025-02-04T09:24:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    24,
                    30,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-23T07:22:33Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    22,
                    33,
                    2,
                    297,
                    0
                ],
                "title": "Process Supervision-Guided Policy Optimization for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Supervision-Guided Policy Optimization for Code Generation"
                },
                "summary": "Reinforcement learning (RL) with unit test feedback has enhanced large\nlanguage models' (LLMs) code generation, but relies on sparse rewards provided\nonly after complete code evaluation, limiting learning efficiency and\nincremental improvements. When generated code fails all unit tests, no learning\nsignal is received, hindering progress on complex tasks. To address this, we\npropose a Process Reward Model (PRM) that delivers dense, line-level feedback\non code correctness during generation, mimicking human code refinement and\nproviding immediate guidance. We explore various strategies for training PRMs\nand integrating them into the RL framework, finding that using PRMs both as\ndense rewards and for value function initialization significantly boosts\nperformance. Our experimental results also highlight the effectiveness of PRMs\nin enhancing RL-driven code generation, especially for long-horizon scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with unit test feedback has enhanced large\nlanguage models' (LLMs) code generation, but relies on sparse rewards provided\nonly after complete code evaluation, limiting learning efficiency and\nincremental improvements. When generated code fails all unit tests, no learning\nsignal is received, hindering progress on complex tasks. To address this, we\npropose a Process Reward Model (PRM) that delivers dense, line-level feedback\non code correctness during generation, mimicking human code refinement and\nproviding immediate guidance. We explore various strategies for training PRMs\nand integrating them into the RL framework, finding that using PRMs both as\ndense rewards and for value function initialization significantly boosts\nperformance. Our experimental results also highlight the effectiveness of PRMs\nin enhancing RL-driven code generation, especially for long-horizon scenarios."
                },
                "authors": [
                    {
                        "name": "Ning Dai"
                    },
                    {
                        "name": "Zheng Wu"
                    },
                    {
                        "name": "Renjie Zheng"
                    },
                    {
                        "name": "Ziyun Wei"
                    },
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Xing Jin"
                    },
                    {
                        "name": "Guanlin Liu"
                    },
                    {
                        "name": "Chen Dun"
                    },
                    {
                        "name": "Liang Huang"
                    },
                    {
                        "name": "Lin Yan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yan"
                },
                "author": "Lin Yan",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02145v1",
                "updated": "2025-02-04T09:19:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    19,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:19:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    19,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "Risk-Aware Driving Scenario Analysis with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk-Aware Driving Scenario Analysis with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can capture nuanced contextual relationships,\nreasoning, and complex problem-solving. By leveraging their ability to process\nand interpret large-scale information, LLMs have shown potential to address\ndomain-specific challenges, including those in autonomous driving systems. This\npaper proposes a novel framework that leverages LLMs for risk-aware analysis of\ngenerated driving scenarios. We hypothesize that LLMs can effectively evaluate\nwhether driving scenarios generated by autonomous driving testing simulators\nare safety-critical. To validate this hypothesis, we conducted an empirical\nevaluation to assess the effectiveness of LLMs in performing this task. This\nframework will also provide feedback to generate the new safety-critical\nscenario by using adversarial method to modify existing non-critical scenarios\nand test their effectiveness in validating motion planning algorithms. Code and\nscenarios are available at:\nhttps://github.com/yuangao-tum/Riskaware-Scenario-analyse",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can capture nuanced contextual relationships,\nreasoning, and complex problem-solving. By leveraging their ability to process\nand interpret large-scale information, LLMs have shown potential to address\ndomain-specific challenges, including those in autonomous driving systems. This\npaper proposes a novel framework that leverages LLMs for risk-aware analysis of\ngenerated driving scenarios. We hypothesize that LLMs can effectively evaluate\nwhether driving scenarios generated by autonomous driving testing simulators\nare safety-critical. To validate this hypothesis, we conducted an empirical\nevaluation to assess the effectiveness of LLMs in performing this task. This\nframework will also provide feedback to generate the new safety-critical\nscenario by using adversarial method to modify existing non-critical scenarios\nand test their effectiveness in validating motion planning algorithms. Code and\nscenarios are available at:\nhttps://github.com/yuangao-tum/Riskaware-Scenario-analyse"
                },
                "authors": [
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Mattia Piccinini"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "arxiv_comment": "IEEE Intelligent Vehicles Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09961v2",
                "updated": "2025-02-04T09:15:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    15,
                    56,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-13T08:42:19Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    42,
                    19,
                    4,
                    348,
                    0
                ],
                "title": "What constitutes a Deep Fake? The blurry line between legitimate\n  processing and manipulation under the EU AI Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What constitutes a Deep Fake? The blurry line between legitimate\n  processing and manipulation under the EU AI Act"
                },
                "summary": "When does a digital image resemble reality? The relevance of this question\nincreases as the generation of synthetic images -- so called deep fakes --\nbecomes increasingly popular. Deep fakes have gained much attention for a\nnumber of reasons -- among others, due to their potential to disrupt the\npolitical climate. In order to mitigate these threats, the EU AI Act implements\nspecific transparency regulations for generating synthetic content or\nmanipulating existing content. However, the distinction between real and\nsynthetic images is -- even from a computer vision perspective -- far from\ntrivial. We argue that the current definition of deep fakes in the AI act and\nthe corresponding obligations are not sufficiently specified to tackle the\nchallenges posed by deep fakes. By analyzing the life cycle of a digital photo\nfrom the camera sensor to the digital editing features, we find that: (1.) Deep\nfakes are ill-defined in the EU AI Act. The definition leaves too much scope\nfor what a deep fake is. (2.) It is unclear how editing functions like Google's\n``best take'' feature can be considered as an exception to transparency\nobligations. (3.) The exception for substantially edited images raises\nquestions about what constitutes substantial editing of content and whether or\nnot this editing must be perceptible by a natural person. Our results\ndemonstrate that complying with the current AI Act transparency obligations is\ndifficult for providers and deployers. As a consequence of the unclear\nprovisions, there is a risk that exceptions may be either too broad or too\nlimited. We intend our analysis to foster the discussion on what constitutes a\ndeep fake and to raise awareness about the pitfalls in the current AI Act\ntransparency obligations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When does a digital image resemble reality? The relevance of this question\nincreases as the generation of synthetic images -- so called deep fakes --\nbecomes increasingly popular. Deep fakes have gained much attention for a\nnumber of reasons -- among others, due to their potential to disrupt the\npolitical climate. In order to mitigate these threats, the EU AI Act implements\nspecific transparency regulations for generating synthetic content or\nmanipulating existing content. However, the distinction between real and\nsynthetic images is -- even from a computer vision perspective -- far from\ntrivial. We argue that the current definition of deep fakes in the AI act and\nthe corresponding obligations are not sufficiently specified to tackle the\nchallenges posed by deep fakes. By analyzing the life cycle of a digital photo\nfrom the camera sensor to the digital editing features, we find that: (1.) Deep\nfakes are ill-defined in the EU AI Act. The definition leaves too much scope\nfor what a deep fake is. (2.) It is unclear how editing functions like Google's\n``best take'' feature can be considered as an exception to transparency\nobligations. (3.) The exception for substantially edited images raises\nquestions about what constitutes substantial editing of content and whether or\nnot this editing must be perceptible by a natural person. Our results\ndemonstrate that complying with the current AI Act transparency obligations is\ndifficult for providers and deployers. As a consequence of the unclear\nprovisions, there is a risk that exceptions may be either too broad or too\nlimited. We intend our analysis to foster the discussion on what constitutes a\ndeep fake and to raise awareness about the pitfalls in the current AI Act\ntransparency obligations."
                },
                "authors": [
                    {
                        "name": "Kristof Meding"
                    },
                    {
                        "name": "Christoph Sorge"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Sorge"
                },
                "author": "Christoph Sorge",
                "arxiv_comment": "Preprint. Accepted at ACM CS&Law '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02141v1",
                "updated": "2025-02-04T09:14:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    14,
                    18,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:14:18Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    14,
                    18,
                    1,
                    35,
                    0
                ],
                "title": "NFV-Enabled Service Recovery in Space-Air-Ground Integrated Networks: A\n  Matching Game Based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NFV-Enabled Service Recovery in Space-Air-Ground Integrated Networks: A\n  Matching Game Based Approach"
                },
                "summary": "To achieve ubiquitous connectivity of the sixth generation communication, the\nspace-air-ground integrated network (SAGIN) is a popular topic. However, the\ndynamic nodes in SAGIN such as satellites and unmanned aerial vehicles, may be\nfragile and out of operation, which can potentially cause service failure.\nTherefore, the research on service recovery in SAGIN under situations of\nresource failure is critical. In order to facilitate the flexible resource\nutilization of SAGIN, the network function virtualization technology (NFV) is\nproposed to be employed. Firstly, the task management is transformed into the\ndeployment of service function chains (SFCs). Then, we design an NFV-based SFC\nrecovery model in SAGIN in the face of resource failure, so that tasks can\nquickly select alternative resources to complete deployments. Moreover, the\nproblem of SFC recovery is formulated to minimize the total time consumption\nfor all completed SFCs. Since it is an NP-hard integer linear programming\nproblem, we propose the efficient recovery algorithm based on the matching\ngame. Finally, via various simulations, the effectiveness of the proposed\nalgorithm and its advantages are verified, where the total time consumption is\noptimized by about 25%, compared with other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve ubiquitous connectivity of the sixth generation communication, the\nspace-air-ground integrated network (SAGIN) is a popular topic. However, the\ndynamic nodes in SAGIN such as satellites and unmanned aerial vehicles, may be\nfragile and out of operation, which can potentially cause service failure.\nTherefore, the research on service recovery in SAGIN under situations of\nresource failure is critical. In order to facilitate the flexible resource\nutilization of SAGIN, the network function virtualization technology (NFV) is\nproposed to be employed. Firstly, the task management is transformed into the\ndeployment of service function chains (SFCs). Then, we design an NFV-based SFC\nrecovery model in SAGIN in the face of resource failure, so that tasks can\nquickly select alternative resources to complete deployments. Moreover, the\nproblem of SFC recovery is formulated to minimize the total time consumption\nfor all completed SFCs. Since it is an NP-hard integer linear programming\nproblem, we propose the efficient recovery algorithm based on the matching\ngame. Finally, via various simulations, the effectiveness of the proposed\nalgorithm and its advantages are verified, where the total time consumption is\noptimized by about 25%, compared with other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Ziye Jia"
                    },
                    {
                        "name": "Yilu Cao"
                    },
                    {
                        "name": "Lijun He"
                    },
                    {
                        "name": "Guangxia Li"
                    },
                    {
                        "name": "Fuhui Zhou"
                    },
                    {
                        "name": "Qihui Wu"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00409v2",
                "updated": "2025-02-04T09:12:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    12,
                    3,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-01T12:08:38Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    12,
                    8,
                    38,
                    5,
                    32,
                    0
                ],
                "title": "Doing More with Less -- Implementing Routing Strategies in Large\n  Language Model-Based Systems: An Extended Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doing More with Less -- Implementing Routing Strategies in Large\n  Language Model-Based Systems: An Extended Survey"
                },
                "summary": "Large Language Models (LLM)-based systems, i.e. interconnected elements that\ninclude an LLM as a central component (e.g., conversational agents), are\ntypically monolithic static architectures that rely on a single LLM for all\nuser queries. However, they often require different preprocessing strategies,\nlevels of reasoning, or knowledge. Generalist LLMs (e.g. GPT-4) trained on very\nlarge multi-topic corpora can perform well in a variety of tasks. They require\nsignificant financial, energy, and hardware resources that may not be justified\nfor basic tasks. This implies potentially investing in unnecessary costs for a\ngiven query. To overcome this problem, a routing mechanism routes user queries\nto the most suitable components, such as smaller LLMs or experts in specific\ntopics. This approach may improve response quality while minimising costs.\nRouting can be expanded to other components of the conversational agent\narchitecture, such as the selection of optimal embedding strategies. This paper\nexplores key considerations for integrating routing into LLM-based systems,\nfocusing on resource management, cost definition, and strategy selection. Our\nmain contributions include a formalisation of the problem, a novel taxonomy of\nexisting approaches emphasising relevance and resource efficiency, and a\ncomparative analysis of these strategies in relation to industry practices.\nFinally, we identify critical challenges and directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM)-based systems, i.e. interconnected elements that\ninclude an LLM as a central component (e.g., conversational agents), are\ntypically monolithic static architectures that rely on a single LLM for all\nuser queries. However, they often require different preprocessing strategies,\nlevels of reasoning, or knowledge. Generalist LLMs (e.g. GPT-4) trained on very\nlarge multi-topic corpora can perform well in a variety of tasks. They require\nsignificant financial, energy, and hardware resources that may not be justified\nfor basic tasks. This implies potentially investing in unnecessary costs for a\ngiven query. To overcome this problem, a routing mechanism routes user queries\nto the most suitable components, such as smaller LLMs or experts in specific\ntopics. This approach may improve response quality while minimising costs.\nRouting can be expanded to other components of the conversational agent\narchitecture, such as the selection of optimal embedding strategies. This paper\nexplores key considerations for integrating routing into LLM-based systems,\nfocusing on resource management, cost definition, and strategy selection. Our\nmain contributions include a formalisation of the problem, a novel taxonomy of\nexisting approaches emphasising relevance and resource efficiency, and a\ncomparative analysis of these strategies in relation to industry practices.\nFinally, we identify critical challenges and directions for future research."
                },
                "authors": [
                    {
                        "name": "Clovis Varangot-Reille"
                    },
                    {
                        "name": "Christophe Bouvard"
                    },
                    {
                        "name": "Antoine Gourru"
                    },
                    {
                        "name": "Mathieu Ciancone"
                    },
                    {
                        "name": "Marion Schaeffer"
                    },
                    {
                        "name": "François Jacquenet"
                    }
                ],
                "author_detail": {
                    "name": "François Jacquenet"
                },
                "author": "François Jacquenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17773v2",
                "updated": "2025-02-04T09:10:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    10,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-29T17:09:28Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    17,
                    9,
                    28,
                    2,
                    29,
                    0
                ],
                "title": "SafePR: Unified Approach for Safe Parallel Robots by Contact Detection\n  and Reaction with Redundancy Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafePR: Unified Approach for Safe Parallel Robots by Contact Detection\n  and Reaction with Redundancy Resolution"
                },
                "summary": "Fast and safe motion is crucial for the successful deployment of physically\ninteractive robots. Parallel robots (PRs) offer the potential for higher speeds\nwhile maintaining the same energy limits due to their low moving masses.\nHowever, they require methods for contact detection and reaction while avoiding\nsingularities and self-collisions. We address this issue and present SafePR - a\nunified approach for the detection and localization, including the distinction\nbetween collision and clamping to perform a reaction that is safe for humans\nand feasible for PRs. Our approach uses information from the encoders and motor\ncurrents to estimate forces via a generalized-momentum observer. Neural\nnetworks and particle filters classify and localize the contacts. We introduce\nreactions with redundancy resolution to avoid type-II singularities and\nself-collisions. Our approach detected and terminated 72 real-world collision\nand clamping contacts with end-effector speeds of up to 1.5 m/s, each within\n25-275 ms. The forces were below the thresholds from ISO/TS 15066. By using\nbuilt-in sensors, SafePR enables safe interaction with already assembled PRs\nwithout the need for new hardware components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and safe motion is crucial for the successful deployment of physically\ninteractive robots. Parallel robots (PRs) offer the potential for higher speeds\nwhile maintaining the same energy limits due to their low moving masses.\nHowever, they require methods for contact detection and reaction while avoiding\nsingularities and self-collisions. We address this issue and present SafePR - a\nunified approach for the detection and localization, including the distinction\nbetween collision and clamping to perform a reaction that is safe for humans\nand feasible for PRs. Our approach uses information from the encoders and motor\ncurrents to estimate forces via a generalized-momentum observer. Neural\nnetworks and particle filters classify and localize the contacts. We introduce\nreactions with redundancy resolution to avoid type-II singularities and\nself-collisions. Our approach detected and terminated 72 real-world collision\nand clamping contacts with end-effector speeds of up to 1.5 m/s, each within\n25-275 ms. The forces were below the thresholds from ISO/TS 15066. By using\nbuilt-in sensors, SafePR enables safe interaction with already assembled PRs\nwithout the need for new hardware components."
                },
                "authors": [
                    {
                        "name": "Aran Mohammad"
                    },
                    {
                        "name": "Tim-Lukas Habich"
                    },
                    {
                        "name": "Thomas Seel"
                    },
                    {
                        "name": "Moritz Schappler"
                    }
                ],
                "author_detail": {
                    "name": "Moritz Schappler"
                },
                "author": "Moritz Schappler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01050v2",
                "updated": "2025-02-04T08:41:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    41,
                    16,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-03T04:51:02Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    51,
                    2,
                    0,
                    34,
                    0
                ],
                "title": "AutoDDG: Automated Dataset Description Generation using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDDG: Automated Dataset Description Generation using Large Language\n  Models"
                },
                "summary": "The proliferation of datasets across open data portals and enterprise data\nlakes presents an opportunity for deriving data-driven insights. However,\nwidely-used dataset search systems rely on keyword searches over dataset\nmetadata, including descriptions, to facilitate discovery. When these\ndescriptions are incomplete, missing, or inconsistent with dataset contents,\nfindability is severely hindered. In this paper, we address the problem of\nautomatic dataset description generation: how to generate informative\ndescriptions that enhance dataset discovery and support relevance assessment.\nWe introduce AutoDDG, a framework for automated dataset description generation\ntailored for tabular data. To derive descriptions that are comprehensive,\naccurate, readable and concise, AutoDDG adopts a data-driven approach to\nsummarize the contents of a dataset, and leverages LLMs to both enrich the\nsummaries with semantic information and to derive human-readable descriptions.\nAn important challenge for this problem is how to evaluate the effectiveness of\nmethods for data description generation and the quality of the descriptions. We\npropose a multi-pronged evaluation strategy that: (1) measures the improvement\nin dataset retrieval within a dataset search engine, (2) compares generated\ndescriptions to existing ones (when available), and (3) evaluates intrinsic\nquality metrics such as readability, faithfulness to the data, and conciseness.\nAdditionally, we introduce two new benchmarks to support this evaluation. Our\nexperimental results, using these benchmarks, demonstrate that AutoDDG\ngenerates high-quality, accurate descriptions and significantly improves\ndataset retrieval performance across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of datasets across open data portals and enterprise data\nlakes presents an opportunity for deriving data-driven insights. However,\nwidely-used dataset search systems rely on keyword searches over dataset\nmetadata, including descriptions, to facilitate discovery. When these\ndescriptions are incomplete, missing, or inconsistent with dataset contents,\nfindability is severely hindered. In this paper, we address the problem of\nautomatic dataset description generation: how to generate informative\ndescriptions that enhance dataset discovery and support relevance assessment.\nWe introduce AutoDDG, a framework for automated dataset description generation\ntailored for tabular data. To derive descriptions that are comprehensive,\naccurate, readable and concise, AutoDDG adopts a data-driven approach to\nsummarize the contents of a dataset, and leverages LLMs to both enrich the\nsummaries with semantic information and to derive human-readable descriptions.\nAn important challenge for this problem is how to evaluate the effectiveness of\nmethods for data description generation and the quality of the descriptions. We\npropose a multi-pronged evaluation strategy that: (1) measures the improvement\nin dataset retrieval within a dataset search engine, (2) compares generated\ndescriptions to existing ones (when available), and (3) evaluates intrinsic\nquality metrics such as readability, faithfulness to the data, and conciseness.\nAdditionally, we introduce two new benchmarks to support this evaluation. Our\nexperimental results, using these benchmarks, demonstrate that AutoDDG\ngenerates high-quality, accurate descriptions and significantly improves\ndataset retrieval performance across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Haoxiang Zhang"
                    },
                    {
                        "name": "Yurong Liu"
                    },
                    {
                        "name": "Wei-Lun"
                    },
                    {
                        "name": "Hung"
                    },
                    {
                        "name": "Aécio Santos"
                    },
                    {
                        "name": "Juliana Freire"
                    }
                ],
                "author_detail": {
                    "name": "Juliana Freire"
                },
                "arxiv_affiliation": "Allen",
                "author": "Juliana Freire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14202v2",
                "updated": "2025-02-04T08:31:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    31,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-18T06:35:17Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    35,
                    17,
                    4,
                    292,
                    0
                ],
                "title": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs"
                },
                "summary": "Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays. The code is available at\nhttps://github.com/BBeeChu/RMTS.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays. The code is available at\nhttps://github.com/BBeeChu/RMTS.git."
                },
                "authors": [
                    {
                        "name": "SeongYeub Chu"
                    },
                    {
                        "name": "JongWoo Kim"
                    },
                    {
                        "name": "Bryan Wong"
                    },
                    {
                        "name": "MunYong Yi"
                    }
                ],
                "author_detail": {
                    "name": "MunYong Yi"
                },
                "author": "MunYong Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02095v1",
                "updated": "2025-02-04T08:25:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    25,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:25:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    25,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via\n  Critique-augmented Stepwise Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via\n  Critique-augmented Stepwise Information"
                },
                "summary": "Long-form generation is crucial for academic writing papers and repo-level\ncode generation. Despite this, current models, including GPT-4o, still exhibit\nunsatisfactory performance. Existing methods that utilize preference learning\nwith outcome supervision often fail to provide detailed feedback for extended\ncontexts. This shortcoming can lead to content that does not fully satisfy\nquery requirements, resulting in issues like length deviations, and diminished\nquality. In this paper, we propose enhancing long-form generation by\nincorporating process supervision. We employ Monte Carlo Tree Search to gather\nstepwise preference pairs, utilizing a global memory pool to maintain\nconsistency. To address the issue of suboptimal candidate selection, we\nintegrate external critiques to refine and improve the quality of the\npreference pairs. Finally, we apply step-level DPO using the collected stepwise\npreference pairs. Experimental results show that our method improves length and\nquality on long-form generation benchmarks, with almost lossless performance on\ngeneral benchmarks across various model backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form generation is crucial for academic writing papers and repo-level\ncode generation. Despite this, current models, including GPT-4o, still exhibit\nunsatisfactory performance. Existing methods that utilize preference learning\nwith outcome supervision often fail to provide detailed feedback for extended\ncontexts. This shortcoming can lead to content that does not fully satisfy\nquery requirements, resulting in issues like length deviations, and diminished\nquality. In this paper, we propose enhancing long-form generation by\nincorporating process supervision. We employ Monte Carlo Tree Search to gather\nstepwise preference pairs, utilizing a global memory pool to maintain\nconsistency. To address the issue of suboptimal candidate selection, we\nintegrate external critiques to refine and improve the quality of the\npreference pairs. Finally, we apply step-level DPO using the collected stepwise\npreference pairs. Experimental results show that our method improves length and\nquality on long-form generation benchmarks, with almost lossless performance on\ngeneral benchmarks across various model backbones."
                },
                "authors": [
                    {
                        "name": "Bowen Ping"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17178v2",
                "updated": "2025-02-04T08:21:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    21,
                    0,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-24T17:01:14Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    1,
                    14,
                    4,
                    24,
                    0
                ],
                "title": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost"
                },
                "summary": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility."
                },
                "authors": [
                    {
                        "name": "David Salinas"
                    },
                    {
                        "name": "Omar Swelam"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19200v2",
                "updated": "2025-02-04T08:01:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    1,
                    59,
                    1,
                    35,
                    0
                ],
                "published": "2024-07-27T08:00:27Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    0,
                    27,
                    5,
                    209,
                    0
                ],
                "title": "On Behalf of the Stakeholders: Trends in NLP Model Interpretability in\n  the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Behalf of the Stakeholders: Trends in NLP Model Interpretability in\n  the Era of LLMs"
                },
                "summary": "Recent advancements in NLP systems, particularly with the introduction of\nLLMs, have led to widespread adoption of these systems by a broad spectrum of\nusers across various domains, impacting decision-making, the job market,\nsociety, and scientific research. This surge in usage has led to an explosion\nin NLP model interpretability and analysis research, accompanied by numerous\ntechnical surveys. Yet, these surveys often overlook the needs and perspectives\nof explanation stakeholders. In this paper, we address three fundamental\nquestions: Why do we need interpretability, what are we interpreting, and how?\nBy exploring these questions, we examine existing interpretability paradigms,\ntheir properties, and their relevance to different stakeholders. We further\nexplore the practical implications of these paradigms by analyzing trends from\nthe past decade across multiple research fields. To this end, we retrieved\nthousands of papers and employed an LLM to characterize them. Our analysis\nreveals significant disparities between NLP developers and non-developer users,\nas well as between research fields, underscoring the diverse needs of\nstakeholders. For example, explanations of internal model components are rarely\nused outside the NLP field. We hope this paper informs the future design,\ndevelopment, and application of methods that align with the objectives and\nrequirements of various stakeholders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in NLP systems, particularly with the introduction of\nLLMs, have led to widespread adoption of these systems by a broad spectrum of\nusers across various domains, impacting decision-making, the job market,\nsociety, and scientific research. This surge in usage has led to an explosion\nin NLP model interpretability and analysis research, accompanied by numerous\ntechnical surveys. Yet, these surveys often overlook the needs and perspectives\nof explanation stakeholders. In this paper, we address three fundamental\nquestions: Why do we need interpretability, what are we interpreting, and how?\nBy exploring these questions, we examine existing interpretability paradigms,\ntheir properties, and their relevance to different stakeholders. We further\nexplore the practical implications of these paradigms by analyzing trends from\nthe past decade across multiple research fields. To this end, we retrieved\nthousands of papers and employed an LLM to characterize them. Our analysis\nreveals significant disparities between NLP developers and non-developer users,\nas well as between research fields, underscoring the diverse needs of\nstakeholders. For example, explanations of internal model components are rarely\nused outside the NLP field. We hope this paper informs the future design,\ndevelopment, and application of methods that align with the objectives and\nrequirements of various stakeholders."
                },
                "authors": [
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02076v1",
                "updated": "2025-02-04T07:53:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    53,
                    23,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:53:23Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    53,
                    23,
                    1,
                    35,
                    0
                ],
                "title": "Position Paper: Building Trust in Synthetic Data for Clinical AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position Paper: Building Trust in Synthetic Data for Clinical AI"
                },
                "summary": "Deep generative models and synthetic medical data have shown significant\npromise in addressing key challenges in healthcare, such as privacy concerns,\ndata bias, and the scarcity of realistic datasets. While research in this area\nhas grown rapidly and demonstrated substantial theoretical potential, its\npractical adoption in clinical settings remains limited. Despite the benefits\nsynthetic data offers, questions surrounding its reliability and credibility\npersist, leading to a lack of trust among clinicians. This position paper\nargues that fostering trust in synthetic medical data is crucial for its\nclinical adoption. It aims to spark a discussion on the viability of synthetic\nmedical data in clinical practice, particularly in the context of current\nadvancements in AI. We present empirical evidence from brain tumor segmentation\nto demonstrate that the quality, diversity, and proportion of synthetic data\ndirectly impact trust in clinical AI models. Our findings provide insights to\nimprove the deployment and acceptance of synthetic data-driven AI systems in\nreal-world clinical workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep generative models and synthetic medical data have shown significant\npromise in addressing key challenges in healthcare, such as privacy concerns,\ndata bias, and the scarcity of realistic datasets. While research in this area\nhas grown rapidly and demonstrated substantial theoretical potential, its\npractical adoption in clinical settings remains limited. Despite the benefits\nsynthetic data offers, questions surrounding its reliability and credibility\npersist, leading to a lack of trust among clinicians. This position paper\nargues that fostering trust in synthetic medical data is crucial for its\nclinical adoption. It aims to spark a discussion on the viability of synthetic\nmedical data in clinical practice, particularly in the context of current\nadvancements in AI. We present empirical evidence from brain tumor segmentation\nto demonstrate that the quality, diversity, and proportion of synthetic data\ndirectly impact trust in clinical AI models. Our findings provide insights to\nimprove the deployment and acceptance of synthetic data-driven AI systems in\nreal-world clinical workflows."
                },
                "authors": [
                    {
                        "name": "Krishan Agyakari Raja Babu"
                    },
                    {
                        "name": "Supriti Mulay"
                    },
                    {
                        "name": "Om Prabhu"
                    },
                    {
                        "name": "Mohanasankar Sivaprakasam"
                    }
                ],
                "author_detail": {
                    "name": "Mohanasankar Sivaprakasam"
                },
                "author": "Mohanasankar Sivaprakasam",
                "arxiv_comment": "7 pages, 8 figures (including sub-figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02074v1",
                "updated": "2025-02-04T07:52:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    52,
                    20,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:52:20Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    52,
                    20,
                    1,
                    35,
                    0
                ],
                "title": "Rethinking stance detection: A theoretically-informed research agenda\n  for user-level inference using language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking stance detection: A theoretically-informed research agenda\n  for user-level inference using language models"
                },
                "summary": "Stance detection has emerged as a popular task in natural language processing\nresearch, enabled largely by the abundance of target-specific social media\ndata. While there has been considerable research on the development of stance\ndetection models, datasets, and application, we highlight important gaps\npertaining to (i) a lack of theoretical conceptualization of stance, and (ii)\nthe treatment of stance at an individual- or user-level, as opposed to\nmessage-level. In this paper, we first review the interdisciplinary origins of\nstance as an individual-level construct to highlight relevant attributes (e.g.,\npsychological features) that might be useful to incorporate in stance detection\nmodels. Further, we argue that recent pre-trained and large language models\n(LLMs) might offer a way to flexibly infer such user-level attributes and/or\nincorporate them in modelling stance. To better illustrate this, we briefly\nreview and synthesize the emerging corpus of studies on using LLMs for\ninferring stance, and specifically on incorporating user attributes in such\ntasks. We conclude by proposing a four-point agenda for pursuing stance\ndetection research that is theoretically informed, inclusive, and practically\nimpactful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection has emerged as a popular task in natural language processing\nresearch, enabled largely by the abundance of target-specific social media\ndata. While there has been considerable research on the development of stance\ndetection models, datasets, and application, we highlight important gaps\npertaining to (i) a lack of theoretical conceptualization of stance, and (ii)\nthe treatment of stance at an individual- or user-level, as opposed to\nmessage-level. In this paper, we first review the interdisciplinary origins of\nstance as an individual-level construct to highlight relevant attributes (e.g.,\npsychological features) that might be useful to incorporate in stance detection\nmodels. Further, we argue that recent pre-trained and large language models\n(LLMs) might offer a way to flexibly infer such user-level attributes and/or\nincorporate them in modelling stance. To better illustrate this, we briefly\nreview and synthesize the emerging corpus of studies on using LLMs for\ninferring stance, and specifically on incorporating user attributes in such\ntasks. We conclude by proposing a four-point agenda for pursuing stance\ndetection research that is theoretically informed, inclusive, and practically\nimpactful."
                },
                "authors": [
                    {
                        "name": "Prasanta Bhattacharya"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Yiming Cao"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Brandon Siyuan Loh"
                    },
                    {
                        "name": "Joseph J. P. Simons"
                    },
                    {
                        "name": "Liang Ze Wong"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ze Wong"
                },
                "author": "Liang Ze Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09099v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09099v6",
                "updated": "2025-02-04T07:46:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    46,
                    33,
                    1,
                    35,
                    0
                ],
                "published": "2024-02-14T11:20:09Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    11,
                    20,
                    9,
                    2,
                    45,
                    0
                ],
                "title": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in\n  Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in\n  Large Models"
                },
                "summary": "In recent years, there has been increasing attention on the capabilities of\nlarge models, particularly in handling complex tasks that small-scale models\nare unable to perform. Notably, large language models (LLMs) have demonstrated\n``intelligent'' abilities such as complex reasoning and abstract language\ncomprehension, reflecting cognitive-like behaviors. However, current research\non emergent abilities in large models predominantly focuses on the relationship\nbetween model performance and size, leaving a significant gap in the systematic\nquantitative analysis of the internal structures and mechanisms driving these\nemergent abilities. Drawing inspiration from neuroscience research on brain\nnetwork structure and self-organization, we propose (i) a general network\nrepresentation of large models, (ii) a new analytical framework, called\nNeuron-based Multifractal Analysis (NeuroMFA), for structural analysis, and\n(iii) a novel structure-based metric as a proxy for emergent abilities of large\nmodels. By linking structural features to the capabilities of large models,\nNeuroMFA provides a quantitative framework for analyzing emergent phenomena in\nlarge models. Our experiments show that the proposed method yields a\ncomprehensive measure of network's evolving heterogeneity and organization,\noffering theoretical foundations and a new perspective for investigating\nemergent abilities in large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been increasing attention on the capabilities of\nlarge models, particularly in handling complex tasks that small-scale models\nare unable to perform. Notably, large language models (LLMs) have demonstrated\n``intelligent'' abilities such as complex reasoning and abstract language\ncomprehension, reflecting cognitive-like behaviors. However, current research\non emergent abilities in large models predominantly focuses on the relationship\nbetween model performance and size, leaving a significant gap in the systematic\nquantitative analysis of the internal structures and mechanisms driving these\nemergent abilities. Drawing inspiration from neuroscience research on brain\nnetwork structure and self-organization, we propose (i) a general network\nrepresentation of large models, (ii) a new analytical framework, called\nNeuron-based Multifractal Analysis (NeuroMFA), for structural analysis, and\n(iii) a novel structure-based metric as a proxy for emergent abilities of large\nmodels. By linking structural features to the capabilities of large models,\nNeuroMFA provides a quantitative framework for analyzing emergent phenomena in\nlarge models. Our experiments show that the proposed method yields a\ncomprehensive measure of network's evolving heterogeneity and organization,\noffering theoretical foundations and a new perspective for investigating\nemergent abilities in large models."
                },
                "authors": [
                    {
                        "name": "Xiongye Xiao"
                    },
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Chenyu Zhou"
                    },
                    {
                        "name": "Defu Cao"
                    },
                    {
                        "name": "Yaxing Li"
                    },
                    {
                        "name": "Yi-Zhuo Zhou"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Nikos Kanakaris"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "arxiv_comment": "ICLR 2025: https://openreview.net/forum?id=nt8gBX58Kh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09099v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09099v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14175v2",
                "updated": "2025-02-04T07:46:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    46,
                    23,
                    1,
                    35,
                    0
                ],
                "published": "2024-09-21T15:32:10Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    15,
                    32,
                    10,
                    5,
                    265,
                    0
                ],
                "title": "QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and\n  Option Shuffling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and\n  Option Shuffling"
                },
                "summary": "Large Language models (LLMs) have brought about substantial advancements in\nthe field of Question Answering (QA) systems. These models do remarkably well\nin addressing intricate inquiries in a variety of disciplines. However, because\nof domain-specific vocabulary, complex technological concepts, and the\nrequirement for exact responses applying LLMs to specialized sectors like\ntelecommunications presents additional obstacles. GPT-3.5 has been used in\nrecent work, to obtain noteworthy accuracy for telecom-related questions in a\nRetrieval Augmented Generation (RAG) framework. Notwithstanding these\ndevelopments, the practical use of models such as GPT-3.5 is restricted by\ntheir proprietary nature and high computing demands. This paper introduces\nQMOS, an innovative approach which uses a Question-Masked loss and Option\nShuffling trick to enhance the performance of LLMs in answering Multiple-Choice\nQuestions in the telecommunications domain. Our focus was on using opensource,\nsmaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework.\nOur multi-faceted approach involves several enhancements to the whole LLM-RAG\npipeline of finetuning, retrieval, prompt engineering and inference. Our\napproaches significantly outperform existing results, achieving accuracy\nimprovements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07%\nto 84.65% with Phi-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have brought about substantial advancements in\nthe field of Question Answering (QA) systems. These models do remarkably well\nin addressing intricate inquiries in a variety of disciplines. However, because\nof domain-specific vocabulary, complex technological concepts, and the\nrequirement for exact responses applying LLMs to specialized sectors like\ntelecommunications presents additional obstacles. GPT-3.5 has been used in\nrecent work, to obtain noteworthy accuracy for telecom-related questions in a\nRetrieval Augmented Generation (RAG) framework. Notwithstanding these\ndevelopments, the practical use of models such as GPT-3.5 is restricted by\ntheir proprietary nature and high computing demands. This paper introduces\nQMOS, an innovative approach which uses a Question-Masked loss and Option\nShuffling trick to enhance the performance of LLMs in answering Multiple-Choice\nQuestions in the telecommunications domain. Our focus was on using opensource,\nsmaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework.\nOur multi-faceted approach involves several enhancements to the whole LLM-RAG\npipeline of finetuning, retrieval, prompt engineering and inference. Our\napproaches significantly outperform existing results, achieving accuracy\nimprovements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07%\nto 84.65% with Phi-2."
                },
                "authors": [
                    {
                        "name": "Blessed Guda"
                    },
                    {
                        "name": "Gabrial Zencha Ashungafac"
                    },
                    {
                        "name": "Lawrence Francis"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "arxiv_journal_ref": "IEEE Globecom Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02072v1",
                "updated": "2025-02-04T07:44:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    44,
                    20,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:44:20Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    44,
                    20,
                    1,
                    35,
                    0
                ],
                "title": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for\n  Detection of Bias, Discrimination and Stereotyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for\n  Detection of Bias, Discrimination and Stereotyping"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has transformed natural\nlanguage processing but raises critical concerns about biases inherent in their\ndeployment and use across diverse linguistic and sociocultural contexts. This\npaper presents a framework named ASCenD BDS (Adaptable, Stochastic and\nContext-aware framework for Detection of Bias, Discrimination and\nStereotyping). The framework presents approach to detecting bias,\ndiscrimination, stereotyping across various categories such as gender, caste,\nage, disability, socioeconomic status, linguistic variations, etc., using an\napproach which is Adaptive, Stochastic and Context-Aware. The existing\nframeworks rely heavily on usage of datasets to generate scenarios for\ndetection of Bias, Discrimination and Stereotyping. Examples include datasets\nsuch as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.\nHowever, such an approach provides point solutions. As a result, these datasets\nprovide a finite number of scenarios for assessment. The current framework\novercomes this limitation by having features which enable Adaptability,\nStochasticity, Context Awareness. Context awareness can be customized for any\nnation or culture or sub-culture (for example an organization's unique\nculture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a\ncommonality of categorization. A framework has been developed using Category,\nSub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,\nStochasticity and Context awareness. The framework has been described in detail\nin Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories\nwere developed by a team of consultants at Saint Fox Consultancy Private Ltd.\nThe concept has been tested out in SFCLabs as part of product development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has transformed natural\nlanguage processing but raises critical concerns about biases inherent in their\ndeployment and use across diverse linguistic and sociocultural contexts. This\npaper presents a framework named ASCenD BDS (Adaptable, Stochastic and\nContext-aware framework for Detection of Bias, Discrimination and\nStereotyping). The framework presents approach to detecting bias,\ndiscrimination, stereotyping across various categories such as gender, caste,\nage, disability, socioeconomic status, linguistic variations, etc., using an\napproach which is Adaptive, Stochastic and Context-Aware. The existing\nframeworks rely heavily on usage of datasets to generate scenarios for\ndetection of Bias, Discrimination and Stereotyping. Examples include datasets\nsuch as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.\nHowever, such an approach provides point solutions. As a result, these datasets\nprovide a finite number of scenarios for assessment. The current framework\novercomes this limitation by having features which enable Adaptability,\nStochasticity, Context Awareness. Context awareness can be customized for any\nnation or culture or sub-culture (for example an organization's unique\nculture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a\ncommonality of categorization. A framework has been developed using Category,\nSub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,\nStochasticity and Context awareness. The framework has been described in detail\nin Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories\nwere developed by a team of consultants at Saint Fox Consultancy Private Ltd.\nThe concept has been tested out in SFCLabs as part of product development."
                },
                "authors": [
                    {
                        "name": "Rajiv Bahl"
                    },
                    {
                        "name": "Venkatesan N"
                    },
                    {
                        "name": "Parimal Aglawe"
                    },
                    {
                        "name": "Aastha Sarasapalli"
                    },
                    {
                        "name": "Bhavya Kancharla"
                    },
                    {
                        "name": "Chaitanya kolukuluri"
                    },
                    {
                        "name": "Harish Mohite"
                    },
                    {
                        "name": "Japneet Hora"
                    },
                    {
                        "name": "Kiran Kakollu"
                    },
                    {
                        "name": "Rahul Diman"
                    },
                    {
                        "name": "Shubham Kapale"
                    },
                    {
                        "name": "Sri Bhagya Kathula"
                    },
                    {
                        "name": "Vamsikrishna Motru"
                    },
                    {
                        "name": "Yogeshwar Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Yogeshwar Reddy"
                },
                "author": "Yogeshwar Reddy",
                "arxiv_comment": "17 pages, 6 Figures and this manuscript will be submitted to Q1,Q2\n  Journals",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02068v1",
                "updated": "2025-02-04T07:35:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    35,
                    28,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:35:28Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    35,
                    28,
                    1,
                    35,
                    0
                ],
                "title": "Robust and Secure Code Watermarking for Large Language Models via\n  ML/Crypto Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Secure Code Watermarking for Large Language Models via\n  ML/Crypto Codesign"
                },
                "summary": "This paper introduces RoSe, the first-of-its-kind ML/Crypto codesign\nwatermarking framework that regulates LLM-generated code to avoid intellectual\nproperty rights violations and inappropriate misuse in software development.\nHigh-quality watermarks adhering to the detectability-fidelity-robustness\ntri-objective are limited due to codes' low-entropy nature. Watermark\nverification, however, often needs to reveal the signature and requires\nre-encoding new ones for code reuse, which potentially compromising the\nsystem's usability. To overcome these challenges, RoSe obtains high-quality\nwatermarks by training the watermark insertion and extraction modules\nend-to-end to ensure (i) unaltered watermarked code functionality and (ii)\nenhanced detectability and robustness leveraging pre-trained CodeT5 as the\ninsertion backbone to enlarge the code syntactic and variable rename\ntransformation search space. In the deployment, RoSe uses zero-knowledge proofs\nfor secure verification without revealing the underlying signatures. Extensive\nevaluations demonstrated RoSe achieves high detection accuracy while preserving\nthe code functionality. RoSe is also robust against attacks and provides\nefficient secure watermark verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RoSe, the first-of-its-kind ML/Crypto codesign\nwatermarking framework that regulates LLM-generated code to avoid intellectual\nproperty rights violations and inappropriate misuse in software development.\nHigh-quality watermarks adhering to the detectability-fidelity-robustness\ntri-objective are limited due to codes' low-entropy nature. Watermark\nverification, however, often needs to reveal the signature and requires\nre-encoding new ones for code reuse, which potentially compromising the\nsystem's usability. To overcome these challenges, RoSe obtains high-quality\nwatermarks by training the watermark insertion and extraction modules\nend-to-end to ensure (i) unaltered watermarked code functionality and (ii)\nenhanced detectability and robustness leveraging pre-trained CodeT5 as the\ninsertion backbone to enlarge the code syntactic and variable rename\ntransformation search space. In the deployment, RoSe uses zero-knowledge proofs\nfor secure verification without revealing the underlying signatures. Extensive\nevaluations demonstrated RoSe achieves high detection accuracy while preserving\nthe code functionality. RoSe is also robust against attacks and provides\nefficient secure watermark verification."
                },
                "authors": [
                    {
                        "name": "Ruisi Zhang"
                    },
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Nojan Sheybani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02067v1",
                "updated": "2025-02-04T07:32:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    32,
                    39,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:32:39Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    32,
                    39,
                    1,
                    35,
                    0
                ],
                "title": "AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement"
                },
                "summary": "Embodied agents assisting humans are often asked to complete a new task in a\nnew scenario. An agent preparing a particular dish in the kitchen based on a\nknown recipe may be asked to prepare a new dish or to perform cleaning tasks in\nthe storeroom. There may not be sufficient resources, e.g., time or labeled\nexamples, to train the agent for these new situations. Large Language Models\n(LLMs) trained on considerable knowledge across many domains are able to\npredict a sequence of abstract actions for such new tasks and scenarios,\nalthough it may not be possible for the agent to execute this action sequence\ndue to task-, agent-, or domain-specific constraints. Our framework addresses\nthese challenges by leveraging the generic predictions provided by LLM and the\nprior domain-specific knowledge encoded in a Knowledge Graph (KG), enabling an\nagent to quickly adapt to new tasks and scenarios. The robot also solicits and\nuses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation over cooking and cleaning tasks in simulation domains,\nwe demonstrate that the interplay between LLM, KG, and human input leads to\nsubstantial performance gains compared with just using the LLM output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied agents assisting humans are often asked to complete a new task in a\nnew scenario. An agent preparing a particular dish in the kitchen based on a\nknown recipe may be asked to prepare a new dish or to perform cleaning tasks in\nthe storeroom. There may not be sufficient resources, e.g., time or labeled\nexamples, to train the agent for these new situations. Large Language Models\n(LLMs) trained on considerable knowledge across many domains are able to\npredict a sequence of abstract actions for such new tasks and scenarios,\nalthough it may not be possible for the agent to execute this action sequence\ndue to task-, agent-, or domain-specific constraints. Our framework addresses\nthese challenges by leveraging the generic predictions provided by LLM and the\nprior domain-specific knowledge encoded in a Knowledge Graph (KG), enabling an\nagent to quickly adapt to new tasks and scenarios. The robot also solicits and\nuses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation over cooking and cleaning tasks in simulation domains,\nwe demonstrate that the interplay between LLM, KG, and human input leads to\nsubstantial performance gains compared with just using the LLM output."
                },
                "authors": [
                    {
                        "name": "Shivam Singh"
                    },
                    {
                        "name": "Karthik Swaminathan"
                    },
                    {
                        "name": "Nabanita Dash"
                    },
                    {
                        "name": "Ramandeep Singh"
                    },
                    {
                        "name": "Snehasis Banerjee"
                    },
                    {
                        "name": "Mohan Sridharan"
                    },
                    {
                        "name": "Madhava Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Madhava Krishna"
                },
                "author": "Madhava Krishna",
                "arxiv_comment": "Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02066v1",
                "updated": "2025-02-04T07:31:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    31,
                    55,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:31:55Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    31,
                    55,
                    1,
                    35,
                    0
                ],
                "title": "Anticipate & Act : Integrating LLMs and Classical Planning for Efficient\n  Task Execution in Household Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anticipate & Act : Integrating LLMs and Classical Planning for Efficient\n  Task Execution in Household Environments"
                },
                "summary": "Assistive agents performing household tasks such as making the bed or cooking\nbreakfast often compute and execute actions that accomplish one task at a time.\nHowever, efficiency can be improved by anticipating upcoming tasks and\ncomputing an action sequence that jointly achieves these tasks.\nState-of-the-art methods for task anticipation use data-driven deep networks\nand Large Language Models (LLMs), but they do so at the level of high-level\ntasks and/or require many training examples. Our framework leverages the\ngeneric knowledge of LLMs through a small number of prompts to perform\nhigh-level task anticipation, using the anticipated tasks as goals in a\nclassical planning system to compute a sequence of finer-granularity actions\nthat jointly achieve these goals. We ground and evaluate our framework's\nabilities in realistic scenarios in the VirtualHome environment and demonstrate\na 31% reduction in execution time compared with a system that does not consider\nupcoming tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistive agents performing household tasks such as making the bed or cooking\nbreakfast often compute and execute actions that accomplish one task at a time.\nHowever, efficiency can be improved by anticipating upcoming tasks and\ncomputing an action sequence that jointly achieves these tasks.\nState-of-the-art methods for task anticipation use data-driven deep networks\nand Large Language Models (LLMs), but they do so at the level of high-level\ntasks and/or require many training examples. Our framework leverages the\ngeneric knowledge of LLMs through a small number of prompts to perform\nhigh-level task anticipation, using the anticipated tasks as goals in a\nclassical planning system to compute a sequence of finer-granularity actions\nthat jointly achieve these goals. We ground and evaluate our framework's\nabilities in realistic scenarios in the VirtualHome environment and demonstrate\na 31% reduction in execution time compared with a system that does not consider\nupcoming tasks."
                },
                "authors": [
                    {
                        "name": "Raghav Arora"
                    },
                    {
                        "name": "Shivam Singh"
                    },
                    {
                        "name": "Karthik Swaminathan"
                    },
                    {
                        "name": "Ahana Datta"
                    },
                    {
                        "name": "Snehasis Banerjee"
                    },
                    {
                        "name": "Brojeshwar Bhowmick"
                    },
                    {
                        "name": "Krishna Murthy Jatavallabhula"
                    },
                    {
                        "name": "Mohan Sridharan"
                    },
                    {
                        "name": "Madhava Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Madhava Krishna"
                },
                "author": "Madhava Krishna",
                "arxiv_comment": "Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20242v4",
                "updated": "2025-02-04T07:24:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    24,
                    35,
                    1,
                    35,
                    0
                ],
                "published": "2024-07-16T13:13:16Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    13,
                    13,
                    16,
                    1,
                    198,
                    0
                ],
                "title": "BadRobot: Jailbreaking Embodied LLMs in the Physical World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadRobot: Jailbreaking Embodied LLMs in the Physical World"
                },
                "summary": "Embodied AI represents systems where AI is integrated into physical entities.\nLarge Language Model (LLM), which exhibits powerful language understanding\nabilities, has been extensively employed in embodied AI by facilitating\nsophisticated task planning. However, a critical safety issue remains\noverlooked: could these embodied LLMs perpetrate harmful behaviors? In\nresponse, we introduce BadRobot, a novel attack paradigm aiming to make\nembodied LLMs violate safety and ethical constraints through typical\nvoice-based user-system interactions. Specifically, three vulnerabilities are\nexploited to achieve this type of attack: (i) manipulation of LLMs within\nrobotic systems, (ii) misalignment between linguistic outputs and physical\nactions, and (iii) unintentional hazardous behaviors caused by world\nknowledge's flaws. Furthermore, we construct a benchmark of various malicious\nphysical action queries to evaluate BadRobot's attack performance. Based on\nthis benchmark, extensive experiments against existing prominent embodied LLM\nframeworks (e.g., Voxposer, Code as Policies, and ProgPrompt) demonstrate the\neffectiveness of our BadRobot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI represents systems where AI is integrated into physical entities.\nLarge Language Model (LLM), which exhibits powerful language understanding\nabilities, has been extensively employed in embodied AI by facilitating\nsophisticated task planning. However, a critical safety issue remains\noverlooked: could these embodied LLMs perpetrate harmful behaviors? In\nresponse, we introduce BadRobot, a novel attack paradigm aiming to make\nembodied LLMs violate safety and ethical constraints through typical\nvoice-based user-system interactions. Specifically, three vulnerabilities are\nexploited to achieve this type of attack: (i) manipulation of LLMs within\nrobotic systems, (ii) misalignment between linguistic outputs and physical\nactions, and (iii) unintentional hazardous behaviors caused by world\nknowledge's flaws. Furthermore, we construct a benchmark of various malicious\nphysical action queries to evaluate BadRobot's attack performance. Based on\nthis benchmark, extensive experiments against existing prominent embodied LLM\nframeworks (e.g., Voxposer, Code as Policies, and ProgPrompt) demonstrate the\neffectiveness of our BadRobot."
                },
                "authors": [
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Changgan Yin"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "arxiv_comment": "Accepted to ICLR 2025. Project page:\n  https://Embodied-LLMs-Safety.github.io",
                "arxiv_journal_ref": "International Conference on Learning Representations (ICLR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00212v2",
                "updated": "2025-02-04T07:20:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    20,
                    28,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-31T23:01:48Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    23,
                    1,
                    48,
                    4,
                    31,
                    0
                ],
                "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and\n  Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and\n  Proving"
                },
                "summary": "A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens\ngenerated during the training in Lean, STP proves 26.3% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (61.1%, pass@3200),\nProofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@64).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens\ngenerated during the training in Lean, STP proves 26.3% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (61.1%, pass@3200),\nProofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@64)."
                },
                "authors": [
                    {
                        "name": "Kefan Dong"
                    },
                    {
                        "name": "Tengyu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Tengyu Ma"
                },
                "author": "Tengyu Ma",
                "arxiv_comment": "22 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02061v1",
                "updated": "2025-02-04T07:17:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    17,
                    54,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:17:54Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    17,
                    54,
                    1,
                    35,
                    0
                ],
                "title": "Large Language Models for Recommendation with Deliberative User\n  Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Recommendation with Deliberative User\n  Preference Alignment"
                },
                "summary": "While recent advancements in aligning Large Language Models (LLMs) with\nrecommendation tasks have shown great potential and promising performance\noverall, these aligned recommendation LLMs still face challenges in complex\nscenarios. This is primarily due to the current alignment approach focusing on\noptimizing LLMs to generate user feedback directly, without incorporating\ndeliberation. To overcome this limitation and develop more reliable LLMs for\nrecommendations, we propose a new Deliberative Recommendation task, which\nincorporates explicit reasoning about user preferences as an additional\nalignment goal. We then introduce the Deliberative User Preference Alignment\nframework, designed to enhance reasoning capabilities by utilizing verbalized\nuser feedback in a step-wise manner to tackle this task. The framework employs\ncollaborative step-wise experts and tailored training strategies for each\nexpert. Experimental results across three real-world datasets demonstrate the\nrationality of the deliberative task formulation and the superior performance\nof the proposed framework in improving both prediction accuracy and reasoning\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent advancements in aligning Large Language Models (LLMs) with\nrecommendation tasks have shown great potential and promising performance\noverall, these aligned recommendation LLMs still face challenges in complex\nscenarios. This is primarily due to the current alignment approach focusing on\noptimizing LLMs to generate user feedback directly, without incorporating\ndeliberation. To overcome this limitation and develop more reliable LLMs for\nrecommendations, we propose a new Deliberative Recommendation task, which\nincorporates explicit reasoning about user preferences as an additional\nalignment goal. We then introduce the Deliberative User Preference Alignment\nframework, designed to enhance reasoning capabilities by utilizing verbalized\nuser feedback in a step-wise manner to tackle this task. The framework employs\ncollaborative step-wise experts and tailored training strategies for each\nexpert. Experimental results across three real-world datasets demonstrate the\nrationality of the deliberative task formulation and the superior performance\nof the proposed framework in improving both prediction accuracy and reasoning\nquality."
                },
                "authors": [
                    {
                        "name": "Yi Fang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14917v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14917v3",
                "updated": "2025-02-04T07:12:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    12,
                    5,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-24T20:54:29Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    20,
                    54,
                    29,
                    4,
                    24,
                    0
                ],
                "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach"
                },
                "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\nHegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical\napproach to emulate internal critiques and then synthesize new ideas by\nresolving the contradicting points. Moreover, this paper investigates the\neffect of LLMs' temperature for generation by establishing a dynamic annealing\napproach, which promotes the creativity in the early stages and gradually\nrefines it by focusing on the nuances, as well as a fixed temperature strategy\nfor generation. Our proposed approach is examined to determine its ability to\ngenerate novel ideas from an initial proposition. Additionally, a Multi Agent\nMajority Voting (MAMV) strategy is leveraged to assess the validity and novelty\nof the generated ideas, which proves beneficial in the absence of domain\nexperts. Our experiments show promise in generating new ideas and provide a\nstepping stone for future research."
                },
                "authors": [
                    {
                        "name": "Sara Abdali"
                    },
                    {
                        "name": "Can Goksen"
                    },
                    {
                        "name": "Saeed Amizadeh"
                    },
                    {
                        "name": "Kazuhito Koishida"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhito Koishida"
                },
                "author": "Kazuhito Koishida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14917v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14917v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12643v2",
                "updated": "2025-02-04T06:55:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    6,
                    55,
                    49,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-19T16:54:30Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    54,
                    30,
                    1,
                    324,
                    0
                ],
                "title": "DLBacktrace: A Model Agnostic Explainability for any Deep Learning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLBacktrace: A Model Agnostic Explainability for any Deep Learning\n  Models"
                },
                "summary": "The rapid growth of AI has led to more complex deep learning models, often\noperating as opaque \"black boxes\" with limited transparency in their\ndecision-making. This lack of interpretability poses challenges, especially in\nhigh-stakes applications where understanding model output is crucial. This work\nhighlights the importance of interpretability in fostering trust,\naccountability, and responsible deployment. To address these challenges, we\nintroduce DLBacktrace, a novel, model-agnostic technique designed to provide\nclear insights into deep learning model decisions across a wide range of\ndomains and architectures, including MLPs, CNNs, and Transformer-based LLM\nmodels. We present a comprehensive overview of DLBacktrace and benchmark its\nperformance against established interpretability methods such as SHAP, LIME,\nand GradCAM. Our results demonstrate that DLBacktrace effectively enhances\nunderstanding of model behavior across diverse tasks. DLBacktrace is compatible\nwith models developed in both PyTorch and TensorFlow, supporting architectures\nsuch as BERT, ResNet, U-Net, and custom DNNs for tabular data. The library is\nopen-sourced and available at https://github.com/AryaXAI/DLBacktrace .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of AI has led to more complex deep learning models, often\noperating as opaque \"black boxes\" with limited transparency in their\ndecision-making. This lack of interpretability poses challenges, especially in\nhigh-stakes applications where understanding model output is crucial. This work\nhighlights the importance of interpretability in fostering trust,\naccountability, and responsible deployment. To address these challenges, we\nintroduce DLBacktrace, a novel, model-agnostic technique designed to provide\nclear insights into deep learning model decisions across a wide range of\ndomains and architectures, including MLPs, CNNs, and Transformer-based LLM\nmodels. We present a comprehensive overview of DLBacktrace and benchmark its\nperformance against established interpretability methods such as SHAP, LIME,\nand GradCAM. Our results demonstrate that DLBacktrace effectively enhances\nunderstanding of model behavior across diverse tasks. DLBacktrace is compatible\nwith models developed in both PyTorch and TensorFlow, supporting architectures\nsuch as BERT, ResNet, U-Net, and custom DNNs for tabular data. The library is\nopen-sourced and available at https://github.com/AryaXAI/DLBacktrace ."
                },
                "authors": [
                    {
                        "name": "Vinay Kumar Sankarapu"
                    },
                    {
                        "name": "Chintan Chitroda"
                    },
                    {
                        "name": "Yashwardhan Rathore"
                    },
                    {
                        "name": "Neeraj Kumar Singh"
                    },
                    {
                        "name": "Pratinav Seth"
                    }
                ],
                "author_detail": {
                    "name": "Pratinav Seth"
                },
                "author": "Pratinav Seth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09078v2",
                "updated": "2025-02-04T06:53:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    6,
                    53,
                    38,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-12T09:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency."
                },
                "authors": [
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Chuanjian Liu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02048v1",
                "updated": "2025-02-04T06:30:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    6,
                    30,
                    12,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T06:30:12Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    6,
                    30,
                    12,
                    1,
                    35,
                    0
                ],
                "title": "Efficient Domain Adaptation of Multimodal Embeddings using Constrastive\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Domain Adaptation of Multimodal Embeddings using Constrastive\n  Learning"
                },
                "summary": "Recent advancements in machine learning (ML), natural language processing\n(NLP), and foundational models have shown promise for real-life applications in\ncritical, albeit compute-constrainted fields like healthcare.\n  In such areas, combining foundational models with supervised ML offers\npotential for automating tasks like diagnosis and treatment planning, but the\nlimited availability of onsite computational resources pose significant\nchallenges before applying these technologies effectively: Current approaches\neither yield subpar results when using pretrained models without task-specific\nadaptation, or require substantial computational resources for fine-tuning,\nwhich is often a barrier to entry in such environments.\n  This renders them inaccessible in applications where performance and quality\nstandards are high, but computational resources are scarce.\n  To bridge the gap between best-in-class performance and accessibility, we\npropose a novel method for adapting foundational, multimodal embeddings to\ndownstream tasks, without the need of expensive fine-tuning processes.\n  Our method leverages frozen embeddings from Large Language Models (LLMs) and\nVision Models, and uses contrastive learning to train a small, task-specific\nnonlinear projection that can be used in the downstream task, without having to\nfine-tune the original foundational models.\n  We show that this efficient procedure leads to significant performance\nimprovements across various downstream tasks, and perhaps more importantly with\nminimal computational overhead, offering a practical solution for the use of\nadvanced, foundational ML models in resource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning (ML), natural language processing\n(NLP), and foundational models have shown promise for real-life applications in\ncritical, albeit compute-constrainted fields like healthcare.\n  In such areas, combining foundational models with supervised ML offers\npotential for automating tasks like diagnosis and treatment planning, but the\nlimited availability of onsite computational resources pose significant\nchallenges before applying these technologies effectively: Current approaches\neither yield subpar results when using pretrained models without task-specific\nadaptation, or require substantial computational resources for fine-tuning,\nwhich is often a barrier to entry in such environments.\n  This renders them inaccessible in applications where performance and quality\nstandards are high, but computational resources are scarce.\n  To bridge the gap between best-in-class performance and accessibility, we\npropose a novel method for adapting foundational, multimodal embeddings to\ndownstream tasks, without the need of expensive fine-tuning processes.\n  Our method leverages frozen embeddings from Large Language Models (LLMs) and\nVision Models, and uses contrastive learning to train a small, task-specific\nnonlinear projection that can be used in the downstream task, without having to\nfine-tune the original foundational models.\n  We show that this efficient procedure leads to significant performance\nimprovements across various downstream tasks, and perhaps more importantly with\nminimal computational overhead, offering a practical solution for the use of\nadvanced, foundational ML models in resource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Georgios Margaritis"
                    },
                    {
                        "name": "Periklis Petridis"
                    },
                    {
                        "name": "Dimitris J. Bertsimas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris J. Bertsimas"
                },
                "author": "Dimitris J. Bertsimas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14304v2",
                "updated": "2025-02-04T06:26:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    6,
                    26,
                    8,
                    1,
                    35,
                    0
                ],
                "published": "2025-01-24T08:01:11Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    1,
                    11,
                    4,
                    24,
                    0
                ],
                "title": "MASTER: A Multi-Agent System with LLM Specialized MCTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASTER: A Multi-Agent System with LLM Specialized MCTS"
                },
                "summary": "Large Language Models (LLM) are increasingly being explored for\nproblem-solving tasks. However, their strategic planning capability is often\nviewed with skepticism. Recent studies have incorporated the Monte Carlo Tree\nSearch (MCTS) algorithm to augment the planning capacity of LLM. Despite its\npotential, MCTS relies on extensive sampling simulations to approximate the\ntrue reward distribution, which leads to two primary issues. Firstly, MCTS is\neffective for tasks like the Game of Go, where simulation results can yield\nobjective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such\nas question answering, the result of a simulation is the answer to the\nquestion, which cannot yield an objective reward without the ground truth.\nSecondly, obtaining statistically significant reward estimations typically\nrequires a sample size exceeding 30 simulations, resulting in excessive token\nusage and time consumption. To address these challenges, we present the\nMulti-Agent System with Tactical Execution and Reasoning using LLM Specialized\nMCTS (MASTER), a novel framework that coordinates agent recruitment and\ncommunication through LLM specialized MCTS. This system autonomously adjusts\nthe number of agents based on task complexity and ensures focused communication\namong them. Comprehensive experiments across various tasks demonstrate the\neffectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA\nand 80% on WebShop, setting new state-of-the-art performance on these datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are increasingly being explored for\nproblem-solving tasks. However, their strategic planning capability is often\nviewed with skepticism. Recent studies have incorporated the Monte Carlo Tree\nSearch (MCTS) algorithm to augment the planning capacity of LLM. Despite its\npotential, MCTS relies on extensive sampling simulations to approximate the\ntrue reward distribution, which leads to two primary issues. Firstly, MCTS is\neffective for tasks like the Game of Go, where simulation results can yield\nobjective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such\nas question answering, the result of a simulation is the answer to the\nquestion, which cannot yield an objective reward without the ground truth.\nSecondly, obtaining statistically significant reward estimations typically\nrequires a sample size exceeding 30 simulations, resulting in excessive token\nusage and time consumption. To address these challenges, we present the\nMulti-Agent System with Tactical Execution and Reasoning using LLM Specialized\nMCTS (MASTER), a novel framework that coordinates agent recruitment and\ncommunication through LLM specialized MCTS. This system autonomously adjusts\nthe number of agents based on task complexity and ensures focused communication\namong them. Comprehensive experiments across various tasks demonstrate the\neffectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA\nand 80% on WebShop, setting new state-of-the-art performance on these datasets."
                },
                "authors": [
                    {
                        "name": "Bingzheng Gan"
                    },
                    {
                        "name": "Yufan Zhao"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yusu Li"
                    },
                    {
                        "name": "Shu Xian Teo"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Wei Shi"
                    }
                ],
                "author_detail": {
                    "name": "Wei Shi"
                },
                "author": "Wei Shi",
                "arxiv_comment": "Accepted by main NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02040v1",
                "updated": "2025-02-04T06:13:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    6,
                    13,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T06:13:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    6,
                    13,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer\n  Inference"
                },
                "summary": "Residual transformations enhance the representational depth and expressive\npower of large language models (LLMs). However, applying static residual\ntransformations across all tokens in auto-regressive generation leads to a\nsuboptimal trade-off between inference efficiency and generation fidelity.\nExisting methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth\naddress this by modulating the residual transformation based on token-level\ncomplexity. Nevertheless, these approaches predominantly consider the distance\ntraversed by tokens through the model layers, neglecting the underlying\nvelocity of residual evolution. We introduce Mixture of Multi-rate Residuals\n(M2R2), a framework that dynamically modulates residual velocity to improve\nearly alignment, enhancing inference efficiency. Evaluations on reasoning\noriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2\nsurpasses state-of-the-art distance-based strategies, balancing generation\nquality and speedup. In self-speculative decoding setup, M2R2 achieves up to\n2.8x speedups on MT-Bench, outperforming methods like 2-model speculative\ndecoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE)\narchitectures, integrating early residual alignment with ahead-of-time expert\nloading into high-bandwidth memory (HBM) accelerates decoding, reduces\nexpert-switching bottlenecks, and achieves a 2.9x speedup, making it highly\neffective in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual transformations enhance the representational depth and expressive\npower of large language models (LLMs). However, applying static residual\ntransformations across all tokens in auto-regressive generation leads to a\nsuboptimal trade-off between inference efficiency and generation fidelity.\nExisting methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth\naddress this by modulating the residual transformation based on token-level\ncomplexity. Nevertheless, these approaches predominantly consider the distance\ntraversed by tokens through the model layers, neglecting the underlying\nvelocity of residual evolution. We introduce Mixture of Multi-rate Residuals\n(M2R2), a framework that dynamically modulates residual velocity to improve\nearly alignment, enhancing inference efficiency. Evaluations on reasoning\noriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2\nsurpasses state-of-the-art distance-based strategies, balancing generation\nquality and speedup. In self-speculative decoding setup, M2R2 achieves up to\n2.8x speedups on MT-Bench, outperforming methods like 2-model speculative\ndecoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE)\narchitectures, integrating early residual alignment with ahead-of-time expert\nloading into high-bandwidth memory (HBM) accelerates decoding, reduces\nexpert-switching bottlenecks, and achieves a 2.9x speedup, making it highly\neffective in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Mahyar Najibi"
                    },
                    {
                        "name": "Devang Naik"
                    },
                    {
                        "name": "Irina Belousova"
                    }
                ],
                "author_detail": {
                    "name": "Irina Belousova"
                },
                "author": "Irina Belousova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00412v2",
                "updated": "2025-02-04T06:11:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    6,
                    11,
                    55,
                    1,
                    35,
                    0
                ],
                "published": "2024-11-01T07:18:31Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    18,
                    31,
                    4,
                    306,
                    0
                ],
                "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with\n  Intelligent Tool Usage Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting While Learning: Grounding LLMs for Scientific Problems with\n  Intelligent Tool Usage Adaptation"
                },
                "summary": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nsimple scientific problems but, even with domain-specific fine-tuning, often\nproduce hallucinations for complex ones. While integrating LLMs with tools can\nmitigate this reliability issue, models finetuned on tool usage only often\nover-rely on them, incurring unnecessary costs from resource-intensive\nscientific tools even for simpler problems. Inspired by how human experts\nassess the complexity of the problem before choosing the solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tools-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we classify questions as easy or hard based on the\nWKL-trained model's accuracy, and train it to maintain direct reasoning for\nsimple problems while switching to tools for challenging ones. We validate our\nmethod on 6 scientific benchmark datasets in climate science, epidemiology, and\nmathematics. Compared to the base 8B model, our trained models achieve 28.27%\nhigher answer accuracy and 13.76% better tool usage accuracy, even surpassing\nstate-of-the-art models including GPT-4 and Claude-3.5 on 4 custom-created\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nsimple scientific problems but, even with domain-specific fine-tuning, often\nproduce hallucinations for complex ones. While integrating LLMs with tools can\nmitigate this reliability issue, models finetuned on tool usage only often\nover-rely on them, incurring unnecessary costs from resource-intensive\nscientific tools even for simpler problems. Inspired by how human experts\nassess the complexity of the problem before choosing the solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tools-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we classify questions as easy or hard based on the\nWKL-trained model's accuracy, and train it to maintain direct reasoning for\nsimple problems while switching to tools for challenging ones. We validate our\nmethod on 6 scientific benchmark datasets in climate science, epidemiology, and\nmathematics. Compared to the base 8B model, our trained models achieve 28.27%\nhigher answer accuracy and 13.76% better tool usage accuracy, even surpassing\nstate-of-the-art models including GPT-4 and Claude-3.5 on 4 custom-created\ndatasets."
                },
                "authors": [
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Yadi Cao"
                    },
                    {
                        "name": "Duncan Watson-Parris"
                    },
                    {
                        "name": "Leon Bergen"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Rose Yu"
                    }
                ],
                "author_detail": {
                    "name": "Rose Yu"
                },
                "author": "Rose Yu",
                "arxiv_comment": "32 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02025v1",
                "updated": "2025-02-04T05:21:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    5,
                    21,
                    29,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T05:21:29Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    5,
                    21,
                    29,
                    1,
                    35,
                    0
                ],
                "title": "From Accidents to Insights: Leveraging Multimodal Data for\n  Scenario-Driven ADS Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Accidents to Insights: Leveraging Multimodal Data for\n  Scenario-Driven ADS Testing"
                },
                "summary": "The rapid advancements in Autonomous Driving Systems (ADS) have necessitated\nrobust software testing to ensure safety and reliability. However, automating\nthe generation of scalable and concrete test scenarios remains a significant\nchallenge. Current scenario-based test case generation methods often face\nlimitations, such as unrealistic scenes and inaccurate vehicle trajectories.\nThese challenges largely result from the loss of map information during data\nextraction and the lack of an effective verification mechanism to mitigate\nhallucinations in large language models (LLMs). This paper introduces TRACE, a\nscenario-based ADS Test case Generation framework for Critical Scenarios. By\nleveraging multimodal data to extract challenging scenarios from real-world car\ncrash reports, TRACE constructs numerous critical test cases with less data,\nsignificantly enhancing ADS bug detection efficiency. Using in-context\nlearning, chain-of-thought prompting, and self-validation approaches, we use\nLLMs to extract environmental and road network information from crash reports.\nFor vehicle trajectory planning, data containing map information and vehicle\ncoordinates serves as a knowledge base to build a ChatGPT-based LLM with\npath-planning capabilities, which we named TrackMate. Based on 50 existing\ncrash reports, our approach successfully tested three ADS models across two\nsimulation platforms, MetaDrive and BeamNG. Of the 290 constructed test\nscenarios, 127 are identified as critical, as they resulted in vehicle\ncollisions. Additionally, user feedback reveals that TRACE demonstrates\nsuperior scenario reconstruction accuracy, with 77.5% of the scenarios being\nrated as 'mostly or 'totally' consistent, compared to only 27% for the most\nrelated SOTA, LCTGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Autonomous Driving Systems (ADS) have necessitated\nrobust software testing to ensure safety and reliability. However, automating\nthe generation of scalable and concrete test scenarios remains a significant\nchallenge. Current scenario-based test case generation methods often face\nlimitations, such as unrealistic scenes and inaccurate vehicle trajectories.\nThese challenges largely result from the loss of map information during data\nextraction and the lack of an effective verification mechanism to mitigate\nhallucinations in large language models (LLMs). This paper introduces TRACE, a\nscenario-based ADS Test case Generation framework for Critical Scenarios. By\nleveraging multimodal data to extract challenging scenarios from real-world car\ncrash reports, TRACE constructs numerous critical test cases with less data,\nsignificantly enhancing ADS bug detection efficiency. Using in-context\nlearning, chain-of-thought prompting, and self-validation approaches, we use\nLLMs to extract environmental and road network information from crash reports.\nFor vehicle trajectory planning, data containing map information and vehicle\ncoordinates serves as a knowledge base to build a ChatGPT-based LLM with\npath-planning capabilities, which we named TrackMate. Based on 50 existing\ncrash reports, our approach successfully tested three ADS models across two\nsimulation platforms, MetaDrive and BeamNG. Of the 290 constructed test\nscenarios, 127 are identified as critical, as they resulted in vehicle\ncollisions. Additionally, user feedback reveals that TRACE demonstrates\nsuperior scenario reconstruction accuracy, with 77.5% of the scenarios being\nrated as 'mostly or 'totally' consistent, compared to only 27% for the most\nrelated SOTA, LCTGen."
                },
                "authors": [
                    {
                        "name": "Siwei Luo"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yao Deng"
                    },
                    {
                        "name": "Xi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zheng"
                },
                "author": "Xi Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]