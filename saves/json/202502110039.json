[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04393v1",
                "updated": "2025-02-06T03:56:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:56:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation"
                },
                "summary": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Wenzhang Sun"
                    },
                    {
                        "name": "Qirui Hou"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Yongjia Ma"
                    },
                    {
                        "name": "Jianxun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jianxun Cui"
                },
                "author": "Jianxun Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v2",
                "updated": "2025-02-05T22:55:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    55,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v2",
                "updated": "2025-02-05T21:44:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    44,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03589v1",
                "updated": "2025-02-05T20:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:09:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference"
                },
                "summary": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Shay Vargaftik"
                    },
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02617v1",
                "updated": "2025-02-04T08:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "PolarQuant: Quantizing KV Caches with Polar Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Quantizing KV Caches with Polar Transformation"
                },
                "summary": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v1",
                "updated": "2025-02-04T03:13:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "14 pages, 11 figures, the first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v1",
                "updated": "2025-02-04T02:23:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v2",
                "updated": "2025-02-03T21:45:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    45,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v1",
                "updated": "2025-02-03T20:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01637v1",
                "updated": "2025-02-03T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Scaling Embedding Layers in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Embedding Layers in Language Models"
                },
                "summary": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS."
                },
                "authors": [
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Edith Cohen"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v1",
                "updated": "2025-02-03T05:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00527v1",
                "updated": "2025-02-01T18:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T18:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration"
                },
                "summary": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models."
                },
                "authors": [
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00439v1",
                "updated": "2025-02-01T14:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T14:16:31Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs"
                },
                "summary": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "11 pages, 4 figures. Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00433v1",
                "updated": "2025-02-01T13:46:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T13:46:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models"
                },
                "summary": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning"
                },
                "authors": [
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00382v1",
                "updated": "2025-02-01T09:41:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T09:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "Masked Generative Nested Transformers with Decode Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Nested Transformers with Decode Time Scaling"
                },
                "summary": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Debapriya Tula"
                    },
                    {
                        "name": "Gagan Jain"
                    },
                    {
                        "name": "Pradeep Shenoy"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Sujoy Paul"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Paul"
                },
                "author": "Sujoy Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v1",
                "updated": "2025-02-01T03:49:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v1",
                "updated": "2025-01-31T18:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v1",
                "updated": "2025-01-31T16:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Mao Xun Huang"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernández"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivanković Nizić"
                    },
                    {
                        "name": "Milko Jaksić"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaquín Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gómez-Albarrán"
                    },
                    {
                        "name": "José-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "José-Luis Sierra"
                },
                "author": "José-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Bürgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jiménez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. García-León"
                    },
                    {
                        "name": "R. García-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqué"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.05177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05177v1",
                "updated": "2025-02-07T18:59:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    59,
                    56,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:59:56Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    59,
                    56,
                    4,
                    38,
                    0
                ],
                "title": "Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with\n  Leading Short-Context Accuray",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with\n  Leading Short-Context Accuray"
                },
                "summary": "Establishing the long-context capability of large vision-language models is\ncrucial for video understanding, high-resolution image understanding,\nmulti-modal agents and reasoning. We introduce Long-VITA, a simple yet\neffective large multi-modal model for long-context visual-language\nunderstanding tasks. It is adept at concurrently processing and analyzing\nmodalities of image, video, and text over 4K frames or 1M tokens while\ndelivering advanced performances on short-context multi-modal tasks. We propose\nan effective multi-modal training schema that starts with large language models\nand proceeds through vision-language alignment, general knowledge learning, and\ntwo sequential stages of long-sequence fine-tuning. We further implement\ncontext-parallelism distributed inference and logits-masked language modeling\nhead to scale Long-VITA to infinitely long inputs of images and texts during\nmodel inference. Regarding training data, Long-VITA is built on a mix of $17$M\nsamples from public datasets only and demonstrates the state-of-the-art\nperformance on various multi-modal benchmarks, compared against recent\ncutting-edge models with internal data. Long-VITA is fully reproducible and\nsupports both NPU and GPU platforms for training and testing. We hope Long-VITA\ncan serve as a competitive baseline and offer valuable insights for the\nopen-source community in advancing long-context multi-modal understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Establishing the long-context capability of large vision-language models is\ncrucial for video understanding, high-resolution image understanding,\nmulti-modal agents and reasoning. We introduce Long-VITA, a simple yet\neffective large multi-modal model for long-context visual-language\nunderstanding tasks. It is adept at concurrently processing and analyzing\nmodalities of image, video, and text over 4K frames or 1M tokens while\ndelivering advanced performances on short-context multi-modal tasks. We propose\nan effective multi-modal training schema that starts with large language models\nand proceeds through vision-language alignment, general knowledge learning, and\ntwo sequential stages of long-sequence fine-tuning. We further implement\ncontext-parallelism distributed inference and logits-masked language modeling\nhead to scale Long-VITA to infinitely long inputs of images and texts during\nmodel inference. Regarding training data, Long-VITA is built on a mix of $17$M\nsamples from public datasets only and demonstrates the state-of-the-art\nperformance on various multi-modal benchmarks, compared against recent\ncutting-edge models with internal data. Long-VITA is fully reproducible and\nsupports both NPU and GPU platforms for training and testing. We hope Long-VITA\ncan serve as a competitive baseline and offer valuable insights for the\nopen-source community in advancing long-context multi-modal understanding."
                },
                "authors": [
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Shaoqi Dong"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Peixian Chen"
                    },
                    {
                        "name": "Mengdan Zhang"
                    },
                    {
                        "name": "Haoyu Cao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "https://github.com/VITA-MLLM/Long-VITA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05174v1",
                "updated": "2025-02-07T18:57:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    57,
                    49,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:57:49Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    57,
                    49,
                    4,
                    38,
                    0
                ],
                "title": "MELON: Indirect Prompt Injection Defense via Masked Re-execution and\n  Tool Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MELON: Indirect Prompt Injection Defense via Masked Re-execution and\n  Tool Comparison"
                },
                "summary": "Recent research has explored that LLM agents are vulnerable to indirect\nprompt injection (IPI) attacks, where malicious tasks embedded in\ntool-retrieved information can redirect the agent to take unauthorized actions.\nExisting defenses against IPI have significant limitations: either require\nessential model training resources, lack effectiveness against sophisticated\nattacks, or harm the normal utilities. We present MELON (Masked re-Execution\nand TooL comparisON), a novel IPI defense. Our approach builds on the\nobservation that under a successful attack, the agent's next action becomes\nless dependent on user tasks and more on malicious tasks. Following this, we\ndesign MELON to detect attacks by re-executing the agent's trajectory with a\nmasked user prompt modified through a masking function. We identify an attack\nif the actions generated in the original and masked executions are similar. We\nalso include three key designs to reduce the potential false positives and\nfalse negatives. Extensive evaluation on the IPI benchmark AgentDojo\ndemonstrates that MELON outperforms SOTA defenses in both attack prevention and\nutility preservation. Moreover, we show that combining MELON with a SOTA prompt\naugmentation defense (denoted as MELON-Aug) further improves its performance.\nWe also conduct a detailed ablation study to validate our key designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has explored that LLM agents are vulnerable to indirect\nprompt injection (IPI) attacks, where malicious tasks embedded in\ntool-retrieved information can redirect the agent to take unauthorized actions.\nExisting defenses against IPI have significant limitations: either require\nessential model training resources, lack effectiveness against sophisticated\nattacks, or harm the normal utilities. We present MELON (Masked re-Execution\nand TooL comparisON), a novel IPI defense. Our approach builds on the\nobservation that under a successful attack, the agent's next action becomes\nless dependent on user tasks and more on malicious tasks. Following this, we\ndesign MELON to detect attacks by re-executing the agent's trajectory with a\nmasked user prompt modified through a masking function. We identify an attack\nif the actions generated in the original and masked executions are similar. We\nalso include three key designs to reduce the potential false positives and\nfalse negatives. Extensive evaluation on the IPI benchmark AgentDojo\ndemonstrates that MELON outperforms SOTA defenses in both attack prevention and\nutility preservation. Moreover, we show that combining MELON with a SOTA prompt\naugmentation defense (denoted as MELON-Aug) further improves its performance.\nWe also conduct a detailed ablation study to validate our key designs."
                },
                "authors": [
                    {
                        "name": "Kaijie Zhu"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Wenbo Guo"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05167v1",
                "updated": "2025-02-07T18:49:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    49,
                    46,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:49:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    49,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoLiMa: Long-Context Evaluation Beyond Literal Matching"
                },
                "summary": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05163v1",
                "updated": "2025-02-07T18:45:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    45,
                    3,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:45:03Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    45,
                    3,
                    4,
                    38,
                    0
                ],
                "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM\n  Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM\n  Guardrails"
                },
                "summary": "The rapid advancement of large language models (LLMs) has increased the need\nfor guardrail models to ensure responsible use, particularly in detecting\nunsafe and illegal content. While substantial safety data exist in English,\nmultilingual guardrail modeling remains underexplored due to the scarcity of\nopen-source safety data in other languages. To address this gap, we propose a\nnovel two-player Reinforcement Learning (RL) framework, where a generator and a\nguardrail model co-evolve adversarially to produce high-quality synthetic data\nfor multilingual guardrail training. We theoretically formalize this\ninteraction as a two-player game, proving convergence to a Nash equilibrium.\nEmpirical evaluations show that our model \\ours outperforms state-of-the-art\nmodels, achieving nearly 10% improvement over LlamaGuard3 (8B) on English\nbenchmarks while being 4.5x faster at inference with a significantly smaller\nmodel (0.5B). We achieve substantial advancements in multilingual safety tasks,\nparticularly in addressing the imbalance for lower-resource languages in a\ncollected real dataset. Ablation studies emphasize the critical role of\nsynthetic data generation in bridging the imbalance in open-source data between\nEnglish and other languages. These findings establish a scalable and efficient\napproach to synthetic data generation, paving the way for improved multilingual\nguardrail models to enhance LLM safety. Code, model, and data will be\nopen-sourced at https://github.com/yihedeng9/DuoGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has increased the need\nfor guardrail models to ensure responsible use, particularly in detecting\nunsafe and illegal content. While substantial safety data exist in English,\nmultilingual guardrail modeling remains underexplored due to the scarcity of\nopen-source safety data in other languages. To address this gap, we propose a\nnovel two-player Reinforcement Learning (RL) framework, where a generator and a\nguardrail model co-evolve adversarially to produce high-quality synthetic data\nfor multilingual guardrail training. We theoretically formalize this\ninteraction as a two-player game, proving convergence to a Nash equilibrium.\nEmpirical evaluations show that our model \\ours outperforms state-of-the-art\nmodels, achieving nearly 10% improvement over LlamaGuard3 (8B) on English\nbenchmarks while being 4.5x faster at inference with a significantly smaller\nmodel (0.5B). We achieve substantial advancements in multilingual safety tasks,\nparticularly in addressing the imbalance for lower-resource languages in a\ncollected real dataset. Ablation studies emphasize the critical role of\nsynthetic data generation in bridging the imbalance in open-source data between\nEnglish and other languages. These findings establish a scalable and efficient\napproach to synthetic data generation, paving the way for improved multilingual\nguardrail models to enhance LLM safety. Code, model, and data will be\nopen-sourced at https://github.com/yihedeng9/DuoGuard."
                },
                "authors": [
                    {
                        "name": "Yihe Deng"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Junkai Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "arxiv_comment": "24 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05159v1",
                "updated": "2025-02-07T18:41:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    41,
                    21,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:41:21Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    41,
                    21,
                    4,
                    38,
                    0
                ],
                "title": "A Lightweight Method to Disrupt Memorized Sequences in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Method to Disrupt Memorized Sequences in LLM"
                },
                "summary": "Large language models (LLMs) demonstrate impressive capabilities across many\ntasks yet risk reproducing copyrighted content verbatim, raising legal and\nethical concerns. Although methods like differential privacy or neuron editing\ncan reduce memorization, they typically require costly retraining or direct\naccess to model weights and may degrade performance. To address these\nchallenges, we propose TokenSwap, a lightweight, post-hoc approach that\nreplaces the probabilities of grammar-related tokens with those from a small\nauxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial\ngrade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method\neffectively reduces well-known cases of memorized generation by upto 10x with\nlittle to no impact on downstream tasks. Our approach offers a uniquely\naccessible and effective solution to users of real-world systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate impressive capabilities across many\ntasks yet risk reproducing copyrighted content verbatim, raising legal and\nethical concerns. Although methods like differential privacy or neuron editing\ncan reduce memorization, they typically require costly retraining or direct\naccess to model weights and may degrade performance. To address these\nchallenges, we propose TokenSwap, a lightweight, post-hoc approach that\nreplaces the probabilities of grammar-related tokens with those from a small\nauxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial\ngrade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method\neffectively reduces well-known cases of memorized generation by upto 10x with\nlittle to no impact on downstream tasks. Our approach offers a uniquely\naccessible and effective solution to users of real-world systems."
                },
                "authors": [
                    {
                        "name": "Parjanya Prajakta Prashant"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Babak Salimi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Salimi"
                },
                "author": "Babak Salimi",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10117v2",
                "updated": "2025-02-07T18:40:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    40,
                    12,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-17T11:05:42Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    5,
                    42,
                    4,
                    17,
                    0
                ],
                "title": "Prediction Sets and Conformal Inference with Censored Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction Sets and Conformal Inference with Censored Outcomes"
                },
                "summary": "Given data on a scalar random variable $Y$, a prediction set for $Y$ with\nmiscoverage level $\\alpha$ is a set of values for $Y$ that contains a randomly\ndrawn $Y$ with probability $1 - \\alpha$, where $\\alpha \\in (0,1)$. Among all\nprediction sets that satisfy this coverage property, the oracle prediction set\nis the one with the smallest volume. This paper provides estimation methods of\nsuch prediction sets given observed conditioning covariates when $Y$ is\n\\textit{censored} or \\textit{measured in intervals}. We first characterise the\noracle prediction set under interval censoring and develop a consistent\nestimator for the shortest prediction {\\it interval} that satisfies this\ncoverage property.These consistency results are extended to accommodate cases\nwhere the prediction set consists of multiple disjoint intervals. We use\nconformal inference to construct a prediction set that achieves finite-sample\nvalidity under censoring and maintains consistency as sample size increases,\nusing a conformity score function designed for interval data. The procedure\naccommodates the prediction uncertainty that is irreducible (due to the\nstochastic nature of outcomes), the modelling uncertainty due to partial\nidentification and also sampling uncertainty that gets reduced as samples get\nlarger. We conduct a set of Monte Carlo simulations and an application to data\nfrom the Current Population Survey. The results highlight the robustness and\nefficiency of the proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given data on a scalar random variable $Y$, a prediction set for $Y$ with\nmiscoverage level $\\alpha$ is a set of values for $Y$ that contains a randomly\ndrawn $Y$ with probability $1 - \\alpha$, where $\\alpha \\in (0,1)$. Among all\nprediction sets that satisfy this coverage property, the oracle prediction set\nis the one with the smallest volume. This paper provides estimation methods of\nsuch prediction sets given observed conditioning covariates when $Y$ is\n\\textit{censored} or \\textit{measured in intervals}. We first characterise the\noracle prediction set under interval censoring and develop a consistent\nestimator for the shortest prediction {\\it interval} that satisfies this\ncoverage property.These consistency results are extended to accommodate cases\nwhere the prediction set consists of multiple disjoint intervals. We use\nconformal inference to construct a prediction set that achieves finite-sample\nvalidity under censoring and maintains consistency as sample size increases,\nusing a conformity score function designed for interval data. The procedure\naccommodates the prediction uncertainty that is irreducible (due to the\nstochastic nature of outcomes), the modelling uncertainty due to partial\nidentification and also sampling uncertainty that gets reduced as samples get\nlarger. We conduct a set of Monte Carlo simulations and an application to data\nfrom the Current Population Survey. The results highlight the robustness and\nefficiency of the proposed methods."
                },
                "authors": [
                    {
                        "name": "Weiguang Liu"
                    },
                    {
                        "name": "Áureo de Paula"
                    },
                    {
                        "name": "Elie Tamer"
                    }
                ],
                "author_detail": {
                    "name": "Elie Tamer"
                },
                "author": "Elie Tamer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14271v2",
                "updated": "2025-02-07T18:37:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    37,
                    21,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-24T06:31:48Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    31,
                    48,
                    4,
                    24,
                    0
                ],
                "title": "TLXML: Task-Level Explanation of Meta-Learning via Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLXML: Task-Level Explanation of Meta-Learning via Influence Functions"
                },
                "summary": "The scheme of adaptation via meta-learning is seen as an ingredient for\nsolving the problem of data shortage or distribution shift in real-world\napplications, but it also brings the new risk of inappropriate updates of the\nmodel in the user environment, which increases the demand for explainability.\nAmong the various types of XAI methods, establishing a method of explanation\nbased on past experience in meta-learning requires special consideration due to\nits bi-level structure of training, which has been left unexplored. In this\nwork, we propose influence functions for explaining meta-learning that measure\nthe sensitivities of training tasks to adaptation and inference. We also argue\nthat the approximation of the Hessian using the Gauss-Newton matrix resolves\ncomputational barriers peculiar to meta-learning. We demonstrate the adequacy\nof the method through experiments on task distinction and task distribution\ndistinction using image classification tasks with MAML and Prototypical\nNetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scheme of adaptation via meta-learning is seen as an ingredient for\nsolving the problem of data shortage or distribution shift in real-world\napplications, but it also brings the new risk of inappropriate updates of the\nmodel in the user environment, which increases the demand for explainability.\nAmong the various types of XAI methods, establishing a method of explanation\nbased on past experience in meta-learning requires special consideration due to\nits bi-level structure of training, which has been left unexplored. In this\nwork, we propose influence functions for explaining meta-learning that measure\nthe sensitivities of training tasks to adaptation and inference. We also argue\nthat the approximation of the Hessian using the Gauss-Newton matrix resolves\ncomputational barriers peculiar to meta-learning. We demonstrate the adequacy\nof the method through experiments on task distinction and task distribution\ndistinction using image classification tasks with MAML and Prototypical\nNetwork."
                },
                "authors": [
                    {
                        "name": "Yoshihiro Mitsuka"
                    },
                    {
                        "name": "Shadan Golestan"
                    },
                    {
                        "name": "Zahin Sufiyan"
                    },
                    {
                        "name": "Sheila Schoepp"
                    },
                    {
                        "name": "Shotaro Miwa"
                    },
                    {
                        "name": "Osmar R. Zaiane"
                    }
                ],
                "author_detail": {
                    "name": "Osmar R. Zaiane"
                },
                "author": "Osmar R. Zaiane",
                "arxiv_comment": "22 pages; v2: modification in metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07163v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07163v3",
                "updated": "2025-02-07T18:34:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    34,
                    28,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-09T17:58:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning"
                },
                "summary": "This work studies the problem of large language model (LLM) unlearning,\naiming to remove unwanted data influences (e.g., copyrighted or harmful\ncontent) while preserving model utility. Despite the increasing demand for\nunlearning, a technically-grounded optimization framework is lacking. Gradient\nascent (GA)-type methods, though widely used, are suboptimal as they reverse\nthe learning process without controlling optimization divergence (i.e.,\ndeviation from the pre-trained state), leading to risks of over-forgetting and\npotential model collapse. Negative preference optimization (NPO) has been\nproposed to address this issue and is considered one of the state-of-the-art\nLLM unlearning approaches. In this work, we revisit NPO and identify another\ncritical issue: reference model bias. This bias arises from using the reference\nmodel (i.e., the model prior to unlearning) to evaluate the unlearning success,\nwhich can compromise NPO's effectiveness. Specifically, it leads to (a) uneven\nallocation of optimization power across forget data with varying difficulty\nlevels and (b) ineffective gradient weight smoothing during the early stages of\nunlearning optimization. To overcome these challenges, we propose a simple yet\neffective unlearning optimization framework, called SimNPO, showing that\n`simplicity' in removing the reliance on a reference model (through the lens of\nsimple preference optimization) benefits unlearning. We provide deeper insights\ninto SimNPO's advantages through an analysis based on mixtures of Markov\nchains. Extensive experiments further validate SimNPO's efficacy on benchmarks\nlike TOFU and MUSE, as well as its robustness against relearning attacks. Codes\nare available at https://github.com/OPTML-Group/Unlearn-Simple.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies the problem of large language model (LLM) unlearning,\naiming to remove unwanted data influences (e.g., copyrighted or harmful\ncontent) while preserving model utility. Despite the increasing demand for\nunlearning, a technically-grounded optimization framework is lacking. Gradient\nascent (GA)-type methods, though widely used, are suboptimal as they reverse\nthe learning process without controlling optimization divergence (i.e.,\ndeviation from the pre-trained state), leading to risks of over-forgetting and\npotential model collapse. Negative preference optimization (NPO) has been\nproposed to address this issue and is considered one of the state-of-the-art\nLLM unlearning approaches. In this work, we revisit NPO and identify another\ncritical issue: reference model bias. This bias arises from using the reference\nmodel (i.e., the model prior to unlearning) to evaluate the unlearning success,\nwhich can compromise NPO's effectiveness. Specifically, it leads to (a) uneven\nallocation of optimization power across forget data with varying difficulty\nlevels and (b) ineffective gradient weight smoothing during the early stages of\nunlearning optimization. To overcome these challenges, we propose a simple yet\neffective unlearning optimization framework, called SimNPO, showing that\n`simplicity' in removing the reliance on a reference model (through the lens of\nsimple preference optimization) benefits unlearning. We provide deeper insights\ninto SimNPO's advantages through an analysis based on mixtures of Markov\nchains. Extensive experiments further validate SimNPO's efficacy on benchmarks\nlike TOFU and MUSE, as well as its robustness against relearning attacks. Codes\nare available at https://github.com/OPTML-Group/Unlearn-Simple."
                },
                "authors": [
                    {
                        "name": "Chongyu Fan"
                    },
                    {
                        "name": "Jiancheng Liu"
                    },
                    {
                        "name": "Licong Lin"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07163v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07163v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05150v1",
                "updated": "2025-02-07T18:26:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    26,
                    15,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:26:15Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    26,
                    15,
                    4,
                    38,
                    0
                ],
                "title": "CodeSCM: Causal Analysis for Multi-Modal Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSCM: Causal Analysis for Multi-Modal Code Generation"
                },
                "summary": "In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for\nanalyzing multi-modal code generation using large language models (LLMs). By\napplying interventions to CodeSCM, we measure the causal effects of different\nprompt modalities, such as natural language, code, and input-output examples,\non the model. CodeSCM introduces latent mediator variables to separate the code\nand natural language semantics of a multi-modal code generation prompt. Using\nthe principles of Causal Mediation Analysis on these mediators we quantify\ndirect effects representing the model's spurious leanings. We find that, in\naddition to natural language instructions, input-output examples significantly\ninfluence code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for\nanalyzing multi-modal code generation using large language models (LLMs). By\napplying interventions to CodeSCM, we measure the causal effects of different\nprompt modalities, such as natural language, code, and input-output examples,\non the model. CodeSCM introduces latent mediator variables to separate the code\nand natural language semantics of a multi-modal code generation prompt. Using\nthe principles of Causal Mediation Analysis on these mediators we quantify\ndirect effects representing the model's spurious leanings. We find that, in\naddition to natural language instructions, input-output examples significantly\ninfluence code generation."
                },
                "authors": [
                    {
                        "name": "Mukur Gupta"
                    },
                    {
                        "name": "Noopur Bhatt"
                    },
                    {
                        "name": "Suman Jana"
                    }
                ],
                "author_detail": {
                    "name": "Suman Jana"
                },
                "author": "Suman Jana",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05148v1",
                "updated": "2025-02-07T18:26:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    26,
                    1,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:26:01Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    26,
                    1,
                    4,
                    38,
                    0
                ],
                "title": "An Annotated Reading of 'The Singer of Tales' in the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Annotated Reading of 'The Singer of Tales' in the LLM Era"
                },
                "summary": "The Parry-Lord oral-formulaic theory was a breakthrough in understanding how\noral narrative poetry is learned, composed, and transmitted by illiterate\nbards. In this paper, we provide an annotated reading of the mechanism\nunderlying this theory from the lens of large language models (LLMs) and\ngenerative artificial intelligence (AI). We point out the the similarities and\ndifferences between oral composition and LLM generation, and comment on the\nimplications to society and AI policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Parry-Lord oral-formulaic theory was a breakthrough in understanding how\noral narrative poetry is learned, composed, and transmitted by illiterate\nbards. In this paper, we provide an annotated reading of the mechanism\nunderlying this theory from the lens of large language models (LLMs) and\ngenerative artificial intelligence (AI). We point out the the similarities and\ndifferences between oral composition and LLM generation, and comment on the\nimplications to society and AI policy."
                },
                "authors": [
                    {
                        "name": "Kush R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Kush R. Varshney"
                },
                "author": "Kush R. Varshney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15485v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15485v3",
                "updated": "2025-02-07T18:25:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    25,
                    16,
                    4,
                    38,
                    0
                ],
                "published": "2024-01-27T19:24:12Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    19,
                    24,
                    12,
                    5,
                    27,
                    0
                ],
                "title": "Constrained Hamiltonian Systems and Physics-Informed Neural Networks:\n  Hamilton-Dirac Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Hamiltonian Systems and Physics-Informed Neural Networks:\n  Hamilton-Dirac Neural Networks"
                },
                "summary": "The effectiveness of the Physics Informed Neural Networks (PINNs) for\nlearning the dynamics of constrained Hamiltonian systems is demonstrated using\nthe Dirac theory of constraints for regular systems with holonomic constraints\nand systems with non-standard Lagrangians. By utilizing Dirac brackets, we\nderive the Hamilton-Dirac equations and minimize their residuals, incorporating\nalso energy conservation and the Dirac constraints, using appropriate\nregularization terms in the loss function. The resulting PINNs, referred to as\nHamilton-Dirac Neural Networks (HDNNs), successfully learn constrained dynamics\nwithout deviating from the constraint manifold. Two examples with holonomic\nconstraints are presented: the nonlinear pendulum in Cartesian coordinates and\na two-dimensional, elliptically restricted harmonic oscillator. In both cases,\nHDNNs exhibit superior performance in preserving energy and constraints\ncompared to traditional explicit solvers. To demonstrate applicability in\nsystems with singular Lagrangians, we computed the guiding center motion in a\nstrong magnetic field starting from the guiding center Lagrangian. The\nimposition of energy conservation during the neural network training proved\nessential for accurately determining the orbits of the guiding center. The HDNN\narchitecture enables the learning of parametric dependencies in constrained\ndynamics by incorporating a problem-specific parameter as an input, in addition\nto the time variable. Additionally, an example of semi-supervised, data-driven\nlearning of guiding center dynamics with parameter inference is presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of the Physics Informed Neural Networks (PINNs) for\nlearning the dynamics of constrained Hamiltonian systems is demonstrated using\nthe Dirac theory of constraints for regular systems with holonomic constraints\nand systems with non-standard Lagrangians. By utilizing Dirac brackets, we\nderive the Hamilton-Dirac equations and minimize their residuals, incorporating\nalso energy conservation and the Dirac constraints, using appropriate\nregularization terms in the loss function. The resulting PINNs, referred to as\nHamilton-Dirac Neural Networks (HDNNs), successfully learn constrained dynamics\nwithout deviating from the constraint manifold. Two examples with holonomic\nconstraints are presented: the nonlinear pendulum in Cartesian coordinates and\na two-dimensional, elliptically restricted harmonic oscillator. In both cases,\nHDNNs exhibit superior performance in preserving energy and constraints\ncompared to traditional explicit solvers. To demonstrate applicability in\nsystems with singular Lagrangians, we computed the guiding center motion in a\nstrong magnetic field starting from the guiding center Lagrangian. The\nimposition of energy conservation during the neural network training proved\nessential for accurately determining the orbits of the guiding center. The HDNN\narchitecture enables the learning of parametric dependencies in constrained\ndynamics by incorporating a problem-specific parameter as an input, in addition\nto the time variable. Additionally, an example of semi-supervised, data-driven\nlearning of guiding center dynamics with parameter inference is presented."
                },
                "authors": [
                    {
                        "name": "Dimitrios A. Kaltsas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios A. Kaltsas"
                },
                "author": "Dimitrios A. Kaltsas",
                "arxiv_doi": "10.1103/PhysRevE.111.025301",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevE.111.025301",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.15485v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15485v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. E 111, 025301 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15797v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15797v3",
                "updated": "2025-02-07T18:24:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    24,
                    26,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-27T05:46:06Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    46,
                    6,
                    0,
                    27,
                    0
                ],
                "title": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models"
                },
                "summary": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language."
                },
                "authors": [
                    {
                        "name": "Tianbo Yang"
                    },
                    {
                        "name": "Mingqi Yang"
                    },
                    {
                        "name": "Hongyi Zhao"
                    },
                    {
                        "name": "Tianshuo Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tianshuo Yang"
                },
                "author": "Tianshuo Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15797v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15797v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05143v1",
                "updated": "2025-02-07T18:19:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    19,
                    12,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:19:12Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    19,
                    12,
                    4,
                    38,
                    0
                ],
                "title": "pyMethods2Test: A Dataset of Python Tests Mapped to Focal Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "pyMethods2Test: A Dataset of Python Tests Mapped to Focal Methods"
                },
                "summary": "Python is one of the fastest-growing programming languages and currently\nranks as the top language in many lists, even recently overtaking JavaScript as\nthe top language on GitHub. Given its importance in data science and machine\nlearning, it is imperative to be able to effectively train LLMs to generate\ngood unit test cases for Python code. This motivates the need for a large\ndataset to provide training and testing data. To date, while other large\ndatasets exist for languages like Java, none publicly exist for Python. Python\nposes difficult challenges in generating such a dataset, due to its less rigid\nnaming requirements. In this work, we consider two commonly used Python unit\ntesting frameworks: Pytest and unittest. We analyze a large corpus of over 88K\nopen-source GitHub projects utilizing these testing frameworks. Using a\ncarefully designed set of heuristics, we are able to locate over 22 million\ntest methods. We then analyze the test and non-test code and map individual\nunit tests to the focal method being tested. This provides an explicit\ntraceability link from the test to the tested method. Our pyMethods2Test\ndataset contains over 2 million of these focal method mappings, as well as the\nability to generate useful context for input to LLMs. The pyMethods2Test\ndataset is publicly available on Zenodo at:\nhttps://doi.org/10.5281/zenodo.14264518",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is one of the fastest-growing programming languages and currently\nranks as the top language in many lists, even recently overtaking JavaScript as\nthe top language on GitHub. Given its importance in data science and machine\nlearning, it is imperative to be able to effectively train LLMs to generate\ngood unit test cases for Python code. This motivates the need for a large\ndataset to provide training and testing data. To date, while other large\ndatasets exist for languages like Java, none publicly exist for Python. Python\nposes difficult challenges in generating such a dataset, due to its less rigid\nnaming requirements. In this work, we consider two commonly used Python unit\ntesting frameworks: Pytest and unittest. We analyze a large corpus of over 88K\nopen-source GitHub projects utilizing these testing frameworks. Using a\ncarefully designed set of heuristics, we are able to locate over 22 million\ntest methods. We then analyze the test and non-test code and map individual\nunit tests to the focal method being tested. This provides an explicit\ntraceability link from the test to the tested method. Our pyMethods2Test\ndataset contains over 2 million of these focal method mappings, as well as the\nability to generate useful context for input to LLMs. The pyMethods2Test\ndataset is publicly available on Zenodo at:\nhttps://doi.org/10.5281/zenodo.14264518"
                },
                "authors": [
                    {
                        "name": "Idriss Abdelmadjid"
                    },
                    {
                        "name": "Robert Dyer"
                    }
                ],
                "author_detail": {
                    "name": "Robert Dyer"
                },
                "author": "Robert Dyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05130v1",
                "updated": "2025-02-07T18:02:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    2,
                    47,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:02:47Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    2,
                    47,
                    4,
                    38,
                    0
                ],
                "title": "Latent Swap Joint Diffusion for Long-Form Audio Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Swap Joint Diffusion for Long-Form Audio Generation"
                },
                "summary": "Previous work on long-form audio generation using global-view diffusion or\niterative generation demands significant training or inference costs. While\nrecent advancements in multi-view joint diffusion for panoramic generation\nprovide an efficient option, they struggle with spectrum generation with severe\noverlap distortions and high cross-view consistency costs. We initially explore\nthis phenomenon through the connectivity inheritance of latent maps and uncover\nthat averaging operations excessively smooth the high-frequency components of\nthe latent map. To address these issues, we propose Swap Forward (SaFa), a\nframe-level latent swap framework that synchronizes multiple diffusions to\nproduce a globally coherent long audio with more spectrum details in a\nforward-only manner. At its core, the bidirectional Self-Loop Latent Swap is\napplied between adjacent views, leveraging stepwise diffusion trajectory to\nadaptively enhance high-frequency components without disrupting low-frequency\ncomponents. Furthermore, to ensure cross-view consistency, the unidirectional\nReference-Guided Latent Swap is applied between the reference and the\nnon-overlap regions of each subview during the early stages, providing\ncentralized trajectory guidance. Quantitative and qualitative experiments\ndemonstrate that SaFa significantly outperforms existing joint diffusion\nmethods and even training-based long audio generation models. Moreover, we find\nthat it also adapts well to panoramic generation, achieving comparable\nstate-of-the-art performance with greater efficiency and model\ngeneralizability. Project page is available at https://swapforward.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous work on long-form audio generation using global-view diffusion or\niterative generation demands significant training or inference costs. While\nrecent advancements in multi-view joint diffusion for panoramic generation\nprovide an efficient option, they struggle with spectrum generation with severe\noverlap distortions and high cross-view consistency costs. We initially explore\nthis phenomenon through the connectivity inheritance of latent maps and uncover\nthat averaging operations excessively smooth the high-frequency components of\nthe latent map. To address these issues, we propose Swap Forward (SaFa), a\nframe-level latent swap framework that synchronizes multiple diffusions to\nproduce a globally coherent long audio with more spectrum details in a\nforward-only manner. At its core, the bidirectional Self-Loop Latent Swap is\napplied between adjacent views, leveraging stepwise diffusion trajectory to\nadaptively enhance high-frequency components without disrupting low-frequency\ncomponents. Furthermore, to ensure cross-view consistency, the unidirectional\nReference-Guided Latent Swap is applied between the reference and the\nnon-overlap regions of each subview during the early stages, providing\ncentralized trajectory guidance. Quantitative and qualitative experiments\ndemonstrate that SaFa significantly outperforms existing joint diffusion\nmethods and even training-based long audio generation models. Moreover, we find\nthat it also adapts well to panoramic generation, achieving comparable\nstate-of-the-art performance with greater efficiency and model\ngeneralizability. Project page is available at https://swapforward.github.io/."
                },
                "authors": [
                    {
                        "name": "Yusheng Dai"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Chang Li"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Kewei Li"
                    },
                    {
                        "name": "Ruoyu Wang"
                    },
                    {
                        "name": "Jiefeng Ma"
                    },
                    {
                        "name": "Lei Sun"
                    },
                    {
                        "name": "Jianqing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianqing Gao"
                },
                "author": "Jianqing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05122v1",
                "updated": "2025-02-07T17:50:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    50,
                    14,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:50:14Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    50,
                    14,
                    4,
                    38,
                    0
                ],
                "title": "Distinguishing Cause from Effect with Causal Velocity Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing Cause from Effect with Causal Velocity Models"
                },
                "summary": "Bivariate structural causal models (SCM) are often used to infer causal\ndirection by examining their goodness-of-fit under restricted model classes. In\nthis paper, we describe a parametrization of bivariate SCMs in terms of a\ncausal velocity by viewing the cause variable as time in a dynamical system.\nThe velocity implicitly defines counterfactual curves via the solution of\ninitial value problems where the observation specifies the initial condition.\nUsing tools from measure transport, we obtain a unique correspondence between\nSCMs and the score function of the generated distribution via its causal\nvelocity. Based on this, we derive an objective function that directly\nregresses the velocity against the score function, the latter of which can be\nestimated non-parametrically from observational data. We use this to develop a\nmethod for bivariate causal discovery that extends beyond known model classes\nsuch as additive or location scale noise, and that requires no assumptions on\nthe noise distributions. When the score is estimated well, the objective is\nalso useful for detecting model non-identifiability and misspecification. We\npresent positive results in simulation and benchmark experiments where many\nexisting methods fail, and perform ablation studies to examine the method's\nsensitivity to accurate score estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bivariate structural causal models (SCM) are often used to infer causal\ndirection by examining their goodness-of-fit under restricted model classes. In\nthis paper, we describe a parametrization of bivariate SCMs in terms of a\ncausal velocity by viewing the cause variable as time in a dynamical system.\nThe velocity implicitly defines counterfactual curves via the solution of\ninitial value problems where the observation specifies the initial condition.\nUsing tools from measure transport, we obtain a unique correspondence between\nSCMs and the score function of the generated distribution via its causal\nvelocity. Based on this, we derive an objective function that directly\nregresses the velocity against the score function, the latter of which can be\nestimated non-parametrically from observational data. We use this to develop a\nmethod for bivariate causal discovery that extends beyond known model classes\nsuch as additive or location scale noise, and that requires no assumptions on\nthe noise distributions. When the score is estimated well, the objective is\nalso useful for detecting model non-identifiability and misspecification. We\npresent positive results in simulation and benchmark experiments where many\nexisting methods fail, and perform ablation studies to examine the method's\nsensitivity to accurate score estimation."
                },
                "authors": [
                    {
                        "name": "Johnny Xi"
                    },
                    {
                        "name": "Hugh Dance"
                    },
                    {
                        "name": "Peter Orbanz"
                    },
                    {
                        "name": "Benjamin Bloem-Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Bloem-Reddy"
                },
                "author": "Benjamin Bloem-Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12924v2",
                "updated": "2025-02-07T17:44:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    44,
                    38,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-16T18:10:50Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    18,
                    10,
                    50,
                    2,
                    290,
                    0
                ],
                "title": "Interpreting token compositionality in LLMs: A robustness analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting token compositionality in LLMs: A robustness analysis"
                },
                "summary": "Understanding the internal mechanisms of large language models (LLMs) is\nintegral to enhancing their reliability, interpretability, and inference\nprocesses. We present Constituent-Aware Pooling (CAP), a methodology designed\nto analyse how LLMs process compositional linguistic structures. Grounded in\nprinciples of compositionality, mechanistic interpretability, and information\ntheory, CAP systematically intervenes in model activations through\nconstituent-based pooling at various model levels. Our experiments on inverse\ndefinition modelling, hypernym and synonym prediction reveal critical insights\ninto transformers' limitations in handling compositional abstractions. No\nspecific layer integrates tokens into unified semantic representations based on\ntheir constituent parts. We observe fragmented information processing, which\nintensifies with model size, suggesting that larger models struggle more with\nthese interventions and exhibit greater information dispersion. This\nfragmentation likely stems from transformers' training objectives and\narchitectural design, preventing systematic and cohesive representations. Our\nfindings highlight fundamental limitations in current transformer architectures\nregarding compositional semantics processing and model interpretability,\nunderscoring the critical need for novel approaches in LLM design to address\nthese challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the internal mechanisms of large language models (LLMs) is\nintegral to enhancing their reliability, interpretability, and inference\nprocesses. We present Constituent-Aware Pooling (CAP), a methodology designed\nto analyse how LLMs process compositional linguistic structures. Grounded in\nprinciples of compositionality, mechanistic interpretability, and information\ntheory, CAP systematically intervenes in model activations through\nconstituent-based pooling at various model levels. Our experiments on inverse\ndefinition modelling, hypernym and synonym prediction reveal critical insights\ninto transformers' limitations in handling compositional abstractions. No\nspecific layer integrates tokens into unified semantic representations based on\ntheir constituent parts. We observe fragmented information processing, which\nintensifies with model size, suggesting that larger models struggle more with\nthese interventions and exhibit greater information dispersion. This\nfragmentation likely stems from transformers' training objectives and\narchitectural design, preventing systematic and cohesive representations. Our\nfindings highlight fundamental limitations in current transformer architectures\nregarding compositional semantics processing and model interpretability,\nunderscoring the critical need for novel approaches in LLM design to address\nthese challenges."
                },
                "authors": [
                    {
                        "name": "Nura Aljaafari"
                    },
                    {
                        "name": "Danilo S. Carvalho"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "arxiv_comment": "19 pages, 2 Figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05115v1",
                "updated": "2025-02-07T17:38:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    38,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:38:10Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    38,
                    10,
                    4,
                    38,
                    0
                ],
                "title": "\"It Felt Like I Was Left in the Dark\": Exploring Information Needs and\n  Design Opportunities for Family Caregivers of Older Adult Patients in\n  Critical Care Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"It Felt Like I Was Left in the Dark\": Exploring Information Needs and\n  Design Opportunities for Family Caregivers of Older Adult Patients in\n  Critical Care Settings"
                },
                "summary": "Older adult patients constitute a rapidly growing subgroup of Intensive Care\nUnit (ICU) patients. In these situations, their family caregivers are expected\nto represent the unconscious patients to access and interpret patients' medical\ninformation. However, caregivers currently have to rely on overloaded\nclinicians for information updates and typically lack the health literacy to\nunderstand complex medical information. Our project aims to explore the\ninformation needs of caregivers of ICU older adult patients, from which we can\npropose design opportunities to guide future AI systems. The project begins\nwith formative interviews with 11 caregivers to identify their challenges in\naccessing and interpreting medical information; From these findings, we then\nsynthesize design requirements and propose an AI system prototype to cope with\ncaregivers' challenges. The system prototype has two key features: a timeline\nvisualization to show the AI extracted and summarized older adult patients' key\nmedical events; and an LLM-based chatbot to provide context-aware informational\nsupport. We conclude our paper by reporting on the follow-up user evaluation of\nthe system and discussing future AI-based systems for ICU caregivers of older\nadults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Older adult patients constitute a rapidly growing subgroup of Intensive Care\nUnit (ICU) patients. In these situations, their family caregivers are expected\nto represent the unconscious patients to access and interpret patients' medical\ninformation. However, caregivers currently have to rely on overloaded\nclinicians for information updates and typically lack the health literacy to\nunderstand complex medical information. Our project aims to explore the\ninformation needs of caregivers of ICU older adult patients, from which we can\npropose design opportunities to guide future AI systems. The project begins\nwith formative interviews with 11 caregivers to identify their challenges in\naccessing and interpreting medical information; From these findings, we then\nsynthesize design requirements and propose an AI system prototype to cope with\ncaregivers' challenges. The system prototype has two key features: a timeline\nvisualization to show the AI extracted and summarized older adult patients' key\nmedical events; and an LLM-based chatbot to provide context-aware informational\nsupport. We conclude our paper by reporting on the follow-up user evaluation of\nthe system and discussing future AI-based systems for ICU caregivers of older\nadults."
                },
                "authors": [
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Smit Desai"
                    },
                    {
                        "name": "Yuqi Hu"
                    },
                    {
                        "name": "Yuling Sun"
                    },
                    {
                        "name": "Samantha Stonbraker"
                    },
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Elizabeth M. Goldberg"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13002v2",
                "updated": "2025-02-07T17:35:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    35,
                    23,
                    4,
                    38,
                    0
                ],
                "published": "2024-08-23T11:44:07Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    11,
                    44,
                    7,
                    4,
                    236,
                    0
                ],
                "title": "Measuring Variable Importance in Heterogeneous Treatment Effects with\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Variable Importance in Heterogeneous Treatment Effects with\n  Confidence"
                },
                "summary": "Causal machine learning (ML) holds promise for estimating individual\ntreatment effects from complex data. For successful real-world applications\nusing machine learning methods, it is of paramount importance to obtain\nreliable insights into which variables drive heterogeneity in the response to\ntreatment. We propose PermuCATE, an algorithm based on the Conditional\nPermutation Importance (CPI) method, for statistically rigorous global variable\nimportance assessment in the estimation of the Conditional Average Treatment\nEffect (CATE). Theoretical analysis of the finite sample regime and empirical\nstudies show that PermuCATE has lower variance than the Leave-One-Covariate-Out\n(LOCO) reference method and provides a reliable measure of variable importance.\nThis property increases statistical power, which is crucial for causal\ninference in the limited-data regime common to biomedical applications. We\nempirically demonstrate the benefits of PermuCATE in simulated and real-world\nhealth datasets, including settings with up to hundreds of correlated\nvariables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal machine learning (ML) holds promise for estimating individual\ntreatment effects from complex data. For successful real-world applications\nusing machine learning methods, it is of paramount importance to obtain\nreliable insights into which variables drive heterogeneity in the response to\ntreatment. We propose PermuCATE, an algorithm based on the Conditional\nPermutation Importance (CPI) method, for statistically rigorous global variable\nimportance assessment in the estimation of the Conditional Average Treatment\nEffect (CATE). Theoretical analysis of the finite sample regime and empirical\nstudies show that PermuCATE has lower variance than the Leave-One-Covariate-Out\n(LOCO) reference method and provides a reliable measure of variable importance.\nThis property increases statistical power, which is crucial for causal\ninference in the limited-data regime common to biomedical applications. We\nempirically demonstrate the benefits of PermuCATE in simulated and real-world\nhealth datasets, including settings with up to hundreds of correlated\nvariables."
                },
                "authors": [
                    {
                        "name": "Joseph Paillard"
                    },
                    {
                        "name": "Angel Reyero Lobo"
                    },
                    {
                        "name": "Vitaliy Kolodyazhniy"
                    },
                    {
                        "name": "Bertrand Thirion"
                    },
                    {
                        "name": "Denis A. Engemann"
                    }
                ],
                "author_detail": {
                    "name": "Denis A. Engemann"
                },
                "author": "Denis A. Engemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05111v1",
                "updated": "2025-02-07T17:35:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    35,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:35:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    35,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Flexible and Efficient Grammar-Constrained Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible and Efficient Grammar-Constrained Decoding"
                },
                "summary": "Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation."
                },
                "authors": [
                    {
                        "name": "Kanghee Park"
                    },
                    {
                        "name": "Timothy Zhou"
                    },
                    {
                        "name": "Loris D'Antoni"
                    }
                ],
                "author_detail": {
                    "name": "Loris D'Antoni"
                },
                "author": "Loris D'Antoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15599v2",
                "updated": "2025-02-07T17:29:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    29,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2024-06-21T18:57:38Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    18,
                    57,
                    38,
                    4,
                    173,
                    0
                ],
                "title": "Pareto-Optimal Learning from Preferences with Hidden Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto-Optimal Learning from Preferences with Hidden Context"
                },
                "summary": "Ensuring AI models align with human values is essential for their safety and\nfunctionality. Reinforcement learning from human feedback (RLHF) leverages\nhuman preferences to achieve this alignment. However, when preferences are\nsourced from diverse populations, point estimates of reward can result in\nsuboptimal performance or be unfair to specific groups. We propose Pareto\nOptimal Preference Learning (POPL), which enables pluralistic alignment by\nframing discrepant group preferences as objectives with potential trade-offs,\naiming for policies that are Pareto-optimal on the preference dataset. POPL\nutilizes lexicase selection, an iterative process that selects diverse and\nPareto-optimal solutions. Our theoretical and empirical evaluations demonstrate\nthat POPL surpasses baseline methods in learning sets of reward functions and\npolicies, effectively catering to distinct groups without access to group\nnumbers or membership labels. We verify the performance of POPL on a stateless\npreference learning setting, a Minigrid RL domain, Metaworld robotics\nbenchmarks, as well as large language model (LLM) fine-tuning. We illustrate\nthat POPL can also serve as a foundation for techniques optimizing specific\nnotions of group fairness, ensuring safe and equitable AI model alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring AI models align with human values is essential for their safety and\nfunctionality. Reinforcement learning from human feedback (RLHF) leverages\nhuman preferences to achieve this alignment. However, when preferences are\nsourced from diverse populations, point estimates of reward can result in\nsuboptimal performance or be unfair to specific groups. We propose Pareto\nOptimal Preference Learning (POPL), which enables pluralistic alignment by\nframing discrepant group preferences as objectives with potential trade-offs,\naiming for policies that are Pareto-optimal on the preference dataset. POPL\nutilizes lexicase selection, an iterative process that selects diverse and\nPareto-optimal solutions. Our theoretical and empirical evaluations demonstrate\nthat POPL surpasses baseline methods in learning sets of reward functions and\npolicies, effectively catering to distinct groups without access to group\nnumbers or membership labels. We verify the performance of POPL on a stateless\npreference learning setting, a Minigrid RL domain, Metaworld robotics\nbenchmarks, as well as large language model (LLM) fine-tuning. We illustrate\nthat POPL can also serve as a foundation for techniques optimizing specific\nnotions of group fairness, ensuring safe and equitable AI model alignment."
                },
                "authors": [
                    {
                        "name": "Ryan Bahlous-Boldi"
                    },
                    {
                        "name": "Li Ding"
                    },
                    {
                        "name": "Lee Spector"
                    },
                    {
                        "name": "Scott Niekum"
                    }
                ],
                "author_detail": {
                    "name": "Scott Niekum"
                },
                "author": "Scott Niekum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12087v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12087v3",
                "updated": "2025-02-07T17:23:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    23,
                    27,
                    4,
                    38,
                    0
                ],
                "published": "2024-04-18T11:13:36Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    11,
                    13,
                    36,
                    3,
                    109,
                    0
                ],
                "title": "Optimizing the diffusion coefficient of overdamped Langevin dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the diffusion coefficient of overdamped Langevin dynamics"
                },
                "summary": "Overdamped Langevin dynamics are reversible stochastic differential equations\nwhich are commonly used to sample probability measures in high-dimensional\nspaces, such as the ones appearing in computational statistical physics and\nBayesian inference. By varying the diffusion coefficient, there are in fact\ninfinitely many overdamped Langevin dynamics which are reversible with respect\nto the target probability measure at hand. This suggests to optimize the\ndiffusion coefficient in order to increase the convergence rate of the\ndynamics, as measured by the spectral gap of the generator associated with the\nstochastic differential equation. We analytically study this problem here,\nobtaining in particular necessary conditions on the optimal diffusion\ncoefficient. We also derive an explicit expression of the optimal diffusion in\nsome appropriate homogenized limit. Numerical results, both relying on\ndiscretizations of the spectral gap problem and Monte Carlo simulations of the\nstochastic dynamics, demonstrate the increased quality of the sampling arising\nfrom an appropriate choice of the diffusion coefficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overdamped Langevin dynamics are reversible stochastic differential equations\nwhich are commonly used to sample probability measures in high-dimensional\nspaces, such as the ones appearing in computational statistical physics and\nBayesian inference. By varying the diffusion coefficient, there are in fact\ninfinitely many overdamped Langevin dynamics which are reversible with respect\nto the target probability measure at hand. This suggests to optimize the\ndiffusion coefficient in order to increase the convergence rate of the\ndynamics, as measured by the spectral gap of the generator associated with the\nstochastic differential equation. We analytically study this problem here,\nobtaining in particular necessary conditions on the optimal diffusion\ncoefficient. We also derive an explicit expression of the optimal diffusion in\nsome appropriate homogenized limit. Numerical results, both relying on\ndiscretizations of the spectral gap problem and Monte Carlo simulations of the\nstochastic dynamics, demonstrate the increased quality of the sampling arising\nfrom an appropriate choice of the diffusion coefficient."
                },
                "authors": [
                    {
                        "name": "Tony Lelièvre"
                    },
                    {
                        "name": "Grigorios A. Pavliotis"
                    },
                    {
                        "name": "Geneviève Robin"
                    },
                    {
                        "name": "Régis Santet"
                    },
                    {
                        "name": "Gabriel Stoltz"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stoltz"
                },
                "author": "Gabriel Stoltz",
                "arxiv_comment": "78 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12087v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12087v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13968v2",
                "updated": "2025-02-07T17:13:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    13,
                    32,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-21T01:19:26Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    1,
                    19,
                    26,
                    5,
                    265,
                    0
                ],
                "title": "LADICA: A Large Shared Display Interface for Generative AI Cognitive\n  Assistance in Co-Located Team Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADICA: A Large Shared Display Interface for Generative AI Cognitive\n  Assistance in Co-Located Team Collaboration"
                },
                "summary": "Large shared displays, such as digital whiteboards, are useful for supporting\nco-located team collaborations by helping members perform cognitive tasks such\nas brainstorming, organizing ideas, and making comparisons. While recent\nadvancement in Large Language Models (LLMs) has catalyzed AI support for these\ndisplays, most existing systems either only offer limited capabilities or\ndiminish human control, neglecting the potential benefits of natural group\ndynamics. Our formative study identified cognitive challenges teams encounter,\nsuch as diverse ideation, knowledge sharing, mutual awareness, idea\norganization, and synchronization of live discussions with the external\nworkspace. In response, we introduce LADICA, a large shared display interface\nthat helps collaborative teams brainstorm, organize, and analyze ideas through\nmultiple analytical lenses, while fostering mutual awareness of ideas and\nconcepts. Furthermore, LADICA facilitates the real-time extraction of key\ninformation from verbal discussions and identifies relevant entities. A lab\nstudy confirmed LADICA's usability and usefulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large shared displays, such as digital whiteboards, are useful for supporting\nco-located team collaborations by helping members perform cognitive tasks such\nas brainstorming, organizing ideas, and making comparisons. While recent\nadvancement in Large Language Models (LLMs) has catalyzed AI support for these\ndisplays, most existing systems either only offer limited capabilities or\ndiminish human control, neglecting the potential benefits of natural group\ndynamics. Our formative study identified cognitive challenges teams encounter,\nsuch as diverse ideation, knowledge sharing, mutual awareness, idea\norganization, and synchronization of live discussions with the external\nworkspace. In response, we introduce LADICA, a large shared display interface\nthat helps collaborative teams brainstorm, organize, and analyze ideas through\nmultiple analytical lenses, while fostering mutual awareness of ideas and\nconcepts. Furthermore, LADICA facilitates the real-time extraction of key\ninformation from verbal discussions and identifies relevant entities. A lab\nstudy confirmed LADICA's usability and usefulness."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Weirui Peng"
                    },
                    {
                        "name": "Xinyue Chen"
                    },
                    {
                        "name": "Luke Cao"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Toby Jia-Jun Li"
                },
                "author": "Toby Jia-Jun Li",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05092v1",
                "updated": "2025-02-07T17:11:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    11,
                    23,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:11:23Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    11,
                    23,
                    4,
                    38,
                    0
                ],
                "title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs"
                },
                "summary": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05087v1",
                "updated": "2025-02-07T17:04:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    4,
                    39,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:04:39Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    4,
                    39,
                    4,
                    38,
                    0
                ],
                "title": "Mitigating Unintended Memorization with LoRA in Federated Learning for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Unintended Memorization with LoRA in Federated Learning for\n  LLMs"
                },
                "summary": "Federated learning (FL) is a popular paradigm for collaborative training\nwhich avoids direct data exposure between clients. However, data privacy issues\nstill remain: FL-trained large language models are capable of memorizing and\ncompleting phrases and sentences contained in training data when given with\ntheir prefixes. Thus, it is possible for adversarial and honest-but-curious\nclients to recover training data of other participants simply through targeted\nprompting. In this work, we demonstrate that a popular and simple fine-tuning\nstrategy, low-rank adaptation (LoRA), reduces memorization during FL up to a\nfactor of 10. We study this effect by performing a medical question-answering\nfine-tuning task and injecting multiple replicas of out-of-distribution\nsensitive sequences drawn from an external clinical dataset. We observe a\nreduction in memorization for a wide variety of Llama 2 and 3 models, and find\nthat LoRA can reduce memorization in centralized learning as well. Furthermore,\nwe show that LoRA can be combined with other privacy-preserving techniques such\nas gradient clipping and Gaussian noising, secure aggregation, and Goldfish\nloss to further improve record-level privacy while maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a popular paradigm for collaborative training\nwhich avoids direct data exposure between clients. However, data privacy issues\nstill remain: FL-trained large language models are capable of memorizing and\ncompleting phrases and sentences contained in training data when given with\ntheir prefixes. Thus, it is possible for adversarial and honest-but-curious\nclients to recover training data of other participants simply through targeted\nprompting. In this work, we demonstrate that a popular and simple fine-tuning\nstrategy, low-rank adaptation (LoRA), reduces memorization during FL up to a\nfactor of 10. We study this effect by performing a medical question-answering\nfine-tuning task and injecting multiple replicas of out-of-distribution\nsensitive sequences drawn from an external clinical dataset. We observe a\nreduction in memorization for a wide variety of Llama 2 and 3 models, and find\nthat LoRA can reduce memorization in centralized learning as well. Furthermore,\nwe show that LoRA can be combined with other privacy-preserving techniques such\nas gradient clipping and Gaussian noising, secure aggregation, and Goldfish\nloss to further improve record-level privacy while maintaining performance."
                },
                "authors": [
                    {
                        "name": "Thierry Bossy"
                    },
                    {
                        "name": "Julien Vignoud"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Juan R. Troncoso Pastoriza"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05085v1",
                "updated": "2025-02-07T17:01:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    1,
                    37,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:01:37Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    1,
                    37,
                    4,
                    38,
                    0
                ],
                "title": "Causality can systematically address the monsters under the bench(marks)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality can systematically address the monsters under the bench(marks)"
                },
                "summary": "Effective and reliable evaluation is essential for advancing empirical\nmachine learning. However, the increasing accessibility of generalist models\nand the progress towards ever more complex, high-level tasks make systematic\nevaluation more challenging. Benchmarks are plagued by various biases,\nartifacts, or leakage, while models may behave unreliably due to poorly\nexplored failure modes. Haphazard treatments and inconsistent formulations of\nsuch \"monsters\" can contribute to a duplication of efforts, a lack of trust in\nresults, and unsupported inferences. In this position paper, we argue causality\noffers an ideal framework to systematically address these challenges. By making\ncausal assumptions in an approach explicit, we can faithfully model phenomena,\nformulate testable hypotheses with explanatory power, and leverage principled\ntools for analysis. To make causal model design more accessible, we identify\nseveral useful Common Abstract Topologies (CATs) in causal graphs which help\ngain insight into the reasoning abilities in large language models. Through a\nseries of case studies, we demonstrate how the precise yet pragmatic language\nof causality clarifies the strengths and limitations of a method and inspires\nnew approaches for systematic progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and reliable evaluation is essential for advancing empirical\nmachine learning. However, the increasing accessibility of generalist models\nand the progress towards ever more complex, high-level tasks make systematic\nevaluation more challenging. Benchmarks are plagued by various biases,\nartifacts, or leakage, while models may behave unreliably due to poorly\nexplored failure modes. Haphazard treatments and inconsistent formulations of\nsuch \"monsters\" can contribute to a duplication of efforts, a lack of trust in\nresults, and unsupported inferences. In this position paper, we argue causality\noffers an ideal framework to systematically address these challenges. By making\ncausal assumptions in an approach explicit, we can faithfully model phenomena,\nformulate testable hypotheses with explanatory power, and leverage principled\ntools for analysis. To make causal model design more accessible, we identify\nseveral useful Common Abstract Topologies (CATs) in causal graphs which help\ngain insight into the reasoning abilities in large language models. Through a\nseries of case studies, we demonstrate how the precise yet pragmatic language\nof causality clarifies the strengths and limitations of a method and inspires\nnew approaches for systematic progress."
                },
                "authors": [
                    {
                        "name": "Felix Leeb"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05084v1",
                "updated": "2025-02-07T16:59:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    59,
                    34,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:59:34Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    59,
                    34,
                    4,
                    38,
                    0
                ],
                "title": "ChallengeMe: An Adversarial Learning-enabled Text Summarization\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChallengeMe: An Adversarial Learning-enabled Text Summarization\n  Framework"
                },
                "summary": "The astonishing performance of large language models (LLMs) and their\nremarkable achievements in production and daily life have led to their\nwidespread application in collaborative tasks. However, current large models\nface challenges such as hallucination and lack of specificity in content\ngeneration in vertical domain tasks. Inspired by the contrast and\nclassification mechanisms in human cognitive processes, this paper constructs\nan adversarial learning-based prompt framework named ChallengeMe, which\nincludes three cascaded solutions: generation prompts, evaluation prompts, and\nfeedback optimization. In this process, we designed seven core optimization\ndimensions and set the threshold for adversarial learning. The results of mixed\ncase studies on the text summarization task show that the proposed framework\ncan generate more accurate and fluent text summaries compared to the current\nadvanced mainstream LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The astonishing performance of large language models (LLMs) and their\nremarkable achievements in production and daily life have led to their\nwidespread application in collaborative tasks. However, current large models\nface challenges such as hallucination and lack of specificity in content\ngeneration in vertical domain tasks. Inspired by the contrast and\nclassification mechanisms in human cognitive processes, this paper constructs\nan adversarial learning-based prompt framework named ChallengeMe, which\nincludes three cascaded solutions: generation prompts, evaluation prompts, and\nfeedback optimization. In this process, we designed seven core optimization\ndimensions and set the threshold for adversarial learning. The results of mixed\ncase studies on the text summarization task show that the proposed framework\ncan generate more accurate and fluent text summaries compared to the current\nadvanced mainstream LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Deng"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Tianmin Guo"
                    },
                    {
                        "name": "Yongzhe Zhang"
                    },
                    {
                        "name": "Zhengjian Kang"
                    },
                    {
                        "name": "Hang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hang Yang"
                },
                "author": "Hang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13633v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13633v3",
                "updated": "2025-02-07T16:58:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    58,
                    33,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-23T13:06:35Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    6,
                    35,
                    3,
                    23,
                    0
                ],
                "title": "Representation of Molecules via Algebraic Data Types : Advancing Beyond\n  SMILES & SELFIES",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation of Molecules via Algebraic Data Types : Advancing Beyond\n  SMILES & SELFIES"
                },
                "summary": "We introduce a novel molecular representation through Algebraic Data Types\n(ADTs) - composite data structures formed through the combination of simpler\ntypes that obey algebraic laws. By explicitly considering how the datatype of a\nrepresentation constrains the operations which may be performed, we ensure\nmeaningful inference can be performed over generative models (programs with\nsample} and score operations). This stands in contrast to string-based\nrepresentations where string-type operations may only indirectly correspond to\nchemical and physical molecular properties, and at worst produce nonsensical\noutput. The ADT presented implements the Dietz representation for molecular\nconstitution via multigraphs and bonding systems, and uses atomic coordinate\ndata to represent 3D information and stereochemical features. This creates a\ngeneral digital molecular representation which surpasses the limitations of the\nstring-based representations and the 2D-graph based models on which they are\nbased. In addition, we present novel support for quantum information through\nrepresentation of shells, subshells, and orbitals, greatly expanding the\nrepresentational scope beyond current approaches, for instance in Molecular\nOrbital theory. The framework's capabilities are demonstrated through key\napplications: Bayesian probabilistic programming is demonstrated through\nintegration with LazyPPL, a lazy probabilistic programming library; molecules\nare made instances of a group under rotation, necessary for geometric learning\ntechniques which exploit the invariance of molecular properties under different\nrepresentations; and the framework's flexibility is demonstrated through an\nextension to model chemical reactions. After critiquing previous\nrepresentations, we provide an open-source solution in Haskell - a type-safe,\npurely functional programming language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel molecular representation through Algebraic Data Types\n(ADTs) - composite data structures formed through the combination of simpler\ntypes that obey algebraic laws. By explicitly considering how the datatype of a\nrepresentation constrains the operations which may be performed, we ensure\nmeaningful inference can be performed over generative models (programs with\nsample} and score operations). This stands in contrast to string-based\nrepresentations where string-type operations may only indirectly correspond to\nchemical and physical molecular properties, and at worst produce nonsensical\noutput. The ADT presented implements the Dietz representation for molecular\nconstitution via multigraphs and bonding systems, and uses atomic coordinate\ndata to represent 3D information and stereochemical features. This creates a\ngeneral digital molecular representation which surpasses the limitations of the\nstring-based representations and the 2D-graph based models on which they are\nbased. In addition, we present novel support for quantum information through\nrepresentation of shells, subshells, and orbitals, greatly expanding the\nrepresentational scope beyond current approaches, for instance in Molecular\nOrbital theory. The framework's capabilities are demonstrated through key\napplications: Bayesian probabilistic programming is demonstrated through\nintegration with LazyPPL, a lazy probabilistic programming library; molecules\nare made instances of a group under rotation, necessary for geometric learning\ntechniques which exploit the invariance of molecular properties under different\nrepresentations; and the framework's flexibility is demonstrated through an\nextension to model chemical reactions. After critiquing previous\nrepresentations, we provide an open-source solution in Haskell - a type-safe,\npurely functional programming language."
                },
                "authors": [
                    {
                        "name": "Oliver Goldstein"
                    },
                    {
                        "name": "Samuel March"
                    }
                ],
                "author_detail": {
                    "name": "Samuel March"
                },
                "author": "Samuel March",
                "arxiv_comment": "1 Figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13633v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13633v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05078v1",
                "updated": "2025-02-07T16:54:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    54,
                    19,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:54:19Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    54,
                    19,
                    4,
                    38,
                    0
                ],
                "title": "Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain,\n  Tree, and Graph Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain,\n  Tree, and Graph Structures"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet their performance is highly dependent on the prompting\nstrategy and model scale. While reinforcement learning and fine-tuning have\nbeen deployed to boost reasoning, these approaches incur substantial\ncomputational and data overhead. In this work, we introduce Adaptive Graph of\nThoughts (AGoT), a dynamic, graph-based inference framework that enhances LLM\nreasoning solely at test time. Rather than relying on fixed-step methods like\nChain of Thought (CoT) or Tree of Thoughts (ToT), AGoT recursively decomposes\ncomplex queries into structured subproblems, forming an dynamic directed\nacyclic graph (DAG) of interdependent reasoning steps. By selectively expanding\nonly those subproblems that require further analysis, AGoT unifies the\nstrengths of chain, tree, and graph paradigms into a cohesive framework that\nallocates computation where it is most needed. We validate our approach on\ndiverse benchmarks spanning multi-hop retrieval, scientific reasoning, and\nmathematical problem-solving, achieving up to 46.2% improvement on scientific\nreasoning tasks (GPQA) - comparable to gains achieved through computationally\nintensive reinforcement learning approaches and outperforming state-of-the-art\niterative approaches. These results suggest that dynamic decomposition and\nstructured recursion offer a scalable, cost-effective alternative to\npost-training modifications, paving the way for more robust, general-purpose\nreasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet their performance is highly dependent on the prompting\nstrategy and model scale. While reinforcement learning and fine-tuning have\nbeen deployed to boost reasoning, these approaches incur substantial\ncomputational and data overhead. In this work, we introduce Adaptive Graph of\nThoughts (AGoT), a dynamic, graph-based inference framework that enhances LLM\nreasoning solely at test time. Rather than relying on fixed-step methods like\nChain of Thought (CoT) or Tree of Thoughts (ToT), AGoT recursively decomposes\ncomplex queries into structured subproblems, forming an dynamic directed\nacyclic graph (DAG) of interdependent reasoning steps. By selectively expanding\nonly those subproblems that require further analysis, AGoT unifies the\nstrengths of chain, tree, and graph paradigms into a cohesive framework that\nallocates computation where it is most needed. We validate our approach on\ndiverse benchmarks spanning multi-hop retrieval, scientific reasoning, and\nmathematical problem-solving, achieving up to 46.2% improvement on scientific\nreasoning tasks (GPQA) - comparable to gains achieved through computationally\nintensive reinforcement learning approaches and outperforming state-of-the-art\niterative approaches. These results suggest that dynamic decomposition and\nstructured recursion offer a scalable, cost-effective alternative to\npost-training modifications, paving the way for more robust, general-purpose\nreasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Tushar Pandey"
                    },
                    {
                        "name": "Ara Ghukasyan"
                    },
                    {
                        "name": "Oktay Goktas"
                    },
                    {
                        "name": "Santosh Kumar Radha"
                    }
                ],
                "author_detail": {
                    "name": "Santosh Kumar Radha"
                },
                "author": "Santosh Kumar Radha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03559v2",
                "updated": "2025-02-07T16:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    51,
                    57,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-05T19:17:24Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    19,
                    17,
                    24,
                    2,
                    36,
                    0
                ],
                "title": "Comprehensive Layer-wise Analysis of SSL Models for Audio Deepfake\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Layer-wise Analysis of SSL Models for Audio Deepfake\n  Detection"
                },
                "summary": "This paper conducts a comprehensive layer-wise analysis of self-supervised\nlearning (SSL) models for audio deepfake detection across diverse contexts,\nincluding multilingual datasets (English, Chinese, Spanish), partial, song, and\nscene-based deepfake scenarios. By systematically evaluating the contributions\nof different transformer layers, we uncover critical insights into model\nbehavior and performance. Our findings reveal that lower layers consistently\nprovide the most discriminative features, while higher layers capture less\nrelevant information. Notably, all models achieve competitive equal error rate\n(EER) scores even when employing a reduced number of layers. This indicates\nthat we can reduce computational costs and increase the inference speed of\ndetecting deepfakes by utilizing only a few lower layers. This work enhances\nour understanding of SSL models in deepfake detection, offering valuable\ninsights applicable across varied linguistic and contextual settings. Our\ntrained models and code are publicly available:\nhttps://github.com/Yaselley/SSL_Layerwise_Deepfake.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper conducts a comprehensive layer-wise analysis of self-supervised\nlearning (SSL) models for audio deepfake detection across diverse contexts,\nincluding multilingual datasets (English, Chinese, Spanish), partial, song, and\nscene-based deepfake scenarios. By systematically evaluating the contributions\nof different transformer layers, we uncover critical insights into model\nbehavior and performance. Our findings reveal that lower layers consistently\nprovide the most discriminative features, while higher layers capture less\nrelevant information. Notably, all models achieve competitive equal error rate\n(EER) scores even when employing a reduced number of layers. This indicates\nthat we can reduce computational costs and increase the inference speed of\ndetecting deepfakes by utilizing only a few lower layers. This work enhances\nour understanding of SSL models in deepfake detection, offering valuable\ninsights applicable across varied linguistic and contextual settings. Our\ntrained models and code are publicly available:\nhttps://github.com/Yaselley/SSL_Layerwise_Deepfake."
                },
                "authors": [
                    {
                        "name": "Yassine El Kheir"
                    },
                    {
                        "name": "Youness Samih"
                    },
                    {
                        "name": "Suraj Maharjan"
                    },
                    {
                        "name": "Tim Polzehl"
                    },
                    {
                        "name": "Sebastian Möller"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Möller"
                },
                "author": "Sebastian Möller",
                "arxiv_comment": "Accepted to NAACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14692v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14692v3",
                "updated": "2025-02-07T16:51:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    51,
                    32,
                    4,
                    38,
                    0
                ],
                "published": "2024-12-19T09:51:45Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    9,
                    51,
                    45,
                    3,
                    354,
                    0
                ],
                "title": "Explicit Relational Reasoning Network for Scene Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Relational Reasoning Network for Scene Text Detection"
                },
                "summary": "Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed."
                },
                "authors": [
                    {
                        "name": "Yuchen Su"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Yongkun Du"
                    },
                    {
                        "name": "Zhilong Ji"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jinfeng Bai"
                    },
                    {
                        "name": "Xieping Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xieping Gao"
                },
                "author": "Xieping Gao",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14692v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14692v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13757v2",
                "updated": "2025-02-07T16:24:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    24,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-11-21T00:01:51Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    1,
                    51,
                    3,
                    326,
                    0
                ],
                "title": "GenBFA: An Evolutionary Optimization Approach to Bit-Flip Attacks on\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenBFA: An Evolutionary Optimization Approach to Bit-Flip Attacks on\n  LLMs"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP), excelling in tasks like text generation and summarization. However,\ntheir increasing adoption in mission-critical applications raises concerns\nabout hardware-based threats, particularly bit-flip attacks (BFAs). BFAs,\nenabled by fault injection methods such as Rowhammer, target model parameters\nin memory, compromising both integrity and performance. Identifying critical\nparameters for BFAs in the vast parameter space of LLMs poses significant\nchallenges. While prior research suggests transformer-based architectures are\ninherently more robust to BFAs compared to traditional deep neural networks, we\nchallenge this assumption. For the first time, we demonstrate that as few as\nthree bit-flips can cause catastrophic performance degradation in an LLM with\nbillions of parameters. Current BFA techniques are inadequate for exploiting\nthis vulnerability due to the difficulty of efficiently identifying critical\nparameters within the immense parameter space. To address this, we propose\nAttentionBreaker, a novel framework tailored for LLMs that enables efficient\ntraversal of the parameter space to identify critical parameters. Additionally,\nwe introduce GenBFA, an evolutionary optimization strategy designed to refine\nthe search further, isolating the most critical bits for an efficient and\neffective attack. Empirical results reveal the profound vulnerability of LLMs\nto AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of\ntotal parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result\nin a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to\n0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings\nunderscore the effectiveness of AttentionBreaker in uncovering and exploiting\ncritical vulnerabilities within LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP), excelling in tasks like text generation and summarization. However,\ntheir increasing adoption in mission-critical applications raises concerns\nabout hardware-based threats, particularly bit-flip attacks (BFAs). BFAs,\nenabled by fault injection methods such as Rowhammer, target model parameters\nin memory, compromising both integrity and performance. Identifying critical\nparameters for BFAs in the vast parameter space of LLMs poses significant\nchallenges. While prior research suggests transformer-based architectures are\ninherently more robust to BFAs compared to traditional deep neural networks, we\nchallenge this assumption. For the first time, we demonstrate that as few as\nthree bit-flips can cause catastrophic performance degradation in an LLM with\nbillions of parameters. Current BFA techniques are inadequate for exploiting\nthis vulnerability due to the difficulty of efficiently identifying critical\nparameters within the immense parameter space. To address this, we propose\nAttentionBreaker, a novel framework tailored for LLMs that enables efficient\ntraversal of the parameter space to identify critical parameters. Additionally,\nwe introduce GenBFA, an evolutionary optimization strategy designed to refine\nthe search further, isolating the most critical bits for an efficient and\neffective attack. Empirical results reveal the profound vulnerability of LLMs\nto AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of\ntotal parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result\nin a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to\n0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings\nunderscore the effectiveness of AttentionBreaker in uncovering and exploiting\ncritical vulnerabilities within LLM architectures."
                },
                "authors": [
                    {
                        "name": "Sanjay Das"
                    },
                    {
                        "name": "Swastik Bhattacharya"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Shamik Kundu"
                    },
                    {
                        "name": "Anand Menon"
                    },
                    {
                        "name": "Arnab Raha"
                    },
                    {
                        "name": "Kanad Basu"
                    }
                ],
                "author_detail": {
                    "name": "Kanad Basu"
                },
                "author": "Kanad Basu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05053v1",
                "updated": "2025-02-07T16:20:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    20,
                    28,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:20:28Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    20,
                    28,
                    4,
                    38,
                    0
                ],
                "title": "Gaze-Guided Robotic Vascular Ultrasound Leveraging Human Intention\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaze-Guided Robotic Vascular Ultrasound Leveraging Human Intention\n  Estimation"
                },
                "summary": "Medical ultrasound has been widely used to examine vascular structure in\nmodern clinical practice. However, traditional ultrasound examination often\nfaces challenges related to inter- and intra-operator variation. The robotic\nultrasound system (RUSS) appears as a potential solution for such challenges\nbecause of its superiority in stability and reproducibility. Given the complex\nanatomy of human vasculature, multiple vessels often appear in ultrasound\nimages, or a single vessel bifurcates into branches, complicating the\nexamination process. To tackle this challenge, this work presents a gaze-guided\nRUSS for vascular applications. A gaze tracker captures the eye movements of\nthe operator. The extracted gaze signal guides the RUSS to follow the correct\nvessel when it bifurcates. Additionally, a gaze-guided segmentation network is\nproposed to enhance segmentation robustness by exploiting gaze information.\nHowever, gaze signals are often noisy, requiring interpretation to accurately\ndiscern the operator's true intentions. To this end, this study proposes a\nstabilization module to process raw gaze data. The inferred attention heatmap\nis utilized as a region proposal to aid segmentation and serve as a trigger\nsignal when the operator needs to adjust the scanning target, such as when a\nbifurcation appears. To ensure appropriate contact between the probe and\nsurface during scanning, an automatic ultrasound confidence-based orientation\ncorrection method is developed. In experiments, we demonstrated the efficiency\nof the proposed gaze-guided segmentation pipeline by comparing it with other\nmethods. Besides, the performance of the proposed gaze-guided RUSS was also\nvalidated as a whole on a realistic arm phantom with an uneven surface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical ultrasound has been widely used to examine vascular structure in\nmodern clinical practice. However, traditional ultrasound examination often\nfaces challenges related to inter- and intra-operator variation. The robotic\nultrasound system (RUSS) appears as a potential solution for such challenges\nbecause of its superiority in stability and reproducibility. Given the complex\nanatomy of human vasculature, multiple vessels often appear in ultrasound\nimages, or a single vessel bifurcates into branches, complicating the\nexamination process. To tackle this challenge, this work presents a gaze-guided\nRUSS for vascular applications. A gaze tracker captures the eye movements of\nthe operator. The extracted gaze signal guides the RUSS to follow the correct\nvessel when it bifurcates. Additionally, a gaze-guided segmentation network is\nproposed to enhance segmentation robustness by exploiting gaze information.\nHowever, gaze signals are often noisy, requiring interpretation to accurately\ndiscern the operator's true intentions. To this end, this study proposes a\nstabilization module to process raw gaze data. The inferred attention heatmap\nis utilized as a region proposal to aid segmentation and serve as a trigger\nsignal when the operator needs to adjust the scanning target, such as when a\nbifurcation appears. To ensure appropriate contact between the probe and\nsurface during scanning, an automatic ultrasound confidence-based orientation\ncorrection method is developed. In experiments, we demonstrated the efficiency\nof the proposed gaze-guided segmentation pipeline by comparing it with other\nmethods. Besides, the performance of the proposed gaze-guided RUSS was also\nvalidated as a whole on a realistic arm phantom with an uneven surface."
                },
                "authors": [
                    {
                        "name": "Yuan Bi"
                    },
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Zhongliang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongliang Jiang"
                },
                "author": "Zhongliang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11613v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11613v4",
                "updated": "2025-02-07T16:18:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    18,
                    20,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-20T17:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems"
                },
                "summary": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Robino"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Robino"
                },
                "author": "Giorgio Robino",
                "arxiv_comment": "Figure 1 substituted. Added smolagents subsection in Other Works.\n  Minor format revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11613v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11613v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05049v1",
                "updated": "2025-02-07T16:11:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    11,
                    39,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:11:39Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    11,
                    39,
                    4,
                    38,
                    0
                ],
                "title": "On the Inference of Sociodemographics on Reddit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Inference of Sociodemographics on Reddit"
                },
                "summary": "Inference of sociodemographic attributes of social media users is an\nessential step for computational social science (CSS) research to link online\nand offline behavior. However, there is a lack of a systematic evaluation and\nclear guidelines for optimal methodologies for this task on Reddit, one of\ntoday's largest social media. In this study, we fill this gap by comparing\nstate-of-the-art (SOTA) and probabilistic models.\n  To this end, first we collect a novel data set of more than 850k\nself-declarations on age, gender, and partisan affiliation from Reddit\ncomments. Then, we systematically compare alternatives to the widely used\nembedding-based model and labeling techniques for the definition of the\nground-truth. We do so on two tasks: ($i$) predicting binary labels\n(classification); and ($ii$)~predicting the prevalence of a demographic class\namong a set of users (quantification).\n  Our findings reveal that Naive Bayes models not only offer transparency and\ninterpretability by design but also consistently outperform the SOTA.\nSpecifically, they achieve an improvement in ROC AUC of up to $19\\%$ and\nmaintain a mean absolute error (MAE) below $15\\%$ in quantification for\nlarge-scale data settings. Finally, we discuss best practices for researchers\nin CSS, emphasizing coverage, interpretability, reliability, and scalability.\n  The code and model weights used for the experiments are publicly\navailable.\\footnote{https://anonymous.4open.science/r/SDI-submission-5234}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of sociodemographic attributes of social media users is an\nessential step for computational social science (CSS) research to link online\nand offline behavior. However, there is a lack of a systematic evaluation and\nclear guidelines for optimal methodologies for this task on Reddit, one of\ntoday's largest social media. In this study, we fill this gap by comparing\nstate-of-the-art (SOTA) and probabilistic models.\n  To this end, first we collect a novel data set of more than 850k\nself-declarations on age, gender, and partisan affiliation from Reddit\ncomments. Then, we systematically compare alternatives to the widely used\nembedding-based model and labeling techniques for the definition of the\nground-truth. We do so on two tasks: ($i$) predicting binary labels\n(classification); and ($ii$)~predicting the prevalence of a demographic class\namong a set of users (quantification).\n  Our findings reveal that Naive Bayes models not only offer transparency and\ninterpretability by design but also consistently outperform the SOTA.\nSpecifically, they achieve an improvement in ROC AUC of up to $19\\%$ and\nmaintain a mean absolute error (MAE) below $15\\%$ in quantification for\nlarge-scale data settings. Finally, we discuss best practices for researchers\nin CSS, emphasizing coverage, interpretability, reliability, and scalability.\n  The code and model weights used for the experiments are publicly\navailable.\\footnote{https://anonymous.4open.science/r/SDI-submission-5234}"
                },
                "authors": [
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Corrado Monti"
                    },
                    {
                        "name": "Paolo Bajardi"
                    },
                    {
                        "name": "Gianmarco De Francisci Morales"
                    }
                ],
                "author_detail": {
                    "name": "Gianmarco De Francisci Morales"
                },
                "author": "Gianmarco De Francisci Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05043v1",
                "updated": "2025-02-07T16:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:09:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "EcoServe: Designing Carbon-Aware AI Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Designing Carbon-Aware AI Inference Systems"
                },
                "summary": "The rapid increase in LLM ubiquity and scale levies unprecedented demands on\ncomputing infrastructure. These demands not only incur large compute and memory\nresources, but also significant energy, yielding large operational and embodied\ncarbon emissions. In this work, we present two main observations. First, while\nGPUs dominate operational carbon, host processing systems (e.g., CPUs, memory,\nstorage) dominate embodied carbon. Second, based on traces from production\ndeployment of two Generative AI services in the cloud, offline, batch-inference\naccounts for a significant portion (up to 55\\%) of serving capacity. We propose\nfour pillars of carbon-conscious infrastructure design for LLM serving systems:\n\\textbf{\\textit{Reduce, Reuse, Rightsize, and Recycle}}. We demonstrate that\nEcoServe can lower carbon emissions by up to 47\\%, compared to performance,\nenergy, and cost-optimized design points, while maintaining performance targets\nand SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in LLM ubiquity and scale levies unprecedented demands on\ncomputing infrastructure. These demands not only incur large compute and memory\nresources, but also significant energy, yielding large operational and embodied\ncarbon emissions. In this work, we present two main observations. First, while\nGPUs dominate operational carbon, host processing systems (e.g., CPUs, memory,\nstorage) dominate embodied carbon. Second, based on traces from production\ndeployment of two Generative AI services in the cloud, offline, batch-inference\naccounts for a significant portion (up to 55\\%) of serving capacity. We propose\nfour pillars of carbon-conscious infrastructure design for LLM serving systems:\n\\textbf{\\textit{Reduce, Reuse, Rightsize, and Recycle}}. We demonstrate that\nEcoServe can lower carbon emissions by up to 47\\%, compared to performance,\nenergy, and cost-optimized design points, while maintaining performance targets\nand SLOs."
                },
                "authors": [
                    {
                        "name": "Yueying"
                    },
                    {
                        "name": "Li"
                    },
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Rodrigo Fonseca"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "arxiv_affiliation": "Lisa",
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05036v1",
                "updated": "2025-02-07T16:03:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    3,
                    8,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:03:08Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    3,
                    8,
                    4,
                    38,
                    0
                ],
                "title": "nvAgent: Automated Data Visualization from Natural Language via\n  Collaborative Agent Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "nvAgent: Automated Data Visualization from Natural Language via\n  Collaborative Agent Workflow"
                },
                "summary": "Natural Language to Visualization (NL2Vis) seeks to convert natural-language\ndescriptions into visual representations of given tables, empowering users to\nderive insights from large-scale data. Recent advancements in Large Language\nModels (LLMs) show promise in automating code generation to transform tabular\ndata into accessible visualizations. However, they often struggle with complex\nqueries that require reasoning across multiple tables. To address this\nlimitation, we propose a collaborative agent workflow, termed nvAgent, for\nNL2Vis. Specifically, nvAgent comprises three agents: a processor agent for\ndatabase processing and context filtering, a composer agent for planning\nvisualization generation, and a validator agent for code translation and output\nverification. Comprehensive evaluations on the new VisEval benchmark\ndemonstrate that nvAgent consistently surpasses state-of-the-art baselines,\nachieving a 7.88% improvement in single-table and a 9.23% improvement in\nmulti-table scenarios. Qualitative analyses further highlight that nvAgent\nmaintains nearly a 20% performance margin over previous models, underscoring\nits capacity to produce high-quality visual representations from complex,\nheterogeneous data sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to Visualization (NL2Vis) seeks to convert natural-language\ndescriptions into visual representations of given tables, empowering users to\nderive insights from large-scale data. Recent advancements in Large Language\nModels (LLMs) show promise in automating code generation to transform tabular\ndata into accessible visualizations. However, they often struggle with complex\nqueries that require reasoning across multiple tables. To address this\nlimitation, we propose a collaborative agent workflow, termed nvAgent, for\nNL2Vis. Specifically, nvAgent comprises three agents: a processor agent for\ndatabase processing and context filtering, a composer agent for planning\nvisualization generation, and a validator agent for code translation and output\nverification. Comprehensive evaluations on the new VisEval benchmark\ndemonstrate that nvAgent consistently surpasses state-of-the-art baselines,\nachieving a 7.88% improvement in single-table and a 9.23% improvement in\nmulti-table scenarios. Qualitative analyses further highlight that nvAgent\nmaintains nearly a 20% performance margin over previous models, underscoring\nits capacity to produce high-quality visual representations from complex,\nheterogeneous data sources."
                },
                "authors": [
                    {
                        "name": "Geliang Ouyang"
                    },
                    {
                        "name": "Jingyao Chen"
                    },
                    {
                        "name": "Zhihe Nie"
                    },
                    {
                        "name": "Yi Gui"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Dongping Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dongping Chen"
                },
                "author": "Dongping Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05025v1",
                "updated": "2025-02-07T15:55:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    55,
                    27,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T15:55:27Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    55,
                    27,
                    4,
                    38,
                    0
                ],
                "title": "Impact of radiative accelerations on the stellar characterization of\n  FGK-type stars using spectroscopic and seismic constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of radiative accelerations on the stellar characterization of\n  FGK-type stars using spectroscopic and seismic constraints"
                },
                "summary": "Chemical transport mechanisms are fundamental processes in stellar evolution\nmodels. They are responsible for the chemical distribution, and their impact\ndetermines how accurately we can characterize stars. Radiative accelerations\nare one of these processes. They allow the accumulation of elements at\ndifferent depths in the star. We aim to assess the impact of radiative\naccelerations on the modeling of FGK-type stars and their impact on the\nprediction of surface abundances. To reduce the cost of the computation of\nradiative accelerations, we implemented the single-valued parameters (SVP)\nmethod in the stellar evolution code MESA. The SVP method is more efficient in\ncalculating radiative accelerations, which enables computations of large enough\ngrids of models for stellar characterization. Compared to models that include\natomic diffusion (with only gravitational settling), the inclusion of radiative\naccelerations has a small effect on the inference of fundamental properties,\nwith an impact of 2\\%, 0.7\\%, and 5\\% for mass, radius, and age. However, the\ntreatment of radiative accelerations is necessary to predict the chemical\ncomposition of and accurately characterize stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical transport mechanisms are fundamental processes in stellar evolution\nmodels. They are responsible for the chemical distribution, and their impact\ndetermines how accurately we can characterize stars. Radiative accelerations\nare one of these processes. They allow the accumulation of elements at\ndifferent depths in the star. We aim to assess the impact of radiative\naccelerations on the modeling of FGK-type stars and their impact on the\nprediction of surface abundances. To reduce the cost of the computation of\nradiative accelerations, we implemented the single-valued parameters (SVP)\nmethod in the stellar evolution code MESA. The SVP method is more efficient in\ncalculating radiative accelerations, which enables computations of large enough\ngrids of models for stellar characterization. Compared to models that include\natomic diffusion (with only gravitational settling), the inclusion of radiative\naccelerations has a small effect on the inference of fundamental properties,\nwith an impact of 2\\%, 0.7\\%, and 5\\% for mass, radius, and age. However, the\ntreatment of radiative accelerations is necessary to predict the chemical\ncomposition of and accurately characterize stars."
                },
                "authors": [
                    {
                        "name": "Nuno Moedas"
                    },
                    {
                        "name": "Diego Bossini"
                    },
                    {
                        "name": "Morgan Deal"
                    }
                ],
                "author_detail": {
                    "name": "Morgan Deal"
                },
                "author": "Morgan Deal",
                "arxiv_comment": "12 pafes,13 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09508v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09508v3",
                "updated": "2025-02-07T15:49:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    49,
                    58,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-12T12:10:14Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    12,
                    10,
                    14,
                    5,
                    286,
                    0
                ],
                "title": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing"
                },
                "summary": "Collaborative learning of large language models (LLMs) has emerged as a new\nparadigm for utilizing private data from different parties to guarantee\nefficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also\ngarnered increased attention due to its ability to manipulate the behaviors of\nLLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits\nof multiple parties are aggregated in a privacy-preserving and continual\nmanner) unexamined. To this end, this manuscript dives into the first\ninvestigation of collaborative KE, in which we start by carefully identifying\nthe unique three challenges therein, including knowledge overlap, knowledge\nconflict, and knowledge forgetting. We then propose a non-destructive\ncollaborative KE framework, COLLABEDIT, which employs a novel model merging\nmechanism to mimic the global KE behavior while preventing the severe\nperformance drop. Extensive experiments on two canonical datasets demonstrate\nthe superiority of COLLABEDIT compared to other destructive baselines, and\nresults shed light on addressing three collaborative KE challenges and future\napplications. Our code is available at https://github.com/LINs-lab/CollabEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative learning of large language models (LLMs) has emerged as a new\nparadigm for utilizing private data from different parties to guarantee\nefficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also\ngarnered increased attention due to its ability to manipulate the behaviors of\nLLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits\nof multiple parties are aggregated in a privacy-preserving and continual\nmanner) unexamined. To this end, this manuscript dives into the first\ninvestigation of collaborative KE, in which we start by carefully identifying\nthe unique three challenges therein, including knowledge overlap, knowledge\nconflict, and knowledge forgetting. We then propose a non-destructive\ncollaborative KE framework, COLLABEDIT, which employs a novel model merging\nmechanism to mimic the global KE behavior while preventing the severe\nperformance drop. Extensive experiments on two canonical datasets demonstrate\nthe superiority of COLLABEDIT compared to other destructive baselines, and\nresults shed light on addressing three collaborative KE challenges and future\napplications. Our code is available at https://github.com/LINs-lab/CollabEdit."
                },
                "authors": [
                    {
                        "name": "Jiamu Zheng"
                    },
                    {
                        "name": "Jinghuai Zhang"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "20 pages, 11 figures. Published as a conference paper at ICLR 2025.\n  Code at https://github.com/LINs-lab/CollabEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09508v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09508v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05004v1",
                "updated": "2025-02-07T15:24:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    24,
                    19,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T15:24:19Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    24,
                    19,
                    4,
                    38,
                    0
                ],
                "title": "R3F: An R package for evolutionary dates, rates, and priors using the\n  relative rate framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R3F: An R package for evolutionary dates, rates, and priors using the\n  relative rate framework"
                },
                "summary": "The relative rate framework (RRF) can estimate divergence times from branch\nlengths in a phylogeny, which is the theoretical basis of the RelTime method\nfrequently applied, a relaxed clock approach for molecular dating that scales\nwell for large phylogenies. The use of RRF has also enabled the development of\ncomputationally efficient and accurate methods for testing the autocorrelation\nof lineage rates in a phylogeny (CorrTest) and selecting data-driven parameters\nof the birth-death speciation model (ddBD), which can be used to specify priors\nin Bayesian molecular dating. We have developed R3F, an R package implementing\nRRF to estimate divergence times, infer lineage rates, conduct CorrTest, and\nbuild a ddBD tree prior for Bayesian dating in molecular phylogenies. Here, we\ndescribe R3F functionality and explain how to interpret and use its outputs in\nother visualization software and packages, such as MEGA, ggtree, and FigTree.\nUltimately, R3F is intended to enable the dating of the Tree of Life with\ngreater accuracy and precision, which would have important implications for\nstudies of organism evolution, diversification dynamics, phylogeography, and\nbiogeography. Availability and Implementation: The source codes and related\ninstructions for installing and implementing R3F are available from GitHub\n(https://github.com/cathyqqtao/R3F).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The relative rate framework (RRF) can estimate divergence times from branch\nlengths in a phylogeny, which is the theoretical basis of the RelTime method\nfrequently applied, a relaxed clock approach for molecular dating that scales\nwell for large phylogenies. The use of RRF has also enabled the development of\ncomputationally efficient and accurate methods for testing the autocorrelation\nof lineage rates in a phylogeny (CorrTest) and selecting data-driven parameters\nof the birth-death speciation model (ddBD), which can be used to specify priors\nin Bayesian molecular dating. We have developed R3F, an R package implementing\nRRF to estimate divergence times, infer lineage rates, conduct CorrTest, and\nbuild a ddBD tree prior for Bayesian dating in molecular phylogenies. Here, we\ndescribe R3F functionality and explain how to interpret and use its outputs in\nother visualization software and packages, such as MEGA, ggtree, and FigTree.\nUltimately, R3F is intended to enable the dating of the Tree of Life with\ngreater accuracy and precision, which would have important implications for\nstudies of organism evolution, diversification dynamics, phylogeography, and\nbiogeography. Availability and Implementation: The source codes and related\ninstructions for installing and implementing R3F are available from GitHub\n(https://github.com/cathyqqtao/R3F)."
                },
                "authors": [
                    {
                        "name": "Qiqing Tao"
                    },
                    {
                        "name": "Sudip Sharma"
                    },
                    {
                        "name": "Koichiro Tamura"
                    },
                    {
                        "name": "Sudhir Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sudhir Kumar"
                },
                "author": "Sudhir Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05003v1",
                "updated": "2025-02-07T15:23:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    23,
                    34,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T15:23:34Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    23,
                    34,
                    4,
                    38,
                    0
                ],
                "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations"
                },
                "summary": "One approach to reducing the massive costs of large language models (LLMs) is\nthe use of quantized or sparse representations for training or deployment.\nWhile post-training compression methods are very popular, the question of\nobtaining even more accurate compressed models by directly training over such\nrepresentations, i.e., Quantization-Aware Training (QAT), is still open: for\nexample, a recent study (arXiv:2411.04330v2) put the \"optimal\" bit-width at\nwhich models can be trained using QAT, while staying accuracy-competitive with\nstandard FP16/BF16 precision, at 8-bits weights and activations.\n  We advance this state-of-the-art via a new method called QuEST, which is\nPareto-competitive with FP16, i.e., it provides better accuracy at lower model\nsize, while training models with weights and activations in 4-bits or less.\nMoreover, QuEST allows stable training with 1-bit weights and activations.\nQuEST achieves this by improving two key aspects of QAT methods: (1) accurate\nand fast quantization of the (continuous) distributions of weights and\nactivations via Hadamard normalization and MSE-optimal fitting; (2) a new trust\ngradient estimator based on the idea of explicitly minimizing the error between\nthe noisy gradient computed over quantized states and the \"true\" (but unknown)\nfull-precision gradient. Experiments on Llama-type architectures show that\nQuEST induces stable scaling laws across the entire range of hardware-supported\nprecisions, and can be extended to sparse representations. We provide GPU\nkernel support showing that models produced by QuEST can be executed\nefficiently. Our code is available at https://github.com/IST-DASLab/QuEST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One approach to reducing the massive costs of large language models (LLMs) is\nthe use of quantized or sparse representations for training or deployment.\nWhile post-training compression methods are very popular, the question of\nobtaining even more accurate compressed models by directly training over such\nrepresentations, i.e., Quantization-Aware Training (QAT), is still open: for\nexample, a recent study (arXiv:2411.04330v2) put the \"optimal\" bit-width at\nwhich models can be trained using QAT, while staying accuracy-competitive with\nstandard FP16/BF16 precision, at 8-bits weights and activations.\n  We advance this state-of-the-art via a new method called QuEST, which is\nPareto-competitive with FP16, i.e., it provides better accuracy at lower model\nsize, while training models with weights and activations in 4-bits or less.\nMoreover, QuEST allows stable training with 1-bit weights and activations.\nQuEST achieves this by improving two key aspects of QAT methods: (1) accurate\nand fast quantization of the (continuous) distributions of weights and\nactivations via Hadamard normalization and MSE-optimal fitting; (2) a new trust\ngradient estimator based on the idea of explicitly minimizing the error between\nthe noisy gradient computed over quantized states and the \"true\" (but unknown)\nfull-precision gradient. Experiments on Llama-type architectures show that\nQuEST induces stable scaling laws across the entire range of hardware-supported\nprecisions, and can be extended to sparse representations. We provide GPU\nkernel support showing that models produced by QuEST can be executed\nefficiently. Our code is available at https://github.com/IST-DASLab/QuEST."
                },
                "authors": [
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04997v1",
                "updated": "2025-02-07T15:19:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    19,
                    40,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T15:19:40Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    19,
                    40,
                    4,
                    38,
                    0
                ],
                "title": "Aligning Black-box Language Models with Human Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Black-box Language Models with Human Judgments"
                },
                "summary": "Large language models (LLMs) are increasingly used as automated judges to\nevaluate recommendation systems, search engines, and other subjective tasks,\nwhere relying on human evaluators can be costly, time-consuming, and\nunscalable. LLMs offer an efficient solution for continuous, automated\nevaluation. However, since the systems that are built and improved with these\njudgments are ultimately designed for human use, it is crucial that LLM\njudgments align closely with human evaluators to ensure such systems remain\nhuman-centered. On the other hand, aligning LLM judgments with human evaluators\nis challenging due to individual variability and biases in human judgments. We\npropose a simple yet effective framework to align LLM judgments with individual\nhuman evaluators or their aggregated judgments, without retraining or\nfine-tuning the LLM. Our approach learns a linear mapping between the LLM's\noutputs and human judgments, achieving over 142% average improvement in\nagreement across 29 tasks with only a small number of calibration examples used\nfor training. Notably, our method works in zero-shot and few-shot settings,\nexceeds inter-human agreement on four out of six tasks, and enables smaller\nLLMs to achieve performance comparable to that of larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used as automated judges to\nevaluate recommendation systems, search engines, and other subjective tasks,\nwhere relying on human evaluators can be costly, time-consuming, and\nunscalable. LLMs offer an efficient solution for continuous, automated\nevaluation. However, since the systems that are built and improved with these\njudgments are ultimately designed for human use, it is crucial that LLM\njudgments align closely with human evaluators to ensure such systems remain\nhuman-centered. On the other hand, aligning LLM judgments with human evaluators\nis challenging due to individual variability and biases in human judgments. We\npropose a simple yet effective framework to align LLM judgments with individual\nhuman evaluators or their aggregated judgments, without retraining or\nfine-tuning the LLM. Our approach learns a linear mapping between the LLM's\noutputs and human judgments, achieving over 142% average improvement in\nagreement across 29 tasks with only a small number of calibration examples used\nfor training. Notably, our method works in zero-shot and few-shot settings,\nexceeds inter-human agreement on four out of six tasks, and enables smaller\nLLMs to achieve performance comparable to that of larger models."
                },
                "authors": [
                    {
                        "name": "Gerrit J. J. van den Burg"
                    },
                    {
                        "name": "Gen Suzuki"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Murat Sensoy"
                    }
                ],
                "author_detail": {
                    "name": "Murat Sensoy"
                },
                "author": "Murat Sensoy",
                "arxiv_comment": "Accepted for publication at NAACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04990v1",
                "updated": "2025-02-07T15:10:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    10,
                    42,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T15:10:42Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    10,
                    42,
                    4,
                    38,
                    0
                ],
                "title": "Probabilistic Programming with Sufficient Statistics for faster Bayesian\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Programming with Sufficient Statistics for faster Bayesian\n  Computation"
                },
                "summary": "Probabilistic programming methods have revolutionised Bayesian inference,\nmaking it easier than ever for practitioners to perform\nMarkov-chain-Monte-Carlo sampling from non-conjugate posterior distributions.\nHere we focus on Stan, arguably the most used probabilistic programming tool\nfor Bayesian inference (Carpenter et al., 2017), and its interface with R via\nthe brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages.\nAlthough easy to implement, these tools can become computationally prohibitive\nwhen applied to datasets with many observations or models with numerous\nparameters. While the use of sufficient statistics is well-established in\ntheory, it has been surprisingly overlooked in state-of-the-art Stan software.\nWe show that when the likelihood can be written in terms of sufficient\nstatistics, considerable computational improvements can be made to current\nimplementations. We demonstrate how this approach provides accurate inference\nat a fraction of the time than state-of-the-art implementations for Gaussian\nlinear regression models with non-conjugate priors, hierarchical random effects\nmodels, and factor analysis models. Our results also show that moderate\ncomputational gains can be achieved even in models where the likelihood can\nonly be partially written in terms of sufficient statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic programming methods have revolutionised Bayesian inference,\nmaking it easier than ever for practitioners to perform\nMarkov-chain-Monte-Carlo sampling from non-conjugate posterior distributions.\nHere we focus on Stan, arguably the most used probabilistic programming tool\nfor Bayesian inference (Carpenter et al., 2017), and its interface with R via\nthe brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages.\nAlthough easy to implement, these tools can become computationally prohibitive\nwhen applied to datasets with many observations or models with numerous\nparameters. While the use of sufficient statistics is well-established in\ntheory, it has been surprisingly overlooked in state-of-the-art Stan software.\nWe show that when the likelihood can be written in terms of sufficient\nstatistics, considerable computational improvements can be made to current\nimplementations. We demonstrate how this approach provides accurate inference\nat a fraction of the time than state-of-the-art implementations for Gaussian\nlinear regression models with non-conjugate priors, hierarchical random effects\nmodels, and factor analysis models. Our results also show that moderate\ncomputational gains can be achieved even in models where the likelihood can\nonly be partially written in terms of sufficient statistics."
                },
                "authors": [
                    {
                        "name": "Clemens Pichler"
                    },
                    {
                        "name": "Jack Jewson"
                    },
                    {
                        "name": "Alejandra Avalos-Pacheco"
                    }
                ],
                "author_detail": {
                    "name": "Alejandra Avalos-Pacheco"
                },
                "author": "Alejandra Avalos-Pacheco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04983v1",
                "updated": "2025-02-07T15:03:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    3,
                    8,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T15:03:08Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    3,
                    8,
                    4,
                    38,
                    0
                ],
                "title": "MoGraphGPT: Creating Interactive Scenes Using Modular LLM and Graphical\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoGraphGPT: Creating Interactive Scenes Using Modular LLM and Graphical\n  Control"
                },
                "summary": "Creating interactive scenes often involves complex programming tasks.\nAlthough large language models (LLMs) like ChatGPT can generate code from\nnatural language, their output is often error-prone, particularly when\nscripting interactions among multiple elements. The linear conversational\nstructure limits the editing of individual elements, and lacking graphical and\nprecise control complicates visual integration. To address these issues, we\nintegrate an element-level modularization technique that processes textual\ndescriptions for individual elements through separate LLM modules, with a\ncentral module managing interactions among elements. This modular approach\nallows for refining each element independently. We design a graphical user\ninterface, MoGraphGPT , which combines modular LLMs with enhanced graphical\ncontrol to generate codes for 2D interactive scenes. It enables direct\nintegration of graphical information and offers quick, precise control through\nautomatically generated sliders. Our comparative evaluation against an AI\ncoding tool, Cursor Composer, as the baseline system and a usability study show\nMoGraphGPT significantly improves easiness, controllability, and refinement in\ncreating complex 2D interactive scenes with multiple visual elements in a\ncoding-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating interactive scenes often involves complex programming tasks.\nAlthough large language models (LLMs) like ChatGPT can generate code from\nnatural language, their output is often error-prone, particularly when\nscripting interactions among multiple elements. The linear conversational\nstructure limits the editing of individual elements, and lacking graphical and\nprecise control complicates visual integration. To address these issues, we\nintegrate an element-level modularization technique that processes textual\ndescriptions for individual elements through separate LLM modules, with a\ncentral module managing interactions among elements. This modular approach\nallows for refining each element independently. We design a graphical user\ninterface, MoGraphGPT , which combines modular LLMs with enhanced graphical\ncontrol to generate codes for 2D interactive scenes. It enables direct\nintegration of graphical information and offers quick, precise control through\nautomatically generated sliders. Our comparative evaluation against an AI\ncoding tool, Cursor Composer, as the baseline system and a usability study show\nMoGraphGPT significantly improves easiness, controllability, and refinement in\ncreating complex 2D interactive scenes with multiple visual elements in a\ncoding-free manner."
                },
                "authors": [
                    {
                        "name": "Hui Ye"
                    },
                    {
                        "name": "Chufeng Xiao"
                    },
                    {
                        "name": "Jiaye Leng"
                    },
                    {
                        "name": "Pengfei Xu"
                    },
                    {
                        "name": "Hongbo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Hongbo Fu"
                },
                "author": "Hongbo Fu",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04964v1",
                "updated": "2025-02-07T14:30:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    30,
                    12,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T14:30:12Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    30,
                    12,
                    4,
                    38,
                    0
                ],
                "title": "CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs"
                },
                "summary": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches."
                },
                "authors": [
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Maxim Panov"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Panov"
                },
                "arxiv_affiliation": "Mohamed bin Zayed University of Artificial Intelligence",
                "author": "Maxim Panov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04955v1",
                "updated": "2025-02-07T14:20:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    20,
                    45,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T14:20:45Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    20,
                    45,
                    4,
                    38,
                    0
                ],
                "title": "Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics"
                },
                "summary": "In this paper, we explore the problem of Claim Extraction using one-to-many\ntext generation methods, comparing LLMs, small summarization models finetuned\nfor the task, and a previous NER-centric baseline QACG. As the current\npublications on Claim Extraction, Fact Extraction, Claim Generation and\nCheck-worthy Claim Detection are quite scattered in their means and\nterminology, we compile their common objectives, releasing the FEVERFact\ndataset, with 17K atomic factual claims extracted from 4K contextualised\nWikipedia sentences, adapted from the original FEVER. We compile the known\nobjectives into an Evaluation framework of: Atomicity, Fluency,\nDecontextualization, Faithfulness checked for each generated claim separately,\nand Focus and Coverage measured against the full set of predicted claims for a\nsingle input. For each metric, we implement a scale using a reduction to an\nalready-explored NLP task. We validate our metrics against human grading of\ngeneric claims, to see that the model ranking on $F_{fact}$, our hardest\nmetric, did not change and the evaluation framework approximates human grading\nvery closely in terms of $F_1$ and RMSE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the problem of Claim Extraction using one-to-many\ntext generation methods, comparing LLMs, small summarization models finetuned\nfor the task, and a previous NER-centric baseline QACG. As the current\npublications on Claim Extraction, Fact Extraction, Claim Generation and\nCheck-worthy Claim Detection are quite scattered in their means and\nterminology, we compile their common objectives, releasing the FEVERFact\ndataset, with 17K atomic factual claims extracted from 4K contextualised\nWikipedia sentences, adapted from the original FEVER. We compile the known\nobjectives into an Evaluation framework of: Atomicity, Fluency,\nDecontextualization, Faithfulness checked for each generated claim separately,\nand Focus and Coverage measured against the full set of predicted claims for a\nsingle input. For each metric, we implement a scale using a reduction to an\nalready-explored NLP task. We validate our metrics against human grading of\ngeneric claims, to see that the model ranking on $F_{fact}$, our hardest\nmetric, did not change and the evaluation framework approximates human grading\nvery closely in terms of $F_1$ and RMSE."
                },
                "authors": [
                    {
                        "name": "Herbert Ullrich"
                    },
                    {
                        "name": "Tomáš Mlynář"
                    },
                    {
                        "name": "Jan Drchal"
                    }
                ],
                "author_detail": {
                    "name": "Jan Drchal"
                },
                "author": "Jan Drchal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04951v1",
                "updated": "2025-02-07T14:15:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    15,
                    46,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T14:15:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    15,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "The Rising Threat to Emerging AI-Powered Search Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rising Threat to Emerging AI-Powered Search Engines"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering\nprecise and efficient responses by integrating external databases with\npre-existing knowledge. However, we observe that these AIPSEs raise risks such\nas quoting malicious content or citing malicious websites, leading to harmful\nor unverified information dissemination. In this study, we conduct the first\nsafety risk quantification on seven production AIPSEs by systematically\ndefining the threat model, risk level, and evaluating responses to various\nquery types. With data collected from PhishTank, ThreatBook, and LevelBlue, our\nfindings reveal that AIPSEs frequently generate harmful content that contains\nmalicious URLs even with benign queries (e.g., with benign keywords). We also\nobserve that directly query URL will increase the risk level while query with\nnatural language will mitigate such risk. We further perform two case studies\non online document spoofing and phishing to show the ease of deceiving AIPSEs\nin the real-world setting. To mitigate these risks, we develop an agent-based\ndefense with a GPT-4o-based content refinement tool and an XGBoost-based URL\ndetector. Our evaluation shows that our defense can effectively reduce the risk\nbut with the cost of reducing available information. Our research highlights\nthe urgent need for robust safety measures in AIPSEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering\nprecise and efficient responses by integrating external databases with\npre-existing knowledge. However, we observe that these AIPSEs raise risks such\nas quoting malicious content or citing malicious websites, leading to harmful\nor unverified information dissemination. In this study, we conduct the first\nsafety risk quantification on seven production AIPSEs by systematically\ndefining the threat model, risk level, and evaluating responses to various\nquery types. With data collected from PhishTank, ThreatBook, and LevelBlue, our\nfindings reveal that AIPSEs frequently generate harmful content that contains\nmalicious URLs even with benign queries (e.g., with benign keywords). We also\nobserve that directly query URL will increase the risk level while query with\nnatural language will mitigate such risk. We further perform two case studies\non online document spoofing and phishing to show the ease of deceiving AIPSEs\nin the real-world setting. To mitigate these risks, we develop an agent-based\ndefense with a GPT-4o-based content refinement tool and an XGBoost-based URL\ndetector. Our evaluation shows that our defense can effectively reduce the risk\nbut with the cost of reducing available information. Our research highlights\nthe urgent need for robust safety measures in AIPSEs."
                },
                "authors": [
                    {
                        "name": "Zeren Luo"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Mingchen Li"
                    },
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18712v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18712v4",
                "updated": "2025-02-07T14:14:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    14,
                    7,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-30T19:15:41Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    19,
                    15,
                    41,
                    3,
                    30,
                    0
                ],
                "title": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps"
                },
                "summary": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts."
                },
                "authors": [
                    {
                        "name": "Devansh Bhardwaj"
                    },
                    {
                        "name": "Naman Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Naman Mishra"
                },
                "author": "Naman Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18712v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18712v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04949v1",
                "updated": "2025-02-07T14:13:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    13,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T14:13:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    13,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Does Unsupervised Domain Adaptation Improve the Robustness of Amortized\n  Bayesian Inference? A Systematic Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Unsupervised Domain Adaptation Improve the Robustness of Amortized\n  Bayesian Inference? A Systematic Evaluation"
                },
                "summary": "Neural networks are fragile when confronted with data that significantly\ndeviates from their training distribution. This is true in particular for\nsimulation-based inference methods, such as neural amortized Bayesian inference\n(ABI), where models trained on simulated data are deployed on noisy real-world\nobservations. Recent robust approaches employ unsupervised domain adaptation\n(UDA) to match the embedding spaces of simulated and observed data. However,\nthe lack of comprehensive evaluations across different domain mismatches raises\nconcerns about the reliability in high-stakes applications. We address this gap\nby systematically testing UDA approaches across a wide range of\nmisspecification scenarios in both a controlled and a high-dimensional\nbenchmark. We demonstrate that aligning summary spaces between domains\neffectively mitigates the impact of unmodeled phenomena or noise. However, the\nsame alignment mechanism can lead to failures under prior misspecifications - a\ncritical finding with practical consequences. Our results underscore the need\nfor careful consideration of misspecification types when using UDA techniques\nto increase the robustness of ABI in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks are fragile when confronted with data that significantly\ndeviates from their training distribution. This is true in particular for\nsimulation-based inference methods, such as neural amortized Bayesian inference\n(ABI), where models trained on simulated data are deployed on noisy real-world\nobservations. Recent robust approaches employ unsupervised domain adaptation\n(UDA) to match the embedding spaces of simulated and observed data. However,\nthe lack of comprehensive evaluations across different domain mismatches raises\nconcerns about the reliability in high-stakes applications. We address this gap\nby systematically testing UDA approaches across a wide range of\nmisspecification scenarios in both a controlled and a high-dimensional\nbenchmark. We demonstrate that aligning summary spaces between domains\neffectively mitigates the impact of unmodeled phenomena or noise. However, the\nsame alignment mechanism can lead to failures under prior misspecifications - a\ncritical finding with practical consequences. Our results underscore the need\nfor careful consideration of misspecification types when using UDA techniques\nto increase the robustness of ABI in practice."
                },
                "authors": [
                    {
                        "name": "Lasse Elsemüller"
                    },
                    {
                        "name": "Valentin Pratz"
                    },
                    {
                        "name": "Mischa von Krause"
                    },
                    {
                        "name": "Andreas Voss"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    },
                    {
                        "name": "Stefan T. Radev"
                    }
                ],
                "author_detail": {
                    "name": "Stefan T. Radev"
                },
                "author": "Stefan T. Radev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05209v2",
                "updated": "2025-02-07T14:09:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    9,
                    26,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-07T17:14:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    14,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "Neural Networks for cosmological model selection and feature importance\n  using Cosmic Microwave Background data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Networks for cosmological model selection and feature importance\n  using Cosmic Microwave Background data"
                },
                "summary": "The measurements of the temperature and polarisation anisotropies of the\nCosmic Microwave Background (CMB) by the ESA Planck mission have strongly\nsupported the current concordance model of cosmology. However, the latest\ncosmological data release from ESA Planck mission still has a powerful\npotential to test new data science algorithms and inference techniques. In this\npaper, we use advanced Machine Learning (ML) algorithms, such as Neural\nNetworks (NNs), to discern among different underlying cosmological models at\nthe angular power spectra level, using both temperature and polarisation Planck\n18 data. We test two different models beyond $\\Lambda$CDM: a modified gravity\nmodel: the Hu-Sawicki model, and an alternative inflationary model: a\nfeature-template in the primordial power spectrum. Furthermore, we also\nimplemented an interpretability method based on SHAP values to evaluate the\nlearning process and identify the most relevant elements that drive our\narchitecture to certain outcomes. We find that our NN is able to distinguish\nbetween different angular power spectra successfully for both alternative\nmodels and $\\Lambda$CDM. We conclude by explaining how archival scientific data\nhas still a strong potential to test novel data science algorithms that are\ninteresting for the next generation of cosmological experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurements of the temperature and polarisation anisotropies of the\nCosmic Microwave Background (CMB) by the ESA Planck mission have strongly\nsupported the current concordance model of cosmology. However, the latest\ncosmological data release from ESA Planck mission still has a powerful\npotential to test new data science algorithms and inference techniques. In this\npaper, we use advanced Machine Learning (ML) algorithms, such as Neural\nNetworks (NNs), to discern among different underlying cosmological models at\nthe angular power spectra level, using both temperature and polarisation Planck\n18 data. We test two different models beyond $\\Lambda$CDM: a modified gravity\nmodel: the Hu-Sawicki model, and an alternative inflationary model: a\nfeature-template in the primordial power spectrum. Furthermore, we also\nimplemented an interpretability method based on SHAP values to evaluate the\nlearning process and identify the most relevant elements that drive our\narchitecture to certain outcomes. We find that our NN is able to distinguish\nbetween different angular power spectra successfully for both alternative\nmodels and $\\Lambda$CDM. We conclude by explaining how archival scientific data\nhas still a strong potential to test novel data science algorithms that are\ninteresting for the next generation of cosmological experiments."
                },
                "authors": [
                    {
                        "name": "I. Ocampo"
                    },
                    {
                        "name": "G. Cañas-Herrera"
                    },
                    {
                        "name": "S. Nesseris"
                    }
                ],
                "author_detail": {
                    "name": "S. Nesseris"
                },
                "author": "S. Nesseris",
                "arxiv_doi": "10.1088/1475-7516/2025/02/004",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2025/02/004",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.05209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 9 figures, 2 tables, comments welcome",
                "arxiv_journal_ref": "JCAP02(2025)004",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04935v1",
                "updated": "2025-02-07T13:57:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    57,
                    47,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:57:47Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    57,
                    47,
                    4,
                    38,
                    0
                ],
                "title": "Conformal Prediction for Electricity Price Forecasting in the Day-Ahead\n  and Real-Time Balancing Market",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Prediction for Electricity Price Forecasting in the Day-Ahead\n  and Real-Time Balancing Market"
                },
                "summary": "The integration of renewable energy into electricity markets poses\nsignificant challenges to price stability and increases the complexity of\nmarket operations. Accurate and reliable electricity price forecasting is\ncrucial for effective market participation, where price dynamics can be\nsignificantly more challenging to predict. Probabilistic forecasting, through\nprediction intervals, efficiently quantifies the inherent uncertainties in\nelectricity prices, supporting better decision-making for market participants.\nThis study explores the enhancement of probabilistic price prediction using\nConformal Prediction (CP) techniques, specifically Ensemble Batch Prediction\nIntervals and Sequential Predictive Conformal Inference. These methods provide\nprecise and reliable prediction intervals, outperforming traditional models in\nvalidity metrics. We propose an ensemble approach that combines the efficiency\nof quantile regression models with the robust coverage properties of time\nseries adapted CP techniques. This ensemble delivers both narrow prediction\nintervals and high coverage, leading to more reliable and accurate forecasts.\nWe further evaluate the practical implications of CP techniques through a\nsimulated trading algorithm applied to a battery storage system. The ensemble\napproach demonstrates improved financial returns in energy trading in both the\nDay-Ahead and Balancing Markets, highlighting its practical benefits for market\nparticipants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of renewable energy into electricity markets poses\nsignificant challenges to price stability and increases the complexity of\nmarket operations. Accurate and reliable electricity price forecasting is\ncrucial for effective market participation, where price dynamics can be\nsignificantly more challenging to predict. Probabilistic forecasting, through\nprediction intervals, efficiently quantifies the inherent uncertainties in\nelectricity prices, supporting better decision-making for market participants.\nThis study explores the enhancement of probabilistic price prediction using\nConformal Prediction (CP) techniques, specifically Ensemble Batch Prediction\nIntervals and Sequential Predictive Conformal Inference. These methods provide\nprecise and reliable prediction intervals, outperforming traditional models in\nvalidity metrics. We propose an ensemble approach that combines the efficiency\nof quantile regression models with the robust coverage properties of time\nseries adapted CP techniques. This ensemble delivers both narrow prediction\nintervals and high coverage, leading to more reliable and accurate forecasts.\nWe further evaluate the practical implications of CP techniques through a\nsimulated trading algorithm applied to a battery storage system. The ensemble\napproach demonstrates improved financial returns in energy trading in both the\nDay-Ahead and Balancing Markets, highlighting its practical benefits for market\nparticipants."
                },
                "authors": [
                    {
                        "name": "Ciaran O'Connor"
                    },
                    {
                        "name": "Mohamed Bahloul"
                    },
                    {
                        "name": "Roberto Rossi"
                    },
                    {
                        "name": "Steven Prestwich"
                    },
                    {
                        "name": "Andrea Visentin"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Visentin"
                },
                "author": "Andrea Visentin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04933v1",
                "updated": "2025-02-07T13:53:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    53,
                    15,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:53:15Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    53,
                    15,
                    4,
                    38,
                    0
                ],
                "title": "Mobile Network-specialized Large Language Models for 6G: Architectures,\n  Innovations, Challenges, and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Network-specialized Large Language Models for 6G: Architectures,\n  Innovations, Challenges, and Future Trends"
                },
                "summary": "Conventional 5G network management mechanisms, that operate in isolated silos\nacross different network segments, will experience significant limitations in\nhandling the unprecedented hyper-complexity and massive scale of the sixth\ngeneration (6G). Holistic intelligence and end-to-end automation are, thus,\npositioned as key enablers of forthcoming 6G networks. The Large Language Model\n(LLM) technology, a major breakthrough in the Generative Artificial\nIntelligence (AI) field, enjoys robust human-like language processing, advanced\ncontextual reasoning and multi-modal capabilities. These features foster a\nholistic understanding of network behavior and an autonomous decision-making.\nThis paper investigates four possible architectural designs for integrated LLM\nand 6G networks, detailing the inherent technical intricacies, the merits and\nthe limitations of each design. As an internal functional building block of\nfuture 6G networks, the LLM will natively benefit from their improved\ndesign-driven security policies from the early design and specification stages.\nAn illustrative scenario of slicing conflicts is used to prove the\neffectiveness of our architectural framework in autonomously dealing with\ncomplicated network anomalies. We finally conclude the paper with an overview\nof the key challenges and the relevant research trends for enabling Mobile\nNetworkspecialized LLMs. This study is intended to provide Mobile Network\nOperators (MNOs) with a comprehensive guidance in their paths towards embracing\nthe LLM technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 5G network management mechanisms, that operate in isolated silos\nacross different network segments, will experience significant limitations in\nhandling the unprecedented hyper-complexity and massive scale of the sixth\ngeneration (6G). Holistic intelligence and end-to-end automation are, thus,\npositioned as key enablers of forthcoming 6G networks. The Large Language Model\n(LLM) technology, a major breakthrough in the Generative Artificial\nIntelligence (AI) field, enjoys robust human-like language processing, advanced\ncontextual reasoning and multi-modal capabilities. These features foster a\nholistic understanding of network behavior and an autonomous decision-making.\nThis paper investigates four possible architectural designs for integrated LLM\nand 6G networks, detailing the inherent technical intricacies, the merits and\nthe limitations of each design. As an internal functional building block of\nfuture 6G networks, the LLM will natively benefit from their improved\ndesign-driven security policies from the early design and specification stages.\nAn illustrative scenario of slicing conflicts is used to prove the\neffectiveness of our architectural framework in autonomously dealing with\ncomplicated network anomalies. We finally conclude the paper with an overview\nof the key challenges and the relevant research trends for enabling Mobile\nNetworkspecialized LLMs. This study is intended to provide Mobile Network\nOperators (MNOs) with a comprehensive guidance in their paths towards embracing\nthe LLM technology."
                },
                "authors": [
                    {
                        "name": "Abdelaali Chaoub"
                    },
                    {
                        "name": "Muslim Elkotob"
                    }
                ],
                "author_detail": {
                    "name": "Muslim Elkotob"
                },
                "author": "Muslim Elkotob",
                "arxiv_comment": "9 pages, 4 figures, 1 table. This paper has been submitted to IEEE\n  for publication. Copyright may change without notice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04931v1",
                "updated": "2025-02-07T13:52:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    52,
                    37,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:52:37Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    52,
                    37,
                    4,
                    38,
                    0
                ],
                "title": "Breaking the News: A LLM-based Game where Players Act as Influencer or\n  Debunker for Raising Awareness About Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the News: A LLM-based Game where Players Act as Influencer or\n  Debunker for Raising Awareness About Misinformation"
                },
                "summary": "Game-based interventions are widely used to combat misinformation online by\nemploying the \"inoculation approach\". However, most current interventions are\ndesigned as single-player games, presenting players with limited predefined\nchoices. Such restrictions reduce replayability and may lead to an overly\nsimplistic understanding of the processes of misinformation phenomenon and the\ndebunking. This study seeks to address these issues, and empower people to\nbetter understand the opinion influencing and misinformation debunking\nprocesses. We did this by creating a Player versus Player (PvP) game where\nparticipants attempt to either generate or debunk misinformation to convince\nLLM-represented public opinion. Using a within-subjects mixed-methods study\ndesign (N=47), we found that this game significantly raised participants' media\nliteracy and improved their ability to identify misinformation. Our qualitative\nexploration revealed how participants' use of debunking and content creation\nstrategies deepened their understanding of the nature of disinformation. We\ndemonstrate how LLMs can be integrated into PvP games to foster greater\nunderstanding of contrasting viewpoints and highlight social challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game-based interventions are widely used to combat misinformation online by\nemploying the \"inoculation approach\". However, most current interventions are\ndesigned as single-player games, presenting players with limited predefined\nchoices. Such restrictions reduce replayability and may lead to an overly\nsimplistic understanding of the processes of misinformation phenomenon and the\ndebunking. This study seeks to address these issues, and empower people to\nbetter understand the opinion influencing and misinformation debunking\nprocesses. We did this by creating a Player versus Player (PvP) game where\nparticipants attempt to either generate or debunk misinformation to convince\nLLM-represented public opinion. Using a within-subjects mixed-methods study\ndesign (N=47), we found that this game significantly raised participants' media\nliteracy and improved their ability to identify misinformation. Our qualitative\nexploration revealed how participants' use of debunking and content creation\nstrategies deepened their understanding of the nature of disinformation. We\ndemonstrate how LLMs can be integrated into PvP games to foster greater\nunderstanding of contrasting viewpoints and highlight social challenges."
                },
                "authors": [
                    {
                        "name": "Huiyun Tang"
                    },
                    {
                        "name": "Songqi Sun"
                    },
                    {
                        "name": "Kexin Nie"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Anastasia Sergeeva"
                    },
                    {
                        "name": "Ray LC"
                    }
                ],
                "author_detail": {
                    "name": "Ray LC"
                },
                "author": "Ray LC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01659v2",
                "updated": "2025-02-07T13:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    44,
                    24,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-31T22:05:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    22,
                    5,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse\n  Graph Processing Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Longer Attention Span: Increasing Transformer Context Length with Sparse\n  Graph Processing Techniques"
                },
                "summary": "Transformers have demonstrated great success in numerous domains including\nnatural language processing and bioinformatics. This success stems from the use\nof the attention mechanism by these models in order to represent and propagate\npairwise interactions between individual tokens of sequential data. However,\nthe primary limitation of this operation is its quadratic memory and time\ncomplexity in relation to the input's context length - the length of a sequence\nover which the interactions need to be captured. This significantly limits the\nlength of sequences that can be inferred upon by these models. Extensive\nresearch has been conducted to reduce the number of pairwise interactions to\nsub-quadratic in relation to the context length by introducing sparsity into\nthe attention mechanism through the development of sparse attention masks.\nHowever, efficient implementations that achieve \"true sparsity\" are lacking.\n  In this work, we address this issue by proposing a graph computing view of\nattention where tokens are perceived as nodes of the graph and the attention\nmask determines the edges of the graph. Using this view, we develop graph\nprocessing algorithms to implement the attention mechanism. Both theoretically\nand empirically, we demonstrate that our algorithms only perform the needed\ncomputations, i.e., they are work optimal. We also perform extensive\nexperimentation using popular attention masks to explore the impact of sparsity\non execution time and achievable context length. Our experiments demonstrate\nsignificant speedups in execution times compared to state-of-the-art attention\nimplementations such as FlashAttention for large sequence lengths. We also\ndemonstrate that our algorithms are able to achieve extremely long sequence\nlengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have demonstrated great success in numerous domains including\nnatural language processing and bioinformatics. This success stems from the use\nof the attention mechanism by these models in order to represent and propagate\npairwise interactions between individual tokens of sequential data. However,\nthe primary limitation of this operation is its quadratic memory and time\ncomplexity in relation to the input's context length - the length of a sequence\nover which the interactions need to be captured. This significantly limits the\nlength of sequences that can be inferred upon by these models. Extensive\nresearch has been conducted to reduce the number of pairwise interactions to\nsub-quadratic in relation to the context length by introducing sparsity into\nthe attention mechanism through the development of sparse attention masks.\nHowever, efficient implementations that achieve \"true sparsity\" are lacking.\n  In this work, we address this issue by proposing a graph computing view of\nattention where tokens are perceived as nodes of the graph and the attention\nmask determines the edges of the graph. Using this view, we develop graph\nprocessing algorithms to implement the attention mechanism. Both theoretically\nand empirically, we demonstrate that our algorithms only perform the needed\ncomputations, i.e., they are work optimal. We also perform extensive\nexperimentation using popular attention masks to explore the impact of sparsity\non execution time and achievable context length. Our experiments demonstrate\nsignificant speedups in execution times compared to state-of-the-art attention\nimplementations such as FlashAttention for large sequence lengths. We also\ndemonstrate that our algorithms are able to achieve extremely long sequence\nlengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB)."
                },
                "authors": [
                    {
                        "name": "Nathaniel Tomczak"
                    },
                    {
                        "name": "Sanmukh Kuppannagari"
                    }
                ],
                "author_detail": {
                    "name": "Sanmukh Kuppannagari"
                },
                "author": "Sanmukh Kuppannagari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04916v1",
                "updated": "2025-02-07T13:33:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    33,
                    40,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:33:40Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    33,
                    40,
                    4,
                    38,
                    0
                ],
                "title": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability"
                },
                "summary": "New regulations are continuously introduced to ensure that software\ndevelopment complies with the ethical concerns and prioritizes public safety. A\nprerequisite for demonstrating compliance involves tracing software\nrequirements to legal provisions. Requirements traceability is a fundamental\ntask where requirements engineers are supposed to analyze technical\nrequirements against target artifacts, often under limited time budget. Doing\nthis analysis manually for complex systems with hundreds of requirements is\ninfeasible. The legal dimension introduces additional challenges that only\nexacerbate manual effort.\n  In this paper, we investigate two automated solutions based on large language\nmodels (LLMs) to predict trace links between requirements and legal provisions.\nThe first solution, Kashif, is a classifier that leverages sentence\ntransformers. The second solution prompts a recent generative LLM based on\nRice, a prompt engineering framework.\n  On a benchmark dataset, we empirically evaluate Kashif and compare it against\na baseline classifier from the literature. Kashif can identify trace links with\nan average recall of ~67%, outperforming the baseline with a substantial gain\nof 54 percentage points (pp) in recall. However, on unseen, more complex\nrequirements documents traced to the European general data protection\nregulation (GDPR), Kashif performs poorly, yielding an average recall of 15%.\nOn the same documents, however, our Rice-based solution yields an average\nrecall of 84%, with a remarkable gain of about 69 pp over Kashif. Our results\nsuggest that requirements traceability in the legal context cannot be simply\naddressed by building classifiers, as such solutions do not generalize and fail\nto perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New regulations are continuously introduced to ensure that software\ndevelopment complies with the ethical concerns and prioritizes public safety. A\nprerequisite for demonstrating compliance involves tracing software\nrequirements to legal provisions. Requirements traceability is a fundamental\ntask where requirements engineers are supposed to analyze technical\nrequirements against target artifacts, often under limited time budget. Doing\nthis analysis manually for complex systems with hundreds of requirements is\ninfeasible. The legal dimension introduces additional challenges that only\nexacerbate manual effort.\n  In this paper, we investigate two automated solutions based on large language\nmodels (LLMs) to predict trace links between requirements and legal provisions.\nThe first solution, Kashif, is a classifier that leverages sentence\ntransformers. The second solution prompts a recent generative LLM based on\nRice, a prompt engineering framework.\n  On a benchmark dataset, we empirically evaluate Kashif and compare it against\na baseline classifier from the literature. Kashif can identify trace links with\nan average recall of ~67%, outperforming the baseline with a substantial gain\nof 54 percentage points (pp) in recall. However, on unseen, more complex\nrequirements documents traced to the European general data protection\nregulation (GDPR), Kashif performs poorly, yielding an average recall of 15%.\nOn the same documents, however, our Rice-based solution yields an average\nrecall of 84%, with a remarkable gain of about 69 pp over Kashif. Our results\nsuggest that requirements traceability in the legal context cannot be simply\naddressed by building classifiers, as such solutions do not generalize and fail\nto perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative."
                },
                "authors": [
                    {
                        "name": "Romina Etezadi"
                    },
                    {
                        "name": "Sallam Abualhaija"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10198v2",
                "updated": "2025-02-07T13:26:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    26,
                    18,
                    4,
                    38,
                    0
                ],
                "published": "2024-12-13T15:15:24Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    15,
                    24,
                    4,
                    348,
                    0
                ],
                "title": "From Allies to Adversaries: Manipulating LLM Tool-Calling through\n  Adversarial Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Allies to Adversaries: Manipulating LLM Tool-Calling through\n  Adversarial Injection"
                },
                "summary": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems."
                },
                "authors": [
                    {
                        "name": "Haowei Wang"
                    },
                    {
                        "name": "Rupeng Zhang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Yuekai Huang"
                    },
                    {
                        "name": "Dandan Wang"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04892v1",
                "updated": "2025-02-07T12:57:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    57,
                    26,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T12:57:26Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    57,
                    26,
                    4,
                    38,
                    0
                ],
                "title": "A Foundational Brain Dynamics Model via Stochastic Optimal Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Foundational Brain Dynamics Model via Stochastic Optimal Control"
                },
                "summary": "We introduce a foundational model for brain dynamics that utilizes stochastic\noptimal control (SOC) and amortized inference. Our method features a\ncontinuous-discrete state space model (SSM) that can robustly handle the\nintricate and noisy nature of fMRI signals. To address computational\nlimitations, we implement an approximation strategy grounded in the SOC\nframework. Additionally, we present a simulation-free latent dynamics approach\nthat employs locally linear approximations, facilitating efficient and scalable\ninference. For effective representation learning, we derive an Evidence Lower\nBound (ELBO) from the SOC formulation, which integrates smoothly with recent\nadvancements in self-supervised learning (SSL), thereby promoting robust and\ntransferable representations. Pre-trained on extensive datasets such as the\nUKB, our model attains state-of-the-art results across a variety of downstream\ntasks, including demographic prediction, trait analysis, disease diagnosis, and\nprognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and\nADHD200 further validates its superior abilities and resilience across\ndifferent demographic and clinical distributions. Our foundational model\nprovides a scalable and efficient approach for deciphering brain dynamics,\nopening up numerous applications in neuroscience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a foundational model for brain dynamics that utilizes stochastic\noptimal control (SOC) and amortized inference. Our method features a\ncontinuous-discrete state space model (SSM) that can robustly handle the\nintricate and noisy nature of fMRI signals. To address computational\nlimitations, we implement an approximation strategy grounded in the SOC\nframework. Additionally, we present a simulation-free latent dynamics approach\nthat employs locally linear approximations, facilitating efficient and scalable\ninference. For effective representation learning, we derive an Evidence Lower\nBound (ELBO) from the SOC formulation, which integrates smoothly with recent\nadvancements in self-supervised learning (SSL), thereby promoting robust and\ntransferable representations. Pre-trained on extensive datasets such as the\nUKB, our model attains state-of-the-art results across a variety of downstream\ntasks, including demographic prediction, trait analysis, disease diagnosis, and\nprognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and\nADHD200 further validates its superior abilities and resilience across\ndifferent demographic and clinical distributions. Our foundational model\nprovides a scalable and efficient approach for deciphering brain dynamics,\nopening up numerous applications in neuroscience."
                },
                "authors": [
                    {
                        "name": "Joonhyeong Park"
                    },
                    {
                        "name": "Byoungwoo Park"
                    },
                    {
                        "name": "Chang-Bae Bang"
                    },
                    {
                        "name": "Jungwon Choi"
                    },
                    {
                        "name": "Hyungjin Chung"
                    },
                    {
                        "name": "Byung-Hoon Kim"
                    },
                    {
                        "name": "Juho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Juho Lee"
                },
                "author": "Juho Lee",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11633v2",
                "updated": "2025-02-07T12:36:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    36,
                    53,
                    4,
                    38,
                    0
                ],
                "published": "2024-02-18T16:20:43Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    16,
                    20,
                    43,
                    6,
                    49,
                    0
                ],
                "title": "Self-seeding and Multi-intent Self-instructing LLMs for Generating\n  Intent-aware Information-Seeking dialogs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-seeding and Multi-intent Self-instructing LLMs for Generating\n  Intent-aware Information-Seeking dialogs"
                },
                "summary": "Identifying user intents in information-seeking dialogs is crucial for a\nsystem to meet user's information needs. Intent prediction (IP) is challenging\nand demands sufficient dialogs with human-labeled intents for training.\nHowever, manually annotating intents is resource-intensive. While large\nlanguage models (LLMs) have been shown to be effective in generating synthetic\ndata, there is no study on using LLMs to generate intent-aware\ninformation-seeking dialogs. In this paper, we focus on leveraging LLMs for\nzero-shot generation of large-scale, open-domain, and intent-aware\ninformation-seeking dialogs. We propose SOLID, which has novel self-seeding and\nmulti-intent self-instructing schemes. The former improves the generation\nquality by using the LLM's own knowledge scope to initiate dialog generation;\nthe latter prompts the LLM to generate utterances sequentially, and mitigates\nthe need for manual prompt design by asking the LLM to autonomously adapt its\nprompt instruction when generating complex multi-intent utterances.\nFurthermore, we propose SOLID-RL, which is further trained to generate a dialog\nin one step on the data generated by SOLID. We propose a length-based quality\nestimation mechanism to assign varying weights to SOLID-generated dialogs based\non their quality during the training process of SOLID-RL. We use SOLID and\nSOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size\nof existing datasets. Experiments show that IP methods trained on dialogs\ngenerated by SOLID and SOLID-RL achieve better IP quality than ones trained on\nhuman-generated dialogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying user intents in information-seeking dialogs is crucial for a\nsystem to meet user's information needs. Intent prediction (IP) is challenging\nand demands sufficient dialogs with human-labeled intents for training.\nHowever, manually annotating intents is resource-intensive. While large\nlanguage models (LLMs) have been shown to be effective in generating synthetic\ndata, there is no study on using LLMs to generate intent-aware\ninformation-seeking dialogs. In this paper, we focus on leveraging LLMs for\nzero-shot generation of large-scale, open-domain, and intent-aware\ninformation-seeking dialogs. We propose SOLID, which has novel self-seeding and\nmulti-intent self-instructing schemes. The former improves the generation\nquality by using the LLM's own knowledge scope to initiate dialog generation;\nthe latter prompts the LLM to generate utterances sequentially, and mitigates\nthe need for manual prompt design by asking the LLM to autonomously adapt its\nprompt instruction when generating complex multi-intent utterances.\nFurthermore, we propose SOLID-RL, which is further trained to generate a dialog\nin one step on the data generated by SOLID. We propose a length-based quality\nestimation mechanism to assign varying weights to SOLID-generated dialogs based\non their quality during the training process of SOLID-RL. We use SOLID and\nSOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size\nof existing datasets. Experiments show that IP methods trained on dialogs\ngenerated by SOLID and SOLID-RL achieve better IP quality than ones trained on\nhuman-generated dialogs."
                },
                "authors": [
                    {
                        "name": "Arian Askari"
                    },
                    {
                        "name": "Roxana Petcu"
                    },
                    {
                        "name": "Chuan Meng"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Amin Abolghasemi"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04879v1",
                "updated": "2025-02-07T12:36:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    36,
                    23,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T12:36:23Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    36,
                    23,
                    4,
                    38,
                    0
                ],
                "title": "Statistical Collusion by Collectives on Learning Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Collusion by Collectives on Learning Platforms"
                },
                "summary": "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain."
                },
                "authors": [
                    {
                        "name": "Etienne Gauthier"
                    },
                    {
                        "name": "Francis Bach"
                    },
                    {
                        "name": "Michael I. Jordan"
                    }
                ],
                "author_detail": {
                    "name": "Michael I. Jordan"
                },
                "author": "Michael I. Jordan",
                "arxiv_comment": "Code available at: https://github.com/GauthierE/statistical-collusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04878v1",
                "updated": "2025-02-07T12:33:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    33,
                    8,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T12:33:08Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    33,
                    8,
                    4,
                    38,
                    0
                ],
                "title": "Sparse Autoencoders Do Not Find Canonical Units of Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Do Not Find Canonical Units of Analysis"
                },
                "summary": "A common goal of mechanistic interpretability is to decompose the activations\nof neural networks into features: interpretable properties of the input\ncomputed by the model. Sparse autoencoders (SAEs) are a popular method for\nfinding these features in LLMs, and it has been postulated that they can be\nused to find a \\textit{canonical} set of units: a unique and complete list of\natomic features. We cast doubt on this belief using two novel techniques: SAE\nstitching to show they are incomplete, and meta-SAEs to show they are not\natomic. SAE stitching involves inserting or swapping latents from a larger SAE\ninto a smaller one. Latents from the larger SAE can be divided into two\ncategories: \\emph{novel latents}, which improve performance when added to the\nsmaller SAE, indicating they capture novel information, and\n\\emph{reconstruction latents}, which can replace corresponding latents in the\nsmaller SAE that have similar behavior. The existence of novel features\nindicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on\nthe decoder matrix of another SAE -- we find that latents in SAEs often\ndecompose into combinations of latents from a smaller SAE, showing that larger\nSAE latents are not atomic. The resulting decompositions are often\ninterpretable; e.g. a latent representing ``Einstein'' decomposes into\n``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find\ncanonical units of analysis, they may still be useful tools. We suggest that\nfuture research should either pursue different approaches for identifying such\nunits, or pragmatically choose the SAE size suited to their task. We provide an\ninteractive dashboard to explore meta-SAEs: https://metasaes.streamlit.app/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common goal of mechanistic interpretability is to decompose the activations\nof neural networks into features: interpretable properties of the input\ncomputed by the model. Sparse autoencoders (SAEs) are a popular method for\nfinding these features in LLMs, and it has been postulated that they can be\nused to find a \\textit{canonical} set of units: a unique and complete list of\natomic features. We cast doubt on this belief using two novel techniques: SAE\nstitching to show they are incomplete, and meta-SAEs to show they are not\natomic. SAE stitching involves inserting or swapping latents from a larger SAE\ninto a smaller one. Latents from the larger SAE can be divided into two\ncategories: \\emph{novel latents}, which improve performance when added to the\nsmaller SAE, indicating they capture novel information, and\n\\emph{reconstruction latents}, which can replace corresponding latents in the\nsmaller SAE that have similar behavior. The existence of novel features\nindicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on\nthe decoder matrix of another SAE -- we find that latents in SAEs often\ndecompose into combinations of latents from a smaller SAE, showing that larger\nSAE latents are not atomic. The resulting decompositions are often\ninterpretable; e.g. a latent representing ``Einstein'' decomposes into\n``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find\ncanonical units of analysis, they may still be useful tools. We suggest that\nfuture research should either pursue different approaches for identifying such\nunits, or pragmatically choose the SAE size suited to their task. We provide an\ninteractive dashboard to explore meta-SAEs: https://metasaes.streamlit.app/"
                },
                "authors": [
                    {
                        "name": "Patrick Leask"
                    },
                    {
                        "name": "Bart Bussmann"
                    },
                    {
                        "name": "Michael Pearce"
                    },
                    {
                        "name": "Joseph Bloom"
                    },
                    {
                        "name": "Curt Tigges"
                    },
                    {
                        "name": "Noura Al Moubayed"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12952v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12952v3",
                "updated": "2025-02-07T12:33:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    33,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2024-06-18T14:54:37Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    54,
                    37,
                    1,
                    170,
                    0
                ],
                "title": "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents"
                },
                "summary": "Rigorous software testing is crucial for developing and maintaining\nhigh-quality code, making automated test generation a promising avenue for both\nimproving software quality and boosting the effectiveness of code generation\nmethods. However, while code generation with Large Language Models (LLMs) is an\nextraordinarily active research area, test generation remains relatively\nunexplored. We address this gap and investigate the capability of LLM-based\nCode Agents to formalize user issues into test cases. To this end, we propose a\nnovel benchmark based on popular GitHub repositories, containing real-world\nissues, ground-truth bug-fixes, and golden tests. We find that LLMs generally\nperform surprisingly well at generating relevant test cases, with Code Agents\ndesigned for code repair exceeding the performance of systems designed\nspecifically for test generation. Further, as test generation is a similar but\nmore structured task than code generation, it allows for a more fine-grained\nanalysis using issue reproduction rate and coverage changes, providing a dual\nmetric for analyzing systems designed for code repair. Finally, we find that\ngenerated tests are an effective filter for proposed code fixes, doubling the\nprecision of SWE-Agent. We release all data and code at\nhttps://github.com/logic-star-ai/SWT-Bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous software testing is crucial for developing and maintaining\nhigh-quality code, making automated test generation a promising avenue for both\nimproving software quality and boosting the effectiveness of code generation\nmethods. However, while code generation with Large Language Models (LLMs) is an\nextraordinarily active research area, test generation remains relatively\nunexplored. We address this gap and investigate the capability of LLM-based\nCode Agents to formalize user issues into test cases. To this end, we propose a\nnovel benchmark based on popular GitHub repositories, containing real-world\nissues, ground-truth bug-fixes, and golden tests. We find that LLMs generally\nperform surprisingly well at generating relevant test cases, with Code Agents\ndesigned for code repair exceeding the performance of systems designed\nspecifically for test generation. Further, as test generation is a similar but\nmore structured task than code generation, it allows for a more fine-grained\nanalysis using issue reproduction rate and coverage changes, providing a dual\nmetric for analyzing systems designed for code repair. Finally, we find that\ngenerated tests are an effective filter for proposed code fixes, doubling the\nprecision of SWE-Agent. We release all data and code at\nhttps://github.com/logic-star-ai/SWT-Bench"
                },
                "authors": [
                    {
                        "name": "Niels Mündler"
                    },
                    {
                        "name": "Mark Niklas Müller"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "20 pages, 14 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12952v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12952v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00026v2",
                "updated": "2025-02-07T12:23:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    23,
                    59,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-21T17:10:52Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    10,
                    52,
                    1,
                    21,
                    0
                ],
                "title": "Pushing the Limits of BFP on Narrow Precision LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of BFP on Narrow Precision LLM Inference"
                },
                "summary": "The substantial computational and memory demands of Large Language Models\n(LLMs) hinder their deployment. Block Floating Point (BFP) has proven effective\nin accelerating linear operations, a cornerstone of LLM workloads. However, as\nsequence lengths grow, nonlinear operations, such as Attention, increasingly\nbecome performance bottlenecks due to their quadratic computational complexity.\nThese nonlinear operations are predominantly executed using inefficient\nfloating-point formats, which renders the system challenging to optimize\nsoftware efficiency and hardware overhead. In this paper, we delve into the\nlimitations and potential of applying BFP to nonlinear operations. Given our\nfindings, we introduce a hardware-software co-design framework (DB-Attn),\nincluding: (i) DBFP, an advanced BFP version, overcomes nonlinear operation\nchallenges with a pivot-focus strategy for diverse data and an adaptive\ngrouping strategy for flexible exponent sharing. (ii) DH-LUT, a novel lookup\ntable algorithm dedicated to accelerating nonlinear operations with DBFP\nformat. (iii) An RTL-level DBFP-based engine is implemented to support DB-Attn,\napplicable to FPGA and ASIC. Results show that DB-Attn provides significant\nperformance improvements with negligible accuracy loss, achieving 74% GPU\nspeedup on Softmax of LLaMA and 10x low overhead performance improvement over\nSOTA designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The substantial computational and memory demands of Large Language Models\n(LLMs) hinder their deployment. Block Floating Point (BFP) has proven effective\nin accelerating linear operations, a cornerstone of LLM workloads. However, as\nsequence lengths grow, nonlinear operations, such as Attention, increasingly\nbecome performance bottlenecks due to their quadratic computational complexity.\nThese nonlinear operations are predominantly executed using inefficient\nfloating-point formats, which renders the system challenging to optimize\nsoftware efficiency and hardware overhead. In this paper, we delve into the\nlimitations and potential of applying BFP to nonlinear operations. Given our\nfindings, we introduce a hardware-software co-design framework (DB-Attn),\nincluding: (i) DBFP, an advanced BFP version, overcomes nonlinear operation\nchallenges with a pivot-focus strategy for diverse data and an adaptive\ngrouping strategy for flexible exponent sharing. (ii) DH-LUT, a novel lookup\ntable algorithm dedicated to accelerating nonlinear operations with DBFP\nformat. (iii) An RTL-level DBFP-based engine is implemented to support DB-Attn,\napplicable to FPGA and ASIC. Results show that DB-Attn provides significant\nperformance improvements with negligible accuracy loss, achieving 74% GPU\nspeedup on Softmax of LLaMA and 10x low overhead performance improvement over\nSOTA designs."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04861v1",
                "updated": "2025-02-07T11:56:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    56,
                    18,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T11:56:18Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    56,
                    18,
                    4,
                    38,
                    0
                ],
                "title": "Optimal Low degree hardness for Broadcasting on Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Low degree hardness for Broadcasting on Trees"
                },
                "summary": "Broadcasting on trees is a fundamental model from statistical physics that\nplays an important role in information theory, noisy computation and\nphylogenetic reconstruction within computational biology and linguistics. While\nthis model permits efficient linear-time algorithms for the inference of the\nroot from the leaves, recent work suggests that non-trivial computational\ncomplexity may be required for inference.\n  The inference of the root state can be performed using the celebrated Belief\nPropagation (BP) algorithm, which achieves Bayes-optimal performance. Although\nBP runs in linear time using real arithmetic operations, recent research\nindicates that it requires non-trivial computational complexity using more\nrefined complexity measures.\n  Moitra, Mossel, and Sandon demonstrated such complexity by constructing a\nMarkov chain for which estimating the root better than random guessing (for\ntypical inputs) is $NC^1$-complete. Kohler and Mossel constructed chains where,\nfor trees with $N$ leaves, achieving better-than-random root recovery requires\npolynomials of degree $N^{\\Omega(1)}$. The papers above raised the question of\nwhether such complexity bounds hold generally below the celebrated\nKesten-Stigum bound.\n  In a recent work, Huang and Mossel established a general degree lower bound\nof $\\Omega(\\log N)$ below the Kesten-Stigum bound. Specifically, they proved\nthat any function expressed as a linear combination of functions of at most\n$O(log N)$ leaves has vanishing correlation with the root. In this work, we get\nan exponential improvement of this lower bound by establishing an\n$N^{\\Omega(1)}$ degree lower bound, for any broadcast process in the whole\nregime below the Kesten-Stigum bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broadcasting on trees is a fundamental model from statistical physics that\nplays an important role in information theory, noisy computation and\nphylogenetic reconstruction within computational biology and linguistics. While\nthis model permits efficient linear-time algorithms for the inference of the\nroot from the leaves, recent work suggests that non-trivial computational\ncomplexity may be required for inference.\n  The inference of the root state can be performed using the celebrated Belief\nPropagation (BP) algorithm, which achieves Bayes-optimal performance. Although\nBP runs in linear time using real arithmetic operations, recent research\nindicates that it requires non-trivial computational complexity using more\nrefined complexity measures.\n  Moitra, Mossel, and Sandon demonstrated such complexity by constructing a\nMarkov chain for which estimating the root better than random guessing (for\ntypical inputs) is $NC^1$-complete. Kohler and Mossel constructed chains where,\nfor trees with $N$ leaves, achieving better-than-random root recovery requires\npolynomials of degree $N^{\\Omega(1)}$. The papers above raised the question of\nwhether such complexity bounds hold generally below the celebrated\nKesten-Stigum bound.\n  In a recent work, Huang and Mossel established a general degree lower bound\nof $\\Omega(\\log N)$ below the Kesten-Stigum bound. Specifically, they proved\nthat any function expressed as a linear combination of functions of at most\n$O(log N)$ leaves has vanishing correlation with the root. In this work, we get\nan exponential improvement of this lower bound by establishing an\n$N^{\\Omega(1)}$ degree lower bound, for any broadcast process in the whole\nregime below the Kesten-Stigum bound."
                },
                "authors": [
                    {
                        "name": "Han Huang"
                    },
                    {
                        "name": "Elchanan Mossel"
                    }
                ],
                "author_detail": {
                    "name": "Elchanan Mossel"
                },
                "author": "Elchanan Mossel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13203v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13203v3",
                "updated": "2025-02-07T11:45:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    45,
                    11,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-20T04:17:13Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    4,
                    17,
                    13,
                    4,
                    264,
                    0
                ],
                "title": "Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks"
                },
                "summary": "In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13203v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13203v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12382v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12382v4",
                "updated": "2025-02-07T11:37:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    37,
                    20,
                    4,
                    38,
                    0
                ],
                "published": "2024-06-18T08:14:28Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    8,
                    14,
                    28,
                    1,
                    170,
                    0
                ],
                "title": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions"
                },
                "summary": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Yanchao Hao"
                    },
                    {
                        "name": "Shengping Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12382v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12382v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04847v1",
                "updated": "2025-02-07T11:36:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    36,
                    36,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T11:36:36Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    36,
                    36,
                    4,
                    38,
                    0
                ],
                "title": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion\n  Video Generation"
                },
                "summary": "Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios."
                },
                "authors": [
                    {
                        "name": "Qijun Gan"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhenhui Ye"
                    },
                    {
                        "name": "Pan Xie"
                    },
                    {
                        "name": "Xiang Yin"
                    },
                    {
                        "name": "Zehuan Yuan"
                    },
                    {
                        "name": "Bingyue Peng"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "https://agnjason.github.io/HumanDiT-page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04816v1",
                "updated": "2025-02-07T10:40:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    10,
                    40,
                    54,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T10:40:54Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    10,
                    40,
                    54,
                    4,
                    38,
                    0
                ],
                "title": "The XXL Survey LIV. X-ray Luminosity Function and Luminosity-Mass\n  Relation of Optically Selected Galaxy Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The XXL Survey LIV. X-ray Luminosity Function and Luminosity-Mass\n  Relation of Optically Selected Galaxy Groups"
                },
                "summary": "The overlap between the GAMA spectroscopic survey and the XXL X-ray survey\nwas used to study the X-ray properties of optically-selected groups of\ngalaxies. Forced X-ray aperture photometry was applied to an optically-selected\nsample of 235 groups (containing at least five member galaxies) to measure\ntheir X-ray luminosities in the regime of low signal to noise X-ray data. The\nsample encompasses X-ray luminosities over an order of magnitude fainter than\ntypical X-ray selected samples, and avoids X-ray selection biases. This gives\naccess to low mass groups where the effects of non-gravitational processes,\nsuch as AGN-feedback, should be most apparent and could inhibit their detection\nin an X-ray survey. We measured the X-ray luminosity function (XLF) of the\nsample, and found it to be consistent with the extrapolation of the XLF from\nX-ray selected samples at higher luminosities. The XLF was combined with a\ntheoretical halo mass function to infer the form of the scaling relation\nbetween X-ray luminosity and mass (LM relation) for the GAMA groups. We found a\nslope of $1.87 \\pm 0.12$, which is steeper than self similarity in this mass\nregime. When comparing with other measurements of the LM relation, we find\nevidence for a steepening of the slope in the low mass regime, likely due to\nthe impact of non-gravitational processes. Our approach can be translated to\neROSITA data using multi-wavelength surveys to constrain the X-ray properties\nof galaxy groups in the limits of high redshift and low mass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The overlap between the GAMA spectroscopic survey and the XXL X-ray survey\nwas used to study the X-ray properties of optically-selected groups of\ngalaxies. Forced X-ray aperture photometry was applied to an optically-selected\nsample of 235 groups (containing at least five member galaxies) to measure\ntheir X-ray luminosities in the regime of low signal to noise X-ray data. The\nsample encompasses X-ray luminosities over an order of magnitude fainter than\ntypical X-ray selected samples, and avoids X-ray selection biases. This gives\naccess to low mass groups where the effects of non-gravitational processes,\nsuch as AGN-feedback, should be most apparent and could inhibit their detection\nin an X-ray survey. We measured the X-ray luminosity function (XLF) of the\nsample, and found it to be consistent with the extrapolation of the XLF from\nX-ray selected samples at higher luminosities. The XLF was combined with a\ntheoretical halo mass function to infer the form of the scaling relation\nbetween X-ray luminosity and mass (LM relation) for the GAMA groups. We found a\nslope of $1.87 \\pm 0.12$, which is steeper than self similarity in this mass\nregime. When comparing with other measurements of the LM relation, we find\nevidence for a steepening of the slope in the low mass regime, likely due to\nthe impact of non-gravitational processes. Our approach can be translated to\neROSITA data using multi-wavelength surveys to constrain the X-ray properties\nof galaxy groups in the limits of high redshift and low mass."
                },
                "authors": [
                    {
                        "name": "C. Wood"
                    },
                    {
                        "name": "B. J. Maughan"
                    },
                    {
                        "name": "J. P. Crossett"
                    },
                    {
                        "name": "D. Eckert"
                    },
                    {
                        "name": "M. Pierre"
                    },
                    {
                        "name": "M. E. Ramos-Ceja"
                    },
                    {
                        "name": "A. S. G. Robotham"
                    },
                    {
                        "name": "C. Adami"
                    },
                    {
                        "name": "L. Faccioli"
                    },
                    {
                        "name": "E. Koulouridis"
                    },
                    {
                        "name": "S. L. McGee"
                    },
                    {
                        "name": "F. Pacaud"
                    },
                    {
                        "name": "S. Phillipps"
                    }
                ],
                "author_detail": {
                    "name": "S. Phillipps"
                },
                "author": "S. Phillipps",
                "arxiv_comment": "22 pages, 15 figures, Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14940v3",
                "updated": "2025-02-07T10:23:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    10,
                    23,
                    16,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-24T21:55:14Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    21,
                    55,
                    14,
                    4,
                    24,
                    0
                ],
                "title": "CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models"
                },
                "summary": "Aligning large language models (LLMs) with human values is essential for\ntheir safe deployment and widespread adoption. Current LLM safety benchmarks\noften focus solely on the refusal of individual problematic queries, which\noverlooks the importance of the context where the query occurs and may cause\nundesired refusal of queries under safe contexts that diminish user experience.\nAddressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark\nthat integrates context into safety assessments of LLMs. CASE-Bench assigns\ndistinct, formally described contexts to categorized queries based on\nContextual Integrity theory. Additionally, in contrast to previous studies\nwhich mainly rely on majority voting from just a few annotators, we recruited a\nsufficient number of annotators necessary to ensure the detection of\nstatistically significant differences among the experimental conditions based\non power analysis. Our extensive analysis using CASE-Bench on various\nopen-source and commercial LLMs reveals a substantial and significant influence\nof context on human judgments (p<0.0001 from a z-test), underscoring the\nnecessity of context in safety evaluations. We also identify notable mismatches\nbetween human judgments and LLM responses, particularly in commercial models\nwithin safe contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human values is essential for\ntheir safe deployment and widespread adoption. Current LLM safety benchmarks\noften focus solely on the refusal of individual problematic queries, which\noverlooks the importance of the context where the query occurs and may cause\nundesired refusal of queries under safe contexts that diminish user experience.\nAddressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark\nthat integrates context into safety assessments of LLMs. CASE-Bench assigns\ndistinct, formally described contexts to categorized queries based on\nContextual Integrity theory. Additionally, in contrast to previous studies\nwhich mainly rely on majority voting from just a few annotators, we recruited a\nsufficient number of annotators necessary to ensure the detection of\nstatistically significant differences among the experimental conditions based\non power analysis. Our extensive analysis using CASE-Bench on various\nopen-source and commercial LLMs reveals a substantial and significant influence\nof context on human judgments (p<0.0001 from a z-test), underscoring the\nnecessity of context in safety evaluations. We also identify notable mismatches\nbetween human judgments and LLM responses, particularly in commercial models\nwithin safe contexts."
                },
                "authors": [
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Xiao Zhan"
                    },
                    {
                        "name": "Shutong Feng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    },
                    {
                        "name": "Jose Such"
                    }
                ],
                "author_detail": {
                    "name": "Jose Such"
                },
                "author": "Jose Such",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04797v1",
                "updated": "2025-02-07T10:01:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    10,
                    1,
                    32,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T10:01:32Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    10,
                    1,
                    32,
                    4,
                    38,
                    0
                ],
                "title": "Self-Rationalization in the Wild: A Large Scale Out-of-Distribution\n  Evaluation on NLI-related tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Rationalization in the Wild: A Large Scale Out-of-Distribution\n  Evaluation on NLI-related tasks"
                },
                "summary": "Free-text explanations are expressive and easy to understand, but many\ndatasets lack annotated explanation data, making it challenging to train models\nfor explainable predictions. To address this, we investigate how to use\nexisting explanation datasets for self-rationalization and evaluate models'\nout-of-distribution (OOD) performance. We fine-tune T5-Large and OLMo-7B models\nand assess the impact of fine-tuning data quality, the number of fine-tuning\nsamples, and few-shot selection methods. The models are evaluated on 19 diverse\nOOD datasets across three tasks: natural language inference (NLI),\nfact-checking, and hallucination detection in abstractive summarization. For\nthe generated explanation evaluation, we conduct a human study on 13 selected\nmodels and study its correlation with the Acceptability score (T5-11B) and\nthree other LLM-based reference-free metrics. Human evaluation shows that the\nAcceptability score correlates most strongly with human judgments,\ndemonstrating its effectiveness in evaluating free-text explanations. Our\nfindings reveal: 1) few annotated examples effectively adapt models for OOD\nexplanation generation; 2) compared to sample selection strategies, fine-tuning\ndata source has a larger impact on OOD performance; and 3) models with higher\nlabel prediction accuracy tend to produce better explanations, as reflected by\nhigher Acceptability scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-text explanations are expressive and easy to understand, but many\ndatasets lack annotated explanation data, making it challenging to train models\nfor explainable predictions. To address this, we investigate how to use\nexisting explanation datasets for self-rationalization and evaluate models'\nout-of-distribution (OOD) performance. We fine-tune T5-Large and OLMo-7B models\nand assess the impact of fine-tuning data quality, the number of fine-tuning\nsamples, and few-shot selection methods. The models are evaluated on 19 diverse\nOOD datasets across three tasks: natural language inference (NLI),\nfact-checking, and hallucination detection in abstractive summarization. For\nthe generated explanation evaluation, we conduct a human study on 13 selected\nmodels and study its correlation with the Acceptability score (T5-11B) and\nthree other LLM-based reference-free metrics. Human evaluation shows that the\nAcceptability score correlates most strongly with human judgments,\ndemonstrating its effectiveness in evaluating free-text explanations. Our\nfindings reveal: 1) few annotated examples effectively adapt models for OOD\nexplanation generation; 2) compared to sample selection strategies, fine-tuning\ndata source has a larger impact on OOD performance; and 3) models with higher\nlabel prediction accuracy tend to produce better explanations, as reflected by\nhigher Acceptability scores."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Max Glockner"
                    },
                    {
                        "name": "Anderson Rocha"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted at TACL; pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12539v2",
                "updated": "2025-02-07T09:54:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    54,
                    53,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-16T13:20:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    20,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "Counterfactual Effect Decomposition in Multi-Agent Sequential Decision\n  Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Effect Decomposition in Multi-Agent Sequential Decision\n  Making"
                },
                "summary": "We address the challenge of explaining counterfactual outcomes in multi-agent\nMarkov decision processes. In particular, we aim to explain the total\ncounterfactual effect of an agent's action on the outcome of a realized\nscenario through its influence on the environment dynamics and the agents'\nbehavior. To achieve this, we introduce a novel causal explanation formula that\ndecomposes the counterfactual effect by attributing to each agent and state\nvariable a score reflecting their respective contributions to the effect.\nFirst, we show that the total counterfactual effect of an agent's action can be\ndecomposed into two components: one measuring the effect that propagates\nthrough all subsequent agents' actions and another related to the effect that\npropagates through the state transitions. Building on recent advancements in\ncausal contribution analysis, we further decompose these two effects as\nfollows. For the former, we consider agent-specific effects -- a causal concept\nthat quantifies the counterfactual effect of an agent's action that propagates\nthrough a subset of agents. Based on this notion, we use Shapley value to\nattribute the effect to individual agents. For the latter, we consider the\nconcept of structure-preserving interventions and attribute the effect to state\nvariables based on their \"intrinsic\" contributions. Through extensive\nexperimentation, we demonstrate the interpretability of our approach in a\nGridworld environment with LLM-assisted agents and a sepsis management\nsimulator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of explaining counterfactual outcomes in multi-agent\nMarkov decision processes. In particular, we aim to explain the total\ncounterfactual effect of an agent's action on the outcome of a realized\nscenario through its influence on the environment dynamics and the agents'\nbehavior. To achieve this, we introduce a novel causal explanation formula that\ndecomposes the counterfactual effect by attributing to each agent and state\nvariable a score reflecting their respective contributions to the effect.\nFirst, we show that the total counterfactual effect of an agent's action can be\ndecomposed into two components: one measuring the effect that propagates\nthrough all subsequent agents' actions and another related to the effect that\npropagates through the state transitions. Building on recent advancements in\ncausal contribution analysis, we further decompose these two effects as\nfollows. For the former, we consider agent-specific effects -- a causal concept\nthat quantifies the counterfactual effect of an agent's action that propagates\nthrough a subset of agents. Based on this notion, we use Shapley value to\nattribute the effect to individual agents. For the latter, we consider the\nconcept of structure-preserving interventions and attribute the effect to state\nvariables based on their \"intrinsic\" contributions. Through extensive\nexperimentation, we demonstrate the interpretability of our approach in a\nGridworld environment with LLM-assisted agents and a sepsis management\nsimulator."
                },
                "authors": [
                    {
                        "name": "Stelios Triantafyllou"
                    },
                    {
                        "name": "Aleksa Sukovic"
                    },
                    {
                        "name": "Yasaman Zolfimoselo"
                    },
                    {
                        "name": "Goran Radanovic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Radanovic"
                },
                "author": "Goran Radanovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04790v1",
                "updated": "2025-02-07T09:49:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    49,
                    56,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T09:49:56Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    49,
                    56,
                    4,
                    38,
                    0
                ],
                "title": "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate\n  Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate\n  Efficiency"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious natural language processing (NLP) scenarios, but they still face\nchallenges when handling complex arithmetic and logical reasoning tasks. While\nChain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction\nstrategies have attempted to guide models in sequential, multi-step reasoning,\nMulti-agent Debate (MAD) has emerged as a viable approach for enhancing the\nreasoning capabilities of LLMs. By increasing both the number of agents and the\nfrequency of debates, the performance of LLMs improves significantly. However,\nthis strategy results in a significant increase in token costs, presenting a\nbarrier to scalability. To address this challenge, we introduce a novel\nsparsification strategy designed to reduce token costs within MAD. This\napproach minimizes ineffective exchanges of information and unproductive\ndiscussions among agents, thereby enhancing the overall efficiency of the\ndebate process. We conduct comparative experiments on multiple datasets across\nvarious models, demonstrating that our approach significantly reduces the token\ncosts in MAD to a considerable extent. Specifically, compared to MAD, our\napproach achieves an impressive reduction of up to 94.5\\% in token costs while\nmaintaining performance degradation below 2.0\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious natural language processing (NLP) scenarios, but they still face\nchallenges when handling complex arithmetic and logical reasoning tasks. While\nChain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction\nstrategies have attempted to guide models in sequential, multi-step reasoning,\nMulti-agent Debate (MAD) has emerged as a viable approach for enhancing the\nreasoning capabilities of LLMs. By increasing both the number of agents and the\nfrequency of debates, the performance of LLMs improves significantly. However,\nthis strategy results in a significant increase in token costs, presenting a\nbarrier to scalability. To address this challenge, we introduce a novel\nsparsification strategy designed to reduce token costs within MAD. This\napproach minimizes ineffective exchanges of information and unproductive\ndiscussions among agents, thereby enhancing the overall efficiency of the\ndebate process. We conduct comparative experiments on multiple datasets across\nvarious models, demonstrating that our approach significantly reduces the token\ncosts in MAD to a considerable extent. Specifically, compared to MAD, our\napproach achieves an impressive reduction of up to 94.5\\% in token costs while\nmaintaining performance degradation below 2.0\\%."
                },
                "authors": [
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Xitai Jin"
                    },
                    {
                        "name": "Chen Tianying Tiana"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Xiaohua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Xu"
                },
                "author": "Xiaohua Xu",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04789v1",
                "updated": "2025-02-07T09:49:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    49,
                    13,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T09:49:13Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    49,
                    13,
                    4,
                    38,
                    0
                ],
                "title": "Probing Internal Representations of Multi-Word Verbs in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Internal Representations of Multi-Word Verbs in Large Language\n  Models"
                },
                "summary": "This study investigates the internal representations of verb-particle\ncombinations, called multi-word verbs, within transformer-based large language\nmodels (LLMs), specifically examining how these models capture lexical and\nsyntactic properties at different neural network layers. Using the BERT\narchitecture, we analyze the representations of its layers for two different\nverb-particle constructions: phrasal verbs like 'give up' and prepositional\nverbs like 'look at'. Our methodology includes training probing classifiers on\nthe internal representations to classify these categories at both word and\nsentence levels. The results indicate that the model's middle layers achieve\nthe highest classification accuracies. To further analyze the nature of these\ndistinctions, we conduct a data separability test using the Generalized\nDiscrimination Value (GDV). While GDV results show weak linear separability\nbetween the two verb types, probing classifiers still achieve high accuracy,\nsuggesting that representations of these linguistic categories may be\nnon-linearly separable. This aligns with previous research indicating that\nlinguistic distinctions in neural networks are not always encoded in a linearly\nseparable manner. These findings computationally support usage-based claims on\nthe representation of verb-particle constructions and highlight the complex\ninteraction between neural network architectures and linguistic structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the internal representations of verb-particle\ncombinations, called multi-word verbs, within transformer-based large language\nmodels (LLMs), specifically examining how these models capture lexical and\nsyntactic properties at different neural network layers. Using the BERT\narchitecture, we analyze the representations of its layers for two different\nverb-particle constructions: phrasal verbs like 'give up' and prepositional\nverbs like 'look at'. Our methodology includes training probing classifiers on\nthe internal representations to classify these categories at both word and\nsentence levels. The results indicate that the model's middle layers achieve\nthe highest classification accuracies. To further analyze the nature of these\ndistinctions, we conduct a data separability test using the Generalized\nDiscrimination Value (GDV). While GDV results show weak linear separability\nbetween the two verb types, probing classifiers still achieve high accuracy,\nsuggesting that representations of these linguistic categories may be\nnon-linearly separable. This aligns with previous research indicating that\nlinguistic distinctions in neural networks are not always encoded in a linearly\nseparable manner. These findings computationally support usage-based claims on\nthe representation of verb-particle constructions and highlight the complex\ninteraction between neural network architectures and linguistic structures."
                },
                "authors": [
                    {
                        "name": "Hassane Kissane"
                    },
                    {
                        "name": "Achim Schilling"
                    },
                    {
                        "name": "Patrick Krauss"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Krauss"
                },
                "author": "Patrick Krauss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04780v1",
                "updated": "2025-02-07T09:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    33,
                    44,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T09:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    33,
                    44,
                    4,
                    38,
                    0
                ],
                "title": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning"
                },
                "summary": "Multi-agent AI systems powered by large language models (LLMs) are\nincreasingly applied to solve complex tasks. However, these systems often rely\non fragile, manually designed prompts and heuristics, making optimization\ndifficult. A key challenge in optimizing multi-agent systems is acquiring\nsuitable training data for specialized agents. We introduce SiriuS, a\nself-improving, reasoning-driven optimization framework for multi-agent\nsystems. Central to our approach is the construction of an experience library:\na repository of high-quality reasoning trajectories. The library is built by\nretaining reasoning steps that lead to successful outcomes, providing a robust\ntraining set for optimizing multi-agent system. Additionally, we introduce a\nlibrary augmentation procedure that refines unsuccessful trajectories, further\nenriching the library. SiriuS boosts performance by 2.86\\% to 21.88\\% on\nreasoning and biomedical QA and enhances agent negotiation in competitive\nsettings. Our results show that SiriuS enhances multi-agent performance while\ngenerating reusable data for self-correction and self-play enhancement in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent AI systems powered by large language models (LLMs) are\nincreasingly applied to solve complex tasks. However, these systems often rely\non fragile, manually designed prompts and heuristics, making optimization\ndifficult. A key challenge in optimizing multi-agent systems is acquiring\nsuitable training data for specialized agents. We introduce SiriuS, a\nself-improving, reasoning-driven optimization framework for multi-agent\nsystems. Central to our approach is the construction of an experience library:\na repository of high-quality reasoning trajectories. The library is built by\nretaining reasoning steps that lead to successful outcomes, providing a robust\ntraining set for optimizing multi-agent system. Additionally, we introduce a\nlibrary augmentation procedure that refines unsuccessful trajectories, further\nenriching the library. SiriuS boosts performance by 2.86\\% to 21.88\\% on\nreasoning and biomedical QA and enhances agent negotiation in competitive\nsettings. Our results show that SiriuS enhances multi-agent performance while\ngenerating reusable data for self-correction and self-play enhancement in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Mert Yuksekgonul"
                    },
                    {
                        "name": "Shirley Wu"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12983v2",
                "updated": "2025-02-07T09:31:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    31,
                    9,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-22T16:12:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    12,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "LLM4WM: Adapting LLM for Wireless Multi-Tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4WM: Adapting LLM for Wireless Multi-Tasking"
                },
                "summary": "The wireless channel is fundamental to communication, encompassing numerous\ntasks collectively referred to as channel-associated tasks. These tasks can\nleverage joint learning based on channel characteristics to share\nrepresentations and enhance system design. To capitalize on this advantage,\nLLM4WM is proposed--a large language model (LLM) multi-task fine-tuning\nframework specifically tailored for channel-associated tasks. This framework\nutilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for\nmulti-task fine-tuning, enabling the transfer of the pre-trained LLM's general\nknowledge to these tasks. Given the unique characteristics of wireless channel\ndata, preprocessing modules, adapter modules, and multi-task output layers are\ndesigned to align the channel data with the LLM's semantic feature space.\nExperiments on a channel-associated multi-task dataset demonstrate that LLM4WM\noutperforms existing methodologies in both full-sample and few-shot\nevaluations, owing to its robust multi-task joint modeling and transfer\nlearning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wireless channel is fundamental to communication, encompassing numerous\ntasks collectively referred to as channel-associated tasks. These tasks can\nleverage joint learning based on channel characteristics to share\nrepresentations and enhance system design. To capitalize on this advantage,\nLLM4WM is proposed--a large language model (LLM) multi-task fine-tuning\nframework specifically tailored for channel-associated tasks. This framework\nutilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for\nmulti-task fine-tuning, enabling the transfer of the pre-trained LLM's general\nknowledge to these tasks. Given the unique characteristics of wireless channel\ndata, preprocessing modules, adapter modules, and multi-task output layers are\ndesigned to align the channel data with the LLM's semantic feature space.\nExperiments on a channel-associated multi-task dataset demonstrate that LLM4WM\noutperforms existing methodologies in both full-sample and few-shot\nevaluations, owing to its robust multi-task joint modeling and transfer\nlearning capabilities."
                },
                "authors": [
                    {
                        "name": "Xuanyu Liu"
                    },
                    {
                        "name": "Shijian Gao"
                    },
                    {
                        "name": "Boxun Liu"
                    },
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Liuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liuqing Yang"
                },
                "author": "Liuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04776v1",
                "updated": "2025-02-07T09:24:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    24,
                    13,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T09:24:13Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    24,
                    13,
                    4,
                    38,
                    0
                ],
                "title": "Constraining parity and Lorentz violations in gravity with future\n  ground- and space-based gravitational wave detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining parity and Lorentz violations in gravity with future\n  ground- and space-based gravitational wave detectors"
                },
                "summary": "The future ground- and space-based gravitational wave (GW) detectors offer\nunprecedented opportunities to test general relativity (GR) with greater\nprecision. In this work, we investigate the capability of future ground-based\nGW detectors, the Einstein Telescope (ET) and the Cosmic Explorer (CE), and\nspace-based GW detectors, LISA, Taiji, and TianQin, for constraining parity and\nLorentz violations in gravity. We inject several typical GW signals from\ncompact binary systems into GW detectors and perform Bayesian inferences with\nthe modified waveforms with parity and Lorentz-violating effects. These effects\nare modeled in the amplitude and phase corrections to the GW waveforms with\ntheir frequency-dependence described by factors $\\beta_{\\nu}$, $\\beta_{\\mu}$,\n$\\beta_{\\bar \\nu}$, and $\\beta_{\\bar \\mu}$. Our results show that the combined\nobservations of ET and CE will impose significantly tighter bounds on the\nenergy scale of parity and Lorentz violations ($M_{\\rm PV}$ and $M_{\\rm LV}$)\ncompared to those given by LIGO-Virgo-KAGRA (LVK) detectors. For cases with\npositive values of $\\beta_{\\nu}$, $\\beta_{\\mu}$, $\\beta_{\\bar \\nu}$, and\n$\\beta_{\\bar \\mu}$, the constraints on $M_{\\rm PV}$ and $M_{\\rm LV}$ from\nground-based detectors are tighter than those from the space-based detectors.\nFor the $\\beta_{\\mu} = -1$ case, space-based GW detectors provide constraints\non $M_{\\rm PV}$ that are better than current LVK observations and comparable to\nthose from ET and CE. Additionally, space-based detectors exhibit superior\nsensitivity in constraining $M_{\\rm LV}$ for $\\beta_{\\bar \\mu} = -2$ case,\nwhich is approximately three orders of magnitude tighter than those from\nground-based GW detectors. This scenario also enables bounds on the graviton\nmass at $m_g \\lesssim 10^{-35}\\; {\\rm GeV}$. These findings highlight the\npromising role of future GW observatories in probing fundamental physics beyond\nGR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The future ground- and space-based gravitational wave (GW) detectors offer\nunprecedented opportunities to test general relativity (GR) with greater\nprecision. In this work, we investigate the capability of future ground-based\nGW detectors, the Einstein Telescope (ET) and the Cosmic Explorer (CE), and\nspace-based GW detectors, LISA, Taiji, and TianQin, for constraining parity and\nLorentz violations in gravity. We inject several typical GW signals from\ncompact binary systems into GW detectors and perform Bayesian inferences with\nthe modified waveforms with parity and Lorentz-violating effects. These effects\nare modeled in the amplitude and phase corrections to the GW waveforms with\ntheir frequency-dependence described by factors $\\beta_{\\nu}$, $\\beta_{\\mu}$,\n$\\beta_{\\bar \\nu}$, and $\\beta_{\\bar \\mu}$. Our results show that the combined\nobservations of ET and CE will impose significantly tighter bounds on the\nenergy scale of parity and Lorentz violations ($M_{\\rm PV}$ and $M_{\\rm LV}$)\ncompared to those given by LIGO-Virgo-KAGRA (LVK) detectors. For cases with\npositive values of $\\beta_{\\nu}$, $\\beta_{\\mu}$, $\\beta_{\\bar \\nu}$, and\n$\\beta_{\\bar \\mu}$, the constraints on $M_{\\rm PV}$ and $M_{\\rm LV}$ from\nground-based detectors are tighter than those from the space-based detectors.\nFor the $\\beta_{\\mu} = -1$ case, space-based GW detectors provide constraints\non $M_{\\rm PV}$ that are better than current LVK observations and comparable to\nthose from ET and CE. Additionally, space-based detectors exhibit superior\nsensitivity in constraining $M_{\\rm LV}$ for $\\beta_{\\bar \\mu} = -2$ case,\nwhich is approximately three orders of magnitude tighter than those from\nground-based GW detectors. This scenario also enables bounds on the graviton\nmass at $m_g \\lesssim 10^{-35}\\; {\\rm GeV}$. These findings highlight the\npromising role of future GW observatories in probing fundamental physics beyond\nGR."
                },
                "authors": [
                    {
                        "name": "Bo-Yang Zhang"
                    },
                    {
                        "name": "Tao Zhu"
                    },
                    {
                        "name": "Jian-Ming Yan"
                    },
                    {
                        "name": "Jing-Fei Zhang"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04774v1",
                "updated": "2025-02-07T09:20:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    20,
                    11,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T09:20:11Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    20,
                    11,
                    4,
                    38,
                    0
                ],
                "title": "SeDi-Instruct: Enhancing Alignment of Language Models through\n  Self-Directed Instruction Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeDi-Instruct: Enhancing Alignment of Language Models through\n  Self-Directed Instruction Generation"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has enabled the industry\nto develop various AI-based services. Instruction tuning is considered\nessential in adapting foundation models for target domains to provide\nhigh-quality services to customers. A key challenge in instruction tuning is\nobtaining high-quality instruction data. Self-Instruct, which automatically\ngenerates instruction data using ChatGPT APIs, alleviates the data scarcity\nproblem. To improve the quality of instruction data, Self-Instruct discards\nmany of the instructions generated from ChatGPT, even though it is inefficient\nin terms of cost owing to many useless API calls. To generate high-quality\ninstruction data at a low cost, we propose a novel data generation framework,\nSelf-Direct Instruction generation (SeDi-Instruct), which employs\ndiversity-based filtering and iterative feedback task generation.\nDiversity-based filtering maintains model accuracy without excessively\ndiscarding low-quality generated instructions by enhancing the diversity of\ninstructions in a batch. This reduces the cost of synthesizing instruction\ndata. The iterative feedback task generation integrates instruction generation\nand training tasks and utilizes information obtained during the training to\ncreate high-quality instruction sets. Our results show that SeDi-Instruct\nenhances the accuracy of AI models by 5.2%, compared with traditional methods,\nwhile reducing data generation costs by 36%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has enabled the industry\nto develop various AI-based services. Instruction tuning is considered\nessential in adapting foundation models for target domains to provide\nhigh-quality services to customers. A key challenge in instruction tuning is\nobtaining high-quality instruction data. Self-Instruct, which automatically\ngenerates instruction data using ChatGPT APIs, alleviates the data scarcity\nproblem. To improve the quality of instruction data, Self-Instruct discards\nmany of the instructions generated from ChatGPT, even though it is inefficient\nin terms of cost owing to many useless API calls. To generate high-quality\ninstruction data at a low cost, we propose a novel data generation framework,\nSelf-Direct Instruction generation (SeDi-Instruct), which employs\ndiversity-based filtering and iterative feedback task generation.\nDiversity-based filtering maintains model accuracy without excessively\ndiscarding low-quality generated instructions by enhancing the diversity of\ninstructions in a batch. This reduces the cost of synthesizing instruction\ndata. The iterative feedback task generation integrates instruction generation\nand training tasks and utilizes information obtained during the training to\ncreate high-quality instruction sets. Our results show that SeDi-Instruct\nenhances the accuracy of AI models by 5.2%, compared with traditional methods,\nwhile reducing data generation costs by 36%."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12229v2",
                "updated": "2025-02-07T09:08:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    8,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-16T04:44:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    4,
                    44,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems"
                },
                "summary": "In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive world knowledge and remarkable reasoning capabilities of LLMs enable\nour method to supplement missing facts in KGs. Additionally, their powerful\ntext understanding abilities allow for better utilization of semantic\ninformation. Specifically, CoLaKG extracts useful information from the KG at\nboth local and global levels. By employing item-centered subgraph extraction\nand prompt engineering, it accurately captures the local KG. Subsequently,\nthrough retrieval-based neighbor enhancement, it supplements the current item\nby capturing related items from the entire KG, thereby effectively utilizing\nglobal information. The local and global information extracted by the LLM are\neffectively integrated into the recommendation model through a representation\nfusion module and a retrieval-augmented representation learning module,\nrespectively, thereby improving recommendation performance. Extensive\nexperiments on four real-world datasets demonstrate the superiority of our\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive world knowledge and remarkable reasoning capabilities of LLMs enable\nour method to supplement missing facts in KGs. Additionally, their powerful\ntext understanding abilities allow for better utilization of semantic\ninformation. Specifically, CoLaKG extracts useful information from the KG at\nboth local and global levels. By employing item-centered subgraph extraction\nand prompt engineering, it accurately captures the local KG. Subsequently,\nthrough retrieval-based neighbor enhancement, it supplements the current item\nby capturing related items from the entire KG, thereby effectively utilizing\nglobal information. The local and global information extracted by the LLM are\neffectively integrated into the recommendation model through a representation\nfusion module and a retrieval-augmented representation learning module,\nrespectively, thereby improving recommendation performance. Extensive\nexperiments on four real-world datasets demonstrate the superiority of our\nmethod."
                },
                "authors": [
                    {
                        "name": "Ziqiang Cui"
                    },
                    {
                        "name": "Yunpeng Weng"
                    },
                    {
                        "name": "Xing Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Dugang Liu"
                    },
                    {
                        "name": "Xiuqiang He"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10559v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10559v3",
                "updated": "2025-02-07T08:56:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    56,
                    56,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-14T14:38:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    38,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "Scale-dependence in $Λ$CDM parameters inferred from the CMB: a\n  possible sign of Early Dark Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-dependence in $Λ$CDM parameters inferred from the CMB: a\n  possible sign of Early Dark Energy"
                },
                "summary": "The early dark energy (EDE) model is one of the promising solutions to the\nHubble tension. One of the successes of the EDE model is that it can provide a\nsimilar fit to the $\\Lambda$CDM model for the CMB power spectrum. In this work,\nI analyze the phenomenology of the EDE and $\\Lambda$CDM parameters on the CMB\ntemperature power spectrum and notice that this cannot hold on all scales.\nThus, if the real cosmology is as described by the EDE model, the $\\Lambda$CDM\nparameters will be scale-dependent when fitting the CMB power spectrum with the\n$\\Lambda$CDM model, which can be hints for the EDE model. I examine CMB-S4-like\nobservations through mock data analysis and find that parameter shifts are\nnotable. As observations include smaller scales, I find lower $H_0$, $n_s$,\n$\\omega_b$ and higher $\\omega_m$, $A_s e^{-2\\tau}$, which will also constitute\nnew tensions with other observations. They can serve as a possible signal for\nthe EDE model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The early dark energy (EDE) model is one of the promising solutions to the\nHubble tension. One of the successes of the EDE model is that it can provide a\nsimilar fit to the $\\Lambda$CDM model for the CMB power spectrum. In this work,\nI analyze the phenomenology of the EDE and $\\Lambda$CDM parameters on the CMB\ntemperature power spectrum and notice that this cannot hold on all scales.\nThus, if the real cosmology is as described by the EDE model, the $\\Lambda$CDM\nparameters will be scale-dependent when fitting the CMB power spectrum with the\n$\\Lambda$CDM model, which can be hints for the EDE model. I examine CMB-S4-like\nobservations through mock data analysis and find that parameter shifts are\nnotable. As observations include smaller scales, I find lower $H_0$, $n_s$,\n$\\omega_b$ and higher $\\omega_m$, $A_s e^{-2\\tau}$, which will also constitute\nnew tensions with other observations. They can serve as a possible signal for\nthe EDE model."
                },
                "authors": [
                    {
                        "name": "Jun-Qian Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Qian Jiang"
                },
                "author": "Jun-Qian Jiang",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10559v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10559v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13412v2",
                "updated": "2025-02-07T08:55:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    55,
                    41,
                    4,
                    38,
                    0
                ],
                "published": "2024-08-24T00:28:59Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    0,
                    28,
                    59,
                    5,
                    237,
                    0
                ],
                "title": "Sommerfeld-Bethe analysis of ZT in inhomogeneous thermoelectrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sommerfeld-Bethe analysis of ZT in inhomogeneous thermoelectrics"
                },
                "summary": "The development of good thermoelectric materials exhibiting high $ZT$\n(=$\\frac{PF}{\\kappa} T$) requires maximizing power factor, $PF$, mainly\ngoverned by electrons, and minimizing thermal conductivity, $\\kappa$,\nassociated not only with electrons but also with phonons. In the present work,\nwe focus on the GeTe and Mg$_3$Sb$_2$ as high $ZT$ materials with inhomogeneous\nstructures and analyze both electrical conductivity, $L_{11}$, and Seebeck\ncoefficient, $S$, with help of Sommerfeld-Bethe formula, resulting in\nunderstanding the temperature dependence of $PF$ and the identification of\nelectrons contribution to thermal conductivity, $\\kappa_{\\rm el}$. Comparing\nthe obtained $\\kappa_{\\rm el}$ and experimentally measured $\\kappa$, the\ntemperature dependence of phonons contribution to thermal conductivity,\n$\\kappa_{\\rm ph}=\\kappa-\\kappa_{\\rm el}$, is inferred and analyzed based on the\nformula by Holland. Comparison of the GeTe and Mg$_3$Sb$_2$ with different\ntypes of crystal structures, i.e., GeTe being of a semiordered zigzag\nnanostructure like a disrupted herringbone structure while Mg$_3$Sb$_2$ of\nrather uniform amorphous structure, discloses that size effects on temperature\ndependence of $\\kappa_{\\rm ph}$ is large in the former, while very small in the\nlatter. Hence, it is concluded that not only the size of the grain but also its\nshape has an important influence on $\\kappa_{\\rm ph}$ and then $ZT$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of good thermoelectric materials exhibiting high $ZT$\n(=$\\frac{PF}{\\kappa} T$) requires maximizing power factor, $PF$, mainly\ngoverned by electrons, and minimizing thermal conductivity, $\\kappa$,\nassociated not only with electrons but also with phonons. In the present work,\nwe focus on the GeTe and Mg$_3$Sb$_2$ as high $ZT$ materials with inhomogeneous\nstructures and analyze both electrical conductivity, $L_{11}$, and Seebeck\ncoefficient, $S$, with help of Sommerfeld-Bethe formula, resulting in\nunderstanding the temperature dependence of $PF$ and the identification of\nelectrons contribution to thermal conductivity, $\\kappa_{\\rm el}$. Comparing\nthe obtained $\\kappa_{\\rm el}$ and experimentally measured $\\kappa$, the\ntemperature dependence of phonons contribution to thermal conductivity,\n$\\kappa_{\\rm ph}=\\kappa-\\kappa_{\\rm el}$, is inferred and analyzed based on the\nformula by Holland. Comparison of the GeTe and Mg$_3$Sb$_2$ with different\ntypes of crystal structures, i.e., GeTe being of a semiordered zigzag\nnanostructure like a disrupted herringbone structure while Mg$_3$Sb$_2$ of\nrather uniform amorphous structure, discloses that size effects on temperature\ndependence of $\\kappa_{\\rm ph}$ is large in the former, while very small in the\nlatter. Hence, it is concluded that not only the size of the grain but also its\nshape has an important influence on $\\kappa_{\\rm ph}$ and then $ZT$."
                },
                "authors": [
                    {
                        "name": "Manaho Matsubara"
                    },
                    {
                        "name": "Takahiro Yamamoto"
                    },
                    {
                        "name": "Hidetoshi Fukuyama"
                    }
                ],
                "author_detail": {
                    "name": "Hidetoshi Fukuyama"
                },
                "author": "Hidetoshi Fukuyama",
                "arxiv_doi": "10.7566/JPSJ.94.024601",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.7566/JPSJ.94.024601",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.13412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "J. Phys. Soc. Jpn. 94, 024601 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00005v3",
                "updated": "2025-02-07T08:49:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    49,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-16T11:57:14Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    57,
                    14,
                    2,
                    290,
                    0
                ],
                "title": "Mastering the Craft of Data Synthesis for CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mastering the Craft of Data Synthesis for CodeLLMs"
                },
                "summary": "Large language models (LLMs) have shown impressive performance in \\emph{code}\nunderstanding and generation, making coding tasks a key focus for researchers\ndue to their practical applications and value as a testbed for LLM evaluation.\nData synthesis and filtering techniques have been widely adopted and shown to\nbe highly effective in this context. In this paper, we present a focused survey\nand taxonomy of these techniques, emphasizing recent advancements. We highlight\nkey challenges, explore future research directions, and offer practical\nguidance for new researchers entering the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance in \\emph{code}\nunderstanding and generation, making coding tasks a key focus for researchers\ndue to their practical applications and value as a testbed for LLM evaluation.\nData synthesis and filtering techniques have been widely adopted and shown to\nbe highly effective in this context. In this paper, we present a focused survey\nand taxonomy of these techniques, emphasizing recent advancements. We highlight\nkey challenges, explore future research directions, and offer practical\nguidance for new researchers entering the field."
                },
                "authors": [
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Philip Arthur"
                    },
                    {
                        "name": "Qianyu Feng"
                    },
                    {
                        "name": "Cong Duy Vu Hoang"
                    },
                    {
                        "name": "Yu-Heng Hong"
                    },
                    {
                        "name": "Mahdi Kazemi Moghaddam"
                    },
                    {
                        "name": "Omid Nezami"
                    },
                    {
                        "name": "Thien Nguyen"
                    },
                    {
                        "name": "Gioacchino Tangari"
                    },
                    {
                        "name": "Duy Vu"
                    },
                    {
                        "name": "Thanh Vu"
                    },
                    {
                        "name": "Mark Johnson"
                    },
                    {
                        "name": "Krishnaram Kenthapadi"
                    },
                    {
                        "name": "Don Dharmasiri"
                    },
                    {
                        "name": "Long Duong"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Fang Li"
                },
                "author": "Yuan-Fang Li",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04759v1",
                "updated": "2025-02-07T08:45:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    45,
                    50,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:45:50Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    45,
                    50,
                    4,
                    38,
                    0
                ],
                "title": "Enhancing Phishing Email Identification with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Phishing Email Identification with Large Language Models"
                },
                "summary": "Phishing has long been a common tactic used by cybercriminals and continues\nto pose a significant threat in today's digital world. When phishing attacks\nbecome more advanced and sophisticated, there is an increasing need for\neffective methods to detect and prevent them. To address the challenging\nproblem of detecting phishing emails, researchers have developed numerous\nsolutions, in particular those based on machine learning (ML) algorithms. In\nthis work, we take steps to study the efficacy of large language models (LLMs)\nin detecting phishing emails. The experiments show that the LLM achieves a high\naccuracy rate at high precision; importantly, it also provides interpretable\nevidence for the decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing has long been a common tactic used by cybercriminals and continues\nto pose a significant threat in today's digital world. When phishing attacks\nbecome more advanced and sophisticated, there is an increasing need for\neffective methods to detect and prevent them. To address the challenging\nproblem of detecting phishing emails, researchers have developed numerous\nsolutions, in particular those based on machine learning (ML) algorithms. In\nthis work, we take steps to study the efficacy of large language models (LLMs)\nin detecting phishing emails. The experiments show that the LLM achieves a high\naccuracy rate at high precision; importantly, it also provides interpretable\nevidence for the decisions."
                },
                "authors": [
                    {
                        "name": "Catherine Lee"
                    }
                ],
                "author_detail": {
                    "name": "Catherine Lee"
                },
                "author": "Catherine Lee",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04756v1",
                "updated": "2025-02-07T08:42:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    42,
                    34,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:42:34Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    42,
                    34,
                    4,
                    38,
                    0
                ],
                "title": "Concept Navigation and Classification via Open Source Large Language\n  Model Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Navigation and Classification via Open Source Large Language\n  Model Processing"
                },
                "summary": "This paper presents a novel methodological framework for detecting and\nclassifying latent constructs, including frames, narratives, and topics, from\ntextual data using Open-Source Large Language Models (LLMs). The proposed\nhybrid approach combines automated summarization with human-in-the-loop\nvalidation to enhance the accuracy and interpretability of construct\nidentification. By employing iterative sampling coupled with expert refinement,\nthe framework guarantees methodological robustness and ensures conceptual\nprecision. Applied to diverse data sets, including AI policy debates, newspaper\narticles on encryption, and the 20 Newsgroups data set, this approach\ndemonstrates its versatility in systematically analyzing complex political\ndiscourses, media framing, and topic classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel methodological framework for detecting and\nclassifying latent constructs, including frames, narratives, and topics, from\ntextual data using Open-Source Large Language Models (LLMs). The proposed\nhybrid approach combines automated summarization with human-in-the-loop\nvalidation to enhance the accuracy and interpretability of construct\nidentification. By employing iterative sampling coupled with expert refinement,\nthe framework guarantees methodological robustness and ensures conceptual\nprecision. Applied to diverse data sets, including AI policy debates, newspaper\narticles on encryption, and the 20 Newsgroups data set, this approach\ndemonstrates its versatility in systematically analyzing complex political\ndiscourses, media framing, and topic classification tasks."
                },
                "authors": [
                    {
                        "name": "Maël Kubli"
                    }
                ],
                "author_detail": {
                    "name": "Maël Kubli"
                },
                "author": "Maël Kubli",
                "arxiv_comment": "35 pages, 1 figure, 7 tabels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14059v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14059v3",
                "updated": "2025-02-07T08:37:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    37,
                    3,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-17T22:03:52Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    22,
                    3,
                    52,
                    3,
                    291,
                    0
                ],
                "title": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language\n  Models"
                },
                "summary": "This paper introduces the UCFE: User-Centric Financial Expertise benchmark,\nan innovative framework designed to evaluate the ability of large language\nmodels (LLMs) to handle complex real-world financial tasks. UCFE benchmark\nadopts a hybrid approach that combines human expert evaluations with dynamic,\ntask-specific interactions to simulate the complexities of evolving financial\nscenarios. Firstly, we conducted a user study involving 804 participants,\ncollecting their feedback on financial tasks. Secondly, based on this feedback,\nwe created our dataset that encompasses a wide range of user intents and\ninteractions. This dataset serves as the foundation for benchmarking 11 LLMs\nservices using the LLM-as-Judge methodology. Our results show a significant\nalignment between benchmark scores and human preferences, with a Pearson\ncorrelation coefficient of 0.78, confirming the effectiveness of the UCFE\ndataset and our evaluation approach. UCFE benchmark not only reveals the\npotential of LLMs in the financial domain but also provides a robust framework\nfor assessing their performance and user satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the UCFE: User-Centric Financial Expertise benchmark,\nan innovative framework designed to evaluate the ability of large language\nmodels (LLMs) to handle complex real-world financial tasks. UCFE benchmark\nadopts a hybrid approach that combines human expert evaluations with dynamic,\ntask-specific interactions to simulate the complexities of evolving financial\nscenarios. Firstly, we conducted a user study involving 804 participants,\ncollecting their feedback on financial tasks. Secondly, based on this feedback,\nwe created our dataset that encompasses a wide range of user intents and\ninteractions. This dataset serves as the foundation for benchmarking 11 LLMs\nservices using the LLM-as-Judge methodology. Our results show a significant\nalignment between benchmark scores and human preferences, with a Pearson\ncorrelation coefficient of 0.78, confirming the effectiveness of the UCFE\ndataset and our evaluation approach. UCFE benchmark not only reveals the\npotential of LLMs in the financial domain but also provides a robust framework\nfor assessing their performance and user satisfaction."
                },
                "authors": [
                    {
                        "name": "Yuzhe Yang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Yilin Guo"
                    },
                    {
                        "name": "Ruoli Gan"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Mingcong Lei"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Haining Wang"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Honghai Yu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14059v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14059v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04751v1",
                "updated": "2025-02-07T08:36:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    36,
                    39,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:36:39Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    36,
                    39,
                    4,
                    38,
                    0
                ],
                "title": "Holistically Guided Monte Carlo Tree Search for Intricate Information\n  Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistically Guided Monte Carlo Tree Search for Intricate Information\n  Seeking"
                },
                "summary": "In the era of vast digital information, the sheer volume and heterogeneity of\navailable information present significant challenges for intricate information\nseeking. Users frequently face multistep web search tasks that involve\nnavigating vast and varied data sources. This complexity demands every step\nremains comprehensive, accurate, and relevant. However, traditional search\nmethods often struggle to balance the need for localized precision with the\nbroader context required for holistic understanding, leaving critical facets of\nintricate queries underexplored. In this paper, we introduce an LLM-based\nsearch assistant that adopts a new information seeking paradigm with\nholistically guided Monte Carlo tree search (HG-MCTS). We reformulate the task\nas a progressive information collection process with a knowledge memory and\nunite an adaptive checklist with multi-perspective reward modeling in MCTS. The\nadaptive checklist provides explicit sub-goals to guide the MCTS process toward\ncomprehensive coverage of complex user queries. Simultaneously, our\nmulti-perspective reward modeling offers both exploration and retrieval\nrewards, along with progress feedback that tracks completed and remaining\nsub-goals, refining the checklist as the tree search progresses. By striking a\nbalance between localized tree expansion and global guidance, HG-MCTS reduces\nredundancy in search paths and ensures that all crucial aspects of an intricate\nquery are properly addressed. Extensive experiments on real-world intricate\ninformation seeking tasks demonstrate that HG-MCTS acquires thorough knowledge\ncollections and delivers more accurate final responses compared with existing\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of vast digital information, the sheer volume and heterogeneity of\navailable information present significant challenges for intricate information\nseeking. Users frequently face multistep web search tasks that involve\nnavigating vast and varied data sources. This complexity demands every step\nremains comprehensive, accurate, and relevant. However, traditional search\nmethods often struggle to balance the need for localized precision with the\nbroader context required for holistic understanding, leaving critical facets of\nintricate queries underexplored. In this paper, we introduce an LLM-based\nsearch assistant that adopts a new information seeking paradigm with\nholistically guided Monte Carlo tree search (HG-MCTS). We reformulate the task\nas a progressive information collection process with a knowledge memory and\nunite an adaptive checklist with multi-perspective reward modeling in MCTS. The\nadaptive checklist provides explicit sub-goals to guide the MCTS process toward\ncomprehensive coverage of complex user queries. Simultaneously, our\nmulti-perspective reward modeling offers both exploration and retrieval\nrewards, along with progress feedback that tracks completed and remaining\nsub-goals, refining the checklist as the tree search progresses. By striking a\nbalance between localized tree expansion and global guidance, HG-MCTS reduces\nredundancy in search paths and ensures that all crucial aspects of an intricate\nquery are properly addressed. Extensive experiments on real-world intricate\ninformation seeking tasks demonstrate that HG-MCTS acquires thorough knowledge\ncollections and delivers more accurate final responses compared with existing\nbaselines."
                },
                "authors": [
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02959v2",
                "updated": "2025-02-07T08:32:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    32,
                    24,
                    4,
                    38,
                    0
                ],
                "published": "2024-11-05T09:58:36Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    58,
                    36,
                    1,
                    310,
                    0
                ],
                "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial RAG\nsystems have used Web search engines as their major retrieval systems.\nTypically, such RAG systems retrieve search results, download HTML sources of\nthe results, and then extract plain texts from the HTML sources. Plain text\ndocuments or chunks are fed into the LLMs to augment the generation. However,\nmuch of the structural and semantic information inherent in HTML, such as\nheadings and table structures, is lost during this plain-text-based RAG\nprocess. To alleviate this problem, we propose HtmlRAG, which uses HTML instead\nof plain text as the format of retrieved knowledge in RAG. We believe HTML is\nbetter than plain text in modeling knowledge in external documents, and most\nLLMs possess robust capacities to understand HTML. However, utilizing HTML\npresents new challenges. HTML contains additional content such as tags,\nJavaScript, and CSS specifications, which bring extra input tokens and noise to\nthe RAG system. To address this issue, we propose HTML cleaning, compression,\nand a two-step block-tree-based pruning strategy, to shorten the HTML while\nminimizing the loss of information. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial RAG\nsystems have used Web search engines as their major retrieval systems.\nTypically, such RAG systems retrieve search results, download HTML sources of\nthe results, and then extract plain texts from the HTML sources. Plain text\ndocuments or chunks are fed into the LLMs to augment the generation. However,\nmuch of the structural and semantic information inherent in HTML, such as\nheadings and table structures, is lost during this plain-text-based RAG\nprocess. To alleviate this problem, we propose HtmlRAG, which uses HTML instead\nof plain text as the format of retrieved knowledge in RAG. We believe HTML is\nbetter than plain text in modeling knowledge in external documents, and most\nLLMs possess robust capacities to understand HTML. However, utilizing HTML\npresents new challenges. HTML contains additional content such as tags,\nJavaScript, and CSS specifications, which bring extra input tokens and noise to\nthe RAG system. To address this issue, we propose HTML cleaning, compression,\nand a two-step block-tree-based pruning strategy, to shorten the HTML while\nminimizing the loss of information. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems."
                },
                "authors": [
                    {
                        "name": "Jiejun Tan"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_doi": "10.1145/3696410.3714546",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714546",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 main conference. Repo:\n  https://github.com/plageon/HtmlRAG",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04747v1",
                "updated": "2025-02-07T08:29:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    29,
                    9,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:29:09Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    29,
                    9,
                    4,
                    38,
                    0
                ],
                "title": "Every Software as an Agent: Blueprint and Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every Software as an Agent: Blueprint and Case Study"
                },
                "summary": "The rise of (multimodal) large language models (LLMs) has shed light on\nsoftware agent -- where software can understand and follow user instructions in\nnatural language. However, existing approaches such as API-based and GUI-based\nagents are far from satisfactory at accuracy and efficiency aspects. Instead,\nwe advocate to endow LLMs with access to the software internals (source code\nand runtime context) and the permission to dynamically inject generated code\ninto software for execution. In such a whitebox setting, one may better\nleverage the software context and the coding ability of LLMs. We then present\nan overall design architecture and case studies on two popular web-based\ndesktop applications. We also give in-depth discussion of the challenges and\nfuture directions. We deem that such a new paradigm has the potential to\nfundamentally overturn the existing software agent design, and finally creating\na digital world in which software can comprehend, operate, collaborate, and\neven think to meet complex user needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of (multimodal) large language models (LLMs) has shed light on\nsoftware agent -- where software can understand and follow user instructions in\nnatural language. However, existing approaches such as API-based and GUI-based\nagents are far from satisfactory at accuracy and efficiency aspects. Instead,\nwe advocate to endow LLMs with access to the software internals (source code\nand runtime context) and the permission to dynamically inject generated code\ninto software for execution. In such a whitebox setting, one may better\nleverage the software context and the coding ability of LLMs. We then present\nan overall design architecture and case studies on two popular web-based\ndesktop applications. We also give in-depth discussion of the challenges and\nfuture directions. We deem that such a new paradigm has the potential to\nfundamentally overturn the existing software agent design, and finally creating\na digital world in which software can comprehend, operate, collaborate, and\neven think to meet complex user needs."
                },
                "authors": [
                    {
                        "name": "Mengwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengwei Xu"
                },
                "author": "Mengwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06589v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06589v4",
                "updated": "2025-02-07T08:23:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    23,
                    57,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T17:06:30Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    17,
                    6,
                    30,
                    5,
                    11,
                    0
                ],
                "title": "Ladder-residual: parallelism-aware architecture for accelerating large\n  model inference with communication overlapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ladder-residual: parallelism-aware architecture for accelerating large\n  model inference with communication overlapping"
                },
                "summary": "Large language model inference is both memory-intensive and time-consuming,\noften requiring distributed algorithms to efficiently scale. Various model\nparallelism strategies are used in multi-gpu training and inference to\npartition computation across multiple devices, reducing memory load and\ncomputation time. However, using model parallelism necessitates communication\nof information between GPUs, which has been a major bottleneck and limits the\ngains obtained by scaling up the number of devices. We introduce Ladder\nResidual, a simple architectural modification applicable to all residual-based\nmodels that enables straightforward overlapping that effectively hides the\nlatency of communication. Our insight is that in addition to systems\noptimization, one can also redesign the model architecture to decouple\ncommunication from computation. While Ladder Residual can allow\ncommunication-computation decoupling in conventional parallelism patterns, we\nfocus on Tensor Parallelism in this paper, which is particularly bottlenecked\nby its heavy communication. For a Transformer model with 70B parameters,\napplying Ladder Residual to all its layers can achieve 29% end-to-end wall\nclock speed up at inference time with TP sharding over 8 devices. We refer the\nresulting Transformer model as the Ladder Transformer. We train a 1B and 3B\nLadder Transformer from scratch and observe comparable performance to a\nstandard dense transformer baseline. We also show that it is possible to\nconvert parts of the Llama-3.1 8B model to our Ladder Residual architecture\nwith minimal accuracy degradation by only retraining for 3B tokens. We release\nour code for training and inference for easier replication of experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model inference is both memory-intensive and time-consuming,\noften requiring distributed algorithms to efficiently scale. Various model\nparallelism strategies are used in multi-gpu training and inference to\npartition computation across multiple devices, reducing memory load and\ncomputation time. However, using model parallelism necessitates communication\nof information between GPUs, which has been a major bottleneck and limits the\ngains obtained by scaling up the number of devices. We introduce Ladder\nResidual, a simple architectural modification applicable to all residual-based\nmodels that enables straightforward overlapping that effectively hides the\nlatency of communication. Our insight is that in addition to systems\noptimization, one can also redesign the model architecture to decouple\ncommunication from computation. While Ladder Residual can allow\ncommunication-computation decoupling in conventional parallelism patterns, we\nfocus on Tensor Parallelism in this paper, which is particularly bottlenecked\nby its heavy communication. For a Transformer model with 70B parameters,\napplying Ladder Residual to all its layers can achieve 29% end-to-end wall\nclock speed up at inference time with TP sharding over 8 devices. We refer the\nresulting Transformer model as the Ladder Transformer. We train a 1B and 3B\nLadder Transformer from scratch and observe comparable performance to a\nstandard dense transformer baseline. We also show that it is possible to\nconvert parts of the Llama-3.1 8B model to our Ladder Residual architecture\nwith minimal accuracy degradation by only retraining for 3B tokens. We release\nour code for training and inference for easier replication of experiments."
                },
                "authors": [
                    {
                        "name": "Muru Zhang"
                    },
                    {
                        "name": "Mayank Mishra"
                    },
                    {
                        "name": "Zhongzhu Zhou"
                    },
                    {
                        "name": "William Brandon"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "Jonathan Ragan-Kelley"
                    },
                    {
                        "name": "Shuaiwen Leon Song"
                    },
                    {
                        "name": "Ben Athiwaratkun"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06589v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06589v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12812v2",
                "updated": "2025-02-07T08:18:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    18,
                    0,
                    4,
                    38,
                    0
                ],
                "published": "2024-08-23T03:16:26Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    16,
                    26,
                    4,
                    236,
                    0
                ],
                "title": "Grounding Fallacies Misrepresenting Scientific Publications in Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Fallacies Misrepresenting Scientific Publications in Evidence"
                },
                "summary": "Health-related misinformation claims often falsely cite a credible biomedical\npublication as evidence. These publications only superficially seem to support\nthe false claim, when logical fallacies are applied. In this work, we aim to\ndetect and to highlight such fallacies, which requires assessing the exact\ncontent of the misrepresented publications. To achieve this, we introduce\nMissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus\nextends Missci by grounding the applied fallacies in real-world passages from\nmisrepresented studies. This creates a realistic test-bed for detecting and\nverbalizing fallacies under real-world input conditions, and enables new and\nrealistic passage-retrieval tasks. MissciPlus is the first logical fallacy\ndataset which pairs the real-world misrepresented evidence with incorrect\nclaims, identical to the input to evidence-based fact-checking models. With\nMissciPlus, we i) benchmark retrieval models in identifying passages that\nsupport claims only with fallacious reasoning, ii) evaluate how well LLMs\nverbalize fallacious reasoning based on misrepresented scientific passages, and\niii) assess the effectiveness of fact-checking models in refuting claims that\nmisrepresent biomedical research. Our findings show that current fact-checking\nmodels struggle to use misrepresented scientific passages to refute\nmisinformation. Moreover, these passages can mislead LLMs into accepting false\nclaims as true.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-related misinformation claims often falsely cite a credible biomedical\npublication as evidence. These publications only superficially seem to support\nthe false claim, when logical fallacies are applied. In this work, we aim to\ndetect and to highlight such fallacies, which requires assessing the exact\ncontent of the misrepresented publications. To achieve this, we introduce\nMissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus\nextends Missci by grounding the applied fallacies in real-world passages from\nmisrepresented studies. This creates a realistic test-bed for detecting and\nverbalizing fallacies under real-world input conditions, and enables new and\nrealistic passage-retrieval tasks. MissciPlus is the first logical fallacy\ndataset which pairs the real-world misrepresented evidence with incorrect\nclaims, identical to the input to evidence-based fact-checking models. With\nMissciPlus, we i) benchmark retrieval models in identifying passages that\nsupport claims only with fallacious reasoning, ii) evaluate how well LLMs\nverbalize fallacious reasoning based on misrepresented scientific passages, and\niii) assess the effectiveness of fact-checking models in refuting claims that\nmisrepresent biomedical research. Our findings show that current fact-checking\nmodels struggle to use misrepresented scientific passages to refute\nmisinformation. Moreover, these passages can mislead LLMs into accepting false\nclaims as true."
                },
                "authors": [
                    {
                        "name": "Max Glockner"
                    },
                    {
                        "name": "Yufang Hou"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15639v3",
                "updated": "2025-02-07T08:06:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    6,
                    50,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-21T04:57:09Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    4,
                    57,
                    9,
                    0,
                    295,
                    0
                ],
                "title": "Can Large Language Models Invent Algorithms to Improve Themselves?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Invent Algorithms to Improve Themselves?"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance improvements\nand are rapidly gaining adoption in industry. However, the methods for\nimproving LLMs are still designed by humans, which restricts the invention of\nnew model-improving algorithms to human expertise and imagination. To address\nthis, we propose the Self-Developing framework, which enables LLMs to\nautonomously generate and learn model-improvement algorithms. In this\nframework, the seed model generates, applies, and learns model-improving\nalgorithms, continuously improving both the seed model and the algorithms\nthemselves. Among model-improving strategies, we focus on model merging\nalgorithms. In mathematical reasoning tasks, Self-Developing discovers novel\nmerging strategies and outperforms human-designed methods. On GSM8k, the\ndiscovered algorithms improve the seed model by 6% and surpass human-designed\nmethods by 4.3%. Moreover, they exhibit strong transferability, achieving a\n7.4% performance gain on out-of-domain models. These results suggest that LLMs\ncan autonomously develop effective model-improvement techniques beyond human\nintuition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance improvements\nand are rapidly gaining adoption in industry. However, the methods for\nimproving LLMs are still designed by humans, which restricts the invention of\nnew model-improving algorithms to human expertise and imagination. To address\nthis, we propose the Self-Developing framework, which enables LLMs to\nautonomously generate and learn model-improvement algorithms. In this\nframework, the seed model generates, applies, and learns model-improving\nalgorithms, continuously improving both the seed model and the algorithms\nthemselves. Among model-improving strategies, we focus on model merging\nalgorithms. In mathematical reasoning tasks, Self-Developing discovers novel\nmerging strategies and outperforms human-designed methods. On GSM8k, the\ndiscovered algorithms improve the seed model by 6% and surpass human-designed\nmethods by 4.3%. Moreover, they exhibit strong transferability, achieving a\n7.4% performance gain on out-of-domain models. These results suggest that LLMs\ncan autonomously develop effective model-improvement techniques beyond human\nintuition."
                },
                "authors": [
                    {
                        "name": "Yoichi Ishibashi"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "arxiv_comment": "Accepted at NAACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14095v2",
                "updated": "2025-02-07T08:03:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    3,
                    50,
                    4,
                    38,
                    0
                ],
                "published": "2024-07-19T07:59:04Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    7,
                    59,
                    4,
                    4,
                    201,
                    0
                ],
                "title": "People use fast, goal-directed simulation to reason about novel games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People use fast, goal-directed simulation to reason about novel games"
                },
                "summary": "People can evaluate features of problems and their potential solutions well\nbefore we can effectively solve them. When considering a game we have never\nplayed, for instance, we might infer whether it is likely to be challenging,\nfair, or fun simply from hearing the game rules, prior to deciding whether to\ninvest time in learning the game or trying to play it well. Many studies of\ngame play have focused on optimality and expertise, characterizing how people\nand computational models play based on moderate to extensive search and after\nplaying a game dozens (if not thousands or millions) of times. Here, we study\nhow people reason about a range of simple but novel Connect-N style board\ngames. We ask people to judge how fair and how fun the games are from very\nlittle experience: just thinking about the game for a minute or so, before they\nhave ever actually played with anyone else, and we propose a resource-limited\nmodel that captures their judgments using only a small number of partial game\nsimulations and almost no look-ahead search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People can evaluate features of problems and their potential solutions well\nbefore we can effectively solve them. When considering a game we have never\nplayed, for instance, we might infer whether it is likely to be challenging,\nfair, or fun simply from hearing the game rules, prior to deciding whether to\ninvest time in learning the game or trying to play it well. Many studies of\ngame play have focused on optimality and expertise, characterizing how people\nand computational models play based on moderate to extensive search and after\nplaying a game dozens (if not thousands or millions) of times. Here, we study\nhow people reason about a range of simple but novel Connect-N style board\ngames. We ask people to judge how fair and how fun the games are from very\nlittle experience: just thinking about the game for a minute or so, before they\nhave ever actually played with anyone else, and we propose a resource-limited\nmodel that captures their judgments using only a small number of partial game\nsimulations and almost no look-ahead search."
                },
                "authors": [
                    {
                        "name": "Cedegao E. Zhang"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Lionel Wong"
                    },
                    {
                        "name": "Mauricio Barba"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Joshua B. Tenenbaum"
                },
                "author": "Joshua B. Tenenbaum",
                "arxiv_comment": "Accepted at CogSci 2024 as a talk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04730v1",
                "updated": "2025-02-07T07:58:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    58,
                    47,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T07:58:47Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    58,
                    47,
                    4,
                    38,
                    0
                ],
                "title": "PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational\n  Autoencoders"
                },
                "summary": "Learning informative representations of phylogenetic tree structures is\nessential for analyzing evolutionary relationships. Classical distance-based\nmethods have been widely used to project phylogenetic trees into Euclidean\nspace, but they are often sensitive to the choice of distance metric and may\nlack sufficient resolution. In this paper, we introduce phylogenetic\nvariational autoencoders (PhyloVAEs), an unsupervised learning framework\ndesigned for representation learning and generative modeling of tree\ntopologies. Leveraging an efficient encoding mechanism inspired by\nautoregressive tree topology generation, we develop a deep latent-variable\ngenerative model that facilitates fast, parallelized topology generation.\nPhyloVAE combines this generative model with a collaborative inference model\nbased on learnable topological features, allowing for high-resolution\nrepresentations of phylogenetic tree samples. Extensive experiments demonstrate\nPhyloVAE's robust representation learning capabilities and fast generation of\nphylogenetic tree topologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning informative representations of phylogenetic tree structures is\nessential for analyzing evolutionary relationships. Classical distance-based\nmethods have been widely used to project phylogenetic trees into Euclidean\nspace, but they are often sensitive to the choice of distance metric and may\nlack sufficient resolution. In this paper, we introduce phylogenetic\nvariational autoencoders (PhyloVAEs), an unsupervised learning framework\ndesigned for representation learning and generative modeling of tree\ntopologies. Leveraging an efficient encoding mechanism inspired by\nautoregressive tree topology generation, we develop a deep latent-variable\ngenerative model that facilitates fast, parallelized topology generation.\nPhyloVAE combines this generative model with a collaborative inference model\nbased on learnable topological features, allowing for high-resolution\nrepresentations of phylogenetic tree samples. Extensive experiments demonstrate\nPhyloVAE's robust representation learning capabilities and fast generation of\nphylogenetic tree topologies."
                },
                "authors": [
                    {
                        "name": "Tianyu Xie"
                    },
                    {
                        "name": "Harry Richman"
                    },
                    {
                        "name": "Jiansi Gao"
                    },
                    {
                        "name": "Frederick A. Matsen IV"
                    },
                    {
                        "name": "Cheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Zhang"
                },
                "author": "Cheng Zhang",
                "arxiv_comment": "ICLR 2025. 22 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04728v1",
                "updated": "2025-02-07T07:52:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    52,
                    25,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T07:52:25Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    52,
                    25,
                    4,
                    38,
                    0
                ],
                "title": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models"
                },
                "summary": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domain, achieving over 50%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domain, achieving over 50%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks."
                },
                "authors": [
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Yuhuan Yuan"
                    },
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Fuxiang Frank Xia"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ge Lin"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Technical Report v1 (32 pages, 6 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04718v1",
                "updated": "2025-02-07T07:39:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    39,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T07:39:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    39,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Evaluating Text Style Transfer Evaluation: Are There Any Reliable\n  Metrics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Text Style Transfer Evaluation: Are There Any Reliable\n  Metrics?"
                },
                "summary": "Text Style Transfer (TST) is the task of transforming a text to reflect a\nparticular style while preserving its original content. Evaluating TST outputs\nis a multidimensional challenge, requiring the assessment of style transfer\naccuracy, content preservation, and naturalness. Using human evaluation is\nideal but costly, same as in other natural language processing (NLP) tasks,\nhowever, automatic metrics for TST have not received as much attention as\nmetrics for, e.g., machine translation or summarization. In this paper, we\nexamine both set of existing and novel metrics from broader NLP tasks for TST\nevaluation, focusing on two popular subtasks-sentiment transfer and\ndetoxification-in a multilingual context comprising English, Hindi, and\nBengali. By conducting meta-evaluation through correlation with human\njudgments, we demonstrate the effectiveness of these metrics when used\nindividually and in ensembles. Additionally, we investigate the potential of\nLarge Language Models (LLMs) as tools for TST evaluation. Our findings\nhighlight that certain advanced NLP metrics and experimental-hybrid-techniques,\nprovide better insights than existing TST metrics for delivering more accurate,\nconsistent, and reproducible TST evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Style Transfer (TST) is the task of transforming a text to reflect a\nparticular style while preserving its original content. Evaluating TST outputs\nis a multidimensional challenge, requiring the assessment of style transfer\naccuracy, content preservation, and naturalness. Using human evaluation is\nideal but costly, same as in other natural language processing (NLP) tasks,\nhowever, automatic metrics for TST have not received as much attention as\nmetrics for, e.g., machine translation or summarization. In this paper, we\nexamine both set of existing and novel metrics from broader NLP tasks for TST\nevaluation, focusing on two popular subtasks-sentiment transfer and\ndetoxification-in a multilingual context comprising English, Hindi, and\nBengali. By conducting meta-evaluation through correlation with human\njudgments, we demonstrate the effectiveness of these metrics when used\nindividually and in ensembles. Additionally, we investigate the potential of\nLarge Language Models (LLMs) as tools for TST evaluation. Our findings\nhighlight that certain advanced NLP metrics and experimental-hybrid-techniques,\nprovide better insights than existing TST metrics for delivering more accurate,\nconsistent, and reproducible TST evaluations."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Atul Kr. Ojha"
                    },
                    {
                        "name": "John P. McCrae"
                    },
                    {
                        "name": "Ondrej Dusek"
                    }
                ],
                "author_detail": {
                    "name": "Ondrej Dusek"
                },
                "author": "Ondrej Dusek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19477v2",
                "updated": "2025-02-07T07:08:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    8,
                    29,
                    4,
                    38,
                    0
                ],
                "published": "2024-11-29T05:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models"
                },
                "summary": "We propose two simple yet principled algorithms that enjoy provable scaling\nlaws for the test-time compute of large language models (LLMs), which require a\nblack-box LLM and nothing else (e.g., no external verifier or reward model) for\na minimalistic implementation. (i) The first one is a two-stage knockout-style\nalgorithm: given an input problem, it first generates multiple candidate\nsolutions, and then aggregate them for a final output, via a knockout\ntournament where pairwise comparisons among the candidates are conducted.\nAssuming that the LLM can generate a correct solution with non-zero probability\nand do better than a random guess in comparing a pair of correct and incorrect\nsolutions, we prove theoretically that the failure probability of this\nalgorithm decays to zero exponentially or by a power law (depending on the\nspecific way of scaling) as its test-time compute grows. (ii) The second one is\na two-stage league-style algorithm, where each candidate solution is evaluated\nby its average win rate against multiple opponents, rather than eliminated upon\nloss to a single opponent. Under certain technical assumptions that are\nanalogous to but more robust than those required by the knockout-style\nalgorithm, we prove theoretically that the failure probability of the\nleague-style algorithm also decays to zero exponentially as its test-time\ncompute grows. Through extensive experiments with two challenging benchmarks,\nnamely GPQA and MMLU-Pro, we validate the proposed theories and demonstrate the\noutstanding scaling properties of both algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose two simple yet principled algorithms that enjoy provable scaling\nlaws for the test-time compute of large language models (LLMs), which require a\nblack-box LLM and nothing else (e.g., no external verifier or reward model) for\na minimalistic implementation. (i) The first one is a two-stage knockout-style\nalgorithm: given an input problem, it first generates multiple candidate\nsolutions, and then aggregate them for a final output, via a knockout\ntournament where pairwise comparisons among the candidates are conducted.\nAssuming that the LLM can generate a correct solution with non-zero probability\nand do better than a random guess in comparing a pair of correct and incorrect\nsolutions, we prove theoretically that the failure probability of this\nalgorithm decays to zero exponentially or by a power law (depending on the\nspecific way of scaling) as its test-time compute grows. (ii) The second one is\na two-stage league-style algorithm, where each candidate solution is evaluated\nby its average win rate against multiple opponents, rather than eliminated upon\nloss to a single opponent. Under certain technical assumptions that are\nanalogous to but more robust than those required by the knockout-style\nalgorithm, we prove theoretically that the failure probability of the\nleague-style algorithm also decays to zero exponentially as its test-time\ncompute grows. Through extensive experiments with two challenging benchmarks,\nnamely GPQA and MMLU-Pro, we validate the proposed theories and demonstrate the\noutstanding scaling properties of both algorithms."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "arXiv v2 update: additional algorithms, theories and experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04700v1",
                "updated": "2025-02-07T07:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    7,
                    4,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T07:07:04Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    7,
                    4,
                    4,
                    38,
                    0
                ],
                "title": "EigenLoRAx: Recycling Adapters to Find Principal Subspaces for\n  Resource-Efficient Adaptation and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EigenLoRAx: Recycling Adapters to Find Principal Subspaces for\n  Resource-Efficient Adaptation and Inference"
                },
                "summary": "The rapid growth of large models has raised concerns about their\nenvironmental impact and equity in accessibility due to significant\ncomputational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for\nfinetuning large models, resulting in an abundance of publicly available\nadapters tailored to diverse domains. We ask: Can these pretrained adapters be\nleveraged to further streamline adaptation to new tasks while addressing these\nchallenges? We introduce EigenLoRAx, a parameter-efficient finetuning method\nthat recycles existing adapters to create a principal subspace aligned with\ntheir shared domain knowledge which can be further augmented with orthogonal\nbasis vectors in low-resource scenarios. This enables rapid adaptation to new\ntasks by learning only lightweight coefficients on the principal components of\nthe subspace - eliminating the need to finetune entire adapters. EigenLoRAx\nrequires significantly fewer parameters and memory, improving efficiency for\nboth training and inference. Our method demonstrates strong performance across\ndiverse domains and tasks, offering a scalable for edge-based applications,\npersonalization, and equitable deployment of large models in\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of large models has raised concerns about their\nenvironmental impact and equity in accessibility due to significant\ncomputational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for\nfinetuning large models, resulting in an abundance of publicly available\nadapters tailored to diverse domains. We ask: Can these pretrained adapters be\nleveraged to further streamline adaptation to new tasks while addressing these\nchallenges? We introduce EigenLoRAx, a parameter-efficient finetuning method\nthat recycles existing adapters to create a principal subspace aligned with\ntheir shared domain knowledge which can be further augmented with orthogonal\nbasis vectors in low-resource scenarios. This enables rapid adaptation to new\ntasks by learning only lightweight coefficients on the principal components of\nthe subspace - eliminating the need to finetune entire adapters. EigenLoRAx\nrequires significantly fewer parameters and memory, improving efficiency for\nboth training and inference. Our method demonstrates strong performance across\ndiverse domains and tasks, offering a scalable for edge-based applications,\npersonalization, and equitable deployment of large models in\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Prakhar Kaushik"
                    },
                    {
                        "name": "Ankit Vaidya"
                    },
                    {
                        "name": "Shravan Chaudhari"
                    },
                    {
                        "name": "Alan Yuille"
                    }
                ],
                "author_detail": {
                    "name": "Alan Yuille"
                },
                "author": "Alan Yuille",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09639v2",
                "updated": "2025-02-07T07:02:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    2,
                    26,
                    4,
                    38,
                    0
                ],
                "published": "2024-08-19T01:53:47Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    53,
                    47,
                    0,
                    232,
                    0
                ],
                "title": "How to Make the Most of LLMs' Grammatical Knowledge for Acceptability\n  Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Make the Most of LLMs' Grammatical Knowledge for Acceptability\n  Judgments"
                },
                "summary": "The grammatical knowledge of language models (LMs) is often measured using a\nbenchmark of linguistic minimal pairs, where the LMs are presented with a pair\nof acceptable and unacceptable sentences and required to judge which is more\nacceptable. Conventional approaches directly compare sentence probabilities\nassigned by LMs, but recent large language models (LLMs) are trained to perform\ntasks via prompting, and thus, the raw probabilities they assign may not fully\nreflect their grammatical knowledge. In this study, we attempt to derive more\naccurate acceptability judgments from LLMs using prompts and templates. Through\nextensive experiments in English and Chinese, we compare nine judgment methods\nand find two of them, a probability readout method -- in-template LP and a\nprompt-based method -- Yes/No probability computing, achieve higher accuracy\nthan the conventional ones. Our analysis reveals that these methods excel in\ndifferent linguistic phenomena, suggesting they access different aspects of\nLLMs' knowledge. We also find that ensembling the two methods outperforms\nsingle methods. Consequently, we recommend these techniques, either\nindividually or ensembled, as more effective alternatives to conventional\napproaches for assessing grammatical knowledge in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The grammatical knowledge of language models (LMs) is often measured using a\nbenchmark of linguistic minimal pairs, where the LMs are presented with a pair\nof acceptable and unacceptable sentences and required to judge which is more\nacceptable. Conventional approaches directly compare sentence probabilities\nassigned by LMs, but recent large language models (LLMs) are trained to perform\ntasks via prompting, and thus, the raw probabilities they assign may not fully\nreflect their grammatical knowledge. In this study, we attempt to derive more\naccurate acceptability judgments from LLMs using prompts and templates. Through\nextensive experiments in English and Chinese, we compare nine judgment methods\nand find two of them, a probability readout method -- in-template LP and a\nprompt-based method -- Yes/No probability computing, achieve higher accuracy\nthan the conventional ones. Our analysis reveals that these methods excel in\ndifferent linguistic phenomena, suggesting they access different aspects of\nLLMs' knowledge. We also find that ensembling the two methods outperforms\nsingle methods. Consequently, we recommend these techniques, either\nindividually or ensembled, as more effective alternatives to conventional\napproaches for assessing grammatical knowledge in LLMs."
                },
                "authors": [
                    {
                        "name": "Yusuke Ide"
                    },
                    {
                        "name": "Yuto Nishida"
                    },
                    {
                        "name": "Justin Vasselli"
                    },
                    {
                        "name": "Miyu Oba"
                    },
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "NAACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02481v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02481v3",
                "updated": "2025-02-07T06:59:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    59,
                    27,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-04T16:57:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    57,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study"
                },
                "summary": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo."
                },
                "authors": [
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Pengzhi Gao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "Accept to NAACL2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02481v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02481v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04692v1",
                "updated": "2025-02-07T06:37:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    37,
                    5,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T06:37:05Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    37,
                    5,
                    4,
                    38,
                    0
                ],
                "title": "STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion"
                },
                "summary": "Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving significant\nimprovements in efficiency and task performance. Using STRIDE-generated\nrewards, simulated humanoid robots achieve sprint-level locomotion across\ncomplex terrains, highlighting its ability to advance DRL workflows and\nhumanoid robotics research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving significant\nimprovements in efficiency and task performance. Using STRIDE-generated\nrewards, simulated humanoid robots achieve sprint-level locomotion across\ncomplex terrains, highlighting its ability to advance DRL workflows and\nhumanoid robotics research."
                },
                "authors": [
                    {
                        "name": "Zhenwei Wu"
                    },
                    {
                        "name": "Jinxiong Lu"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Luhui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Luhui Hu"
                },
                "author": "Luhui Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11301v2",
                "updated": "2025-02-07T06:34:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    34,
                    14,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-20T07:05:15Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    7,
                    5,
                    15,
                    0,
                    20,
                    0
                ],
                "title": "Question-to-Question Retrieval for Hallucination-Free Knowledge Access:\n  An Approach for Wikipedia and Wikidata Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-to-Question Retrieval for Hallucination-Free Knowledge Access:\n  An Approach for Wikipedia and Wikidata Question Answering"
                },
                "summary": "This paper introduces an approach to question answering over knowledge bases\nlike Wikipedia and Wikidata by performing \"question-to-question\" matching and\nretrieval from a dense vector embedding store. Instead of embedding document\ncontent, we generate a comprehensive set of questions for each logical content\nunit using an instruction-tuned LLM. These questions are vector-embedded and\nstored, mapping to the corresponding content. Vector embedding of user queries\nare then matched against this question vector store. The highest similarity\nscore leads to direct retrieval of the associated article content, eliminating\nthe need for answer generation. Our method achieves high cosine similarity ( >\n0.9 ) for relevant question pairs, enabling highly precise retrieval. This\napproach offers several advantages including computational efficiency, rapid\nresponse times, and increased scalability. We demonstrate its effectiveness on\nWikipedia and Wikidata, including multimedia content through structured fact\nretrieval from Wikidata, opening up new pathways for multimodal question\nanswering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an approach to question answering over knowledge bases\nlike Wikipedia and Wikidata by performing \"question-to-question\" matching and\nretrieval from a dense vector embedding store. Instead of embedding document\ncontent, we generate a comprehensive set of questions for each logical content\nunit using an instruction-tuned LLM. These questions are vector-embedded and\nstored, mapping to the corresponding content. Vector embedding of user queries\nare then matched against this question vector store. The highest similarity\nscore leads to direct retrieval of the associated article content, eliminating\nthe need for answer generation. Our method achieves high cosine similarity ( >\n0.9 ) for relevant question pairs, enabling highly precise retrieval. This\napproach offers several advantages including computational efficiency, rapid\nresponse times, and increased scalability. We demonstrate its effectiveness on\nWikipedia and Wikidata, including multimedia content through structured fact\nretrieval from Wikidata, opening up new pathways for multimodal question\nanswering."
                },
                "authors": [
                    {
                        "name": "Santhosh Thottingal"
                    }
                ],
                "author_detail": {
                    "name": "Santhosh Thottingal"
                },
                "author": "Santhosh Thottingal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.05174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05174v1",
                "updated": "2025-02-07T18:57:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    57,
                    49,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:57:49Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    57,
                    49,
                    4,
                    38,
                    0
                ],
                "title": "MELON: Indirect Prompt Injection Defense via Masked Re-execution and\n  Tool Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MELON: Indirect Prompt Injection Defense via Masked Re-execution and\n  Tool Comparison"
                },
                "summary": "Recent research has explored that LLM agents are vulnerable to indirect\nprompt injection (IPI) attacks, where malicious tasks embedded in\ntool-retrieved information can redirect the agent to take unauthorized actions.\nExisting defenses against IPI have significant limitations: either require\nessential model training resources, lack effectiveness against sophisticated\nattacks, or harm the normal utilities. We present MELON (Masked re-Execution\nand TooL comparisON), a novel IPI defense. Our approach builds on the\nobservation that under a successful attack, the agent's next action becomes\nless dependent on user tasks and more on malicious tasks. Following this, we\ndesign MELON to detect attacks by re-executing the agent's trajectory with a\nmasked user prompt modified through a masking function. We identify an attack\nif the actions generated in the original and masked executions are similar. We\nalso include three key designs to reduce the potential false positives and\nfalse negatives. Extensive evaluation on the IPI benchmark AgentDojo\ndemonstrates that MELON outperforms SOTA defenses in both attack prevention and\nutility preservation. Moreover, we show that combining MELON with a SOTA prompt\naugmentation defense (denoted as MELON-Aug) further improves its performance.\nWe also conduct a detailed ablation study to validate our key designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has explored that LLM agents are vulnerable to indirect\nprompt injection (IPI) attacks, where malicious tasks embedded in\ntool-retrieved information can redirect the agent to take unauthorized actions.\nExisting defenses against IPI have significant limitations: either require\nessential model training resources, lack effectiveness against sophisticated\nattacks, or harm the normal utilities. We present MELON (Masked re-Execution\nand TooL comparisON), a novel IPI defense. Our approach builds on the\nobservation that under a successful attack, the agent's next action becomes\nless dependent on user tasks and more on malicious tasks. Following this, we\ndesign MELON to detect attacks by re-executing the agent's trajectory with a\nmasked user prompt modified through a masking function. We identify an attack\nif the actions generated in the original and masked executions are similar. We\nalso include three key designs to reduce the potential false positives and\nfalse negatives. Extensive evaluation on the IPI benchmark AgentDojo\ndemonstrates that MELON outperforms SOTA defenses in both attack prevention and\nutility preservation. Moreover, we show that combining MELON with a SOTA prompt\naugmentation defense (denoted as MELON-Aug) further improves its performance.\nWe also conduct a detailed ablation study to validate our key designs."
                },
                "authors": [
                    {
                        "name": "Kaijie Zhu"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Wenbo Guo"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05167v1",
                "updated": "2025-02-07T18:49:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    49,
                    46,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:49:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    49,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoLiMa: Long-Context Evaluation Beyond Literal Matching"
                },
                "summary": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05163v1",
                "updated": "2025-02-07T18:45:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    45,
                    3,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:45:03Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    45,
                    3,
                    4,
                    38,
                    0
                ],
                "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM\n  Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM\n  Guardrails"
                },
                "summary": "The rapid advancement of large language models (LLMs) has increased the need\nfor guardrail models to ensure responsible use, particularly in detecting\nunsafe and illegal content. While substantial safety data exist in English,\nmultilingual guardrail modeling remains underexplored due to the scarcity of\nopen-source safety data in other languages. To address this gap, we propose a\nnovel two-player Reinforcement Learning (RL) framework, where a generator and a\nguardrail model co-evolve adversarially to produce high-quality synthetic data\nfor multilingual guardrail training. We theoretically formalize this\ninteraction as a two-player game, proving convergence to a Nash equilibrium.\nEmpirical evaluations show that our model \\ours outperforms state-of-the-art\nmodels, achieving nearly 10% improvement over LlamaGuard3 (8B) on English\nbenchmarks while being 4.5x faster at inference with a significantly smaller\nmodel (0.5B). We achieve substantial advancements in multilingual safety tasks,\nparticularly in addressing the imbalance for lower-resource languages in a\ncollected real dataset. Ablation studies emphasize the critical role of\nsynthetic data generation in bridging the imbalance in open-source data between\nEnglish and other languages. These findings establish a scalable and efficient\napproach to synthetic data generation, paving the way for improved multilingual\nguardrail models to enhance LLM safety. Code, model, and data will be\nopen-sourced at https://github.com/yihedeng9/DuoGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has increased the need\nfor guardrail models to ensure responsible use, particularly in detecting\nunsafe and illegal content. While substantial safety data exist in English,\nmultilingual guardrail modeling remains underexplored due to the scarcity of\nopen-source safety data in other languages. To address this gap, we propose a\nnovel two-player Reinforcement Learning (RL) framework, where a generator and a\nguardrail model co-evolve adversarially to produce high-quality synthetic data\nfor multilingual guardrail training. We theoretically formalize this\ninteraction as a two-player game, proving convergence to a Nash equilibrium.\nEmpirical evaluations show that our model \\ours outperforms state-of-the-art\nmodels, achieving nearly 10% improvement over LlamaGuard3 (8B) on English\nbenchmarks while being 4.5x faster at inference with a significantly smaller\nmodel (0.5B). We achieve substantial advancements in multilingual safety tasks,\nparticularly in addressing the imbalance for lower-resource languages in a\ncollected real dataset. Ablation studies emphasize the critical role of\nsynthetic data generation in bridging the imbalance in open-source data between\nEnglish and other languages. These findings establish a scalable and efficient\napproach to synthetic data generation, paving the way for improved multilingual\nguardrail models to enhance LLM safety. Code, model, and data will be\nopen-sourced at https://github.com/yihedeng9/DuoGuard."
                },
                "authors": [
                    {
                        "name": "Yihe Deng"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Junkai Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "arxiv_comment": "24 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05159v1",
                "updated": "2025-02-07T18:41:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    41,
                    21,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:41:21Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    41,
                    21,
                    4,
                    38,
                    0
                ],
                "title": "A Lightweight Method to Disrupt Memorized Sequences in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Method to Disrupt Memorized Sequences in LLM"
                },
                "summary": "Large language models (LLMs) demonstrate impressive capabilities across many\ntasks yet risk reproducing copyrighted content verbatim, raising legal and\nethical concerns. Although methods like differential privacy or neuron editing\ncan reduce memorization, they typically require costly retraining or direct\naccess to model weights and may degrade performance. To address these\nchallenges, we propose TokenSwap, a lightweight, post-hoc approach that\nreplaces the probabilities of grammar-related tokens with those from a small\nauxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial\ngrade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method\neffectively reduces well-known cases of memorized generation by upto 10x with\nlittle to no impact on downstream tasks. Our approach offers a uniquely\naccessible and effective solution to users of real-world systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate impressive capabilities across many\ntasks yet risk reproducing copyrighted content verbatim, raising legal and\nethical concerns. Although methods like differential privacy or neuron editing\ncan reduce memorization, they typically require costly retraining or direct\naccess to model weights and may degrade performance. To address these\nchallenges, we propose TokenSwap, a lightweight, post-hoc approach that\nreplaces the probabilities of grammar-related tokens with those from a small\nauxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial\ngrade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method\neffectively reduces well-known cases of memorized generation by upto 10x with\nlittle to no impact on downstream tasks. Our approach offers a uniquely\naccessible and effective solution to users of real-world systems."
                },
                "authors": [
                    {
                        "name": "Parjanya Prajakta Prashant"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Babak Salimi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Salimi"
                },
                "author": "Babak Salimi",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07163v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07163v3",
                "updated": "2025-02-07T18:34:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    34,
                    28,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-09T17:58:12Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    58,
                    12,
                    2,
                    283,
                    0
                ],
                "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning"
                },
                "summary": "This work studies the problem of large language model (LLM) unlearning,\naiming to remove unwanted data influences (e.g., copyrighted or harmful\ncontent) while preserving model utility. Despite the increasing demand for\nunlearning, a technically-grounded optimization framework is lacking. Gradient\nascent (GA)-type methods, though widely used, are suboptimal as they reverse\nthe learning process without controlling optimization divergence (i.e.,\ndeviation from the pre-trained state), leading to risks of over-forgetting and\npotential model collapse. Negative preference optimization (NPO) has been\nproposed to address this issue and is considered one of the state-of-the-art\nLLM unlearning approaches. In this work, we revisit NPO and identify another\ncritical issue: reference model bias. This bias arises from using the reference\nmodel (i.e., the model prior to unlearning) to evaluate the unlearning success,\nwhich can compromise NPO's effectiveness. Specifically, it leads to (a) uneven\nallocation of optimization power across forget data with varying difficulty\nlevels and (b) ineffective gradient weight smoothing during the early stages of\nunlearning optimization. To overcome these challenges, we propose a simple yet\neffective unlearning optimization framework, called SimNPO, showing that\n`simplicity' in removing the reliance on a reference model (through the lens of\nsimple preference optimization) benefits unlearning. We provide deeper insights\ninto SimNPO's advantages through an analysis based on mixtures of Markov\nchains. Extensive experiments further validate SimNPO's efficacy on benchmarks\nlike TOFU and MUSE, as well as its robustness against relearning attacks. Codes\nare available at https://github.com/OPTML-Group/Unlearn-Simple.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies the problem of large language model (LLM) unlearning,\naiming to remove unwanted data influences (e.g., copyrighted or harmful\ncontent) while preserving model utility. Despite the increasing demand for\nunlearning, a technically-grounded optimization framework is lacking. Gradient\nascent (GA)-type methods, though widely used, are suboptimal as they reverse\nthe learning process without controlling optimization divergence (i.e.,\ndeviation from the pre-trained state), leading to risks of over-forgetting and\npotential model collapse. Negative preference optimization (NPO) has been\nproposed to address this issue and is considered one of the state-of-the-art\nLLM unlearning approaches. In this work, we revisit NPO and identify another\ncritical issue: reference model bias. This bias arises from using the reference\nmodel (i.e., the model prior to unlearning) to evaluate the unlearning success,\nwhich can compromise NPO's effectiveness. Specifically, it leads to (a) uneven\nallocation of optimization power across forget data with varying difficulty\nlevels and (b) ineffective gradient weight smoothing during the early stages of\nunlearning optimization. To overcome these challenges, we propose a simple yet\neffective unlearning optimization framework, called SimNPO, showing that\n`simplicity' in removing the reliance on a reference model (through the lens of\nsimple preference optimization) benefits unlearning. We provide deeper insights\ninto SimNPO's advantages through an analysis based on mixtures of Markov\nchains. Extensive experiments further validate SimNPO's efficacy on benchmarks\nlike TOFU and MUSE, as well as its robustness against relearning attacks. Codes\nare available at https://github.com/OPTML-Group/Unlearn-Simple."
                },
                "authors": [
                    {
                        "name": "Chongyu Fan"
                    },
                    {
                        "name": "Jiancheng Liu"
                    },
                    {
                        "name": "Licong Lin"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07163v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07163v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05150v1",
                "updated": "2025-02-07T18:26:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    26,
                    15,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:26:15Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    26,
                    15,
                    4,
                    38,
                    0
                ],
                "title": "CodeSCM: Causal Analysis for Multi-Modal Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSCM: Causal Analysis for Multi-Modal Code Generation"
                },
                "summary": "In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for\nanalyzing multi-modal code generation using large language models (LLMs). By\napplying interventions to CodeSCM, we measure the causal effects of different\nprompt modalities, such as natural language, code, and input-output examples,\non the model. CodeSCM introduces latent mediator variables to separate the code\nand natural language semantics of a multi-modal code generation prompt. Using\nthe principles of Causal Mediation Analysis on these mediators we quantify\ndirect effects representing the model's spurious leanings. We find that, in\naddition to natural language instructions, input-output examples significantly\ninfluence code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for\nanalyzing multi-modal code generation using large language models (LLMs). By\napplying interventions to CodeSCM, we measure the causal effects of different\nprompt modalities, such as natural language, code, and input-output examples,\non the model. CodeSCM introduces latent mediator variables to separate the code\nand natural language semantics of a multi-modal code generation prompt. Using\nthe principles of Causal Mediation Analysis on these mediators we quantify\ndirect effects representing the model's spurious leanings. We find that, in\naddition to natural language instructions, input-output examples significantly\ninfluence code generation."
                },
                "authors": [
                    {
                        "name": "Mukur Gupta"
                    },
                    {
                        "name": "Noopur Bhatt"
                    },
                    {
                        "name": "Suman Jana"
                    }
                ],
                "author_detail": {
                    "name": "Suman Jana"
                },
                "author": "Suman Jana",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05148v1",
                "updated": "2025-02-07T18:26:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    26,
                    1,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:26:01Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    26,
                    1,
                    4,
                    38,
                    0
                ],
                "title": "An Annotated Reading of 'The Singer of Tales' in the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Annotated Reading of 'The Singer of Tales' in the LLM Era"
                },
                "summary": "The Parry-Lord oral-formulaic theory was a breakthrough in understanding how\noral narrative poetry is learned, composed, and transmitted by illiterate\nbards. In this paper, we provide an annotated reading of the mechanism\nunderlying this theory from the lens of large language models (LLMs) and\ngenerative artificial intelligence (AI). We point out the the similarities and\ndifferences between oral composition and LLM generation, and comment on the\nimplications to society and AI policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Parry-Lord oral-formulaic theory was a breakthrough in understanding how\noral narrative poetry is learned, composed, and transmitted by illiterate\nbards. In this paper, we provide an annotated reading of the mechanism\nunderlying this theory from the lens of large language models (LLMs) and\ngenerative artificial intelligence (AI). We point out the the similarities and\ndifferences between oral composition and LLM generation, and comment on the\nimplications to society and AI policy."
                },
                "authors": [
                    {
                        "name": "Kush R. Varshney"
                    }
                ],
                "author_detail": {
                    "name": "Kush R. Varshney"
                },
                "author": "Kush R. Varshney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15797v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15797v3",
                "updated": "2025-02-07T18:24:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    24,
                    26,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-27T05:46:06Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    46,
                    6,
                    0,
                    27,
                    0
                ],
                "title": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models"
                },
                "summary": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language."
                },
                "authors": [
                    {
                        "name": "Tianbo Yang"
                    },
                    {
                        "name": "Mingqi Yang"
                    },
                    {
                        "name": "Hongyi Zhao"
                    },
                    {
                        "name": "Tianshuo Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tianshuo Yang"
                },
                "author": "Tianshuo Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15797v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15797v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05143v1",
                "updated": "2025-02-07T18:19:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    19,
                    12,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:19:12Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    19,
                    12,
                    4,
                    38,
                    0
                ],
                "title": "pyMethods2Test: A Dataset of Python Tests Mapped to Focal Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "pyMethods2Test: A Dataset of Python Tests Mapped to Focal Methods"
                },
                "summary": "Python is one of the fastest-growing programming languages and currently\nranks as the top language in many lists, even recently overtaking JavaScript as\nthe top language on GitHub. Given its importance in data science and machine\nlearning, it is imperative to be able to effectively train LLMs to generate\ngood unit test cases for Python code. This motivates the need for a large\ndataset to provide training and testing data. To date, while other large\ndatasets exist for languages like Java, none publicly exist for Python. Python\nposes difficult challenges in generating such a dataset, due to its less rigid\nnaming requirements. In this work, we consider two commonly used Python unit\ntesting frameworks: Pytest and unittest. We analyze a large corpus of over 88K\nopen-source GitHub projects utilizing these testing frameworks. Using a\ncarefully designed set of heuristics, we are able to locate over 22 million\ntest methods. We then analyze the test and non-test code and map individual\nunit tests to the focal method being tested. This provides an explicit\ntraceability link from the test to the tested method. Our pyMethods2Test\ndataset contains over 2 million of these focal method mappings, as well as the\nability to generate useful context for input to LLMs. The pyMethods2Test\ndataset is publicly available on Zenodo at:\nhttps://doi.org/10.5281/zenodo.14264518",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is one of the fastest-growing programming languages and currently\nranks as the top language in many lists, even recently overtaking JavaScript as\nthe top language on GitHub. Given its importance in data science and machine\nlearning, it is imperative to be able to effectively train LLMs to generate\ngood unit test cases for Python code. This motivates the need for a large\ndataset to provide training and testing data. To date, while other large\ndatasets exist for languages like Java, none publicly exist for Python. Python\nposes difficult challenges in generating such a dataset, due to its less rigid\nnaming requirements. In this work, we consider two commonly used Python unit\ntesting frameworks: Pytest and unittest. We analyze a large corpus of over 88K\nopen-source GitHub projects utilizing these testing frameworks. Using a\ncarefully designed set of heuristics, we are able to locate over 22 million\ntest methods. We then analyze the test and non-test code and map individual\nunit tests to the focal method being tested. This provides an explicit\ntraceability link from the test to the tested method. Our pyMethods2Test\ndataset contains over 2 million of these focal method mappings, as well as the\nability to generate useful context for input to LLMs. The pyMethods2Test\ndataset is publicly available on Zenodo at:\nhttps://doi.org/10.5281/zenodo.14264518"
                },
                "authors": [
                    {
                        "name": "Idriss Abdelmadjid"
                    },
                    {
                        "name": "Robert Dyer"
                    }
                ],
                "author_detail": {
                    "name": "Robert Dyer"
                },
                "author": "Robert Dyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05129v1",
                "updated": "2025-02-07T18:02:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    2,
                    28,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:02:28Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    2,
                    28,
                    4,
                    38,
                    0
                ],
                "title": "Counting Fish with Temporal Representations of Sonar Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counting Fish with Temporal Representations of Sonar Video"
                },
                "summary": "Accurate estimates of salmon escapement - the number of fish migrating\nupstream to spawn - are key data for conservation and fishery management.\nExisting methods for salmon counting using high-resolution imaging sonar\nhardware are non-invasive and compatible with computer vision processing. Prior\nwork in this area has utilized object detection and tracking based methods for\nautomated salmon counting. However, these techniques remain inaccessible to\nmany sonar deployment sites due to limited compute and connectivity in the\nfield. We propose an alternative lightweight computer vision method for fish\ncounting based on analyzing echograms - temporal representations that compress\nseveral hundred frames of imaging sonar video into a single image. We predict\nupstream and downstream counts within 200-frame time windows directly from\nechograms using a ResNet-18 model, and propose a set of domain-specific image\naugmentations and a weakly-supervised training protocol to further improve\nresults. We achieve a count error of 23% on representative data from the Kenai\nRiver in Alaska, demonstrating the feasibility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate estimates of salmon escapement - the number of fish migrating\nupstream to spawn - are key data for conservation and fishery management.\nExisting methods for salmon counting using high-resolution imaging sonar\nhardware are non-invasive and compatible with computer vision processing. Prior\nwork in this area has utilized object detection and tracking based methods for\nautomated salmon counting. However, these techniques remain inaccessible to\nmany sonar deployment sites due to limited compute and connectivity in the\nfield. We propose an alternative lightweight computer vision method for fish\ncounting based on analyzing echograms - temporal representations that compress\nseveral hundred frames of imaging sonar video into a single image. We predict\nupstream and downstream counts within 200-frame time windows directly from\nechograms using a ResNet-18 model, and propose a set of domain-specific image\naugmentations and a weakly-supervised training protocol to further improve\nresults. We achieve a count error of 23% on representative data from the Kenai\nRiver in Alaska, demonstrating the feasibility of our approach."
                },
                "authors": [
                    {
                        "name": "Kai Van Brunt"
                    },
                    {
                        "name": "Justin Kay"
                    },
                    {
                        "name": "Timm Haucke"
                    },
                    {
                        "name": "Pietro Perona"
                    },
                    {
                        "name": "Grant Van Horn"
                    },
                    {
                        "name": "Sara Beery"
                    }
                ],
                "author_detail": {
                    "name": "Sara Beery"
                },
                "author": "Sara Beery",
                "arxiv_comment": "ECCV 2024. 6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05127v1",
                "updated": "2025-02-07T18:00:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    0,
                    36,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T18:00:36Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    0,
                    36,
                    4,
                    38,
                    0
                ],
                "title": "Self-supervised Conformal Prediction for Uncertainty Quantification in\n  Imaging Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised Conformal Prediction for Uncertainty Quantification in\n  Imaging Problems"
                },
                "summary": "Most image restoration problems are ill-conditioned or ill-posed and hence\ninvolve significant uncertainty. Quantifying this uncertainty is crucial for\nreliably interpreting experimental results, particularly when reconstructed\nimages inform critical decisions and science. However, most existing image\nrestoration methods either fail to quantify uncertainty or provide estimates\nthat are highly inaccurate. Conformal prediction has recently emerged as a\nflexible framework to equip any estimator with uncertainty quantification\ncapabilities that, by construction, have nearly exact marginal coverage. To\nachieve this, conformal prediction relies on abundant ground truth data for\ncalibration. However, in image restoration problems, reliable ground truth data\nis often expensive or not possible to acquire. Also, reliance on ground truth\ndata can introduce large biases in situations of distribution shift between\ncalibration and deployment. This paper seeks to develop a more robust approach\nto conformal prediction for image restoration problems by proposing a\nself-supervised conformal prediction method that leverages Stein's Unbiased\nRisk Estimator (SURE) to self-calibrate itself directly from the observed noisy\nmeasurements, bypassing the need for ground truth. The method is suitable for\nany linear imaging inverse problem that is ill-conditioned, and it is\nespecially powerful when used with modern self-supervised image restoration\ntechniques that can also be trained directly from measurement data. The\nproposed approach is demonstrated through numerical experiments on image\ndenoising and deblurring, where it delivers results that are remarkably\naccurate and comparable to those obtained by supervised conformal prediction\nwith ground truth data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most image restoration problems are ill-conditioned or ill-posed and hence\ninvolve significant uncertainty. Quantifying this uncertainty is crucial for\nreliably interpreting experimental results, particularly when reconstructed\nimages inform critical decisions and science. However, most existing image\nrestoration methods either fail to quantify uncertainty or provide estimates\nthat are highly inaccurate. Conformal prediction has recently emerged as a\nflexible framework to equip any estimator with uncertainty quantification\ncapabilities that, by construction, have nearly exact marginal coverage. To\nachieve this, conformal prediction relies on abundant ground truth data for\ncalibration. However, in image restoration problems, reliable ground truth data\nis often expensive or not possible to acquire. Also, reliance on ground truth\ndata can introduce large biases in situations of distribution shift between\ncalibration and deployment. This paper seeks to develop a more robust approach\nto conformal prediction for image restoration problems by proposing a\nself-supervised conformal prediction method that leverages Stein's Unbiased\nRisk Estimator (SURE) to self-calibrate itself directly from the observed noisy\nmeasurements, bypassing the need for ground truth. The method is suitable for\nany linear imaging inverse problem that is ill-conditioned, and it is\nespecially powerful when used with modern self-supervised image restoration\ntechniques that can also be trained directly from measurement data. The\nproposed approach is demonstrated through numerical experiments on image\ndenoising and deblurring, where it delivers results that are remarkably\naccurate and comparable to those obtained by supervised conformal prediction\nwith ground truth data."
                },
                "authors": [
                    {
                        "name": "Jasper M. Everink"
                    },
                    {
                        "name": "Bernardin Tamo Amougou"
                    },
                    {
                        "name": "Marcelo Pereyra"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Pereyra"
                },
                "author": "Marcelo Pereyra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12924v2",
                "updated": "2025-02-07T17:44:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    44,
                    38,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-16T18:10:50Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    18,
                    10,
                    50,
                    2,
                    290,
                    0
                ],
                "title": "Interpreting token compositionality in LLMs: A robustness analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting token compositionality in LLMs: A robustness analysis"
                },
                "summary": "Understanding the internal mechanisms of large language models (LLMs) is\nintegral to enhancing their reliability, interpretability, and inference\nprocesses. We present Constituent-Aware Pooling (CAP), a methodology designed\nto analyse how LLMs process compositional linguistic structures. Grounded in\nprinciples of compositionality, mechanistic interpretability, and information\ntheory, CAP systematically intervenes in model activations through\nconstituent-based pooling at various model levels. Our experiments on inverse\ndefinition modelling, hypernym and synonym prediction reveal critical insights\ninto transformers' limitations in handling compositional abstractions. No\nspecific layer integrates tokens into unified semantic representations based on\ntheir constituent parts. We observe fragmented information processing, which\nintensifies with model size, suggesting that larger models struggle more with\nthese interventions and exhibit greater information dispersion. This\nfragmentation likely stems from transformers' training objectives and\narchitectural design, preventing systematic and cohesive representations. Our\nfindings highlight fundamental limitations in current transformer architectures\nregarding compositional semantics processing and model interpretability,\nunderscoring the critical need for novel approaches in LLM design to address\nthese challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the internal mechanisms of large language models (LLMs) is\nintegral to enhancing their reliability, interpretability, and inference\nprocesses. We present Constituent-Aware Pooling (CAP), a methodology designed\nto analyse how LLMs process compositional linguistic structures. Grounded in\nprinciples of compositionality, mechanistic interpretability, and information\ntheory, CAP systematically intervenes in model activations through\nconstituent-based pooling at various model levels. Our experiments on inverse\ndefinition modelling, hypernym and synonym prediction reveal critical insights\ninto transformers' limitations in handling compositional abstractions. No\nspecific layer integrates tokens into unified semantic representations based on\ntheir constituent parts. We observe fragmented information processing, which\nintensifies with model size, suggesting that larger models struggle more with\nthese interventions and exhibit greater information dispersion. This\nfragmentation likely stems from transformers' training objectives and\narchitectural design, preventing systematic and cohesive representations. Our\nfindings highlight fundamental limitations in current transformer architectures\nregarding compositional semantics processing and model interpretability,\nunderscoring the critical need for novel approaches in LLM design to address\nthese challenges."
                },
                "authors": [
                    {
                        "name": "Nura Aljaafari"
                    },
                    {
                        "name": "Danilo S. Carvalho"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "arxiv_comment": "19 pages, 2 Figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05115v1",
                "updated": "2025-02-07T17:38:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    38,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:38:10Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    38,
                    10,
                    4,
                    38,
                    0
                ],
                "title": "\"It Felt Like I Was Left in the Dark\": Exploring Information Needs and\n  Design Opportunities for Family Caregivers of Older Adult Patients in\n  Critical Care Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"It Felt Like I Was Left in the Dark\": Exploring Information Needs and\n  Design Opportunities for Family Caregivers of Older Adult Patients in\n  Critical Care Settings"
                },
                "summary": "Older adult patients constitute a rapidly growing subgroup of Intensive Care\nUnit (ICU) patients. In these situations, their family caregivers are expected\nto represent the unconscious patients to access and interpret patients' medical\ninformation. However, caregivers currently have to rely on overloaded\nclinicians for information updates and typically lack the health literacy to\nunderstand complex medical information. Our project aims to explore the\ninformation needs of caregivers of ICU older adult patients, from which we can\npropose design opportunities to guide future AI systems. The project begins\nwith formative interviews with 11 caregivers to identify their challenges in\naccessing and interpreting medical information; From these findings, we then\nsynthesize design requirements and propose an AI system prototype to cope with\ncaregivers' challenges. The system prototype has two key features: a timeline\nvisualization to show the AI extracted and summarized older adult patients' key\nmedical events; and an LLM-based chatbot to provide context-aware informational\nsupport. We conclude our paper by reporting on the follow-up user evaluation of\nthe system and discussing future AI-based systems for ICU caregivers of older\nadults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Older adult patients constitute a rapidly growing subgroup of Intensive Care\nUnit (ICU) patients. In these situations, their family caregivers are expected\nto represent the unconscious patients to access and interpret patients' medical\ninformation. However, caregivers currently have to rely on overloaded\nclinicians for information updates and typically lack the health literacy to\nunderstand complex medical information. Our project aims to explore the\ninformation needs of caregivers of ICU older adult patients, from which we can\npropose design opportunities to guide future AI systems. The project begins\nwith formative interviews with 11 caregivers to identify their challenges in\naccessing and interpreting medical information; From these findings, we then\nsynthesize design requirements and propose an AI system prototype to cope with\ncaregivers' challenges. The system prototype has two key features: a timeline\nvisualization to show the AI extracted and summarized older adult patients' key\nmedical events; and an LLM-based chatbot to provide context-aware informational\nsupport. We conclude our paper by reporting on the follow-up user evaluation of\nthe system and discussing future AI-based systems for ICU caregivers of older\nadults."
                },
                "authors": [
                    {
                        "name": "Shihan Fu"
                    },
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Smit Desai"
                    },
                    {
                        "name": "Yuqi Hu"
                    },
                    {
                        "name": "Yuling Sun"
                    },
                    {
                        "name": "Samantha Stonbraker"
                    },
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Elizabeth M. Goldberg"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05111v1",
                "updated": "2025-02-07T17:35:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    35,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:35:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    35,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Flexible and Efficient Grammar-Constrained Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible and Efficient Grammar-Constrained Decoding"
                },
                "summary": "Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation."
                },
                "authors": [
                    {
                        "name": "Kanghee Park"
                    },
                    {
                        "name": "Timothy Zhou"
                    },
                    {
                        "name": "Loris D'Antoni"
                    }
                ],
                "author_detail": {
                    "name": "Loris D'Antoni"
                },
                "author": "Loris D'Antoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15599v2",
                "updated": "2025-02-07T17:29:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    29,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2024-06-21T18:57:38Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    18,
                    57,
                    38,
                    4,
                    173,
                    0
                ],
                "title": "Pareto-Optimal Learning from Preferences with Hidden Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto-Optimal Learning from Preferences with Hidden Context"
                },
                "summary": "Ensuring AI models align with human values is essential for their safety and\nfunctionality. Reinforcement learning from human feedback (RLHF) leverages\nhuman preferences to achieve this alignment. However, when preferences are\nsourced from diverse populations, point estimates of reward can result in\nsuboptimal performance or be unfair to specific groups. We propose Pareto\nOptimal Preference Learning (POPL), which enables pluralistic alignment by\nframing discrepant group preferences as objectives with potential trade-offs,\naiming for policies that are Pareto-optimal on the preference dataset. POPL\nutilizes lexicase selection, an iterative process that selects diverse and\nPareto-optimal solutions. Our theoretical and empirical evaluations demonstrate\nthat POPL surpasses baseline methods in learning sets of reward functions and\npolicies, effectively catering to distinct groups without access to group\nnumbers or membership labels. We verify the performance of POPL on a stateless\npreference learning setting, a Minigrid RL domain, Metaworld robotics\nbenchmarks, as well as large language model (LLM) fine-tuning. We illustrate\nthat POPL can also serve as a foundation for techniques optimizing specific\nnotions of group fairness, ensuring safe and equitable AI model alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring AI models align with human values is essential for their safety and\nfunctionality. Reinforcement learning from human feedback (RLHF) leverages\nhuman preferences to achieve this alignment. However, when preferences are\nsourced from diverse populations, point estimates of reward can result in\nsuboptimal performance or be unfair to specific groups. We propose Pareto\nOptimal Preference Learning (POPL), which enables pluralistic alignment by\nframing discrepant group preferences as objectives with potential trade-offs,\naiming for policies that are Pareto-optimal on the preference dataset. POPL\nutilizes lexicase selection, an iterative process that selects diverse and\nPareto-optimal solutions. Our theoretical and empirical evaluations demonstrate\nthat POPL surpasses baseline methods in learning sets of reward functions and\npolicies, effectively catering to distinct groups without access to group\nnumbers or membership labels. We verify the performance of POPL on a stateless\npreference learning setting, a Minigrid RL domain, Metaworld robotics\nbenchmarks, as well as large language model (LLM) fine-tuning. We illustrate\nthat POPL can also serve as a foundation for techniques optimizing specific\nnotions of group fairness, ensuring safe and equitable AI model alignment."
                },
                "authors": [
                    {
                        "name": "Ryan Bahlous-Boldi"
                    },
                    {
                        "name": "Li Ding"
                    },
                    {
                        "name": "Lee Spector"
                    },
                    {
                        "name": "Scott Niekum"
                    }
                ],
                "author_detail": {
                    "name": "Scott Niekum"
                },
                "author": "Scott Niekum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05098v1",
                "updated": "2025-02-07T17:17:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    17,
                    42,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:17:42Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    17,
                    42,
                    4,
                    38,
                    0
                ],
                "title": "Learning Temporal Invariance in Android Malware Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Temporal Invariance in Android Malware Detectors"
                },
                "summary": "Learning-based Android malware detectors degrade over time due to natural\ndistribution drift caused by malware variants and new families. This paper\nsystematically investigates the challenges classifiers trained with empirical\nrisk minimization (ERM) face against such distribution shifts and attributes\ntheir shortcomings to their inability to learn stable discriminative features.\nInvariant learning theory offers a promising solution by encouraging models to\ngenerate stable representations crossing environments that expose the\ninstability of the training set. However, the lack of prior environment labels,\nthe diversity of drift factors, and low-quality representations caused by\ndiverse families make this task challenging. To address these issues, we\npropose TIF, the first temporal invariant training framework for malware\ndetection, which aims to enhance the ability of detectors to learn stable\nrepresentations across time. TIF organizes environments based on application\nobservation dates to reveal temporal drift, integrating specialized multi-proxy\ncontrastive learning and invariant gradient alignment to generate and align\nenvironments with high-quality, stable representations. TIF can be seamlessly\nintegrated into any learning-based detector. Experiments on a decade-long\ndataset show that TIF excels, particularly in early deployment stages,\naddressing real-world needs and outperforming state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based Android malware detectors degrade over time due to natural\ndistribution drift caused by malware variants and new families. This paper\nsystematically investigates the challenges classifiers trained with empirical\nrisk minimization (ERM) face against such distribution shifts and attributes\ntheir shortcomings to their inability to learn stable discriminative features.\nInvariant learning theory offers a promising solution by encouraging models to\ngenerate stable representations crossing environments that expose the\ninstability of the training set. However, the lack of prior environment labels,\nthe diversity of drift factors, and low-quality representations caused by\ndiverse families make this task challenging. To address these issues, we\npropose TIF, the first temporal invariant training framework for malware\ndetection, which aims to enhance the ability of detectors to learn stable\nrepresentations across time. TIF organizes environments based on application\nobservation dates to reveal temporal drift, integrating specialized multi-proxy\ncontrastive learning and invariant gradient alignment to generate and align\nenvironments with high-quality, stable representations. TIF can be seamlessly\nintegrated into any learning-based detector. Experiments on a decade-long\ndataset show that TIF excels, particularly in early deployment stages,\naddressing real-world needs and outperforming state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xinran Zheng"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Edith C. H. Ngai"
                    },
                    {
                        "name": "Suman Jana"
                    },
                    {
                        "name": "Lorenzo Cavallaro"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cavallaro"
                },
                "author": "Lorenzo Cavallaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13968v2",
                "updated": "2025-02-07T17:13:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    13,
                    32,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-21T01:19:26Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    1,
                    19,
                    26,
                    5,
                    265,
                    0
                ],
                "title": "LADICA: A Large Shared Display Interface for Generative AI Cognitive\n  Assistance in Co-Located Team Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADICA: A Large Shared Display Interface for Generative AI Cognitive\n  Assistance in Co-Located Team Collaboration"
                },
                "summary": "Large shared displays, such as digital whiteboards, are useful for supporting\nco-located team collaborations by helping members perform cognitive tasks such\nas brainstorming, organizing ideas, and making comparisons. While recent\nadvancement in Large Language Models (LLMs) has catalyzed AI support for these\ndisplays, most existing systems either only offer limited capabilities or\ndiminish human control, neglecting the potential benefits of natural group\ndynamics. Our formative study identified cognitive challenges teams encounter,\nsuch as diverse ideation, knowledge sharing, mutual awareness, idea\norganization, and synchronization of live discussions with the external\nworkspace. In response, we introduce LADICA, a large shared display interface\nthat helps collaborative teams brainstorm, organize, and analyze ideas through\nmultiple analytical lenses, while fostering mutual awareness of ideas and\nconcepts. Furthermore, LADICA facilitates the real-time extraction of key\ninformation from verbal discussions and identifies relevant entities. A lab\nstudy confirmed LADICA's usability and usefulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large shared displays, such as digital whiteboards, are useful for supporting\nco-located team collaborations by helping members perform cognitive tasks such\nas brainstorming, organizing ideas, and making comparisons. While recent\nadvancement in Large Language Models (LLMs) has catalyzed AI support for these\ndisplays, most existing systems either only offer limited capabilities or\ndiminish human control, neglecting the potential benefits of natural group\ndynamics. Our formative study identified cognitive challenges teams encounter,\nsuch as diverse ideation, knowledge sharing, mutual awareness, idea\norganization, and synchronization of live discussions with the external\nworkspace. In response, we introduce LADICA, a large shared display interface\nthat helps collaborative teams brainstorm, organize, and analyze ideas through\nmultiple analytical lenses, while fostering mutual awareness of ideas and\nconcepts. Furthermore, LADICA facilitates the real-time extraction of key\ninformation from verbal discussions and identifies relevant entities. A lab\nstudy confirmed LADICA's usability and usefulness."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Weirui Peng"
                    },
                    {
                        "name": "Xinyue Chen"
                    },
                    {
                        "name": "Luke Cao"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Toby Jia-Jun Li"
                },
                "author": "Toby Jia-Jun Li",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05092v1",
                "updated": "2025-02-07T17:11:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    11,
                    23,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:11:23Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    11,
                    23,
                    4,
                    38,
                    0
                ],
                "title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs"
                },
                "summary": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05091v1",
                "updated": "2025-02-07T17:10:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    10,
                    22,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:10:22Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    10,
                    22,
                    4,
                    38,
                    0
                ],
                "title": "DCFormer: Efficient 3D Vision-Language Modeling with Decomposed\n  Convolutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCFormer: Efficient 3D Vision-Language Modeling with Decomposed\n  Convolutions"
                },
                "summary": "Vision-language models (VLMs) align visual and textual representations,\nenabling high-performance zero-shot classification and image-text retrieval in\n2D medical imaging. However, extending VLMs to 3D medical imaging remains\ncomputationally challenging. Existing 3D VLMs rely on Vision Transformers\n(ViTs), which are computationally expensive due to self-attention's quadratic\ncomplexity, or 3D convolutions, which demand excessive parameters and FLOPs as\nkernel size increases. We introduce DCFormer, an efficient 3D medical image\nencoder that factorizes 3D convolutions into three parallel 1D convolutions\nalong depth, height, and width. This design preserves spatial information while\nsignificantly reducing computational cost. Integrated into a CLIP-based\nvision-language framework, DCFormer is evaluated on CT-RATE, a dataset of\n50,188 paired 3D chest CT volumes and radiology reports, for zero-shot\nmulti-abnormality detection across 18 pathologies. Compared to ViT, ConvNeXt,\nPoolFormer, and TransUNet, DCFormer achieves superior efficiency and accuracy,\nwith DCFormer-Tiny reaching 62.0% accuracy and a 46.3% F1-score while using\nsignificantly fewer parameters. These results highlight DCFormer's potential\nfor scalable, clinically deployable 3D medical VLMs. Our codes will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) align visual and textual representations,\nenabling high-performance zero-shot classification and image-text retrieval in\n2D medical imaging. However, extending VLMs to 3D medical imaging remains\ncomputationally challenging. Existing 3D VLMs rely on Vision Transformers\n(ViTs), which are computationally expensive due to self-attention's quadratic\ncomplexity, or 3D convolutions, which demand excessive parameters and FLOPs as\nkernel size increases. We introduce DCFormer, an efficient 3D medical image\nencoder that factorizes 3D convolutions into three parallel 1D convolutions\nalong depth, height, and width. This design preserves spatial information while\nsignificantly reducing computational cost. Integrated into a CLIP-based\nvision-language framework, DCFormer is evaluated on CT-RATE, a dataset of\n50,188 paired 3D chest CT volumes and radiology reports, for zero-shot\nmulti-abnormality detection across 18 pathologies. Compared to ViT, ConvNeXt,\nPoolFormer, and TransUNet, DCFormer achieves superior efficiency and accuracy,\nwith DCFormer-Tiny reaching 62.0% accuracy and a 46.3% F1-score while using\nsignificantly fewer parameters. These results highlight DCFormer's potential\nfor scalable, clinically deployable 3D medical VLMs. Our codes will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Gorkem Can Ates"
                    },
                    {
                        "name": "Kuang Gong"
                    },
                    {
                        "name": "Wei Shao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Shao"
                },
                "author": "Wei Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05087v1",
                "updated": "2025-02-07T17:04:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    4,
                    39,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T17:04:39Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    4,
                    39,
                    4,
                    38,
                    0
                ],
                "title": "Mitigating Unintended Memorization with LoRA in Federated Learning for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Unintended Memorization with LoRA in Federated Learning for\n  LLMs"
                },
                "summary": "Federated learning (FL) is a popular paradigm for collaborative training\nwhich avoids direct data exposure between clients. However, data privacy issues\nstill remain: FL-trained large language models are capable of memorizing and\ncompleting phrases and sentences contained in training data when given with\ntheir prefixes. Thus, it is possible for adversarial and honest-but-curious\nclients to recover training data of other participants simply through targeted\nprompting. In this work, we demonstrate that a popular and simple fine-tuning\nstrategy, low-rank adaptation (LoRA), reduces memorization during FL up to a\nfactor of 10. We study this effect by performing a medical question-answering\nfine-tuning task and injecting multiple replicas of out-of-distribution\nsensitive sequences drawn from an external clinical dataset. We observe a\nreduction in memorization for a wide variety of Llama 2 and 3 models, and find\nthat LoRA can reduce memorization in centralized learning as well. Furthermore,\nwe show that LoRA can be combined with other privacy-preserving techniques such\nas gradient clipping and Gaussian noising, secure aggregation, and Goldfish\nloss to further improve record-level privacy while maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a popular paradigm for collaborative training\nwhich avoids direct data exposure between clients. However, data privacy issues\nstill remain: FL-trained large language models are capable of memorizing and\ncompleting phrases and sentences contained in training data when given with\ntheir prefixes. Thus, it is possible for adversarial and honest-but-curious\nclients to recover training data of other participants simply through targeted\nprompting. In this work, we demonstrate that a popular and simple fine-tuning\nstrategy, low-rank adaptation (LoRA), reduces memorization during FL up to a\nfactor of 10. We study this effect by performing a medical question-answering\nfine-tuning task and injecting multiple replicas of out-of-distribution\nsensitive sequences drawn from an external clinical dataset. We observe a\nreduction in memorization for a wide variety of Llama 2 and 3 models, and find\nthat LoRA can reduce memorization in centralized learning as well. Furthermore,\nwe show that LoRA can be combined with other privacy-preserving techniques such\nas gradient clipping and Gaussian noising, secure aggregation, and Goldfish\nloss to further improve record-level privacy while maintaining performance."
                },
                "authors": [
                    {
                        "name": "Thierry Bossy"
                    },
                    {
                        "name": "Julien Vignoud"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Juan R. Troncoso Pastoriza"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05084v1",
                "updated": "2025-02-07T16:59:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    59,
                    34,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:59:34Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    59,
                    34,
                    4,
                    38,
                    0
                ],
                "title": "ChallengeMe: An Adversarial Learning-enabled Text Summarization\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChallengeMe: An Adversarial Learning-enabled Text Summarization\n  Framework"
                },
                "summary": "The astonishing performance of large language models (LLMs) and their\nremarkable achievements in production and daily life have led to their\nwidespread application in collaborative tasks. However, current large models\nface challenges such as hallucination and lack of specificity in content\ngeneration in vertical domain tasks. Inspired by the contrast and\nclassification mechanisms in human cognitive processes, this paper constructs\nan adversarial learning-based prompt framework named ChallengeMe, which\nincludes three cascaded solutions: generation prompts, evaluation prompts, and\nfeedback optimization. In this process, we designed seven core optimization\ndimensions and set the threshold for adversarial learning. The results of mixed\ncase studies on the text summarization task show that the proposed framework\ncan generate more accurate and fluent text summaries compared to the current\nadvanced mainstream LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The astonishing performance of large language models (LLMs) and their\nremarkable achievements in production and daily life have led to their\nwidespread application in collaborative tasks. However, current large models\nface challenges such as hallucination and lack of specificity in content\ngeneration in vertical domain tasks. Inspired by the contrast and\nclassification mechanisms in human cognitive processes, this paper constructs\nan adversarial learning-based prompt framework named ChallengeMe, which\nincludes three cascaded solutions: generation prompts, evaluation prompts, and\nfeedback optimization. In this process, we designed seven core optimization\ndimensions and set the threshold for adversarial learning. The results of mixed\ncase studies on the text summarization task show that the proposed framework\ncan generate more accurate and fluent text summaries compared to the current\nadvanced mainstream LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Deng"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Tianmin Guo"
                    },
                    {
                        "name": "Yongzhe Zhang"
                    },
                    {
                        "name": "Zhengjian Kang"
                    },
                    {
                        "name": "Hang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hang Yang"
                },
                "author": "Hang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05078v1",
                "updated": "2025-02-07T16:54:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    54,
                    19,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:54:19Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    54,
                    19,
                    4,
                    38,
                    0
                ],
                "title": "Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain,\n  Tree, and Graph Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain,\n  Tree, and Graph Structures"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet their performance is highly dependent on the prompting\nstrategy and model scale. While reinforcement learning and fine-tuning have\nbeen deployed to boost reasoning, these approaches incur substantial\ncomputational and data overhead. In this work, we introduce Adaptive Graph of\nThoughts (AGoT), a dynamic, graph-based inference framework that enhances LLM\nreasoning solely at test time. Rather than relying on fixed-step methods like\nChain of Thought (CoT) or Tree of Thoughts (ToT), AGoT recursively decomposes\ncomplex queries into structured subproblems, forming an dynamic directed\nacyclic graph (DAG) of interdependent reasoning steps. By selectively expanding\nonly those subproblems that require further analysis, AGoT unifies the\nstrengths of chain, tree, and graph paradigms into a cohesive framework that\nallocates computation where it is most needed. We validate our approach on\ndiverse benchmarks spanning multi-hop retrieval, scientific reasoning, and\nmathematical problem-solving, achieving up to 46.2% improvement on scientific\nreasoning tasks (GPQA) - comparable to gains achieved through computationally\nintensive reinforcement learning approaches and outperforming state-of-the-art\niterative approaches. These results suggest that dynamic decomposition and\nstructured recursion offer a scalable, cost-effective alternative to\npost-training modifications, paving the way for more robust, general-purpose\nreasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet their performance is highly dependent on the prompting\nstrategy and model scale. While reinforcement learning and fine-tuning have\nbeen deployed to boost reasoning, these approaches incur substantial\ncomputational and data overhead. In this work, we introduce Adaptive Graph of\nThoughts (AGoT), a dynamic, graph-based inference framework that enhances LLM\nreasoning solely at test time. Rather than relying on fixed-step methods like\nChain of Thought (CoT) or Tree of Thoughts (ToT), AGoT recursively decomposes\ncomplex queries into structured subproblems, forming an dynamic directed\nacyclic graph (DAG) of interdependent reasoning steps. By selectively expanding\nonly those subproblems that require further analysis, AGoT unifies the\nstrengths of chain, tree, and graph paradigms into a cohesive framework that\nallocates computation where it is most needed. We validate our approach on\ndiverse benchmarks spanning multi-hop retrieval, scientific reasoning, and\nmathematical problem-solving, achieving up to 46.2% improvement on scientific\nreasoning tasks (GPQA) - comparable to gains achieved through computationally\nintensive reinforcement learning approaches and outperforming state-of-the-art\niterative approaches. These results suggest that dynamic decomposition and\nstructured recursion offer a scalable, cost-effective alternative to\npost-training modifications, paving the way for more robust, general-purpose\nreasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Tushar Pandey"
                    },
                    {
                        "name": "Ara Ghukasyan"
                    },
                    {
                        "name": "Oktay Goktas"
                    },
                    {
                        "name": "Santosh Kumar Radha"
                    }
                ],
                "author_detail": {
                    "name": "Santosh Kumar Radha"
                },
                "author": "Santosh Kumar Radha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13757v2",
                "updated": "2025-02-07T16:24:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    24,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-11-21T00:01:51Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    1,
                    51,
                    3,
                    326,
                    0
                ],
                "title": "GenBFA: An Evolutionary Optimization Approach to Bit-Flip Attacks on\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenBFA: An Evolutionary Optimization Approach to Bit-Flip Attacks on\n  LLMs"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP), excelling in tasks like text generation and summarization. However,\ntheir increasing adoption in mission-critical applications raises concerns\nabout hardware-based threats, particularly bit-flip attacks (BFAs). BFAs,\nenabled by fault injection methods such as Rowhammer, target model parameters\nin memory, compromising both integrity and performance. Identifying critical\nparameters for BFAs in the vast parameter space of LLMs poses significant\nchallenges. While prior research suggests transformer-based architectures are\ninherently more robust to BFAs compared to traditional deep neural networks, we\nchallenge this assumption. For the first time, we demonstrate that as few as\nthree bit-flips can cause catastrophic performance degradation in an LLM with\nbillions of parameters. Current BFA techniques are inadequate for exploiting\nthis vulnerability due to the difficulty of efficiently identifying critical\nparameters within the immense parameter space. To address this, we propose\nAttentionBreaker, a novel framework tailored for LLMs that enables efficient\ntraversal of the parameter space to identify critical parameters. Additionally,\nwe introduce GenBFA, an evolutionary optimization strategy designed to refine\nthe search further, isolating the most critical bits for an efficient and\neffective attack. Empirical results reveal the profound vulnerability of LLMs\nto AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of\ntotal parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result\nin a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to\n0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings\nunderscore the effectiveness of AttentionBreaker in uncovering and exploiting\ncritical vulnerabilities within LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP), excelling in tasks like text generation and summarization. However,\ntheir increasing adoption in mission-critical applications raises concerns\nabout hardware-based threats, particularly bit-flip attacks (BFAs). BFAs,\nenabled by fault injection methods such as Rowhammer, target model parameters\nin memory, compromising both integrity and performance. Identifying critical\nparameters for BFAs in the vast parameter space of LLMs poses significant\nchallenges. While prior research suggests transformer-based architectures are\ninherently more robust to BFAs compared to traditional deep neural networks, we\nchallenge this assumption. For the first time, we demonstrate that as few as\nthree bit-flips can cause catastrophic performance degradation in an LLM with\nbillions of parameters. Current BFA techniques are inadequate for exploiting\nthis vulnerability due to the difficulty of efficiently identifying critical\nparameters within the immense parameter space. To address this, we propose\nAttentionBreaker, a novel framework tailored for LLMs that enables efficient\ntraversal of the parameter space to identify critical parameters. Additionally,\nwe introduce GenBFA, an evolutionary optimization strategy designed to refine\nthe search further, isolating the most critical bits for an efficient and\neffective attack. Empirical results reveal the profound vulnerability of LLMs\nto AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of\ntotal parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result\nin a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to\n0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings\nunderscore the effectiveness of AttentionBreaker in uncovering and exploiting\ncritical vulnerabilities within LLM architectures."
                },
                "authors": [
                    {
                        "name": "Sanjay Das"
                    },
                    {
                        "name": "Swastik Bhattacharya"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Shamik Kundu"
                    },
                    {
                        "name": "Anand Menon"
                    },
                    {
                        "name": "Arnab Raha"
                    },
                    {
                        "name": "Kanad Basu"
                    }
                ],
                "author_detail": {
                    "name": "Kanad Basu"
                },
                "author": "Kanad Basu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11613v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11613v4",
                "updated": "2025-02-07T16:18:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    18,
                    20,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-20T17:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems"
                },
                "summary": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Robino"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Robino"
                },
                "author": "Giorgio Robino",
                "arxiv_comment": "Figure 1 substituted. Added smolagents subsection in Other Works.\n  Minor format revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11613v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11613v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05043v1",
                "updated": "2025-02-07T16:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:09:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "EcoServe: Designing Carbon-Aware AI Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Designing Carbon-Aware AI Inference Systems"
                },
                "summary": "The rapid increase in LLM ubiquity and scale levies unprecedented demands on\ncomputing infrastructure. These demands not only incur large compute and memory\nresources, but also significant energy, yielding large operational and embodied\ncarbon emissions. In this work, we present two main observations. First, while\nGPUs dominate operational carbon, host processing systems (e.g., CPUs, memory,\nstorage) dominate embodied carbon. Second, based on traces from production\ndeployment of two Generative AI services in the cloud, offline, batch-inference\naccounts for a significant portion (up to 55\\%) of serving capacity. We propose\nfour pillars of carbon-conscious infrastructure design for LLM serving systems:\n\\textbf{\\textit{Reduce, Reuse, Rightsize, and Recycle}}. We demonstrate that\nEcoServe can lower carbon emissions by up to 47\\%, compared to performance,\nenergy, and cost-optimized design points, while maintaining performance targets\nand SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in LLM ubiquity and scale levies unprecedented demands on\ncomputing infrastructure. These demands not only incur large compute and memory\nresources, but also significant energy, yielding large operational and embodied\ncarbon emissions. In this work, we present two main observations. First, while\nGPUs dominate operational carbon, host processing systems (e.g., CPUs, memory,\nstorage) dominate embodied carbon. Second, based on traces from production\ndeployment of two Generative AI services in the cloud, offline, batch-inference\naccounts for a significant portion (up to 55\\%) of serving capacity. We propose\nfour pillars of carbon-conscious infrastructure design for LLM serving systems:\n\\textbf{\\textit{Reduce, Reuse, Rightsize, and Recycle}}. We demonstrate that\nEcoServe can lower carbon emissions by up to 47\\%, compared to performance,\nenergy, and cost-optimized design points, while maintaining performance targets\nand SLOs."
                },
                "authors": [
                    {
                        "name": "Yueying"
                    },
                    {
                        "name": "Li"
                    },
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Rodrigo Fonseca"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "arxiv_affiliation": "Lisa",
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05036v1",
                "updated": "2025-02-07T16:03:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    3,
                    8,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T16:03:08Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    3,
                    8,
                    4,
                    38,
                    0
                ],
                "title": "nvAgent: Automated Data Visualization from Natural Language via\n  Collaborative Agent Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "nvAgent: Automated Data Visualization from Natural Language via\n  Collaborative Agent Workflow"
                },
                "summary": "Natural Language to Visualization (NL2Vis) seeks to convert natural-language\ndescriptions into visual representations of given tables, empowering users to\nderive insights from large-scale data. Recent advancements in Large Language\nModels (LLMs) show promise in automating code generation to transform tabular\ndata into accessible visualizations. However, they often struggle with complex\nqueries that require reasoning across multiple tables. To address this\nlimitation, we propose a collaborative agent workflow, termed nvAgent, for\nNL2Vis. Specifically, nvAgent comprises three agents: a processor agent for\ndatabase processing and context filtering, a composer agent for planning\nvisualization generation, and a validator agent for code translation and output\nverification. Comprehensive evaluations on the new VisEval benchmark\ndemonstrate that nvAgent consistently surpasses state-of-the-art baselines,\nachieving a 7.88% improvement in single-table and a 9.23% improvement in\nmulti-table scenarios. Qualitative analyses further highlight that nvAgent\nmaintains nearly a 20% performance margin over previous models, underscoring\nits capacity to produce high-quality visual representations from complex,\nheterogeneous data sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to Visualization (NL2Vis) seeks to convert natural-language\ndescriptions into visual representations of given tables, empowering users to\nderive insights from large-scale data. Recent advancements in Large Language\nModels (LLMs) show promise in automating code generation to transform tabular\ndata into accessible visualizations. However, they often struggle with complex\nqueries that require reasoning across multiple tables. To address this\nlimitation, we propose a collaborative agent workflow, termed nvAgent, for\nNL2Vis. Specifically, nvAgent comprises three agents: a processor agent for\ndatabase processing and context filtering, a composer agent for planning\nvisualization generation, and a validator agent for code translation and output\nverification. Comprehensive evaluations on the new VisEval benchmark\ndemonstrate that nvAgent consistently surpasses state-of-the-art baselines,\nachieving a 7.88% improvement in single-table and a 9.23% improvement in\nmulti-table scenarios. Qualitative analyses further highlight that nvAgent\nmaintains nearly a 20% performance margin over previous models, underscoring\nits capacity to produce high-quality visual representations from complex,\nheterogeneous data sources."
                },
                "authors": [
                    {
                        "name": "Geliang Ouyang"
                    },
                    {
                        "name": "Jingyao Chen"
                    },
                    {
                        "name": "Zhihe Nie"
                    },
                    {
                        "name": "Yi Gui"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Dongping Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dongping Chen"
                },
                "author": "Dongping Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09508v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09508v3",
                "updated": "2025-02-07T15:49:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    49,
                    58,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-12T12:10:14Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    12,
                    10,
                    14,
                    5,
                    286,
                    0
                ],
                "title": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing"
                },
                "summary": "Collaborative learning of large language models (LLMs) has emerged as a new\nparadigm for utilizing private data from different parties to guarantee\nefficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also\ngarnered increased attention due to its ability to manipulate the behaviors of\nLLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits\nof multiple parties are aggregated in a privacy-preserving and continual\nmanner) unexamined. To this end, this manuscript dives into the first\ninvestigation of collaborative KE, in which we start by carefully identifying\nthe unique three challenges therein, including knowledge overlap, knowledge\nconflict, and knowledge forgetting. We then propose a non-destructive\ncollaborative KE framework, COLLABEDIT, which employs a novel model merging\nmechanism to mimic the global KE behavior while preventing the severe\nperformance drop. Extensive experiments on two canonical datasets demonstrate\nthe superiority of COLLABEDIT compared to other destructive baselines, and\nresults shed light on addressing three collaborative KE challenges and future\napplications. Our code is available at https://github.com/LINs-lab/CollabEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative learning of large language models (LLMs) has emerged as a new\nparadigm for utilizing private data from different parties to guarantee\nefficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also\ngarnered increased attention due to its ability to manipulate the behaviors of\nLLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits\nof multiple parties are aggregated in a privacy-preserving and continual\nmanner) unexamined. To this end, this manuscript dives into the first\ninvestigation of collaborative KE, in which we start by carefully identifying\nthe unique three challenges therein, including knowledge overlap, knowledge\nconflict, and knowledge forgetting. We then propose a non-destructive\ncollaborative KE framework, COLLABEDIT, which employs a novel model merging\nmechanism to mimic the global KE behavior while preventing the severe\nperformance drop. Extensive experiments on two canonical datasets demonstrate\nthe superiority of COLLABEDIT compared to other destructive baselines, and\nresults shed light on addressing three collaborative KE challenges and future\napplications. Our code is available at https://github.com/LINs-lab/CollabEdit."
                },
                "authors": [
                    {
                        "name": "Jiamu Zheng"
                    },
                    {
                        "name": "Jinghuai Zhang"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "20 pages, 11 figures. Published as a conference paper at ICLR 2025.\n  Code at https://github.com/LINs-lab/CollabEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09508v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09508v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05003v1",
                "updated": "2025-02-07T15:23:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    23,
                    34,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T15:23:34Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    23,
                    34,
                    4,
                    38,
                    0
                ],
                "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations"
                },
                "summary": "One approach to reducing the massive costs of large language models (LLMs) is\nthe use of quantized or sparse representations for training or deployment.\nWhile post-training compression methods are very popular, the question of\nobtaining even more accurate compressed models by directly training over such\nrepresentations, i.e., Quantization-Aware Training (QAT), is still open: for\nexample, a recent study (arXiv:2411.04330v2) put the \"optimal\" bit-width at\nwhich models can be trained using QAT, while staying accuracy-competitive with\nstandard FP16/BF16 precision, at 8-bits weights and activations.\n  We advance this state-of-the-art via a new method called QuEST, which is\nPareto-competitive with FP16, i.e., it provides better accuracy at lower model\nsize, while training models with weights and activations in 4-bits or less.\nMoreover, QuEST allows stable training with 1-bit weights and activations.\nQuEST achieves this by improving two key aspects of QAT methods: (1) accurate\nand fast quantization of the (continuous) distributions of weights and\nactivations via Hadamard normalization and MSE-optimal fitting; (2) a new trust\ngradient estimator based on the idea of explicitly minimizing the error between\nthe noisy gradient computed over quantized states and the \"true\" (but unknown)\nfull-precision gradient. Experiments on Llama-type architectures show that\nQuEST induces stable scaling laws across the entire range of hardware-supported\nprecisions, and can be extended to sparse representations. We provide GPU\nkernel support showing that models produced by QuEST can be executed\nefficiently. Our code is available at https://github.com/IST-DASLab/QuEST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One approach to reducing the massive costs of large language models (LLMs) is\nthe use of quantized or sparse representations for training or deployment.\nWhile post-training compression methods are very popular, the question of\nobtaining even more accurate compressed models by directly training over such\nrepresentations, i.e., Quantization-Aware Training (QAT), is still open: for\nexample, a recent study (arXiv:2411.04330v2) put the \"optimal\" bit-width at\nwhich models can be trained using QAT, while staying accuracy-competitive with\nstandard FP16/BF16 precision, at 8-bits weights and activations.\n  We advance this state-of-the-art via a new method called QuEST, which is\nPareto-competitive with FP16, i.e., it provides better accuracy at lower model\nsize, while training models with weights and activations in 4-bits or less.\nMoreover, QuEST allows stable training with 1-bit weights and activations.\nQuEST achieves this by improving two key aspects of QAT methods: (1) accurate\nand fast quantization of the (continuous) distributions of weights and\nactivations via Hadamard normalization and MSE-optimal fitting; (2) a new trust\ngradient estimator based on the idea of explicitly minimizing the error between\nthe noisy gradient computed over quantized states and the \"true\" (but unknown)\nfull-precision gradient. Experiments on Llama-type architectures show that\nQuEST induces stable scaling laws across the entire range of hardware-supported\nprecisions, and can be extended to sparse representations. We provide GPU\nkernel support showing that models produced by QuEST can be executed\nefficiently. Our code is available at https://github.com/IST-DASLab/QuEST."
                },
                "authors": [
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04997v1",
                "updated": "2025-02-07T15:19:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    19,
                    40,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T15:19:40Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    19,
                    40,
                    4,
                    38,
                    0
                ],
                "title": "Aligning Black-box Language Models with Human Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Black-box Language Models with Human Judgments"
                },
                "summary": "Large language models (LLMs) are increasingly used as automated judges to\nevaluate recommendation systems, search engines, and other subjective tasks,\nwhere relying on human evaluators can be costly, time-consuming, and\nunscalable. LLMs offer an efficient solution for continuous, automated\nevaluation. However, since the systems that are built and improved with these\njudgments are ultimately designed for human use, it is crucial that LLM\njudgments align closely with human evaluators to ensure such systems remain\nhuman-centered. On the other hand, aligning LLM judgments with human evaluators\nis challenging due to individual variability and biases in human judgments. We\npropose a simple yet effective framework to align LLM judgments with individual\nhuman evaluators or their aggregated judgments, without retraining or\nfine-tuning the LLM. Our approach learns a linear mapping between the LLM's\noutputs and human judgments, achieving over 142% average improvement in\nagreement across 29 tasks with only a small number of calibration examples used\nfor training. Notably, our method works in zero-shot and few-shot settings,\nexceeds inter-human agreement on four out of six tasks, and enables smaller\nLLMs to achieve performance comparable to that of larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used as automated judges to\nevaluate recommendation systems, search engines, and other subjective tasks,\nwhere relying on human evaluators can be costly, time-consuming, and\nunscalable. LLMs offer an efficient solution for continuous, automated\nevaluation. However, since the systems that are built and improved with these\njudgments are ultimately designed for human use, it is crucial that LLM\njudgments align closely with human evaluators to ensure such systems remain\nhuman-centered. On the other hand, aligning LLM judgments with human evaluators\nis challenging due to individual variability and biases in human judgments. We\npropose a simple yet effective framework to align LLM judgments with individual\nhuman evaluators or their aggregated judgments, without retraining or\nfine-tuning the LLM. Our approach learns a linear mapping between the LLM's\noutputs and human judgments, achieving over 142% average improvement in\nagreement across 29 tasks with only a small number of calibration examples used\nfor training. Notably, our method works in zero-shot and few-shot settings,\nexceeds inter-human agreement on four out of six tasks, and enables smaller\nLLMs to achieve performance comparable to that of larger models."
                },
                "authors": [
                    {
                        "name": "Gerrit J. J. van den Burg"
                    },
                    {
                        "name": "Gen Suzuki"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Murat Sensoy"
                    }
                ],
                "author_detail": {
                    "name": "Murat Sensoy"
                },
                "author": "Murat Sensoy",
                "arxiv_comment": "Accepted for publication at NAACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04983v1",
                "updated": "2025-02-07T15:03:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    3,
                    8,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T15:03:08Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    3,
                    8,
                    4,
                    38,
                    0
                ],
                "title": "MoGraphGPT: Creating Interactive Scenes Using Modular LLM and Graphical\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoGraphGPT: Creating Interactive Scenes Using Modular LLM and Graphical\n  Control"
                },
                "summary": "Creating interactive scenes often involves complex programming tasks.\nAlthough large language models (LLMs) like ChatGPT can generate code from\nnatural language, their output is often error-prone, particularly when\nscripting interactions among multiple elements. The linear conversational\nstructure limits the editing of individual elements, and lacking graphical and\nprecise control complicates visual integration. To address these issues, we\nintegrate an element-level modularization technique that processes textual\ndescriptions for individual elements through separate LLM modules, with a\ncentral module managing interactions among elements. This modular approach\nallows for refining each element independently. We design a graphical user\ninterface, MoGraphGPT , which combines modular LLMs with enhanced graphical\ncontrol to generate codes for 2D interactive scenes. It enables direct\nintegration of graphical information and offers quick, precise control through\nautomatically generated sliders. Our comparative evaluation against an AI\ncoding tool, Cursor Composer, as the baseline system and a usability study show\nMoGraphGPT significantly improves easiness, controllability, and refinement in\ncreating complex 2D interactive scenes with multiple visual elements in a\ncoding-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating interactive scenes often involves complex programming tasks.\nAlthough large language models (LLMs) like ChatGPT can generate code from\nnatural language, their output is often error-prone, particularly when\nscripting interactions among multiple elements. The linear conversational\nstructure limits the editing of individual elements, and lacking graphical and\nprecise control complicates visual integration. To address these issues, we\nintegrate an element-level modularization technique that processes textual\ndescriptions for individual elements through separate LLM modules, with a\ncentral module managing interactions among elements. This modular approach\nallows for refining each element independently. We design a graphical user\ninterface, MoGraphGPT , which combines modular LLMs with enhanced graphical\ncontrol to generate codes for 2D interactive scenes. It enables direct\nintegration of graphical information and offers quick, precise control through\nautomatically generated sliders. Our comparative evaluation against an AI\ncoding tool, Cursor Composer, as the baseline system and a usability study show\nMoGraphGPT significantly improves easiness, controllability, and refinement in\ncreating complex 2D interactive scenes with multiple visual elements in a\ncoding-free manner."
                },
                "authors": [
                    {
                        "name": "Hui Ye"
                    },
                    {
                        "name": "Chufeng Xiao"
                    },
                    {
                        "name": "Jiaye Leng"
                    },
                    {
                        "name": "Pengfei Xu"
                    },
                    {
                        "name": "Hongbo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Hongbo Fu"
                },
                "author": "Hongbo Fu",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04964v1",
                "updated": "2025-02-07T14:30:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    30,
                    12,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T14:30:12Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    30,
                    12,
                    4,
                    38,
                    0
                ],
                "title": "CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs"
                },
                "summary": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches."
                },
                "authors": [
                    {
                        "name": "Roman Vashurin"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Maxim Panov"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Panov"
                },
                "arxiv_affiliation": "Mohamed bin Zayed University of Artificial Intelligence",
                "author": "Maxim Panov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04955v1",
                "updated": "2025-02-07T14:20:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    20,
                    45,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T14:20:45Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    20,
                    45,
                    4,
                    38,
                    0
                ],
                "title": "Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics"
                },
                "summary": "In this paper, we explore the problem of Claim Extraction using one-to-many\ntext generation methods, comparing LLMs, small summarization models finetuned\nfor the task, and a previous NER-centric baseline QACG. As the current\npublications on Claim Extraction, Fact Extraction, Claim Generation and\nCheck-worthy Claim Detection are quite scattered in their means and\nterminology, we compile their common objectives, releasing the FEVERFact\ndataset, with 17K atomic factual claims extracted from 4K contextualised\nWikipedia sentences, adapted from the original FEVER. We compile the known\nobjectives into an Evaluation framework of: Atomicity, Fluency,\nDecontextualization, Faithfulness checked for each generated claim separately,\nand Focus and Coverage measured against the full set of predicted claims for a\nsingle input. For each metric, we implement a scale using a reduction to an\nalready-explored NLP task. We validate our metrics against human grading of\ngeneric claims, to see that the model ranking on $F_{fact}$, our hardest\nmetric, did not change and the evaluation framework approximates human grading\nvery closely in terms of $F_1$ and RMSE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the problem of Claim Extraction using one-to-many\ntext generation methods, comparing LLMs, small summarization models finetuned\nfor the task, and a previous NER-centric baseline QACG. As the current\npublications on Claim Extraction, Fact Extraction, Claim Generation and\nCheck-worthy Claim Detection are quite scattered in their means and\nterminology, we compile their common objectives, releasing the FEVERFact\ndataset, with 17K atomic factual claims extracted from 4K contextualised\nWikipedia sentences, adapted from the original FEVER. We compile the known\nobjectives into an Evaluation framework of: Atomicity, Fluency,\nDecontextualization, Faithfulness checked for each generated claim separately,\nand Focus and Coverage measured against the full set of predicted claims for a\nsingle input. For each metric, we implement a scale using a reduction to an\nalready-explored NLP task. We validate our metrics against human grading of\ngeneric claims, to see that the model ranking on $F_{fact}$, our hardest\nmetric, did not change and the evaluation framework approximates human grading\nvery closely in terms of $F_1$ and RMSE."
                },
                "authors": [
                    {
                        "name": "Herbert Ullrich"
                    },
                    {
                        "name": "Tomáš Mlynář"
                    },
                    {
                        "name": "Jan Drchal"
                    }
                ],
                "author_detail": {
                    "name": "Jan Drchal"
                },
                "author": "Jan Drchal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04951v1",
                "updated": "2025-02-07T14:15:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    15,
                    46,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T14:15:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    15,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "The Rising Threat to Emerging AI-Powered Search Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rising Threat to Emerging AI-Powered Search Engines"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering\nprecise and efficient responses by integrating external databases with\npre-existing knowledge. However, we observe that these AIPSEs raise risks such\nas quoting malicious content or citing malicious websites, leading to harmful\nor unverified information dissemination. In this study, we conduct the first\nsafety risk quantification on seven production AIPSEs by systematically\ndefining the threat model, risk level, and evaluating responses to various\nquery types. With data collected from PhishTank, ThreatBook, and LevelBlue, our\nfindings reveal that AIPSEs frequently generate harmful content that contains\nmalicious URLs even with benign queries (e.g., with benign keywords). We also\nobserve that directly query URL will increase the risk level while query with\nnatural language will mitigate such risk. We further perform two case studies\non online document spoofing and phishing to show the ease of deceiving AIPSEs\nin the real-world setting. To mitigate these risks, we develop an agent-based\ndefense with a GPT-4o-based content refinement tool and an XGBoost-based URL\ndetector. Our evaluation shows that our defense can effectively reduce the risk\nbut with the cost of reducing available information. Our research highlights\nthe urgent need for robust safety measures in AIPSEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering\nprecise and efficient responses by integrating external databases with\npre-existing knowledge. However, we observe that these AIPSEs raise risks such\nas quoting malicious content or citing malicious websites, leading to harmful\nor unverified information dissemination. In this study, we conduct the first\nsafety risk quantification on seven production AIPSEs by systematically\ndefining the threat model, risk level, and evaluating responses to various\nquery types. With data collected from PhishTank, ThreatBook, and LevelBlue, our\nfindings reveal that AIPSEs frequently generate harmful content that contains\nmalicious URLs even with benign queries (e.g., with benign keywords). We also\nobserve that directly query URL will increase the risk level while query with\nnatural language will mitigate such risk. We further perform two case studies\non online document spoofing and phishing to show the ease of deceiving AIPSEs\nin the real-world setting. To mitigate these risks, we develop an agent-based\ndefense with a GPT-4o-based content refinement tool and an XGBoost-based URL\ndetector. Our evaluation shows that our defense can effectively reduce the risk\nbut with the cost of reducing available information. Our research highlights\nthe urgent need for robust safety measures in AIPSEs."
                },
                "authors": [
                    {
                        "name": "Zeren Luo"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Mingchen Li"
                    },
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18712v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18712v4",
                "updated": "2025-02-07T14:14:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    14,
                    7,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-30T19:15:41Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    19,
                    15,
                    41,
                    3,
                    30,
                    0
                ],
                "title": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps"
                },
                "summary": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts."
                },
                "authors": [
                    {
                        "name": "Devansh Bhardwaj"
                    },
                    {
                        "name": "Naman Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Naman Mishra"
                },
                "author": "Naman Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18712v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18712v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04937v1",
                "updated": "2025-02-07T14:00:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    0,
                    4,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T14:00:04Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    14,
                    0,
                    4,
                    4,
                    38,
                    0
                ],
                "title": "Data-driven Modality Fusion: An AI-enabled Framework for Large-Scale\n  Sensor Network Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Modality Fusion: An AI-enabled Framework for Large-Scale\n  Sensor Network Management"
                },
                "summary": "The development and operation of smart cities relyheavily on large-scale\nInternet-of-Things (IoT) networks and sensor infrastructures that continuously\nmonitor various aspects of urban environments. These networks generate vast\namounts of data, posing challenges related to bandwidth usage, energy\nconsumption, and system scalability. This paper introduces a novel sensing\nparadigm called Data-driven Modality Fusion (DMF), designed to enhance the\nefficiency of smart city IoT network management. By leveraging correlations\nbetween timeseries data from different sensing modalities, the proposed DMF\napproach reduces the number of physical sensors required for monitoring,\nthereby minimizing energy expenditure, communication bandwidth, and overall\ndeployment costs. The framework relocates computational complexity from the\nedge devices to the core, ensuring that resource-constrained IoT devices are\nnot burdened with intensive processing tasks. DMF is validated using data from\na real-world IoT deployment in Madrid, demonstrating the effectiveness of the\nproposed system in accurately estimating traffic, environmental, and pollution\nmetrics from a reduced set of sensors. The proposed solution offers a scalable,\nefficient mechanism for managing urban IoT networks, while addressing issues of\nsensor failure and privacy concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development and operation of smart cities relyheavily on large-scale\nInternet-of-Things (IoT) networks and sensor infrastructures that continuously\nmonitor various aspects of urban environments. These networks generate vast\namounts of data, posing challenges related to bandwidth usage, energy\nconsumption, and system scalability. This paper introduces a novel sensing\nparadigm called Data-driven Modality Fusion (DMF), designed to enhance the\nefficiency of smart city IoT network management. By leveraging correlations\nbetween timeseries data from different sensing modalities, the proposed DMF\napproach reduces the number of physical sensors required for monitoring,\nthereby minimizing energy expenditure, communication bandwidth, and overall\ndeployment costs. The framework relocates computational complexity from the\nedge devices to the core, ensuring that resource-constrained IoT devices are\nnot burdened with intensive processing tasks. DMF is validated using data from\na real-world IoT deployment in Madrid, demonstrating the effectiveness of the\nproposed system in accurately estimating traffic, environmental, and pollution\nmetrics from a reduced set of sensors. The proposed solution offers a scalable,\nefficient mechanism for managing urban IoT networks, while addressing issues of\nsensor failure and privacy concerns."
                },
                "authors": [
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Roberto Minerva"
                    },
                    {
                        "name": "Maira Alvi"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04933v1",
                "updated": "2025-02-07T13:53:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    53,
                    15,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:53:15Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    53,
                    15,
                    4,
                    38,
                    0
                ],
                "title": "Mobile Network-specialized Large Language Models for 6G: Architectures,\n  Innovations, Challenges, and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Network-specialized Large Language Models for 6G: Architectures,\n  Innovations, Challenges, and Future Trends"
                },
                "summary": "Conventional 5G network management mechanisms, that operate in isolated silos\nacross different network segments, will experience significant limitations in\nhandling the unprecedented hyper-complexity and massive scale of the sixth\ngeneration (6G). Holistic intelligence and end-to-end automation are, thus,\npositioned as key enablers of forthcoming 6G networks. The Large Language Model\n(LLM) technology, a major breakthrough in the Generative Artificial\nIntelligence (AI) field, enjoys robust human-like language processing, advanced\ncontextual reasoning and multi-modal capabilities. These features foster a\nholistic understanding of network behavior and an autonomous decision-making.\nThis paper investigates four possible architectural designs for integrated LLM\nand 6G networks, detailing the inherent technical intricacies, the merits and\nthe limitations of each design. As an internal functional building block of\nfuture 6G networks, the LLM will natively benefit from their improved\ndesign-driven security policies from the early design and specification stages.\nAn illustrative scenario of slicing conflicts is used to prove the\neffectiveness of our architectural framework in autonomously dealing with\ncomplicated network anomalies. We finally conclude the paper with an overview\nof the key challenges and the relevant research trends for enabling Mobile\nNetworkspecialized LLMs. This study is intended to provide Mobile Network\nOperators (MNOs) with a comprehensive guidance in their paths towards embracing\nthe LLM technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 5G network management mechanisms, that operate in isolated silos\nacross different network segments, will experience significant limitations in\nhandling the unprecedented hyper-complexity and massive scale of the sixth\ngeneration (6G). Holistic intelligence and end-to-end automation are, thus,\npositioned as key enablers of forthcoming 6G networks. The Large Language Model\n(LLM) technology, a major breakthrough in the Generative Artificial\nIntelligence (AI) field, enjoys robust human-like language processing, advanced\ncontextual reasoning and multi-modal capabilities. These features foster a\nholistic understanding of network behavior and an autonomous decision-making.\nThis paper investigates four possible architectural designs for integrated LLM\nand 6G networks, detailing the inherent technical intricacies, the merits and\nthe limitations of each design. As an internal functional building block of\nfuture 6G networks, the LLM will natively benefit from their improved\ndesign-driven security policies from the early design and specification stages.\nAn illustrative scenario of slicing conflicts is used to prove the\neffectiveness of our architectural framework in autonomously dealing with\ncomplicated network anomalies. We finally conclude the paper with an overview\nof the key challenges and the relevant research trends for enabling Mobile\nNetworkspecialized LLMs. This study is intended to provide Mobile Network\nOperators (MNOs) with a comprehensive guidance in their paths towards embracing\nthe LLM technology."
                },
                "authors": [
                    {
                        "name": "Abdelaali Chaoub"
                    },
                    {
                        "name": "Muslim Elkotob"
                    }
                ],
                "author_detail": {
                    "name": "Muslim Elkotob"
                },
                "author": "Muslim Elkotob",
                "arxiv_comment": "9 pages, 4 figures, 1 table. This paper has been submitted to IEEE\n  for publication. Copyright may change without notice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04931v1",
                "updated": "2025-02-07T13:52:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    52,
                    37,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:52:37Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    52,
                    37,
                    4,
                    38,
                    0
                ],
                "title": "Breaking the News: A LLM-based Game where Players Act as Influencer or\n  Debunker for Raising Awareness About Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the News: A LLM-based Game where Players Act as Influencer or\n  Debunker for Raising Awareness About Misinformation"
                },
                "summary": "Game-based interventions are widely used to combat misinformation online by\nemploying the \"inoculation approach\". However, most current interventions are\ndesigned as single-player games, presenting players with limited predefined\nchoices. Such restrictions reduce replayability and may lead to an overly\nsimplistic understanding of the processes of misinformation phenomenon and the\ndebunking. This study seeks to address these issues, and empower people to\nbetter understand the opinion influencing and misinformation debunking\nprocesses. We did this by creating a Player versus Player (PvP) game where\nparticipants attempt to either generate or debunk misinformation to convince\nLLM-represented public opinion. Using a within-subjects mixed-methods study\ndesign (N=47), we found that this game significantly raised participants' media\nliteracy and improved their ability to identify misinformation. Our qualitative\nexploration revealed how participants' use of debunking and content creation\nstrategies deepened their understanding of the nature of disinformation. We\ndemonstrate how LLMs can be integrated into PvP games to foster greater\nunderstanding of contrasting viewpoints and highlight social challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game-based interventions are widely used to combat misinformation online by\nemploying the \"inoculation approach\". However, most current interventions are\ndesigned as single-player games, presenting players with limited predefined\nchoices. Such restrictions reduce replayability and may lead to an overly\nsimplistic understanding of the processes of misinformation phenomenon and the\ndebunking. This study seeks to address these issues, and empower people to\nbetter understand the opinion influencing and misinformation debunking\nprocesses. We did this by creating a Player versus Player (PvP) game where\nparticipants attempt to either generate or debunk misinformation to convince\nLLM-represented public opinion. Using a within-subjects mixed-methods study\ndesign (N=47), we found that this game significantly raised participants' media\nliteracy and improved their ability to identify misinformation. Our qualitative\nexploration revealed how participants' use of debunking and content creation\nstrategies deepened their understanding of the nature of disinformation. We\ndemonstrate how LLMs can be integrated into PvP games to foster greater\nunderstanding of contrasting viewpoints and highlight social challenges."
                },
                "authors": [
                    {
                        "name": "Huiyun Tang"
                    },
                    {
                        "name": "Songqi Sun"
                    },
                    {
                        "name": "Kexin Nie"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Anastasia Sergeeva"
                    },
                    {
                        "name": "Ray LC"
                    }
                ],
                "author_detail": {
                    "name": "Ray LC"
                },
                "author": "Ray LC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04916v1",
                "updated": "2025-02-07T13:33:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    33,
                    40,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:33:40Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    33,
                    40,
                    4,
                    38,
                    0
                ],
                "title": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification or Prompting: A Case Study on Legal Requirements\n  Traceability"
                },
                "summary": "New regulations are continuously introduced to ensure that software\ndevelopment complies with the ethical concerns and prioritizes public safety. A\nprerequisite for demonstrating compliance involves tracing software\nrequirements to legal provisions. Requirements traceability is a fundamental\ntask where requirements engineers are supposed to analyze technical\nrequirements against target artifacts, often under limited time budget. Doing\nthis analysis manually for complex systems with hundreds of requirements is\ninfeasible. The legal dimension introduces additional challenges that only\nexacerbate manual effort.\n  In this paper, we investigate two automated solutions based on large language\nmodels (LLMs) to predict trace links between requirements and legal provisions.\nThe first solution, Kashif, is a classifier that leverages sentence\ntransformers. The second solution prompts a recent generative LLM based on\nRice, a prompt engineering framework.\n  On a benchmark dataset, we empirically evaluate Kashif and compare it against\na baseline classifier from the literature. Kashif can identify trace links with\nan average recall of ~67%, outperforming the baseline with a substantial gain\nof 54 percentage points (pp) in recall. However, on unseen, more complex\nrequirements documents traced to the European general data protection\nregulation (GDPR), Kashif performs poorly, yielding an average recall of 15%.\nOn the same documents, however, our Rice-based solution yields an average\nrecall of 84%, with a remarkable gain of about 69 pp over Kashif. Our results\nsuggest that requirements traceability in the legal context cannot be simply\naddressed by building classifiers, as such solutions do not generalize and fail\nto perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New regulations are continuously introduced to ensure that software\ndevelopment complies with the ethical concerns and prioritizes public safety. A\nprerequisite for demonstrating compliance involves tracing software\nrequirements to legal provisions. Requirements traceability is a fundamental\ntask where requirements engineers are supposed to analyze technical\nrequirements against target artifacts, often under limited time budget. Doing\nthis analysis manually for complex systems with hundreds of requirements is\ninfeasible. The legal dimension introduces additional challenges that only\nexacerbate manual effort.\n  In this paper, we investigate two automated solutions based on large language\nmodels (LLMs) to predict trace links between requirements and legal provisions.\nThe first solution, Kashif, is a classifier that leverages sentence\ntransformers. The second solution prompts a recent generative LLM based on\nRice, a prompt engineering framework.\n  On a benchmark dataset, we empirically evaluate Kashif and compare it against\na baseline classifier from the literature. Kashif can identify trace links with\nan average recall of ~67%, outperforming the baseline with a substantial gain\nof 54 percentage points (pp) in recall. However, on unseen, more complex\nrequirements documents traced to the European general data protection\nregulation (GDPR), Kashif performs poorly, yielding an average recall of 15%.\nOn the same documents, however, our Rice-based solution yields an average\nrecall of 84%, with a remarkable gain of about 69 pp over Kashif. Our results\nsuggest that requirements traceability in the legal context cannot be simply\naddressed by building classifiers, as such solutions do not generalize and fail\nto perform well on complex regulations and requirements. Resorting to\ngenerative LLMs, with careful prompt engineering, is thus a more promising\nalternative."
                },
                "authors": [
                    {
                        "name": "Romina Etezadi"
                    },
                    {
                        "name": "Sallam Abualhaija"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10198v2",
                "updated": "2025-02-07T13:26:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    26,
                    18,
                    4,
                    38,
                    0
                ],
                "published": "2024-12-13T15:15:24Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    15,
                    24,
                    4,
                    348,
                    0
                ],
                "title": "From Allies to Adversaries: Manipulating LLM Tool-Calling through\n  Adversarial Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Allies to Adversaries: Manipulating LLM Tool-Calling through\n  Adversarial Injection"
                },
                "summary": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems."
                },
                "authors": [
                    {
                        "name": "Haowei Wang"
                    },
                    {
                        "name": "Rupeng Zhang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Yuekai Huang"
                    },
                    {
                        "name": "Dandan Wang"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11633v2",
                "updated": "2025-02-07T12:36:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    36,
                    53,
                    4,
                    38,
                    0
                ],
                "published": "2024-02-18T16:20:43Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    16,
                    20,
                    43,
                    6,
                    49,
                    0
                ],
                "title": "Self-seeding and Multi-intent Self-instructing LLMs for Generating\n  Intent-aware Information-Seeking dialogs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-seeding and Multi-intent Self-instructing LLMs for Generating\n  Intent-aware Information-Seeking dialogs"
                },
                "summary": "Identifying user intents in information-seeking dialogs is crucial for a\nsystem to meet user's information needs. Intent prediction (IP) is challenging\nand demands sufficient dialogs with human-labeled intents for training.\nHowever, manually annotating intents is resource-intensive. While large\nlanguage models (LLMs) have been shown to be effective in generating synthetic\ndata, there is no study on using LLMs to generate intent-aware\ninformation-seeking dialogs. In this paper, we focus on leveraging LLMs for\nzero-shot generation of large-scale, open-domain, and intent-aware\ninformation-seeking dialogs. We propose SOLID, which has novel self-seeding and\nmulti-intent self-instructing schemes. The former improves the generation\nquality by using the LLM's own knowledge scope to initiate dialog generation;\nthe latter prompts the LLM to generate utterances sequentially, and mitigates\nthe need for manual prompt design by asking the LLM to autonomously adapt its\nprompt instruction when generating complex multi-intent utterances.\nFurthermore, we propose SOLID-RL, which is further trained to generate a dialog\nin one step on the data generated by SOLID. We propose a length-based quality\nestimation mechanism to assign varying weights to SOLID-generated dialogs based\non their quality during the training process of SOLID-RL. We use SOLID and\nSOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size\nof existing datasets. Experiments show that IP methods trained on dialogs\ngenerated by SOLID and SOLID-RL achieve better IP quality than ones trained on\nhuman-generated dialogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying user intents in information-seeking dialogs is crucial for a\nsystem to meet user's information needs. Intent prediction (IP) is challenging\nand demands sufficient dialogs with human-labeled intents for training.\nHowever, manually annotating intents is resource-intensive. While large\nlanguage models (LLMs) have been shown to be effective in generating synthetic\ndata, there is no study on using LLMs to generate intent-aware\ninformation-seeking dialogs. In this paper, we focus on leveraging LLMs for\nzero-shot generation of large-scale, open-domain, and intent-aware\ninformation-seeking dialogs. We propose SOLID, which has novel self-seeding and\nmulti-intent self-instructing schemes. The former improves the generation\nquality by using the LLM's own knowledge scope to initiate dialog generation;\nthe latter prompts the LLM to generate utterances sequentially, and mitigates\nthe need for manual prompt design by asking the LLM to autonomously adapt its\nprompt instruction when generating complex multi-intent utterances.\nFurthermore, we propose SOLID-RL, which is further trained to generate a dialog\nin one step on the data generated by SOLID. We propose a length-based quality\nestimation mechanism to assign varying weights to SOLID-generated dialogs based\non their quality during the training process of SOLID-RL. We use SOLID and\nSOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size\nof existing datasets. Experiments show that IP methods trained on dialogs\ngenerated by SOLID and SOLID-RL achieve better IP quality than ones trained on\nhuman-generated dialogs."
                },
                "authors": [
                    {
                        "name": "Arian Askari"
                    },
                    {
                        "name": "Roxana Petcu"
                    },
                    {
                        "name": "Chuan Meng"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Amin Abolghasemi"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04878v1",
                "updated": "2025-02-07T12:33:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    33,
                    8,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T12:33:08Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    33,
                    8,
                    4,
                    38,
                    0
                ],
                "title": "Sparse Autoencoders Do Not Find Canonical Units of Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders Do Not Find Canonical Units of Analysis"
                },
                "summary": "A common goal of mechanistic interpretability is to decompose the activations\nof neural networks into features: interpretable properties of the input\ncomputed by the model. Sparse autoencoders (SAEs) are a popular method for\nfinding these features in LLMs, and it has been postulated that they can be\nused to find a \\textit{canonical} set of units: a unique and complete list of\natomic features. We cast doubt on this belief using two novel techniques: SAE\nstitching to show they are incomplete, and meta-SAEs to show they are not\natomic. SAE stitching involves inserting or swapping latents from a larger SAE\ninto a smaller one. Latents from the larger SAE can be divided into two\ncategories: \\emph{novel latents}, which improve performance when added to the\nsmaller SAE, indicating they capture novel information, and\n\\emph{reconstruction latents}, which can replace corresponding latents in the\nsmaller SAE that have similar behavior. The existence of novel features\nindicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on\nthe decoder matrix of another SAE -- we find that latents in SAEs often\ndecompose into combinations of latents from a smaller SAE, showing that larger\nSAE latents are not atomic. The resulting decompositions are often\ninterpretable; e.g. a latent representing ``Einstein'' decomposes into\n``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find\ncanonical units of analysis, they may still be useful tools. We suggest that\nfuture research should either pursue different approaches for identifying such\nunits, or pragmatically choose the SAE size suited to their task. We provide an\ninteractive dashboard to explore meta-SAEs: https://metasaes.streamlit.app/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common goal of mechanistic interpretability is to decompose the activations\nof neural networks into features: interpretable properties of the input\ncomputed by the model. Sparse autoencoders (SAEs) are a popular method for\nfinding these features in LLMs, and it has been postulated that they can be\nused to find a \\textit{canonical} set of units: a unique and complete list of\natomic features. We cast doubt on this belief using two novel techniques: SAE\nstitching to show they are incomplete, and meta-SAEs to show they are not\natomic. SAE stitching involves inserting or swapping latents from a larger SAE\ninto a smaller one. Latents from the larger SAE can be divided into two\ncategories: \\emph{novel latents}, which improve performance when added to the\nsmaller SAE, indicating they capture novel information, and\n\\emph{reconstruction latents}, which can replace corresponding latents in the\nsmaller SAE that have similar behavior. The existence of novel features\nindicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on\nthe decoder matrix of another SAE -- we find that latents in SAEs often\ndecompose into combinations of latents from a smaller SAE, showing that larger\nSAE latents are not atomic. The resulting decompositions are often\ninterpretable; e.g. a latent representing ``Einstein'' decomposes into\n``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find\ncanonical units of analysis, they may still be useful tools. We suggest that\nfuture research should either pursue different approaches for identifying such\nunits, or pragmatically choose the SAE size suited to their task. We provide an\ninteractive dashboard to explore meta-SAEs: https://metasaes.streamlit.app/"
                },
                "authors": [
                    {
                        "name": "Patrick Leask"
                    },
                    {
                        "name": "Bart Bussmann"
                    },
                    {
                        "name": "Michael Pearce"
                    },
                    {
                        "name": "Joseph Bloom"
                    },
                    {
                        "name": "Curt Tigges"
                    },
                    {
                        "name": "Noura Al Moubayed"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12952v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12952v3",
                "updated": "2025-02-07T12:33:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    33,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2024-06-18T14:54:37Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    54,
                    37,
                    1,
                    170,
                    0
                ],
                "title": "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents"
                },
                "summary": "Rigorous software testing is crucial for developing and maintaining\nhigh-quality code, making automated test generation a promising avenue for both\nimproving software quality and boosting the effectiveness of code generation\nmethods. However, while code generation with Large Language Models (LLMs) is an\nextraordinarily active research area, test generation remains relatively\nunexplored. We address this gap and investigate the capability of LLM-based\nCode Agents to formalize user issues into test cases. To this end, we propose a\nnovel benchmark based on popular GitHub repositories, containing real-world\nissues, ground-truth bug-fixes, and golden tests. We find that LLMs generally\nperform surprisingly well at generating relevant test cases, with Code Agents\ndesigned for code repair exceeding the performance of systems designed\nspecifically for test generation. Further, as test generation is a similar but\nmore structured task than code generation, it allows for a more fine-grained\nanalysis using issue reproduction rate and coverage changes, providing a dual\nmetric for analyzing systems designed for code repair. Finally, we find that\ngenerated tests are an effective filter for proposed code fixes, doubling the\nprecision of SWE-Agent. We release all data and code at\nhttps://github.com/logic-star-ai/SWT-Bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous software testing is crucial for developing and maintaining\nhigh-quality code, making automated test generation a promising avenue for both\nimproving software quality and boosting the effectiveness of code generation\nmethods. However, while code generation with Large Language Models (LLMs) is an\nextraordinarily active research area, test generation remains relatively\nunexplored. We address this gap and investigate the capability of LLM-based\nCode Agents to formalize user issues into test cases. To this end, we propose a\nnovel benchmark based on popular GitHub repositories, containing real-world\nissues, ground-truth bug-fixes, and golden tests. We find that LLMs generally\nperform surprisingly well at generating relevant test cases, with Code Agents\ndesigned for code repair exceeding the performance of systems designed\nspecifically for test generation. Further, as test generation is a similar but\nmore structured task than code generation, it allows for a more fine-grained\nanalysis using issue reproduction rate and coverage changes, providing a dual\nmetric for analyzing systems designed for code repair. Finally, we find that\ngenerated tests are an effective filter for proposed code fixes, doubling the\nprecision of SWE-Agent. We release all data and code at\nhttps://github.com/logic-star-ai/SWT-Bench"
                },
                "authors": [
                    {
                        "name": "Niels Mündler"
                    },
                    {
                        "name": "Mark Niklas Müller"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "20 pages, 14 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12952v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12952v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04877v1",
                "updated": "2025-02-07T12:31:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    31,
                    0,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T12:31:00Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    31,
                    0,
                    4,
                    38,
                    0
                ],
                "title": "Terahertz Integrated Sensing and Communication-Empowered UAVs in 6G: A\n  Transceiver Design Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz Integrated Sensing and Communication-Empowered UAVs in 6G: A\n  Transceiver Design Perspective"
                },
                "summary": "Due to their high maneuverability, flexible deployment, and low cost,\nunmanned aerial vehicles (UAVs) are expected to play a pivotal role in not only\ncommunication, but also sensing. Especially by exploiting the ultra-wide\nbandwidth of terahertz (THz) bands, integrated sensing and communication\n(ISAC)-empowered UAV has been a promising technology of 6G space-air-ground\nintegrated networks. In this article, we systematically investigate the key\ntechniques and essential obstacles for THz-ISAC-empowered UAV from a\ntransceiver design perspective, with the highlight of its major challenges and\nkey technologies. Specifically, we discuss the THz-ISAC-UAV wireless\npropagation environment, based on which several channel characteristics for\ncommunication and sensing are revealed. We point out the transceiver payload\ndesign peculiarities for THz-ISAC-UAV from the perspective of antenna design,\nradio frequency front-end, and baseband signal processing. To deal with the\nspecificities faced by the payload, we shed light on three key technologies,\ni.e., hybrid beamforming for ultra-massive MIMO-ISAC, power-efficient THz-ISAC\nwaveform design, as well as communication and sensing channel state information\nacquisition, and extensively elaborate their concepts and key issues. More\nimportantly, future research directions and associated open problems are\npresented, which may unleash the full potential of THz-ISAC-UAV for 6G wireless\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to their high maneuverability, flexible deployment, and low cost,\nunmanned aerial vehicles (UAVs) are expected to play a pivotal role in not only\ncommunication, but also sensing. Especially by exploiting the ultra-wide\nbandwidth of terahertz (THz) bands, integrated sensing and communication\n(ISAC)-empowered UAV has been a promising technology of 6G space-air-ground\nintegrated networks. In this article, we systematically investigate the key\ntechniques and essential obstacles for THz-ISAC-empowered UAV from a\ntransceiver design perspective, with the highlight of its major challenges and\nkey technologies. Specifically, we discuss the THz-ISAC-UAV wireless\npropagation environment, based on which several channel characteristics for\ncommunication and sensing are revealed. We point out the transceiver payload\ndesign peculiarities for THz-ISAC-UAV from the perspective of antenna design,\nradio frequency front-end, and baseband signal processing. To deal with the\nspecificities faced by the payload, we shed light on three key technologies,\ni.e., hybrid beamforming for ultra-massive MIMO-ISAC, power-efficient THz-ISAC\nwaveform design, as well as communication and sensing channel state information\nacquisition, and extensively elaborate their concepts and key issues. More\nimportantly, future research directions and associated open problems are\npresented, which may unleash the full potential of THz-ISAC-UAV for 6G wireless\nnetworks."
                },
                "authors": [
                    {
                        "name": "Ruoyu Zhang"
                    },
                    {
                        "name": "Wen Wu"
                    },
                    {
                        "name": "Xiaoming Chen"
                    },
                    {
                        "name": "Zhen Gao"
                    },
                    {
                        "name": "Yueming Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yueming Cai"
                },
                "author": "Yueming Cai",
                "arxiv_doi": "10.1109/MVT.2025.3531088",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MVT.2025.3531088",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.04877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Vehicular Technology Magazine, 2025",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00026v2",
                "updated": "2025-02-07T12:23:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    12,
                    23,
                    59,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-21T17:10:52Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    10,
                    52,
                    1,
                    21,
                    0
                ],
                "title": "Pushing the Limits of BFP on Narrow Precision LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of BFP on Narrow Precision LLM Inference"
                },
                "summary": "The substantial computational and memory demands of Large Language Models\n(LLMs) hinder their deployment. Block Floating Point (BFP) has proven effective\nin accelerating linear operations, a cornerstone of LLM workloads. However, as\nsequence lengths grow, nonlinear operations, such as Attention, increasingly\nbecome performance bottlenecks due to their quadratic computational complexity.\nThese nonlinear operations are predominantly executed using inefficient\nfloating-point formats, which renders the system challenging to optimize\nsoftware efficiency and hardware overhead. In this paper, we delve into the\nlimitations and potential of applying BFP to nonlinear operations. Given our\nfindings, we introduce a hardware-software co-design framework (DB-Attn),\nincluding: (i) DBFP, an advanced BFP version, overcomes nonlinear operation\nchallenges with a pivot-focus strategy for diverse data and an adaptive\ngrouping strategy for flexible exponent sharing. (ii) DH-LUT, a novel lookup\ntable algorithm dedicated to accelerating nonlinear operations with DBFP\nformat. (iii) An RTL-level DBFP-based engine is implemented to support DB-Attn,\napplicable to FPGA and ASIC. Results show that DB-Attn provides significant\nperformance improvements with negligible accuracy loss, achieving 74% GPU\nspeedup on Softmax of LLaMA and 10x low overhead performance improvement over\nSOTA designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The substantial computational and memory demands of Large Language Models\n(LLMs) hinder their deployment. Block Floating Point (BFP) has proven effective\nin accelerating linear operations, a cornerstone of LLM workloads. However, as\nsequence lengths grow, nonlinear operations, such as Attention, increasingly\nbecome performance bottlenecks due to their quadratic computational complexity.\nThese nonlinear operations are predominantly executed using inefficient\nfloating-point formats, which renders the system challenging to optimize\nsoftware efficiency and hardware overhead. In this paper, we delve into the\nlimitations and potential of applying BFP to nonlinear operations. Given our\nfindings, we introduce a hardware-software co-design framework (DB-Attn),\nincluding: (i) DBFP, an advanced BFP version, overcomes nonlinear operation\nchallenges with a pivot-focus strategy for diverse data and an adaptive\ngrouping strategy for flexible exponent sharing. (ii) DH-LUT, a novel lookup\ntable algorithm dedicated to accelerating nonlinear operations with DBFP\nformat. (iii) An RTL-level DBFP-based engine is implemented to support DB-Attn,\napplicable to FPGA and ASIC. Results show that DB-Attn provides significant\nperformance improvements with negligible accuracy loss, achieving 74% GPU\nspeedup on Softmax of LLaMA and 10x low overhead performance improvement over\nSOTA designs."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13203v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13203v3",
                "updated": "2025-02-07T11:45:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    45,
                    11,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-20T04:17:13Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    4,
                    17,
                    13,
                    4,
                    264,
                    0
                ],
                "title": "Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks"
                },
                "summary": "In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13203v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13203v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12382v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12382v4",
                "updated": "2025-02-07T11:37:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    37,
                    20,
                    4,
                    38,
                    0
                ],
                "published": "2024-06-18T08:14:28Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    8,
                    14,
                    28,
                    1,
                    170,
                    0
                ],
                "title": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions"
                },
                "summary": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Yanchao Hao"
                    },
                    {
                        "name": "Shengping Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12382v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12382v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04834v1",
                "updated": "2025-02-07T11:08:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    8,
                    32,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T11:08:32Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    11,
                    8,
                    32,
                    4,
                    38,
                    0
                ],
                "title": "Lightweight Operations for Visual Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Operations for Visual Speech Recognition"
                },
                "summary": "Visual speech recognition (VSR), which decodes spoken words from video data,\noffers significant benefits, particularly when audio is unavailable. However,\nthe high dimensionality of video data leads to prohibitive computational costs\nthat demand powerful hardware, limiting VSR deployment on resource-constrained\ndevices. This work addresses this limitation by developing lightweight VSR\narchitectures. Leveraging efficient operation design paradigms, we create\ncompact yet powerful models with reduced resource requirements and minimal\naccuracy loss. We train and evaluate our models on a large-scale public dataset\nfor recognition of words from video sequences, demonstrating their\neffectiveness for practical applications. We also conduct an extensive array of\nablative experiments to thoroughly analyze the size and complexity of each\nmodel. Code and trained models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual speech recognition (VSR), which decodes spoken words from video data,\noffers significant benefits, particularly when audio is unavailable. However,\nthe high dimensionality of video data leads to prohibitive computational costs\nthat demand powerful hardware, limiting VSR deployment on resource-constrained\ndevices. This work addresses this limitation by developing lightweight VSR\narchitectures. Leveraging efficient operation design paradigms, we create\ncompact yet powerful models with reduced resource requirements and minimal\naccuracy loss. We train and evaluate our models on a large-scale public dataset\nfor recognition of words from video sequences, demonstrating their\neffectiveness for practical applications. We also conduct an extensive array of\nablative experiments to thoroughly analyze the size and complexity of each\nmodel. Code and trained models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Iason Ioannis Panagos"
                    },
                    {
                        "name": "Giorgos Sfikas"
                    },
                    {
                        "name": "Christophoros Nikou"
                    }
                ],
                "author_detail": {
                    "name": "Christophoros Nikou"
                },
                "author": "Christophoros Nikou",
                "arxiv_comment": "10 pages (double column format), 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14940v3",
                "updated": "2025-02-07T10:23:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    10,
                    23,
                    16,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-24T21:55:14Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    21,
                    55,
                    14,
                    4,
                    24,
                    0
                ],
                "title": "CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models"
                },
                "summary": "Aligning large language models (LLMs) with human values is essential for\ntheir safe deployment and widespread adoption. Current LLM safety benchmarks\noften focus solely on the refusal of individual problematic queries, which\noverlooks the importance of the context where the query occurs and may cause\nundesired refusal of queries under safe contexts that diminish user experience.\nAddressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark\nthat integrates context into safety assessments of LLMs. CASE-Bench assigns\ndistinct, formally described contexts to categorized queries based on\nContextual Integrity theory. Additionally, in contrast to previous studies\nwhich mainly rely on majority voting from just a few annotators, we recruited a\nsufficient number of annotators necessary to ensure the detection of\nstatistically significant differences among the experimental conditions based\non power analysis. Our extensive analysis using CASE-Bench on various\nopen-source and commercial LLMs reveals a substantial and significant influence\nof context on human judgments (p<0.0001 from a z-test), underscoring the\nnecessity of context in safety evaluations. We also identify notable mismatches\nbetween human judgments and LLM responses, particularly in commercial models\nwithin safe contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human values is essential for\ntheir safe deployment and widespread adoption. Current LLM safety benchmarks\noften focus solely on the refusal of individual problematic queries, which\noverlooks the importance of the context where the query occurs and may cause\nundesired refusal of queries under safe contexts that diminish user experience.\nAddressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark\nthat integrates context into safety assessments of LLMs. CASE-Bench assigns\ndistinct, formally described contexts to categorized queries based on\nContextual Integrity theory. Additionally, in contrast to previous studies\nwhich mainly rely on majority voting from just a few annotators, we recruited a\nsufficient number of annotators necessary to ensure the detection of\nstatistically significant differences among the experimental conditions based\non power analysis. Our extensive analysis using CASE-Bench on various\nopen-source and commercial LLMs reveals a substantial and significant influence\nof context on human judgments (p<0.0001 from a z-test), underscoring the\nnecessity of context in safety evaluations. We also identify notable mismatches\nbetween human judgments and LLM responses, particularly in commercial models\nwithin safe contexts."
                },
                "authors": [
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Xiao Zhan"
                    },
                    {
                        "name": "Shutong Feng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    },
                    {
                        "name": "Jose Such"
                    }
                ],
                "author_detail": {
                    "name": "Jose Such"
                },
                "author": "Jose Such",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04797v1",
                "updated": "2025-02-07T10:01:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    10,
                    1,
                    32,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T10:01:32Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    10,
                    1,
                    32,
                    4,
                    38,
                    0
                ],
                "title": "Self-Rationalization in the Wild: A Large Scale Out-of-Distribution\n  Evaluation on NLI-related tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Rationalization in the Wild: A Large Scale Out-of-Distribution\n  Evaluation on NLI-related tasks"
                },
                "summary": "Free-text explanations are expressive and easy to understand, but many\ndatasets lack annotated explanation data, making it challenging to train models\nfor explainable predictions. To address this, we investigate how to use\nexisting explanation datasets for self-rationalization and evaluate models'\nout-of-distribution (OOD) performance. We fine-tune T5-Large and OLMo-7B models\nand assess the impact of fine-tuning data quality, the number of fine-tuning\nsamples, and few-shot selection methods. The models are evaluated on 19 diverse\nOOD datasets across three tasks: natural language inference (NLI),\nfact-checking, and hallucination detection in abstractive summarization. For\nthe generated explanation evaluation, we conduct a human study on 13 selected\nmodels and study its correlation with the Acceptability score (T5-11B) and\nthree other LLM-based reference-free metrics. Human evaluation shows that the\nAcceptability score correlates most strongly with human judgments,\ndemonstrating its effectiveness in evaluating free-text explanations. Our\nfindings reveal: 1) few annotated examples effectively adapt models for OOD\nexplanation generation; 2) compared to sample selection strategies, fine-tuning\ndata source has a larger impact on OOD performance; and 3) models with higher\nlabel prediction accuracy tend to produce better explanations, as reflected by\nhigher Acceptability scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-text explanations are expressive and easy to understand, but many\ndatasets lack annotated explanation data, making it challenging to train models\nfor explainable predictions. To address this, we investigate how to use\nexisting explanation datasets for self-rationalization and evaluate models'\nout-of-distribution (OOD) performance. We fine-tune T5-Large and OLMo-7B models\nand assess the impact of fine-tuning data quality, the number of fine-tuning\nsamples, and few-shot selection methods. The models are evaluated on 19 diverse\nOOD datasets across three tasks: natural language inference (NLI),\nfact-checking, and hallucination detection in abstractive summarization. For\nthe generated explanation evaluation, we conduct a human study on 13 selected\nmodels and study its correlation with the Acceptability score (T5-11B) and\nthree other LLM-based reference-free metrics. Human evaluation shows that the\nAcceptability score correlates most strongly with human judgments,\ndemonstrating its effectiveness in evaluating free-text explanations. Our\nfindings reveal: 1) few annotated examples effectively adapt models for OOD\nexplanation generation; 2) compared to sample selection strategies, fine-tuning\ndata source has a larger impact on OOD performance; and 3) models with higher\nlabel prediction accuracy tend to produce better explanations, as reflected by\nhigher Acceptability scores."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Max Glockner"
                    },
                    {
                        "name": "Anderson Rocha"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted at TACL; pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12539v2",
                "updated": "2025-02-07T09:54:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    54,
                    53,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-16T13:20:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    20,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "Counterfactual Effect Decomposition in Multi-Agent Sequential Decision\n  Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Effect Decomposition in Multi-Agent Sequential Decision\n  Making"
                },
                "summary": "We address the challenge of explaining counterfactual outcomes in multi-agent\nMarkov decision processes. In particular, we aim to explain the total\ncounterfactual effect of an agent's action on the outcome of a realized\nscenario through its influence on the environment dynamics and the agents'\nbehavior. To achieve this, we introduce a novel causal explanation formula that\ndecomposes the counterfactual effect by attributing to each agent and state\nvariable a score reflecting their respective contributions to the effect.\nFirst, we show that the total counterfactual effect of an agent's action can be\ndecomposed into two components: one measuring the effect that propagates\nthrough all subsequent agents' actions and another related to the effect that\npropagates through the state transitions. Building on recent advancements in\ncausal contribution analysis, we further decompose these two effects as\nfollows. For the former, we consider agent-specific effects -- a causal concept\nthat quantifies the counterfactual effect of an agent's action that propagates\nthrough a subset of agents. Based on this notion, we use Shapley value to\nattribute the effect to individual agents. For the latter, we consider the\nconcept of structure-preserving interventions and attribute the effect to state\nvariables based on their \"intrinsic\" contributions. Through extensive\nexperimentation, we demonstrate the interpretability of our approach in a\nGridworld environment with LLM-assisted agents and a sepsis management\nsimulator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of explaining counterfactual outcomes in multi-agent\nMarkov decision processes. In particular, we aim to explain the total\ncounterfactual effect of an agent's action on the outcome of a realized\nscenario through its influence on the environment dynamics and the agents'\nbehavior. To achieve this, we introduce a novel causal explanation formula that\ndecomposes the counterfactual effect by attributing to each agent and state\nvariable a score reflecting their respective contributions to the effect.\nFirst, we show that the total counterfactual effect of an agent's action can be\ndecomposed into two components: one measuring the effect that propagates\nthrough all subsequent agents' actions and another related to the effect that\npropagates through the state transitions. Building on recent advancements in\ncausal contribution analysis, we further decompose these two effects as\nfollows. For the former, we consider agent-specific effects -- a causal concept\nthat quantifies the counterfactual effect of an agent's action that propagates\nthrough a subset of agents. Based on this notion, we use Shapley value to\nattribute the effect to individual agents. For the latter, we consider the\nconcept of structure-preserving interventions and attribute the effect to state\nvariables based on their \"intrinsic\" contributions. Through extensive\nexperimentation, we demonstrate the interpretability of our approach in a\nGridworld environment with LLM-assisted agents and a sepsis management\nsimulator."
                },
                "authors": [
                    {
                        "name": "Stelios Triantafyllou"
                    },
                    {
                        "name": "Aleksa Sukovic"
                    },
                    {
                        "name": "Yasaman Zolfimoselo"
                    },
                    {
                        "name": "Goran Radanovic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Radanovic"
                },
                "author": "Goran Radanovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04790v1",
                "updated": "2025-02-07T09:49:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    49,
                    56,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T09:49:56Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    49,
                    56,
                    4,
                    38,
                    0
                ],
                "title": "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate\n  Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate\n  Efficiency"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious natural language processing (NLP) scenarios, but they still face\nchallenges when handling complex arithmetic and logical reasoning tasks. While\nChain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction\nstrategies have attempted to guide models in sequential, multi-step reasoning,\nMulti-agent Debate (MAD) has emerged as a viable approach for enhancing the\nreasoning capabilities of LLMs. By increasing both the number of agents and the\nfrequency of debates, the performance of LLMs improves significantly. However,\nthis strategy results in a significant increase in token costs, presenting a\nbarrier to scalability. To address this challenge, we introduce a novel\nsparsification strategy designed to reduce token costs within MAD. This\napproach minimizes ineffective exchanges of information and unproductive\ndiscussions among agents, thereby enhancing the overall efficiency of the\ndebate process. We conduct comparative experiments on multiple datasets across\nvarious models, demonstrating that our approach significantly reduces the token\ncosts in MAD to a considerable extent. Specifically, compared to MAD, our\napproach achieves an impressive reduction of up to 94.5\\% in token costs while\nmaintaining performance degradation below 2.0\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious natural language processing (NLP) scenarios, but they still face\nchallenges when handling complex arithmetic and logical reasoning tasks. While\nChain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction\nstrategies have attempted to guide models in sequential, multi-step reasoning,\nMulti-agent Debate (MAD) has emerged as a viable approach for enhancing the\nreasoning capabilities of LLMs. By increasing both the number of agents and the\nfrequency of debates, the performance of LLMs improves significantly. However,\nthis strategy results in a significant increase in token costs, presenting a\nbarrier to scalability. To address this challenge, we introduce a novel\nsparsification strategy designed to reduce token costs within MAD. This\napproach minimizes ineffective exchanges of information and unproductive\ndiscussions among agents, thereby enhancing the overall efficiency of the\ndebate process. We conduct comparative experiments on multiple datasets across\nvarious models, demonstrating that our approach significantly reduces the token\ncosts in MAD to a considerable extent. Specifically, compared to MAD, our\napproach achieves an impressive reduction of up to 94.5\\% in token costs while\nmaintaining performance degradation below 2.0\\%."
                },
                "authors": [
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Xitai Jin"
                    },
                    {
                        "name": "Chen Tianying Tiana"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Xiaohua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Xu"
                },
                "author": "Xiaohua Xu",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04789v1",
                "updated": "2025-02-07T09:49:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    49,
                    13,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T09:49:13Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    49,
                    13,
                    4,
                    38,
                    0
                ],
                "title": "Probing Internal Representations of Multi-Word Verbs in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Internal Representations of Multi-Word Verbs in Large Language\n  Models"
                },
                "summary": "This study investigates the internal representations of verb-particle\ncombinations, called multi-word verbs, within transformer-based large language\nmodels (LLMs), specifically examining how these models capture lexical and\nsyntactic properties at different neural network layers. Using the BERT\narchitecture, we analyze the representations of its layers for two different\nverb-particle constructions: phrasal verbs like 'give up' and prepositional\nverbs like 'look at'. Our methodology includes training probing classifiers on\nthe internal representations to classify these categories at both word and\nsentence levels. The results indicate that the model's middle layers achieve\nthe highest classification accuracies. To further analyze the nature of these\ndistinctions, we conduct a data separability test using the Generalized\nDiscrimination Value (GDV). While GDV results show weak linear separability\nbetween the two verb types, probing classifiers still achieve high accuracy,\nsuggesting that representations of these linguistic categories may be\nnon-linearly separable. This aligns with previous research indicating that\nlinguistic distinctions in neural networks are not always encoded in a linearly\nseparable manner. These findings computationally support usage-based claims on\nthe representation of verb-particle constructions and highlight the complex\ninteraction between neural network architectures and linguistic structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the internal representations of verb-particle\ncombinations, called multi-word verbs, within transformer-based large language\nmodels (LLMs), specifically examining how these models capture lexical and\nsyntactic properties at different neural network layers. Using the BERT\narchitecture, we analyze the representations of its layers for two different\nverb-particle constructions: phrasal verbs like 'give up' and prepositional\nverbs like 'look at'. Our methodology includes training probing classifiers on\nthe internal representations to classify these categories at both word and\nsentence levels. The results indicate that the model's middle layers achieve\nthe highest classification accuracies. To further analyze the nature of these\ndistinctions, we conduct a data separability test using the Generalized\nDiscrimination Value (GDV). While GDV results show weak linear separability\nbetween the two verb types, probing classifiers still achieve high accuracy,\nsuggesting that representations of these linguistic categories may be\nnon-linearly separable. This aligns with previous research indicating that\nlinguistic distinctions in neural networks are not always encoded in a linearly\nseparable manner. These findings computationally support usage-based claims on\nthe representation of verb-particle constructions and highlight the complex\ninteraction between neural network architectures and linguistic structures."
                },
                "authors": [
                    {
                        "name": "Hassane Kissane"
                    },
                    {
                        "name": "Achim Schilling"
                    },
                    {
                        "name": "Patrick Krauss"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Krauss"
                },
                "author": "Patrick Krauss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04780v1",
                "updated": "2025-02-07T09:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    33,
                    44,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T09:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    33,
                    44,
                    4,
                    38,
                    0
                ],
                "title": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning"
                },
                "summary": "Multi-agent AI systems powered by large language models (LLMs) are\nincreasingly applied to solve complex tasks. However, these systems often rely\non fragile, manually designed prompts and heuristics, making optimization\ndifficult. A key challenge in optimizing multi-agent systems is acquiring\nsuitable training data for specialized agents. We introduce SiriuS, a\nself-improving, reasoning-driven optimization framework for multi-agent\nsystems. Central to our approach is the construction of an experience library:\na repository of high-quality reasoning trajectories. The library is built by\nretaining reasoning steps that lead to successful outcomes, providing a robust\ntraining set for optimizing multi-agent system. Additionally, we introduce a\nlibrary augmentation procedure that refines unsuccessful trajectories, further\nenriching the library. SiriuS boosts performance by 2.86\\% to 21.88\\% on\nreasoning and biomedical QA and enhances agent negotiation in competitive\nsettings. Our results show that SiriuS enhances multi-agent performance while\ngenerating reusable data for self-correction and self-play enhancement in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent AI systems powered by large language models (LLMs) are\nincreasingly applied to solve complex tasks. However, these systems often rely\non fragile, manually designed prompts and heuristics, making optimization\ndifficult. A key challenge in optimizing multi-agent systems is acquiring\nsuitable training data for specialized agents. We introduce SiriuS, a\nself-improving, reasoning-driven optimization framework for multi-agent\nsystems. Central to our approach is the construction of an experience library:\na repository of high-quality reasoning trajectories. The library is built by\nretaining reasoning steps that lead to successful outcomes, providing a robust\ntraining set for optimizing multi-agent system. Additionally, we introduce a\nlibrary augmentation procedure that refines unsuccessful trajectories, further\nenriching the library. SiriuS boosts performance by 2.86\\% to 21.88\\% on\nreasoning and biomedical QA and enhances agent negotiation in competitive\nsettings. Our results show that SiriuS enhances multi-agent performance while\ngenerating reusable data for self-correction and self-play enhancement in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Mert Yuksekgonul"
                    },
                    {
                        "name": "Shirley Wu"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12983v2",
                "updated": "2025-02-07T09:31:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    31,
                    9,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-22T16:12:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    12,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "LLM4WM: Adapting LLM for Wireless Multi-Tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4WM: Adapting LLM for Wireless Multi-Tasking"
                },
                "summary": "The wireless channel is fundamental to communication, encompassing numerous\ntasks collectively referred to as channel-associated tasks. These tasks can\nleverage joint learning based on channel characteristics to share\nrepresentations and enhance system design. To capitalize on this advantage,\nLLM4WM is proposed--a large language model (LLM) multi-task fine-tuning\nframework specifically tailored for channel-associated tasks. This framework\nutilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for\nmulti-task fine-tuning, enabling the transfer of the pre-trained LLM's general\nknowledge to these tasks. Given the unique characteristics of wireless channel\ndata, preprocessing modules, adapter modules, and multi-task output layers are\ndesigned to align the channel data with the LLM's semantic feature space.\nExperiments on a channel-associated multi-task dataset demonstrate that LLM4WM\noutperforms existing methodologies in both full-sample and few-shot\nevaluations, owing to its robust multi-task joint modeling and transfer\nlearning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wireless channel is fundamental to communication, encompassing numerous\ntasks collectively referred to as channel-associated tasks. These tasks can\nleverage joint learning based on channel characteristics to share\nrepresentations and enhance system design. To capitalize on this advantage,\nLLM4WM is proposed--a large language model (LLM) multi-task fine-tuning\nframework specifically tailored for channel-associated tasks. This framework\nutilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for\nmulti-task fine-tuning, enabling the transfer of the pre-trained LLM's general\nknowledge to these tasks. Given the unique characteristics of wireless channel\ndata, preprocessing modules, adapter modules, and multi-task output layers are\ndesigned to align the channel data with the LLM's semantic feature space.\nExperiments on a channel-associated multi-task dataset demonstrate that LLM4WM\noutperforms existing methodologies in both full-sample and few-shot\nevaluations, owing to its robust multi-task joint modeling and transfer\nlearning capabilities."
                },
                "authors": [
                    {
                        "name": "Xuanyu Liu"
                    },
                    {
                        "name": "Shijian Gao"
                    },
                    {
                        "name": "Boxun Liu"
                    },
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Liuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liuqing Yang"
                },
                "author": "Liuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04774v1",
                "updated": "2025-02-07T09:20:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    20,
                    11,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T09:20:11Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    20,
                    11,
                    4,
                    38,
                    0
                ],
                "title": "SeDi-Instruct: Enhancing Alignment of Language Models through\n  Self-Directed Instruction Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeDi-Instruct: Enhancing Alignment of Language Models through\n  Self-Directed Instruction Generation"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has enabled the industry\nto develop various AI-based services. Instruction tuning is considered\nessential in adapting foundation models for target domains to provide\nhigh-quality services to customers. A key challenge in instruction tuning is\nobtaining high-quality instruction data. Self-Instruct, which automatically\ngenerates instruction data using ChatGPT APIs, alleviates the data scarcity\nproblem. To improve the quality of instruction data, Self-Instruct discards\nmany of the instructions generated from ChatGPT, even though it is inefficient\nin terms of cost owing to many useless API calls. To generate high-quality\ninstruction data at a low cost, we propose a novel data generation framework,\nSelf-Direct Instruction generation (SeDi-Instruct), which employs\ndiversity-based filtering and iterative feedback task generation.\nDiversity-based filtering maintains model accuracy without excessively\ndiscarding low-quality generated instructions by enhancing the diversity of\ninstructions in a batch. This reduces the cost of synthesizing instruction\ndata. The iterative feedback task generation integrates instruction generation\nand training tasks and utilizes information obtained during the training to\ncreate high-quality instruction sets. Our results show that SeDi-Instruct\nenhances the accuracy of AI models by 5.2%, compared with traditional methods,\nwhile reducing data generation costs by 36%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has enabled the industry\nto develop various AI-based services. Instruction tuning is considered\nessential in adapting foundation models for target domains to provide\nhigh-quality services to customers. A key challenge in instruction tuning is\nobtaining high-quality instruction data. Self-Instruct, which automatically\ngenerates instruction data using ChatGPT APIs, alleviates the data scarcity\nproblem. To improve the quality of instruction data, Self-Instruct discards\nmany of the instructions generated from ChatGPT, even though it is inefficient\nin terms of cost owing to many useless API calls. To generate high-quality\ninstruction data at a low cost, we propose a novel data generation framework,\nSelf-Direct Instruction generation (SeDi-Instruct), which employs\ndiversity-based filtering and iterative feedback task generation.\nDiversity-based filtering maintains model accuracy without excessively\ndiscarding low-quality generated instructions by enhancing the diversity of\ninstructions in a batch. This reduces the cost of synthesizing instruction\ndata. The iterative feedback task generation integrates instruction generation\nand training tasks and utilizes information obtained during the training to\ncreate high-quality instruction sets. Our results show that SeDi-Instruct\nenhances the accuracy of AI models by 5.2%, compared with traditional methods,\nwhile reducing data generation costs by 36%."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02913v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02913v3",
                "updated": "2025-02-07T09:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    10,
                    9,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-05T06:20:20Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    20,
                    20,
                    2,
                    36,
                    0
                ],
                "title": "Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage"
                },
                "summary": "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications."
                },
                "authors": [
                    {
                        "name": "Jiayang Meng"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xin Shi"
                    },
                    {
                        "name": "Qingyu Huang"
                    },
                    {
                        "name": "Chen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Chen Hou"
                },
                "author": "Chen Hou",
                "arxiv_comment": "There is something wrong with the order of Figures 8-11. And I need\n  to add an experiment with differential privacy quantization mutual\n  information value",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02913v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02913v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12229v2",
                "updated": "2025-02-07T09:08:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    9,
                    8,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-16T04:44:34Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    4,
                    44,
                    34,
                    2,
                    290,
                    0
                ],
                "title": "Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems"
                },
                "summary": "In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive world knowledge and remarkable reasoning capabilities of LLMs enable\nour method to supplement missing facts in KGs. Additionally, their powerful\ntext understanding abilities allow for better utilization of semantic\ninformation. Specifically, CoLaKG extracts useful information from the KG at\nboth local and global levels. By employing item-centered subgraph extraction\nand prompt engineering, it accurately captures the local KG. Subsequently,\nthrough retrieval-based neighbor enhancement, it supplements the current item\nby capturing related items from the entire KG, thereby effectively utilizing\nglobal information. The local and global information extracted by the LLM are\neffectively integrated into the recommendation model through a representation\nfusion module and a retrieval-augmented representation learning module,\nrespectively, thereby improving recommendation performance. Extensive\nexperiments on four real-world datasets demonstrate the superiority of our\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive world knowledge and remarkable reasoning capabilities of LLMs enable\nour method to supplement missing facts in KGs. Additionally, their powerful\ntext understanding abilities allow for better utilization of semantic\ninformation. Specifically, CoLaKG extracts useful information from the KG at\nboth local and global levels. By employing item-centered subgraph extraction\nand prompt engineering, it accurately captures the local KG. Subsequently,\nthrough retrieval-based neighbor enhancement, it supplements the current item\nby capturing related items from the entire KG, thereby effectively utilizing\nglobal information. The local and global information extracted by the LLM are\neffectively integrated into the recommendation model through a representation\nfusion module and a retrieval-augmented representation learning module,\nrespectively, thereby improving recommendation performance. Extensive\nexperiments on four real-world datasets demonstrate the superiority of our\nmethod."
                },
                "authors": [
                    {
                        "name": "Ziqiang Cui"
                    },
                    {
                        "name": "Yunpeng Weng"
                    },
                    {
                        "name": "Xing Tang"
                    },
                    {
                        "name": "Fuyuan Lyu"
                    },
                    {
                        "name": "Dugang Liu"
                    },
                    {
                        "name": "Xiuqiang He"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00005v3",
                "updated": "2025-02-07T08:49:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    49,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-16T11:57:14Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    57,
                    14,
                    2,
                    290,
                    0
                ],
                "title": "Mastering the Craft of Data Synthesis for CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mastering the Craft of Data Synthesis for CodeLLMs"
                },
                "summary": "Large language models (LLMs) have shown impressive performance in \\emph{code}\nunderstanding and generation, making coding tasks a key focus for researchers\ndue to their practical applications and value as a testbed for LLM evaluation.\nData synthesis and filtering techniques have been widely adopted and shown to\nbe highly effective in this context. In this paper, we present a focused survey\nand taxonomy of these techniques, emphasizing recent advancements. We highlight\nkey challenges, explore future research directions, and offer practical\nguidance for new researchers entering the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance in \\emph{code}\nunderstanding and generation, making coding tasks a key focus for researchers\ndue to their practical applications and value as a testbed for LLM evaluation.\nData synthesis and filtering techniques have been widely adopted and shown to\nbe highly effective in this context. In this paper, we present a focused survey\nand taxonomy of these techniques, emphasizing recent advancements. We highlight\nkey challenges, explore future research directions, and offer practical\nguidance for new researchers entering the field."
                },
                "authors": [
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Philip Arthur"
                    },
                    {
                        "name": "Qianyu Feng"
                    },
                    {
                        "name": "Cong Duy Vu Hoang"
                    },
                    {
                        "name": "Yu-Heng Hong"
                    },
                    {
                        "name": "Mahdi Kazemi Moghaddam"
                    },
                    {
                        "name": "Omid Nezami"
                    },
                    {
                        "name": "Thien Nguyen"
                    },
                    {
                        "name": "Gioacchino Tangari"
                    },
                    {
                        "name": "Duy Vu"
                    },
                    {
                        "name": "Thanh Vu"
                    },
                    {
                        "name": "Mark Johnson"
                    },
                    {
                        "name": "Krishnaram Kenthapadi"
                    },
                    {
                        "name": "Don Dharmasiri"
                    },
                    {
                        "name": "Long Duong"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Fang Li"
                },
                "author": "Yuan-Fang Li",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04759v1",
                "updated": "2025-02-07T08:45:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    45,
                    50,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:45:50Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    45,
                    50,
                    4,
                    38,
                    0
                ],
                "title": "Enhancing Phishing Email Identification with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Phishing Email Identification with Large Language Models"
                },
                "summary": "Phishing has long been a common tactic used by cybercriminals and continues\nto pose a significant threat in today's digital world. When phishing attacks\nbecome more advanced and sophisticated, there is an increasing need for\neffective methods to detect and prevent them. To address the challenging\nproblem of detecting phishing emails, researchers have developed numerous\nsolutions, in particular those based on machine learning (ML) algorithms. In\nthis work, we take steps to study the efficacy of large language models (LLMs)\nin detecting phishing emails. The experiments show that the LLM achieves a high\naccuracy rate at high precision; importantly, it also provides interpretable\nevidence for the decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing has long been a common tactic used by cybercriminals and continues\nto pose a significant threat in today's digital world. When phishing attacks\nbecome more advanced and sophisticated, there is an increasing need for\neffective methods to detect and prevent them. To address the challenging\nproblem of detecting phishing emails, researchers have developed numerous\nsolutions, in particular those based on machine learning (ML) algorithms. In\nthis work, we take steps to study the efficacy of large language models (LLMs)\nin detecting phishing emails. The experiments show that the LLM achieves a high\naccuracy rate at high precision; importantly, it also provides interpretable\nevidence for the decisions."
                },
                "authors": [
                    {
                        "name": "Catherine Lee"
                    }
                ],
                "author_detail": {
                    "name": "Catherine Lee"
                },
                "author": "Catherine Lee",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04756v1",
                "updated": "2025-02-07T08:42:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    42,
                    34,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:42:34Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    42,
                    34,
                    4,
                    38,
                    0
                ],
                "title": "Concept Navigation and Classification via Open Source Large Language\n  Model Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Navigation and Classification via Open Source Large Language\n  Model Processing"
                },
                "summary": "This paper presents a novel methodological framework for detecting and\nclassifying latent constructs, including frames, narratives, and topics, from\ntextual data using Open-Source Large Language Models (LLMs). The proposed\nhybrid approach combines automated summarization with human-in-the-loop\nvalidation to enhance the accuracy and interpretability of construct\nidentification. By employing iterative sampling coupled with expert refinement,\nthe framework guarantees methodological robustness and ensures conceptual\nprecision. Applied to diverse data sets, including AI policy debates, newspaper\narticles on encryption, and the 20 Newsgroups data set, this approach\ndemonstrates its versatility in systematically analyzing complex political\ndiscourses, media framing, and topic classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel methodological framework for detecting and\nclassifying latent constructs, including frames, narratives, and topics, from\ntextual data using Open-Source Large Language Models (LLMs). The proposed\nhybrid approach combines automated summarization with human-in-the-loop\nvalidation to enhance the accuracy and interpretability of construct\nidentification. By employing iterative sampling coupled with expert refinement,\nthe framework guarantees methodological robustness and ensures conceptual\nprecision. Applied to diverse data sets, including AI policy debates, newspaper\narticles on encryption, and the 20 Newsgroups data set, this approach\ndemonstrates its versatility in systematically analyzing complex political\ndiscourses, media framing, and topic classification tasks."
                },
                "authors": [
                    {
                        "name": "Maël Kubli"
                    }
                ],
                "author_detail": {
                    "name": "Maël Kubli"
                },
                "author": "Maël Kubli",
                "arxiv_comment": "35 pages, 1 figure, 7 tabels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14059v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14059v3",
                "updated": "2025-02-07T08:37:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    37,
                    3,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-17T22:03:52Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    22,
                    3,
                    52,
                    3,
                    291,
                    0
                ],
                "title": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language\n  Models"
                },
                "summary": "This paper introduces the UCFE: User-Centric Financial Expertise benchmark,\nan innovative framework designed to evaluate the ability of large language\nmodels (LLMs) to handle complex real-world financial tasks. UCFE benchmark\nadopts a hybrid approach that combines human expert evaluations with dynamic,\ntask-specific interactions to simulate the complexities of evolving financial\nscenarios. Firstly, we conducted a user study involving 804 participants,\ncollecting their feedback on financial tasks. Secondly, based on this feedback,\nwe created our dataset that encompasses a wide range of user intents and\ninteractions. This dataset serves as the foundation for benchmarking 11 LLMs\nservices using the LLM-as-Judge methodology. Our results show a significant\nalignment between benchmark scores and human preferences, with a Pearson\ncorrelation coefficient of 0.78, confirming the effectiveness of the UCFE\ndataset and our evaluation approach. UCFE benchmark not only reveals the\npotential of LLMs in the financial domain but also provides a robust framework\nfor assessing their performance and user satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the UCFE: User-Centric Financial Expertise benchmark,\nan innovative framework designed to evaluate the ability of large language\nmodels (LLMs) to handle complex real-world financial tasks. UCFE benchmark\nadopts a hybrid approach that combines human expert evaluations with dynamic,\ntask-specific interactions to simulate the complexities of evolving financial\nscenarios. Firstly, we conducted a user study involving 804 participants,\ncollecting their feedback on financial tasks. Secondly, based on this feedback,\nwe created our dataset that encompasses a wide range of user intents and\ninteractions. This dataset serves as the foundation for benchmarking 11 LLMs\nservices using the LLM-as-Judge methodology. Our results show a significant\nalignment between benchmark scores and human preferences, with a Pearson\ncorrelation coefficient of 0.78, confirming the effectiveness of the UCFE\ndataset and our evaluation approach. UCFE benchmark not only reveals the\npotential of LLMs in the financial domain but also provides a robust framework\nfor assessing their performance and user satisfaction."
                },
                "authors": [
                    {
                        "name": "Yuzhe Yang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Yilin Guo"
                    },
                    {
                        "name": "Ruoli Gan"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Mingcong Lei"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Haining Wang"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Honghai Yu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14059v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14059v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04751v1",
                "updated": "2025-02-07T08:36:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    36,
                    39,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:36:39Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    36,
                    39,
                    4,
                    38,
                    0
                ],
                "title": "Holistically Guided Monte Carlo Tree Search for Intricate Information\n  Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistically Guided Monte Carlo Tree Search for Intricate Information\n  Seeking"
                },
                "summary": "In the era of vast digital information, the sheer volume and heterogeneity of\navailable information present significant challenges for intricate information\nseeking. Users frequently face multistep web search tasks that involve\nnavigating vast and varied data sources. This complexity demands every step\nremains comprehensive, accurate, and relevant. However, traditional search\nmethods often struggle to balance the need for localized precision with the\nbroader context required for holistic understanding, leaving critical facets of\nintricate queries underexplored. In this paper, we introduce an LLM-based\nsearch assistant that adopts a new information seeking paradigm with\nholistically guided Monte Carlo tree search (HG-MCTS). We reformulate the task\nas a progressive information collection process with a knowledge memory and\nunite an adaptive checklist with multi-perspective reward modeling in MCTS. The\nadaptive checklist provides explicit sub-goals to guide the MCTS process toward\ncomprehensive coverage of complex user queries. Simultaneously, our\nmulti-perspective reward modeling offers both exploration and retrieval\nrewards, along with progress feedback that tracks completed and remaining\nsub-goals, refining the checklist as the tree search progresses. By striking a\nbalance between localized tree expansion and global guidance, HG-MCTS reduces\nredundancy in search paths and ensures that all crucial aspects of an intricate\nquery are properly addressed. Extensive experiments on real-world intricate\ninformation seeking tasks demonstrate that HG-MCTS acquires thorough knowledge\ncollections and delivers more accurate final responses compared with existing\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of vast digital information, the sheer volume and heterogeneity of\navailable information present significant challenges for intricate information\nseeking. Users frequently face multistep web search tasks that involve\nnavigating vast and varied data sources. This complexity demands every step\nremains comprehensive, accurate, and relevant. However, traditional search\nmethods often struggle to balance the need for localized precision with the\nbroader context required for holistic understanding, leaving critical facets of\nintricate queries underexplored. In this paper, we introduce an LLM-based\nsearch assistant that adopts a new information seeking paradigm with\nholistically guided Monte Carlo tree search (HG-MCTS). We reformulate the task\nas a progressive information collection process with a knowledge memory and\nunite an adaptive checklist with multi-perspective reward modeling in MCTS. The\nadaptive checklist provides explicit sub-goals to guide the MCTS process toward\ncomprehensive coverage of complex user queries. Simultaneously, our\nmulti-perspective reward modeling offers both exploration and retrieval\nrewards, along with progress feedback that tracks completed and remaining\nsub-goals, refining the checklist as the tree search progresses. By striking a\nbalance between localized tree expansion and global guidance, HG-MCTS reduces\nredundancy in search paths and ensures that all crucial aspects of an intricate\nquery are properly addressed. Extensive experiments on real-world intricate\ninformation seeking tasks demonstrate that HG-MCTS acquires thorough knowledge\ncollections and delivers more accurate final responses compared with existing\nbaselines."
                },
                "authors": [
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02959v2",
                "updated": "2025-02-07T08:32:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    32,
                    24,
                    4,
                    38,
                    0
                ],
                "published": "2024-11-05T09:58:36Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    58,
                    36,
                    1,
                    310,
                    0
                ],
                "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial RAG\nsystems have used Web search engines as their major retrieval systems.\nTypically, such RAG systems retrieve search results, download HTML sources of\nthe results, and then extract plain texts from the HTML sources. Plain text\ndocuments or chunks are fed into the LLMs to augment the generation. However,\nmuch of the structural and semantic information inherent in HTML, such as\nheadings and table structures, is lost during this plain-text-based RAG\nprocess. To alleviate this problem, we propose HtmlRAG, which uses HTML instead\nof plain text as the format of retrieved knowledge in RAG. We believe HTML is\nbetter than plain text in modeling knowledge in external documents, and most\nLLMs possess robust capacities to understand HTML. However, utilizing HTML\npresents new challenges. HTML contains additional content such as tags,\nJavaScript, and CSS specifications, which bring extra input tokens and noise to\nthe RAG system. To address this issue, we propose HTML cleaning, compression,\nand a two-step block-tree-based pruning strategy, to shorten the HTML while\nminimizing the loss of information. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial RAG\nsystems have used Web search engines as their major retrieval systems.\nTypically, such RAG systems retrieve search results, download HTML sources of\nthe results, and then extract plain texts from the HTML sources. Plain text\ndocuments or chunks are fed into the LLMs to augment the generation. However,\nmuch of the structural and semantic information inherent in HTML, such as\nheadings and table structures, is lost during this plain-text-based RAG\nprocess. To alleviate this problem, we propose HtmlRAG, which uses HTML instead\nof plain text as the format of retrieved knowledge in RAG. We believe HTML is\nbetter than plain text in modeling knowledge in external documents, and most\nLLMs possess robust capacities to understand HTML. However, utilizing HTML\npresents new challenges. HTML contains additional content such as tags,\nJavaScript, and CSS specifications, which bring extra input tokens and noise to\nthe RAG system. To address this issue, we propose HTML cleaning, compression,\nand a two-step block-tree-based pruning strategy, to shorten the HTML while\nminimizing the loss of information. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems."
                },
                "authors": [
                    {
                        "name": "Jiejun Tan"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_doi": "10.1145/3696410.3714546",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714546",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 main conference. Repo:\n  https://github.com/plageon/HtmlRAG",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04747v1",
                "updated": "2025-02-07T08:29:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    29,
                    9,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:29:09Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    29,
                    9,
                    4,
                    38,
                    0
                ],
                "title": "Every Software as an Agent: Blueprint and Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every Software as an Agent: Blueprint and Case Study"
                },
                "summary": "The rise of (multimodal) large language models (LLMs) has shed light on\nsoftware agent -- where software can understand and follow user instructions in\nnatural language. However, existing approaches such as API-based and GUI-based\nagents are far from satisfactory at accuracy and efficiency aspects. Instead,\nwe advocate to endow LLMs with access to the software internals (source code\nand runtime context) and the permission to dynamically inject generated code\ninto software for execution. In such a whitebox setting, one may better\nleverage the software context and the coding ability of LLMs. We then present\nan overall design architecture and case studies on two popular web-based\ndesktop applications. We also give in-depth discussion of the challenges and\nfuture directions. We deem that such a new paradigm has the potential to\nfundamentally overturn the existing software agent design, and finally creating\na digital world in which software can comprehend, operate, collaborate, and\neven think to meet complex user needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of (multimodal) large language models (LLMs) has shed light on\nsoftware agent -- where software can understand and follow user instructions in\nnatural language. However, existing approaches such as API-based and GUI-based\nagents are far from satisfactory at accuracy and efficiency aspects. Instead,\nwe advocate to endow LLMs with access to the software internals (source code\nand runtime context) and the permission to dynamically inject generated code\ninto software for execution. In such a whitebox setting, one may better\nleverage the software context and the coding ability of LLMs. We then present\nan overall design architecture and case studies on two popular web-based\ndesktop applications. We also give in-depth discussion of the challenges and\nfuture directions. We deem that such a new paradigm has the potential to\nfundamentally overturn the existing software agent design, and finally creating\na digital world in which software can comprehend, operate, collaborate, and\neven think to meet complex user needs."
                },
                "authors": [
                    {
                        "name": "Mengwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengwei Xu"
                },
                "author": "Mengwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12812v2",
                "updated": "2025-02-07T08:18:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    18,
                    0,
                    4,
                    38,
                    0
                ],
                "published": "2024-08-23T03:16:26Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    3,
                    16,
                    26,
                    4,
                    236,
                    0
                ],
                "title": "Grounding Fallacies Misrepresenting Scientific Publications in Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Fallacies Misrepresenting Scientific Publications in Evidence"
                },
                "summary": "Health-related misinformation claims often falsely cite a credible biomedical\npublication as evidence. These publications only superficially seem to support\nthe false claim, when logical fallacies are applied. In this work, we aim to\ndetect and to highlight such fallacies, which requires assessing the exact\ncontent of the misrepresented publications. To achieve this, we introduce\nMissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus\nextends Missci by grounding the applied fallacies in real-world passages from\nmisrepresented studies. This creates a realistic test-bed for detecting and\nverbalizing fallacies under real-world input conditions, and enables new and\nrealistic passage-retrieval tasks. MissciPlus is the first logical fallacy\ndataset which pairs the real-world misrepresented evidence with incorrect\nclaims, identical to the input to evidence-based fact-checking models. With\nMissciPlus, we i) benchmark retrieval models in identifying passages that\nsupport claims only with fallacious reasoning, ii) evaluate how well LLMs\nverbalize fallacious reasoning based on misrepresented scientific passages, and\niii) assess the effectiveness of fact-checking models in refuting claims that\nmisrepresent biomedical research. Our findings show that current fact-checking\nmodels struggle to use misrepresented scientific passages to refute\nmisinformation. Moreover, these passages can mislead LLMs into accepting false\nclaims as true.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-related misinformation claims often falsely cite a credible biomedical\npublication as evidence. These publications only superficially seem to support\nthe false claim, when logical fallacies are applied. In this work, we aim to\ndetect and to highlight such fallacies, which requires assessing the exact\ncontent of the misrepresented publications. To achieve this, we introduce\nMissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus\nextends Missci by grounding the applied fallacies in real-world passages from\nmisrepresented studies. This creates a realistic test-bed for detecting and\nverbalizing fallacies under real-world input conditions, and enables new and\nrealistic passage-retrieval tasks. MissciPlus is the first logical fallacy\ndataset which pairs the real-world misrepresented evidence with incorrect\nclaims, identical to the input to evidence-based fact-checking models. With\nMissciPlus, we i) benchmark retrieval models in identifying passages that\nsupport claims only with fallacious reasoning, ii) evaluate how well LLMs\nverbalize fallacious reasoning based on misrepresented scientific passages, and\niii) assess the effectiveness of fact-checking models in refuting claims that\nmisrepresent biomedical research. Our findings show that current fact-checking\nmodels struggle to use misrepresented scientific passages to refute\nmisinformation. Moreover, these passages can mislead LLMs into accepting false\nclaims as true."
                },
                "authors": [
                    {
                        "name": "Max Glockner"
                    },
                    {
                        "name": "Yufang Hou"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15639v3",
                "updated": "2025-02-07T08:06:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    6,
                    50,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-21T04:57:09Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    4,
                    57,
                    9,
                    0,
                    295,
                    0
                ],
                "title": "Can Large Language Models Invent Algorithms to Improve Themselves?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Invent Algorithms to Improve Themselves?"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance improvements\nand are rapidly gaining adoption in industry. However, the methods for\nimproving LLMs are still designed by humans, which restricts the invention of\nnew model-improving algorithms to human expertise and imagination. To address\nthis, we propose the Self-Developing framework, which enables LLMs to\nautonomously generate and learn model-improvement algorithms. In this\nframework, the seed model generates, applies, and learns model-improving\nalgorithms, continuously improving both the seed model and the algorithms\nthemselves. Among model-improving strategies, we focus on model merging\nalgorithms. In mathematical reasoning tasks, Self-Developing discovers novel\nmerging strategies and outperforms human-designed methods. On GSM8k, the\ndiscovered algorithms improve the seed model by 6% and surpass human-designed\nmethods by 4.3%. Moreover, they exhibit strong transferability, achieving a\n7.4% performance gain on out-of-domain models. These results suggest that LLMs\ncan autonomously develop effective model-improvement techniques beyond human\nintuition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance improvements\nand are rapidly gaining adoption in industry. However, the methods for\nimproving LLMs are still designed by humans, which restricts the invention of\nnew model-improving algorithms to human expertise and imagination. To address\nthis, we propose the Self-Developing framework, which enables LLMs to\nautonomously generate and learn model-improvement algorithms. In this\nframework, the seed model generates, applies, and learns model-improving\nalgorithms, continuously improving both the seed model and the algorithms\nthemselves. Among model-improving strategies, we focus on model merging\nalgorithms. In mathematical reasoning tasks, Self-Developing discovers novel\nmerging strategies and outperforms human-designed methods. On GSM8k, the\ndiscovered algorithms improve the seed model by 6% and surpass human-designed\nmethods by 4.3%. Moreover, they exhibit strong transferability, achieving a\n7.4% performance gain on out-of-domain models. These results suggest that LLMs\ncan autonomously develop effective model-improvement techniques beyond human\nintuition."
                },
                "authors": [
                    {
                        "name": "Yoichi Ishibashi"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "arxiv_comment": "Accepted at NAACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04728v1",
                "updated": "2025-02-07T07:52:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    52,
                    25,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T07:52:25Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    52,
                    25,
                    4,
                    38,
                    0
                ],
                "title": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models"
                },
                "summary": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domain, achieving over 50%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domain, achieving over 50%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks."
                },
                "authors": [
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Yuhuan Yuan"
                    },
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Fuxiang Frank Xia"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ge Lin"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Technical Report v1 (32 pages, 6 figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04718v1",
                "updated": "2025-02-07T07:39:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    39,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T07:39:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    39,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Evaluating Text Style Transfer Evaluation: Are There Any Reliable\n  Metrics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Text Style Transfer Evaluation: Are There Any Reliable\n  Metrics?"
                },
                "summary": "Text Style Transfer (TST) is the task of transforming a text to reflect a\nparticular style while preserving its original content. Evaluating TST outputs\nis a multidimensional challenge, requiring the assessment of style transfer\naccuracy, content preservation, and naturalness. Using human evaluation is\nideal but costly, same as in other natural language processing (NLP) tasks,\nhowever, automatic metrics for TST have not received as much attention as\nmetrics for, e.g., machine translation or summarization. In this paper, we\nexamine both set of existing and novel metrics from broader NLP tasks for TST\nevaluation, focusing on two popular subtasks-sentiment transfer and\ndetoxification-in a multilingual context comprising English, Hindi, and\nBengali. By conducting meta-evaluation through correlation with human\njudgments, we demonstrate the effectiveness of these metrics when used\nindividually and in ensembles. Additionally, we investigate the potential of\nLarge Language Models (LLMs) as tools for TST evaluation. Our findings\nhighlight that certain advanced NLP metrics and experimental-hybrid-techniques,\nprovide better insights than existing TST metrics for delivering more accurate,\nconsistent, and reproducible TST evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Style Transfer (TST) is the task of transforming a text to reflect a\nparticular style while preserving its original content. Evaluating TST outputs\nis a multidimensional challenge, requiring the assessment of style transfer\naccuracy, content preservation, and naturalness. Using human evaluation is\nideal but costly, same as in other natural language processing (NLP) tasks,\nhowever, automatic metrics for TST have not received as much attention as\nmetrics for, e.g., machine translation or summarization. In this paper, we\nexamine both set of existing and novel metrics from broader NLP tasks for TST\nevaluation, focusing on two popular subtasks-sentiment transfer and\ndetoxification-in a multilingual context comprising English, Hindi, and\nBengali. By conducting meta-evaluation through correlation with human\njudgments, we demonstrate the effectiveness of these metrics when used\nindividually and in ensembles. Additionally, we investigate the potential of\nLarge Language Models (LLMs) as tools for TST evaluation. Our findings\nhighlight that certain advanced NLP metrics and experimental-hybrid-techniques,\nprovide better insights than existing TST metrics for delivering more accurate,\nconsistent, and reproducible TST evaluations."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Atul Kr. Ojha"
                    },
                    {
                        "name": "John P. McCrae"
                    },
                    {
                        "name": "Ondrej Dusek"
                    }
                ],
                "author_detail": {
                    "name": "Ondrej Dusek"
                },
                "author": "Ondrej Dusek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04711v1",
                "updated": "2025-02-07T07:25:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    25,
                    59,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T07:25:59Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    25,
                    59,
                    4,
                    38,
                    0
                ],
                "title": "Dynamic Frequency-Adaptive Knowledge Distillation for Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Frequency-Adaptive Knowledge Distillation for Speech Enhancement"
                },
                "summary": "Deep learning-based speech enhancement (SE) models have recently outperformed\ntraditional techniques, yet their deployment on resource-constrained devices\nremains challenging due to high computational and memory demands. This paper\nintroduces a novel dynamic frequency-adaptive knowledge distillation (DFKD)\napproach to effectively compress SE models. Our method dynamically assesses the\nmodel's output, distinguishing between high and low-frequency components, and\nadapts the learning objectives to meet the unique requirements of different\nfrequency bands, capitalizing on the SE task's inherent characteristics. To\nevaluate the DFKD's efficacy, we conducted experiments on three\nstate-of-the-art models: DCCRN, ConTasNet, and DPTNet. The results demonstrate\nthat our method not only significantly enhances the performance of the\ncompressed model (student model) but also surpasses other logit-based knowledge\ndistillation methods specifically for SE tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based speech enhancement (SE) models have recently outperformed\ntraditional techniques, yet their deployment on resource-constrained devices\nremains challenging due to high computational and memory demands. This paper\nintroduces a novel dynamic frequency-adaptive knowledge distillation (DFKD)\napproach to effectively compress SE models. Our method dynamically assesses the\nmodel's output, distinguishing between high and low-frequency components, and\nadapts the learning objectives to meet the unique requirements of different\nfrequency bands, capitalizing on the SE task's inherent characteristics. To\nevaluate the DFKD's efficacy, we conducted experiments on three\nstate-of-the-art models: DCCRN, ConTasNet, and DPTNet. The results demonstrate\nthat our method not only significantly enhances the performance of the\ncompressed model (student model) but also surpasses other logit-based knowledge\ndistillation methods specifically for SE tasks."
                },
                "authors": [
                    {
                        "name": "Xihao Yuan"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Lu Zhou"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu",
                "arxiv_comment": "5 pages, 2 figures, accepted by ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04710v1",
                "updated": "2025-02-07T07:24:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    24,
                    0,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T07:24:00Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    24,
                    0,
                    4,
                    38,
                    0
                ],
                "title": "Measuring SES-related traits relating to technology usage: Two validated\n  surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring SES-related traits relating to technology usage: Two validated\n  surveys"
                },
                "summary": "Software producers are now recognizing the importance of improving their\nproducts' suitability for diverse populations, but little attention has been\ngiven to measurements to shed light on products' suitability to individuals\nbelow the median socioeconomic status (SES) -- who, by definition, make up half\nthe population. To enable software practitioners to attend to both lower- and\nhigher-SES individuals, this paper provides two new surveys that together\nfacilitate measuring how well a software product serves socioeconomically\ndiverse populations. The first survey (SES-Subjective) is who-oriented: it\nmeasures who their potential or current users are in terms of their subjective\nSES (perceptions of their SES). The second survey (SES-Facets) is why-oriented:\nit collects individuals' values for an evidence-based set of facet values\n(individual traits) that (1) statistically differ by SES and (2) affect how an\nindividual works and problem-solves with software products. Our empirical\nvalidations with deployments at University A and University B (464 and 522\nresponses, respectively) showed that both surveys are reliable. Further, our\nresults statistically agree with both ground truth data on respondents'\nsocioeconomic statuses and with predictions from foundational literature.\nFinally, we explain how the pair of surveys is uniquely actionable by software\npractitioners, such as in requirements gathering, debugging, quality assurance\nactivities, maintenance activities, and fulfilling legal reporting requirements\nsuch as those being drafted by various governments for AI-powered software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software producers are now recognizing the importance of improving their\nproducts' suitability for diverse populations, but little attention has been\ngiven to measurements to shed light on products' suitability to individuals\nbelow the median socioeconomic status (SES) -- who, by definition, make up half\nthe population. To enable software practitioners to attend to both lower- and\nhigher-SES individuals, this paper provides two new surveys that together\nfacilitate measuring how well a software product serves socioeconomically\ndiverse populations. The first survey (SES-Subjective) is who-oriented: it\nmeasures who their potential or current users are in terms of their subjective\nSES (perceptions of their SES). The second survey (SES-Facets) is why-oriented:\nit collects individuals' values for an evidence-based set of facet values\n(individual traits) that (1) statistically differ by SES and (2) affect how an\nindividual works and problem-solves with software products. Our empirical\nvalidations with deployments at University A and University B (464 and 522\nresponses, respectively) showed that both surveys are reliable. Further, our\nresults statistically agree with both ground truth data on respondents'\nsocioeconomic statuses and with predictions from foundational literature.\nFinally, we explain how the pair of surveys is uniquely actionable by software\npractitioners, such as in requirements gathering, debugging, quality assurance\nactivities, maintenance activities, and fulfilling legal reporting requirements\nsuch as those being drafted by various governments for AI-powered software."
                },
                "authors": [
                    {
                        "name": "Chimdi Chikezie"
                    },
                    {
                        "name": "Pannapat Chenpaiseng"
                    },
                    {
                        "name": "Puja Agarwal"
                    },
                    {
                        "name": "Sadia Afroz"
                    },
                    {
                        "name": "Bhavika Madhwani"
                    },
                    {
                        "name": "Rudrajit Choudhuri"
                    },
                    {
                        "name": "Andrew Anderson"
                    },
                    {
                        "name": "Prisha Velhal"
                    },
                    {
                        "name": "Patricia Morreale"
                    },
                    {
                        "name": "Christopher Bogart"
                    },
                    {
                        "name": "Anita Sarma"
                    },
                    {
                        "name": "Margaret Burnett"
                    }
                ],
                "author_detail": {
                    "name": "Margaret Burnett"
                },
                "author": "Margaret Burnett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12311v2",
                "updated": "2025-02-07T07:16:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    16,
                    59,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:14:31Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    14,
                    31,
                    0,
                    141,
                    0
                ],
                "title": "SpotKube: Cost-Optimal Microservices Deployment with Cluster Autoscaling\n  and Spot Pricing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpotKube: Cost-Optimal Microservices Deployment with Cluster Autoscaling\n  and Spot Pricing"
                },
                "summary": "Microservices architecture, known for its agility and efficiency, is an ideal\nframework for cloud-based software development and deployment. When integrated\nwith containerization and orchestration systems, resource management becomes\nmore streamlined. However, cloud computing costs remain a critical concern,\nnecessitating effective strategies to minimize expenses without compromising\nperformance. Cloud platforms like AWS offer transient pricing options, such as\nSpot Pricing, to reduce operational costs. However, unpredictable demand and\nabrupt termination of spot VMs introduce challenges. By leveraging\ncontainerization and intelligent orchestration, microservices deployment costs\ncan be optimized while maintaining performance requirements. We present\nSpotKube, an open-source, Kubernetes-based solution that employs a genetic\nalgorithm for cost optimization. Designed to dynamically scale clusters for\nmicroservice applications on public clouds using spot pricing, SpotKube\nanalyzes application characteristics to recommend optimal resource allocations.\nThis ensures cost-effective deployments without sacrificing performance. Its\nelastic cluster autoscaler adapts to changing demands, gracefully managing node\nterminations to minimize disruptions in system availability.Evaluations\nconducted using real-world public cloud setups demonstrate SpotKube's superior\nperformance and cost efficiency compared to alternative optimization\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microservices architecture, known for its agility and efficiency, is an ideal\nframework for cloud-based software development and deployment. When integrated\nwith containerization and orchestration systems, resource management becomes\nmore streamlined. However, cloud computing costs remain a critical concern,\nnecessitating effective strategies to minimize expenses without compromising\nperformance. Cloud platforms like AWS offer transient pricing options, such as\nSpot Pricing, to reduce operational costs. However, unpredictable demand and\nabrupt termination of spot VMs introduce challenges. By leveraging\ncontainerization and intelligent orchestration, microservices deployment costs\ncan be optimized while maintaining performance requirements. We present\nSpotKube, an open-source, Kubernetes-based solution that employs a genetic\nalgorithm for cost optimization. Designed to dynamically scale clusters for\nmicroservice applications on public clouds using spot pricing, SpotKube\nanalyzes application characteristics to recommend optimal resource allocations.\nThis ensures cost-effective deployments without sacrificing performance. Its\nelastic cluster autoscaler adapts to changing demands, gracefully managing node\nterminations to minimize disruptions in system availability.Evaluations\nconducted using real-world public cloud setups demonstrate SpotKube's superior\nperformance and cost efficiency compared to alternative optimization\nstrategies."
                },
                "authors": [
                    {
                        "name": "Dasith Edirisinghe"
                    },
                    {
                        "name": "Kavinda Rajapakse"
                    },
                    {
                        "name": "Pasindu Abeysinghe"
                    },
                    {
                        "name": "Sunimal Rathnayake"
                    }
                ],
                "author_detail": {
                    "name": "Sunimal Rathnayake"
                },
                "author": "Sunimal Rathnayake",
                "arxiv_doi": "10.1109/CloudCom62794.2024.00026",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CloudCom62794.2024.00026",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages including references, 9 figures, Keywords: Microservices,\n  Cost Optimization, Genetic Algorithm, Transient Pricing, Cluster Auto Scaling",
                "arxiv_journal_ref": "In Proceedings of the 2024 IEEE International Conference on Cloud\n  Computing Technology and Science (CloudCom), pp. 87-94",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19477v2",
                "updated": "2025-02-07T07:08:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    8,
                    29,
                    4,
                    38,
                    0
                ],
                "published": "2024-11-29T05:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models"
                },
                "summary": "We propose two simple yet principled algorithms that enjoy provable scaling\nlaws for the test-time compute of large language models (LLMs), which require a\nblack-box LLM and nothing else (e.g., no external verifier or reward model) for\na minimalistic implementation. (i) The first one is a two-stage knockout-style\nalgorithm: given an input problem, it first generates multiple candidate\nsolutions, and then aggregate them for a final output, via a knockout\ntournament where pairwise comparisons among the candidates are conducted.\nAssuming that the LLM can generate a correct solution with non-zero probability\nand do better than a random guess in comparing a pair of correct and incorrect\nsolutions, we prove theoretically that the failure probability of this\nalgorithm decays to zero exponentially or by a power law (depending on the\nspecific way of scaling) as its test-time compute grows. (ii) The second one is\na two-stage league-style algorithm, where each candidate solution is evaluated\nby its average win rate against multiple opponents, rather than eliminated upon\nloss to a single opponent. Under certain technical assumptions that are\nanalogous to but more robust than those required by the knockout-style\nalgorithm, we prove theoretically that the failure probability of the\nleague-style algorithm also decays to zero exponentially as its test-time\ncompute grows. Through extensive experiments with two challenging benchmarks,\nnamely GPQA and MMLU-Pro, we validate the proposed theories and demonstrate the\noutstanding scaling properties of both algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose two simple yet principled algorithms that enjoy provable scaling\nlaws for the test-time compute of large language models (LLMs), which require a\nblack-box LLM and nothing else (e.g., no external verifier or reward model) for\na minimalistic implementation. (i) The first one is a two-stage knockout-style\nalgorithm: given an input problem, it first generates multiple candidate\nsolutions, and then aggregate them for a final output, via a knockout\ntournament where pairwise comparisons among the candidates are conducted.\nAssuming that the LLM can generate a correct solution with non-zero probability\nand do better than a random guess in comparing a pair of correct and incorrect\nsolutions, we prove theoretically that the failure probability of this\nalgorithm decays to zero exponentially or by a power law (depending on the\nspecific way of scaling) as its test-time compute grows. (ii) The second one is\na two-stage league-style algorithm, where each candidate solution is evaluated\nby its average win rate against multiple opponents, rather than eliminated upon\nloss to a single opponent. Under certain technical assumptions that are\nanalogous to but more robust than those required by the knockout-style\nalgorithm, we prove theoretically that the failure probability of the\nleague-style algorithm also decays to zero exponentially as its test-time\ncompute grows. Through extensive experiments with two challenging benchmarks,\nnamely GPQA and MMLU-Pro, we validate the proposed theories and demonstrate the\noutstanding scaling properties of both algorithms."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "arXiv v2 update: additional algorithms, theories and experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04700v1",
                "updated": "2025-02-07T07:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    7,
                    4,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T07:07:04Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    7,
                    4,
                    4,
                    38,
                    0
                ],
                "title": "EigenLoRAx: Recycling Adapters to Find Principal Subspaces for\n  Resource-Efficient Adaptation and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EigenLoRAx: Recycling Adapters to Find Principal Subspaces for\n  Resource-Efficient Adaptation and Inference"
                },
                "summary": "The rapid growth of large models has raised concerns about their\nenvironmental impact and equity in accessibility due to significant\ncomputational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for\nfinetuning large models, resulting in an abundance of publicly available\nadapters tailored to diverse domains. We ask: Can these pretrained adapters be\nleveraged to further streamline adaptation to new tasks while addressing these\nchallenges? We introduce EigenLoRAx, a parameter-efficient finetuning method\nthat recycles existing adapters to create a principal subspace aligned with\ntheir shared domain knowledge which can be further augmented with orthogonal\nbasis vectors in low-resource scenarios. This enables rapid adaptation to new\ntasks by learning only lightweight coefficients on the principal components of\nthe subspace - eliminating the need to finetune entire adapters. EigenLoRAx\nrequires significantly fewer parameters and memory, improving efficiency for\nboth training and inference. Our method demonstrates strong performance across\ndiverse domains and tasks, offering a scalable for edge-based applications,\npersonalization, and equitable deployment of large models in\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of large models has raised concerns about their\nenvironmental impact and equity in accessibility due to significant\ncomputational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for\nfinetuning large models, resulting in an abundance of publicly available\nadapters tailored to diverse domains. We ask: Can these pretrained adapters be\nleveraged to further streamline adaptation to new tasks while addressing these\nchallenges? We introduce EigenLoRAx, a parameter-efficient finetuning method\nthat recycles existing adapters to create a principal subspace aligned with\ntheir shared domain knowledge which can be further augmented with orthogonal\nbasis vectors in low-resource scenarios. This enables rapid adaptation to new\ntasks by learning only lightweight coefficients on the principal components of\nthe subspace - eliminating the need to finetune entire adapters. EigenLoRAx\nrequires significantly fewer parameters and memory, improving efficiency for\nboth training and inference. Our method demonstrates strong performance across\ndiverse domains and tasks, offering a scalable for edge-based applications,\npersonalization, and equitable deployment of large models in\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Prakhar Kaushik"
                    },
                    {
                        "name": "Ankit Vaidya"
                    },
                    {
                        "name": "Shravan Chaudhari"
                    },
                    {
                        "name": "Alan Yuille"
                    }
                ],
                "author_detail": {
                    "name": "Alan Yuille"
                },
                "author": "Alan Yuille",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09639v2",
                "updated": "2025-02-07T07:02:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    2,
                    26,
                    4,
                    38,
                    0
                ],
                "published": "2024-08-19T01:53:47Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    53,
                    47,
                    0,
                    232,
                    0
                ],
                "title": "How to Make the Most of LLMs' Grammatical Knowledge for Acceptability\n  Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Make the Most of LLMs' Grammatical Knowledge for Acceptability\n  Judgments"
                },
                "summary": "The grammatical knowledge of language models (LMs) is often measured using a\nbenchmark of linguistic minimal pairs, where the LMs are presented with a pair\nof acceptable and unacceptable sentences and required to judge which is more\nacceptable. Conventional approaches directly compare sentence probabilities\nassigned by LMs, but recent large language models (LLMs) are trained to perform\ntasks via prompting, and thus, the raw probabilities they assign may not fully\nreflect their grammatical knowledge. In this study, we attempt to derive more\naccurate acceptability judgments from LLMs using prompts and templates. Through\nextensive experiments in English and Chinese, we compare nine judgment methods\nand find two of them, a probability readout method -- in-template LP and a\nprompt-based method -- Yes/No probability computing, achieve higher accuracy\nthan the conventional ones. Our analysis reveals that these methods excel in\ndifferent linguistic phenomena, suggesting they access different aspects of\nLLMs' knowledge. We also find that ensembling the two methods outperforms\nsingle methods. Consequently, we recommend these techniques, either\nindividually or ensembled, as more effective alternatives to conventional\napproaches for assessing grammatical knowledge in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The grammatical knowledge of language models (LMs) is often measured using a\nbenchmark of linguistic minimal pairs, where the LMs are presented with a pair\nof acceptable and unacceptable sentences and required to judge which is more\nacceptable. Conventional approaches directly compare sentence probabilities\nassigned by LMs, but recent large language models (LLMs) are trained to perform\ntasks via prompting, and thus, the raw probabilities they assign may not fully\nreflect their grammatical knowledge. In this study, we attempt to derive more\naccurate acceptability judgments from LLMs using prompts and templates. Through\nextensive experiments in English and Chinese, we compare nine judgment methods\nand find two of them, a probability readout method -- in-template LP and a\nprompt-based method -- Yes/No probability computing, achieve higher accuracy\nthan the conventional ones. Our analysis reveals that these methods excel in\ndifferent linguistic phenomena, suggesting they access different aspects of\nLLMs' knowledge. We also find that ensembling the two methods outperforms\nsingle methods. Consequently, we recommend these techniques, either\nindividually or ensembled, as more effective alternatives to conventional\napproaches for assessing grammatical knowledge in LLMs."
                },
                "authors": [
                    {
                        "name": "Yusuke Ide"
                    },
                    {
                        "name": "Yuto Nishida"
                    },
                    {
                        "name": "Justin Vasselli"
                    },
                    {
                        "name": "Miyu Oba"
                    },
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "NAACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04697v1",
                "updated": "2025-02-07T06:59:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    59,
                    31,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T06:59:31Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    59,
                    31,
                    4,
                    38,
                    0
                ],
                "title": "Multi-Agent Coverage Control in Non-Convex Annulus Region with Conformal\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Coverage Control in Non-Convex Annulus Region with Conformal\n  Mapping"
                },
                "summary": "Efficiently fulfilling coverage tasks in non-convex regions has long been a\nsignificant challenge for multi-agent systems (MASs). By leveraging conformal\nmapping, this paper introduces a novel sectorial coverage formulation to\ntransform a non-convex annulus region into a topologically equivalent one. This\napproach enables the deployment of MASs in a non-star-shaped region while\noptimizing coverage performance and achieving load balance among sub-regions.\nIt provides a unique perspective on the partitioned sub-regions to highlight\nthe geodesic convex property of the non-star-shaped region. By utilizing the\nsectorial partition mechanism and the diffeomorphism property of conformal\nmapping, a decentralized control law is designed to drive MASs towards a\ndesired configuration, which not only optimizes the global coverage cost but\nalso ensures exponential convergence of equitable workload. Moreover, an\niterative search algorithm is developed to identify the optimal approximation\nof multi-agent deployment in the non-star-shaped region. Theoretical analysis\nis conducted to confirm the asymptotic stability and global convergence with\narbitrary small tolerance of the closed-loop system. Finally, numerical\nsimulations demonstrate the practicality of the proposed coverage formulation\nwith conformal mapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently fulfilling coverage tasks in non-convex regions has long been a\nsignificant challenge for multi-agent systems (MASs). By leveraging conformal\nmapping, this paper introduces a novel sectorial coverage formulation to\ntransform a non-convex annulus region into a topologically equivalent one. This\napproach enables the deployment of MASs in a non-star-shaped region while\noptimizing coverage performance and achieving load balance among sub-regions.\nIt provides a unique perspective on the partitioned sub-regions to highlight\nthe geodesic convex property of the non-star-shaped region. By utilizing the\nsectorial partition mechanism and the diffeomorphism property of conformal\nmapping, a decentralized control law is designed to drive MASs towards a\ndesired configuration, which not only optimizes the global coverage cost but\nalso ensures exponential convergence of equitable workload. Moreover, an\niterative search algorithm is developed to identify the optimal approximation\nof multi-agent deployment in the non-star-shaped region. Theoretical analysis\nis conducted to confirm the asymptotic stability and global convergence with\narbitrary small tolerance of the closed-loop system. Finally, numerical\nsimulations demonstrate the practicality of the proposed coverage formulation\nwith conformal mapping."
                },
                "authors": [
                    {
                        "name": "Xun Feng"
                    },
                    {
                        "name": "Chao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhai"
                },
                "author": "Chao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02481v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02481v3",
                "updated": "2025-02-07T06:59:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    59,
                    27,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-04T16:57:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    57,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study"
                },
                "summary": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo."
                },
                "authors": [
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Pengzhi Gao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "Accept to NAACL2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02481v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02481v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04692v1",
                "updated": "2025-02-07T06:37:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    37,
                    5,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T06:37:05Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    37,
                    5,
                    4,
                    38,
                    0
                ],
                "title": "STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion"
                },
                "summary": "Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving significant\nimprovements in efficiency and task performance. Using STRIDE-generated\nrewards, simulated humanoid robots achieve sprint-level locomotion across\ncomplex terrains, highlighting its ability to advance DRL workflows and\nhumanoid robotics research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving significant\nimprovements in efficiency and task performance. Using STRIDE-generated\nrewards, simulated humanoid robots achieve sprint-level locomotion across\ncomplex terrains, highlighting its ability to advance DRL workflows and\nhumanoid robotics research."
                },
                "authors": [
                    {
                        "name": "Zhenwei Wu"
                    },
                    {
                        "name": "Jinxiong Lu"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Luhui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Luhui Hu"
                },
                "author": "Luhui Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11301v2",
                "updated": "2025-02-07T06:34:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    34,
                    14,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-20T07:05:15Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    7,
                    5,
                    15,
                    0,
                    20,
                    0
                ],
                "title": "Question-to-Question Retrieval for Hallucination-Free Knowledge Access:\n  An Approach for Wikipedia and Wikidata Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question-to-Question Retrieval for Hallucination-Free Knowledge Access:\n  An Approach for Wikipedia and Wikidata Question Answering"
                },
                "summary": "This paper introduces an approach to question answering over knowledge bases\nlike Wikipedia and Wikidata by performing \"question-to-question\" matching and\nretrieval from a dense vector embedding store. Instead of embedding document\ncontent, we generate a comprehensive set of questions for each logical content\nunit using an instruction-tuned LLM. These questions are vector-embedded and\nstored, mapping to the corresponding content. Vector embedding of user queries\nare then matched against this question vector store. The highest similarity\nscore leads to direct retrieval of the associated article content, eliminating\nthe need for answer generation. Our method achieves high cosine similarity ( >\n0.9 ) for relevant question pairs, enabling highly precise retrieval. This\napproach offers several advantages including computational efficiency, rapid\nresponse times, and increased scalability. We demonstrate its effectiveness on\nWikipedia and Wikidata, including multimedia content through structured fact\nretrieval from Wikidata, opening up new pathways for multimodal question\nanswering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an approach to question answering over knowledge bases\nlike Wikipedia and Wikidata by performing \"question-to-question\" matching and\nretrieval from a dense vector embedding store. Instead of embedding document\ncontent, we generate a comprehensive set of questions for each logical content\nunit using an instruction-tuned LLM. These questions are vector-embedded and\nstored, mapping to the corresponding content. Vector embedding of user queries\nare then matched against this question vector store. The highest similarity\nscore leads to direct retrieval of the associated article content, eliminating\nthe need for answer generation. Our method achieves high cosine similarity ( >\n0.9 ) for relevant question pairs, enabling highly precise retrieval. This\napproach offers several advantages including computational efficiency, rapid\nresponse times, and increased scalability. We demonstrate its effectiveness on\nWikipedia and Wikidata, including multimedia content through structured fact\nretrieval from Wikidata, opening up new pathways for multimodal question\nanswering."
                },
                "authors": [
                    {
                        "name": "Santhosh Thottingal"
                    }
                ],
                "author_detail": {
                    "name": "Santhosh Thottingal"
                },
                "author": "Santhosh Thottingal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04689v1",
                "updated": "2025-02-07T06:30:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    30,
                    33,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T06:30:33Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    30,
                    33,
                    4,
                    38,
                    0
                ],
                "title": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning"
                },
                "summary": "Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR."
                },
                "authors": [
                    {
                        "name": "Yuwei Yin"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Carenini"
                },
                "author": "Giuseppe Carenini",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04688v1",
                "updated": "2025-02-07T06:27:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    27,
                    4,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T06:27:04Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    27,
                    4,
                    4,
                    38,
                    0
                ],
                "title": "M-IFEval: Multilingual Instruction-Following Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-IFEval: Multilingual Instruction-Following Evaluation"
                },
                "summary": "Instruction following is a core capability of modern Large language models\n(LLMs), making evaluating this capability essential to understanding these\nmodels. The Instruction Following Evaluation (IFEval) benchmark from the\nliterature does this using objective criteria, offering a measure of LLM\nperformance without subjective AI or human judgement. However, it only includes\nEnglish instructions, limiting its ability to assess LLMs in other languages.\n  We propose the Multilingual Instruction Following Evaluation (M-IFEval)\nbenchmark, expanding the evaluation to French, Japanese, and Spanish, with both\ngeneral and language-specific instructions. Applying this benchmark to 8\nstate-of-the-art LLMs, we find that benchmark performance across languages and\ninstruction types can vary widely, underscoring the importance of a\nmultilingual benchmark for evaluating LLMs in a diverse cultural context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction following is a core capability of modern Large language models\n(LLMs), making evaluating this capability essential to understanding these\nmodels. The Instruction Following Evaluation (IFEval) benchmark from the\nliterature does this using objective criteria, offering a measure of LLM\nperformance without subjective AI or human judgement. However, it only includes\nEnglish instructions, limiting its ability to assess LLMs in other languages.\n  We propose the Multilingual Instruction Following Evaluation (M-IFEval)\nbenchmark, expanding the evaluation to French, Japanese, and Spanish, with both\ngeneral and language-specific instructions. Applying this benchmark to 8\nstate-of-the-art LLMs, we find that benchmark performance across languages and\ninstruction types can vary widely, underscoring the importance of a\nmultilingual benchmark for evaluating LLMs in a diverse cultural context."
                },
                "authors": [
                    {
                        "name": "Antoine Dussolle"
                    },
                    {
                        "name": "Andrea Cardeña Díaz"
                    },
                    {
                        "name": "Shota Sato"
                    },
                    {
                        "name": "Peter Devine"
                    }
                ],
                "author_detail": {
                    "name": "Peter Devine"
                },
                "author": "Peter Devine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02298v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02298v4",
                "updated": "2025-02-07T06:23:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    23,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-03T08:34:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    34,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models"
                },
                "summary": "As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems."
                },
                "authors": [
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Yiting Dong"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by ICLR2025. url: https://openreview.net/forum?id=s20W12XTF8",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02298v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02298v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03885v2",
                "updated": "2025-02-07T06:22:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    22,
                    14,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-06T09:01:24Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    1,
                    24,
                    3,
                    37,
                    0
                ],
                "title": "InfinitePOD: Building Datacenter-Scale High-Bandwidth Domain for LLM\n  with Optical Circuit Switching Transceivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfinitePOD: Building Datacenter-Scale High-Bandwidth Domain for LLM\n  with Optical Circuit Switching Transceivers"
                },
                "summary": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches,\nbut the fault explosion radius remains large at the cube level (e.g., 64 TPUs).\n  We propose InfinitePOD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfinitePOD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt into variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfinitePOD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches,\nbut the fault explosion radius remains large at the cube level (e.g., 64 TPUs).\n  We propose InfinitePOD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfinitePOD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt into variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfinitePOD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node)."
                },
                "authors": [
                    {
                        "name": "Chenchen Shou"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Huaiyu Meng"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Wenqing Lv"
                    },
                    {
                        "name": "Yelong Xu"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Zhang Chen"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yichen Shen"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04686v1",
                "updated": "2025-02-07T06:19:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    19,
                    55,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T06:19:55Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    19,
                    55,
                    4,
                    38,
                    0
                ],
                "title": "Learning Strategic Language Agents in the Werewolf Game with Iterative\n  Latent Space Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Strategic Language Agents in the Werewolf Game with Iterative\n  Latent Space Policy Optimization"
                },
                "summary": "Large language model (LLM)-based agents have recently shown impressive\nprogress in a variety of domains, including open-ended conversation and\nmulti-step decision-making. However, applying these agents to social deduction\ngames such as Werewolf, which requires both strategic decision-making and\nfree-form language interaction, remains non-trivial. Traditional methods based\non Counterfactual Regret Minimization (CFR) or reinforcement learning (RL)\ntypically depend on a predefined action space, making them unsuitable for\nlanguage games with unconstrained text action space. Meanwhile, pure LLM-based\nagents often suffer from intrinsic biases and require prohibitively large\ndatasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO),\nan iterative framework that addresses these challenges by first mapping\nfree-form text to a discrete latent space, where methods like CFR and RL can\nlearn strategic policy more effectively. We then translate the learned policy\nback into natural language dialogues, which are used to fine-tune an LLM via\nDirect Preference Optimization (DPO). By iteratively alternating between these\nstages, our LSPO agent progressively enhances both strategic reasoning and\nlanguage communication. Experiment results on the Werewolf game show that our\nmethod improves the agent's performance in each iteration and outperforms\nexisting Werewolf agents, underscoring its promise for free-form language\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents have recently shown impressive\nprogress in a variety of domains, including open-ended conversation and\nmulti-step decision-making. However, applying these agents to social deduction\ngames such as Werewolf, which requires both strategic decision-making and\nfree-form language interaction, remains non-trivial. Traditional methods based\non Counterfactual Regret Minimization (CFR) or reinforcement learning (RL)\ntypically depend on a predefined action space, making them unsuitable for\nlanguage games with unconstrained text action space. Meanwhile, pure LLM-based\nagents often suffer from intrinsic biases and require prohibitively large\ndatasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO),\nan iterative framework that addresses these challenges by first mapping\nfree-form text to a discrete latent space, where methods like CFR and RL can\nlearn strategic policy more effectively. We then translate the learned policy\nback into natural language dialogues, which are used to fine-tune an LLM via\nDirect Preference Optimization (DPO). By iteratively alternating between these\nstages, our LSPO agent progressively enhances both strategic reasoning and\nlanguage communication. Experiment results on the Werewolf game show that our\nmethod improves the agent's performance in each iteration and outperforms\nexisting Werewolf agents, underscoring its promise for free-form language\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Zelai Xu"
                    },
                    {
                        "name": "Wanjun Gu"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01128v2",
                "updated": "2025-02-07T05:59:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    59,
                    26,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-03T07:47:49Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    7,
                    47,
                    49,
                    0,
                    34,
                    0
                ],
                "title": "C codegen considered unnecessary: go directly to binary, do not pass C.\n  Compilation of Julia code for deployment in model-based engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C codegen considered unnecessary: go directly to binary, do not pass C.\n  Compilation of Julia code for deployment in model-based engineering"
                },
                "summary": "Since time immemorial an old adage has always seemed to ring true: you cannot\nuse a high-level productive programming language like Python or R for real-time\ncontrol and embedded-systems programming, you must rewrite your program in C.\nWe present a counterexample to this mantra by demonstrating how recent compiler\ndevelopments in the Julia programming language allow users of Julia and the\nequation-based modeling language ModelingToolkit to compile and deploy binaries\nfor real-time model-based estimation and control. Contrary to the approach\ntaken by a majority of modeling and simulation tools, we do not generate C\ncode, and instead demonstrate how we may use the native Julia code-generation\npipeline through LLVM to compile architecture-specific binaries from high-level\ncode. This approach avoids many of the restrictions typically placed on\nhigh-level languages to enable C-code generation. As case studies, we include a\nnonlinear state estimator derived from an equation-based model which is\ncompiled into a program that performs state estimation for deployment onto a\nRaspberry Pi, as well as a PID controller library implemented in Julia and\ncompiled into a shared library callable from a C program.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since time immemorial an old adage has always seemed to ring true: you cannot\nuse a high-level productive programming language like Python or R for real-time\ncontrol and embedded-systems programming, you must rewrite your program in C.\nWe present a counterexample to this mantra by demonstrating how recent compiler\ndevelopments in the Julia programming language allow users of Julia and the\nequation-based modeling language ModelingToolkit to compile and deploy binaries\nfor real-time model-based estimation and control. Contrary to the approach\ntaken by a majority of modeling and simulation tools, we do not generate C\ncode, and instead demonstrate how we may use the native Julia code-generation\npipeline through LLVM to compile architecture-specific binaries from high-level\ncode. This approach avoids many of the restrictions typically placed on\nhigh-level languages to enable C-code generation. As case studies, we include a\nnonlinear state estimator derived from an equation-based model which is\ncompiled into a program that performs state estimation for deployment onto a\nRaspberry Pi, as well as a PID controller library implemented in Julia and\ncompiled into a shared library callable from a C program."
                },
                "authors": [
                    {
                        "name": "Fredrik Bagge Carlson"
                    },
                    {
                        "name": "Cody Tapscott"
                    },
                    {
                        "name": "Gabriel Baraldi"
                    },
                    {
                        "name": "Chris Rackauckas"
                    }
                ],
                "author_detail": {
                    "name": "Chris Rackauckas"
                },
                "author": "Chris Rackauckas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04677v1",
                "updated": "2025-02-07T05:49:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    49,
                    50,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T05:49:50Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    49,
                    50,
                    4,
                    38,
                    0
                ],
                "title": "LLM Query Scheduling with Prefix Reuse and Latency Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Query Scheduling with Prefix Reuse and Latency Constraints"
                },
                "summary": "The efficient deployment of large language models (LLMs) in online settings\nrequires optimizing inference performance under stringent latency constraints,\nparticularly the time-to-first-token (TTFT) and time-per-output-token (TPOT).\nThis paper focuses on the query scheduling problem for LLM inference with\nprefix reuse, a technique that leverages shared prefixes across queries to\nreduce computational overhead. Our work reveals previously unknown limitations\nof the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM)\nscheduling strategies with respect to satisfying latency constraints. We\npresent a formal theoretical framework for LLM query scheduling under\nRadixAttention, a prefix reuse mechanism that stores and reuses intermediate\nrepresentations in a radix tree structure. Our analysis establishes the\nNP-hardness of the scheduling problem with prefix reuse under TTFT constraints\nand proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing\nmethods by balancing prefix reuse and fairness in query processing. Theoretical\nguarantees demonstrate that $k$-LPM achieves improved TTFT performance under\nrealistic traffic patterns captured by a data generative model. Empirical\nevaluations in a realistic serving setting validates our findings, showing\nsignificant reductions in P99 TTFT compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient deployment of large language models (LLMs) in online settings\nrequires optimizing inference performance under stringent latency constraints,\nparticularly the time-to-first-token (TTFT) and time-per-output-token (TPOT).\nThis paper focuses on the query scheduling problem for LLM inference with\nprefix reuse, a technique that leverages shared prefixes across queries to\nreduce computational overhead. Our work reveals previously unknown limitations\nof the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM)\nscheduling strategies with respect to satisfying latency constraints. We\npresent a formal theoretical framework for LLM query scheduling under\nRadixAttention, a prefix reuse mechanism that stores and reuses intermediate\nrepresentations in a radix tree structure. Our analysis establishes the\nNP-hardness of the scheduling problem with prefix reuse under TTFT constraints\nand proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing\nmethods by balancing prefix reuse and fairness in query processing. Theoretical\nguarantees demonstrate that $k$-LPM achieves improved TTFT performance under\nrealistic traffic patterns captured by a data generative model. Empirical\nevaluations in a realistic serving setting validates our findings, showing\nsignificant reductions in P99 TTFT compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Gregory Dexter"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Ata Fatahi Baarzi"
                    },
                    {
                        "name": "Qingquan Song"
                    },
                    {
                        "name": "Tejas Dharamsi"
                    },
                    {
                        "name": "Aman Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Aman Gupta"
                },
                "author": "Aman Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04420v2",
                "updated": "2025-02-07T05:38:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    38,
                    55,
                    4,
                    38,
                    0
                ],
                "published": "2024-07-05T11:10:53Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    11,
                    10,
                    53,
                    4,
                    187,
                    0
                ],
                "title": "cosmosage: A Natural-Language Assistant for Cosmologists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cosmosage: A Natural-Language Assistant for Cosmologists"
                },
                "summary": "cosmosage is a natural-language assistant intended for a wide audience, from\nlaypersons interested in cosmology to students, teachers, and professional\ncosmologists. cosmosage provides a novel way to access knowledge and reason\nabout cosmology. Leveraging the power of advanced large language models (LLMs),\ncosmosage has learned from a vast corpus of open-access source texts, including\ntextbooks and papers. cosmosage is found to be state-of-the-art on the narrow\ntask of answering questions about cosmology, outperforming all general-purpose\nmodels. The model parameters and code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cosmosage is a natural-language assistant intended for a wide audience, from\nlaypersons interested in cosmology to students, teachers, and professional\ncosmologists. cosmosage provides a novel way to access knowledge and reason\nabout cosmology. Leveraging the power of advanced large language models (LLMs),\ncosmosage has learned from a vast corpus of open-access source texts, including\ntextbooks and papers. cosmosage is found to be state-of-the-art on the narrow\ntask of answering questions about cosmology, outperforming all general-purpose\nmodels. The model parameters and code are publicly available."
                },
                "authors": [
                    {
                        "name": "Tijmen de Haan"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen de Haan"
                },
                "author": "Tijmen de Haan",
                "arxiv_doi": "10.1016/j.ascom.2025.100934",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ascom.2025.100934",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Astronomy and Computing, Volume 51, 2025, 100934, ISSN 2213-1337",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04667v1",
                "updated": "2025-02-07T05:21:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    21,
                    13,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T05:21:13Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    21,
                    13,
                    4,
                    38,
                    0
                ],
                "title": "Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought\n  Enhances Reasoning Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought\n  Enhances Reasoning Generalization"
                },
                "summary": "Training large language models (LLMs) with high-quality Chain-of-Thought\n(CoT) annotations has become a widely adopted strategy due to its significant\nenhancement of reasoning capabilities. To fully comprehend this approach, two\nquestions naturally arise: (Q1) What advantages does training with CoT offer\ncompared to training without CoT? (Q2) If there are advantages, what are the\nunderlying mechanisms of explicit CoT training? Analyzing the advantages and\nmechanisms of CoT training is challenging due to the many factors involved. To\naddress this, we conduct a detailed analysis using clear and controllable data\ndistributions and, for the first time, reveal that CoT training offers the\nfollowing advantages: (1) Training with CoT markedly improves reasoning\ngeneralization, extending it from in-distribution (ID) to both ID and\nout-of-distribution (OOD) scenarios, while also speeding up convergence; (2)\nEven when training with CoT includes a certain range of erroneous reasoning\nsteps, it still enables the model to learn reasoning patterns, leading to\nsystematic generalization. We further explore the underlying mechanisms from a\ncircuit perspective: (1) The data distribution (e.g., ratio $\\lambda$ and\npattern) plays a crucial role in influencing the model's systematic\ngeneralization; (2) CoT training (with two-hop facts) internalizes reasoning\ninto a two-stage generalizing circuit, where the number of stages corresponds\nto the explicit reasoning steps during training. Our findings elucidate the\nmechanisms underlying explicit CoT training and offer critical insights into\ntuning strategies for LLMs to achieve robust generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) with high-quality Chain-of-Thought\n(CoT) annotations has become a widely adopted strategy due to its significant\nenhancement of reasoning capabilities. To fully comprehend this approach, two\nquestions naturally arise: (Q1) What advantages does training with CoT offer\ncompared to training without CoT? (Q2) If there are advantages, what are the\nunderlying mechanisms of explicit CoT training? Analyzing the advantages and\nmechanisms of CoT training is challenging due to the many factors involved. To\naddress this, we conduct a detailed analysis using clear and controllable data\ndistributions and, for the first time, reveal that CoT training offers the\nfollowing advantages: (1) Training with CoT markedly improves reasoning\ngeneralization, extending it from in-distribution (ID) to both ID and\nout-of-distribution (OOD) scenarios, while also speeding up convergence; (2)\nEven when training with CoT includes a certain range of erroneous reasoning\nsteps, it still enables the model to learn reasoning patterns, leading to\nsystematic generalization. We further explore the underlying mechanisms from a\ncircuit perspective: (1) The data distribution (e.g., ratio $\\lambda$ and\npattern) plays a crucial role in influencing the model's systematic\ngeneralization; (2) CoT training (with two-hop facts) internalizes reasoning\ninto a two-stage generalizing circuit, where the number of stages corresponds\nto the explicit reasoning steps during training. Our findings elucidate the\nmechanisms underlying explicit CoT training and offer critical insights into\ntuning strategies for LLMs to achieve robust generalization."
                },
                "authors": [
                    {
                        "name": "Xinhao Yao"
                    },
                    {
                        "name": "Ruifeng Ren"
                    },
                    {
                        "name": "Yun Liao"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04666v1",
                "updated": "2025-02-07T05:19:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    19,
                    13,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T05:19:13Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    19,
                    13,
                    4,
                    38,
                    0
                ],
                "title": "Enhancing Health Information Retrieval with RAG by Prioritizing Topical\n  Relevance and Factual Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Health Information Retrieval with RAG by Prioritizing Topical\n  Relevance and Factual Accuracy"
                },
                "summary": "The exponential surge in online health information, coupled with its\nincreasing use by non-experts, highlights the pressing need for advanced Health\nInformation Retrieval models that consider not only topical relevance but also\nthe factual accuracy of the retrieved information, given the potential risks\nassociated with health misinformation. To this aim, this paper introduces a\nsolution driven by Retrieval-Augmented Generation (RAG), which leverages the\ncapabilities of generative Large Language Models (LLMs) to enhance the\nretrieval of health-related documents grounded in scientific evidence. In\nparticular, we propose a three-stage model: in the first stage, the user's\nquery is employed to retrieve topically relevant passages with associated\nreferences from a knowledge base constituted by scientific literature. In the\nsecond stage, these passages, alongside the initial query, are processed by\nLLMs to generate a contextually relevant rich text (GenText). In the last\nstage, the documents to be retrieved are evaluated and ranked both from the\npoint of view of topical relevance and factual accuracy by means of their\ncomparison with GenText, either through stance detection or semantic\nsimilarity. In addition to calculating factual accuracy, GenText can offer a\nlayer of explainability for it, aiding users in understanding the reasoning\nbehind the retrieval. Experimental evaluation of our model on benchmark\ndatasets and against baseline models demonstrates its effectiveness in\nenhancing the retrieval of both topically relevant and factually accurate\nhealth information, thus presenting a significant step forward in the health\nmisinformation mitigation problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential surge in online health information, coupled with its\nincreasing use by non-experts, highlights the pressing need for advanced Health\nInformation Retrieval models that consider not only topical relevance but also\nthe factual accuracy of the retrieved information, given the potential risks\nassociated with health misinformation. To this aim, this paper introduces a\nsolution driven by Retrieval-Augmented Generation (RAG), which leverages the\ncapabilities of generative Large Language Models (LLMs) to enhance the\nretrieval of health-related documents grounded in scientific evidence. In\nparticular, we propose a three-stage model: in the first stage, the user's\nquery is employed to retrieve topically relevant passages with associated\nreferences from a knowledge base constituted by scientific literature. In the\nsecond stage, these passages, alongside the initial query, are processed by\nLLMs to generate a contextually relevant rich text (GenText). In the last\nstage, the documents to be retrieved are evaluated and ranked both from the\npoint of view of topical relevance and factual accuracy by means of their\ncomparison with GenText, either through stance detection or semantic\nsimilarity. In addition to calculating factual accuracy, GenText can offer a\nlayer of explainability for it, aiding users in understanding the reasoning\nbehind the retrieval. Experimental evaluation of our model on benchmark\ndatasets and against baseline models demonstrates its effectiveness in\nenhancing the retrieval of both topically relevant and factually accurate\nhealth information, thus presenting a significant step forward in the health\nmisinformation mitigation problem."
                },
                "authors": [
                    {
                        "name": "Rishabh Uapadhyay"
                    },
                    {
                        "name": "Marco Viviani"
                    }
                ],
                "author_detail": {
                    "name": "Marco Viviani"
                },
                "author": "Marco Viviani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18624v2",
                "updated": "2025-02-07T05:11:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    11,
                    54,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-27T05:44:58Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    44,
                    58,
                    0,
                    27,
                    0
                ],
                "title": "Membership Inference Attacks Against Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks Against Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs), built on pre-trained vision encoders and large\nlanguage models (LLMs), have shown exceptional multi-modal understanding and\ndialog capabilities, positioning them as catalysts for the next technological\nrevolution. However, while most VLM research focuses on enhancing multi-modal\ninteraction, the risks of data misuse and leakage have been largely unexplored.\nThis prompts the need for a comprehensive investigation of such risks in VLMs.\nIn this paper, we conduct the first analysis of misuse and leakage detection in\nVLMs through the lens of membership inference attack (MIA). In specific, we\nfocus on the instruction tuning data of VLMs, which is more likely to contain\nsensitive or unauthorized information. To address the limitation of existing\nMIA methods, we introduce a novel approach that infers membership based on a\nset of samples and their sensitivity to temperature, a unique parameter in\nVLMs. Based on this, we propose four membership inference methods, each\ntailored to different levels of background knowledge, ultimately arriving at\nthe most challenging scenario. Our comprehensive evaluations show that these\nmethods can accurately determine membership status, e.g., achieving an AUC\ngreater than 0.8 targeting a small set consisting of only 5 samples on LLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs), built on pre-trained vision encoders and large\nlanguage models (LLMs), have shown exceptional multi-modal understanding and\ndialog capabilities, positioning them as catalysts for the next technological\nrevolution. However, while most VLM research focuses on enhancing multi-modal\ninteraction, the risks of data misuse and leakage have been largely unexplored.\nThis prompts the need for a comprehensive investigation of such risks in VLMs.\nIn this paper, we conduct the first analysis of misuse and leakage detection in\nVLMs through the lens of membership inference attack (MIA). In specific, we\nfocus on the instruction tuning data of VLMs, which is more likely to contain\nsensitive or unauthorized information. To address the limitation of existing\nMIA methods, we introduce a novel approach that infers membership based on a\nset of samples and their sensitivity to temperature, a unique parameter in\nVLMs. Based on this, we propose four membership inference methods, each\ntailored to different levels of background knowledge, ultimately arriving at\nthe most challenging scenario. Our comprehensive evaluations show that these\nmethods can accurately determine membership status, e.g., achieving an AUC\ngreater than 0.8 targeting a small set consisting of only 5 samples on LLaVA."
                },
                "authors": [
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    },
                    {
                        "name": "Chun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chun Chen"
                },
                "author": "Chun Chen",
                "arxiv_comment": "Accepted by USENIX'25; 22 pages, 28 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10198v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10198v3",
                "updated": "2025-02-07T05:11:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    11,
                    18,
                    4,
                    38,
                    0
                ],
                "published": "2024-04-16T00:43:03Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    0,
                    43,
                    3,
                    1,
                    107,
                    0
                ],
                "title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior\n  and external evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior\n  and external evidence"
                },
                "summary": "Retrieval augmented generation (RAG) is frequently used to mitigate\nhallucinations and provide up-to-date knowledge for large language models\n(LLMs). However, given that document retrieval is an imprecise task and\nsometimes results in erroneous or even harmful content being presented in\ncontext, this raises the question of how LLMs handle retrieved information: If\nthe provided content is incorrect, does the model know to ignore it, or does it\nrecapitulate the error? Conversely, when the model's initial response is\nincorrect, does it always know to use the retrieved information to correct\nitself, or does it insist on its wrong prior response? To answer this, we\ncurate a dataset of over 1200 questions across six domains (e.g., drug dosages,\nOlympic records, locations) along with content relevant to answering each\nquestion. We further apply precise perturbations to the answers in the content\nthat range from subtle to blatant errors. We benchmark six top-performing LLMs,\nincluding GPT-4o, on this dataset and find that LLMs are susceptible to\nadopting incorrect retrieved content, overriding their own correct prior\nknowledge over 60% of the time. However, the more unrealistic the retrieved\ncontent is (i.e. more deviated from truth), the less likely the model is to\nadopt it. Also, the less confident a model is in its initial response (via\nmeasuring token probabilities), the more likely it is to adopt the information\nin the retrieved content. We exploit this finding and demonstrate simple\nmethods for improving model accuracy where there is conflicting retrieved\ncontent. Our results highlight a difficult task and benchmark for LLMs --\nnamely, their ability to correctly discern when it is wrong in light of correct\nretrieved content and to reject cases when the provided content is incorrect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is frequently used to mitigate\nhallucinations and provide up-to-date knowledge for large language models\n(LLMs). However, given that document retrieval is an imprecise task and\nsometimes results in erroneous or even harmful content being presented in\ncontext, this raises the question of how LLMs handle retrieved information: If\nthe provided content is incorrect, does the model know to ignore it, or does it\nrecapitulate the error? Conversely, when the model's initial response is\nincorrect, does it always know to use the retrieved information to correct\nitself, or does it insist on its wrong prior response? To answer this, we\ncurate a dataset of over 1200 questions across six domains (e.g., drug dosages,\nOlympic records, locations) along with content relevant to answering each\nquestion. We further apply precise perturbations to the answers in the content\nthat range from subtle to blatant errors. We benchmark six top-performing LLMs,\nincluding GPT-4o, on this dataset and find that LLMs are susceptible to\nadopting incorrect retrieved content, overriding their own correct prior\nknowledge over 60% of the time. However, the more unrealistic the retrieved\ncontent is (i.e. more deviated from truth), the less likely the model is to\nadopt it. Also, the less confident a model is in its initial response (via\nmeasuring token probabilities), the more likely it is to adopt the information\nin the retrieved content. We exploit this finding and demonstrate simple\nmethods for improving model accuracy where there is conflicting retrieved\ncontent. Our results highlight a difficult task and benchmark for LLMs --\nnamely, their ability to correctly discern when it is wrong in light of correct\nretrieved content and to reject cases when the provided content is incorrect."
                },
                "authors": [
                    {
                        "name": "Kevin Wu"
                    },
                    {
                        "name": "Eric Wu"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "arxiv_comment": "Revised June 9 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10198v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10198v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19457v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19457v4",
                "updated": "2025-02-07T05:10:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    10,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-28T21:11:25Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    21,
                    11,
                    25,
                    5,
                    272,
                    0
                ],
                "title": "A Parameter-Efficient Tuning Framework for Language-guided Object\n  Grounding and Robot Grasping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Parameter-Efficient Tuning Framework for Language-guided Object\n  Grounding and Robot Grasping"
                },
                "summary": "The language-guided robot grasping task requires a robot agent to integrate\nmultimodal information from both visual and linguistic inputs to predict\nactions for target-driven grasping. While recent approaches utilizing\nMultimodal Large Language Models (MLLMs) have shown promising results, their\nextensive computation and data demands limit the feasibility of local\ndeployment and customization. To address this, we propose a novel CLIP-based\nmultimodal parameter-efficient tuning (PET) framework designed for three\nlanguage-guided object grounding and grasping tasks: (1) Referring Expression\nSegmentation (RES), (2) Referring Grasp Synthesis (RGS), and (3) Referring\nGrasp Affordance (RGA). Our approach introduces two key innovations: a\nbi-directional vision-language adapter that aligns multimodal inputs for\npixel-level language understanding and a depth fusion branch that incorporates\ngeometric cues to facilitate robot grasping predictions. Experiment results\ndemonstrate superior performance in the RES object grounding task compared with\nexisting CLIP-based full-model tuning or PET approaches. In the RGS and RGA\ntasks, our model not only effectively interprets object attributes based on\nsimple language descriptions but also shows strong potential for comprehending\ncomplex spatial reasoning scenarios, such as multiple identical objects present\nin the workspace. Project page: https://z.umn.edu/etog-etrg",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The language-guided robot grasping task requires a robot agent to integrate\nmultimodal information from both visual and linguistic inputs to predict\nactions for target-driven grasping. While recent approaches utilizing\nMultimodal Large Language Models (MLLMs) have shown promising results, their\nextensive computation and data demands limit the feasibility of local\ndeployment and customization. To address this, we propose a novel CLIP-based\nmultimodal parameter-efficient tuning (PET) framework designed for three\nlanguage-guided object grounding and grasping tasks: (1) Referring Expression\nSegmentation (RES), (2) Referring Grasp Synthesis (RGS), and (3) Referring\nGrasp Affordance (RGA). Our approach introduces two key innovations: a\nbi-directional vision-language adapter that aligns multimodal inputs for\npixel-level language understanding and a depth fusion branch that incorporates\ngeometric cues to facilitate robot grasping predictions. Experiment results\ndemonstrate superior performance in the RES object grounding task compared with\nexisting CLIP-based full-model tuning or PET approaches. In the RGS and RGA\ntasks, our model not only effectively interprets object attributes based on\nsimple language descriptions but also shows strong potential for comprehending\ncomplex spatial reasoning scenarios, such as multiple identical objects present\nin the workspace. Project page: https://z.umn.edu/etog-etrg"
                },
                "authors": [
                    {
                        "name": "Houjian Yu"
                    },
                    {
                        "name": "Mingen Li"
                    },
                    {
                        "name": "Alireza Rezazadeh"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Changhyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Changhyun Choi"
                },
                "author": "Changhyun Choi",
                "arxiv_comment": "Accepted for ICRA 2025. Project page:\n  https://sites.google.com/umn.edu/etog-etrg/home",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19457v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19457v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12846v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12846v3",
                "updated": "2025-02-07T04:36:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    4,
                    36,
                    53,
                    4,
                    38,
                    0
                ],
                "published": "2024-10-10T05:34:00Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    34,
                    0,
                    3,
                    284,
                    0
                ],
                "title": "Accurate and Regret-aware Numerical Problem Solver for Tabular Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and Regret-aware Numerical Problem Solver for Tabular Question\n  Answering"
                },
                "summary": "Question answering on free-form tables (a.k.a. TableQA) is a challenging task\nbecause of the flexible structure and complex schema of tables. Recent studies\nuse Large Language Models (LLMs) for this task, exploiting their capability in\nunderstanding the questions and tabular data, which are typically given in\nnatural language and contain many textual fields, respectively. While this\napproach has shown promising results, it overlooks the challenges brought by\nnumerical values which are common in tabular data, and LLMs are known to\nstruggle with such values. We aim to address this issue, and we propose a model\nnamed TabLaP that uses LLMs as a planner rather than an answer generator. This\napproach exploits LLMs' capability in multi-step reasoning while leaving the\nactual numerical calculations to a Python interpreter for accurate calculation.\nRecognizing the inaccurate nature of LLMs, we further make a first attempt to\nquantify the trustworthiness of the answers produced by TabLaP, such that users\ncan use TabLaP in a regret-aware manner. Experimental results on two benchmark\ndatasets show that TabLaP is substantially more accurate than the\nstate-of-the-art models, improving the answer accuracy by 5.7% and 5.8% on the\ntwo datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering on free-form tables (a.k.a. TableQA) is a challenging task\nbecause of the flexible structure and complex schema of tables. Recent studies\nuse Large Language Models (LLMs) for this task, exploiting their capability in\nunderstanding the questions and tabular data, which are typically given in\nnatural language and contain many textual fields, respectively. While this\napproach has shown promising results, it overlooks the challenges brought by\nnumerical values which are common in tabular data, and LLMs are known to\nstruggle with such values. We aim to address this issue, and we propose a model\nnamed TabLaP that uses LLMs as a planner rather than an answer generator. This\napproach exploits LLMs' capability in multi-step reasoning while leaving the\nactual numerical calculations to a Python interpreter for accurate calculation.\nRecognizing the inaccurate nature of LLMs, we further make a first attempt to\nquantify the trustworthiness of the answers produced by TabLaP, such that users\ncan use TabLaP in a regret-aware manner. Experimental results on two benchmark\ndatasets show that TabLaP is substantially more accurate than the\nstate-of-the-art models, improving the answer accuracy by 5.7% and 5.8% on the\ntwo datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wang"
                    },
                    {
                        "name": "Jianzhong Qi"
                    },
                    {
                        "name": "Junhao Gan"
                    }
                ],
                "author_detail": {
                    "name": "Junhao Gan"
                },
                "author": "Junhao Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12846v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12846v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04644v1",
                "updated": "2025-02-07T04:08:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    4,
                    8,
                    46,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T04:08:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    4,
                    8,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research"
                },
                "summary": "We introduce Agentic Reasoning, a framework that enhances large language\nmodel (LLM) reasoning by integrating external tool-using agents. Unlike\nconventional LLM-based reasoning approaches, which rely solely on internal\ninference, Agentic Reasoning dynamically engages web search, code execution,\nand structured reasoning-context memory to solve complex problems requiring\ndeep research and multi-step logical deduction. Our framework introduces the\nMind Map agent, which constructs a structured knowledge graph to track logical\nrelationships, improving deductive reasoning. Additionally, the integration of\nweb-search and coding agents enables real-time retrieval and computational\nanalysis, enhancing reasoning accuracy and decision-making. Evaluations on\nPhD-level scientific reasoning (GPQA) and domain-specific deep research tasks\ndemonstrate that our approach significantly outperforms existing models,\nincluding leading retrieval-augmented generation (RAG) systems and\nclosed-source LLMs. Moreover, our results indicate that agentic reasoning\nimproves expert-level knowledge synthesis, test-time scalability, and\nstructured problem-solving. The code is at:\nhttps://github.com/theworldofagents/Agentic-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Agentic Reasoning, a framework that enhances large language\nmodel (LLM) reasoning by integrating external tool-using agents. Unlike\nconventional LLM-based reasoning approaches, which rely solely on internal\ninference, Agentic Reasoning dynamically engages web search, code execution,\nand structured reasoning-context memory to solve complex problems requiring\ndeep research and multi-step logical deduction. Our framework introduces the\nMind Map agent, which constructs a structured knowledge graph to track logical\nrelationships, improving deductive reasoning. Additionally, the integration of\nweb-search and coding agents enables real-time retrieval and computational\nanalysis, enhancing reasoning accuracy and decision-making. Evaluations on\nPhD-level scientific reasoning (GPQA) and domain-specific deep research tasks\ndemonstrate that our approach significantly outperforms existing models,\nincluding leading retrieval-augmented generation (RAG) systems and\nclosed-source LLMs. Moreover, our results indicate that agentic reasoning\nimproves expert-level knowledge synthesis, test-time scalability, and\nstructured problem-solving. The code is at:\nhttps://github.com/theworldofagents/Agentic-Reasoning."
                },
                "authors": [
                    {
                        "name": "Junde Wu"
                    },
                    {
                        "name": "Jiayuan Zhu"
                    },
                    {
                        "name": "Yuyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuyuan Liu"
                },
                "author": "Yuyuan Liu",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04643v1",
                "updated": "2025-02-07T04:07:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    4,
                    7,
                    36,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T04:07:36Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    4,
                    7,
                    36,
                    4,
                    38,
                    0
                ],
                "title": "Confidence Elicitation: A New Attack Vector for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Elicitation: A New Attack Vector for Large Language Models"
                },
                "summary": "A fundamental issue in deep learning has been adversarial robustness. As\nthese systems have scaled, such issues have persisted. Currently, large\nlanguage models (LLMs) with billions of parameters suffer from adversarial\nattacks just like their earlier, smaller counterparts. However, the threat\nmodels have changed. Previously, having gray-box access, where input embeddings\nor output logits/probabilities were visible to the user, might have been\nreasonable. However, with the introduction of closed-source models, no\ninformation about the model is available apart from the generated output. This\nmeans that current black-box attacks can only utilize the final prediction to\ndetect if an attack is successful. In this work, we investigate and demonstrate\nthe potential of attack guidance, akin to using output probabilities, while\nhaving only black-box access in a classification setting. This is achieved\nthrough the ability to elicit confidence from the model. We empirically show\nthat the elicited confidence is calibrated and not hallucinated for current\nLLMs. By minimizing the elicited confidence, we can therefore increase the\nlikelihood of misclassification. Our new proposed paradigm demonstrates\npromising state-of-the-art results on three datasets across two models\n(LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique\nto existing hard-label black-box attack methods that introduce word-level\nsubstitutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental issue in deep learning has been adversarial robustness. As\nthese systems have scaled, such issues have persisted. Currently, large\nlanguage models (LLMs) with billions of parameters suffer from adversarial\nattacks just like their earlier, smaller counterparts. However, the threat\nmodels have changed. Previously, having gray-box access, where input embeddings\nor output logits/probabilities were visible to the user, might have been\nreasonable. However, with the introduction of closed-source models, no\ninformation about the model is available apart from the generated output. This\nmeans that current black-box attacks can only utilize the final prediction to\ndetect if an attack is successful. In this work, we investigate and demonstrate\nthe potential of attack guidance, akin to using output probabilities, while\nhaving only black-box access in a classification setting. This is achieved\nthrough the ability to elicit confidence from the model. We empirically show\nthat the elicited confidence is calibrated and not hallucinated for current\nLLMs. By minimizing the elicited confidence, we can therefore increase the\nlikelihood of misclassification. Our new proposed paradigm demonstrates\npromising state-of-the-art results on three datasets across two models\n(LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique\nto existing hard-label black-box attack methods that introduce word-level\nsubstitutions."
                },
                "authors": [
                    {
                        "name": "Brian Formento"
                    },
                    {
                        "name": "Chuan Sheng Foo"
                    },
                    {
                        "name": "See-Kiong Ng"
                    }
                ],
                "author_detail": {
                    "name": "See-Kiong Ng"
                },
                "author": "See-Kiong Ng",
                "arxiv_comment": "Published in ICLR 2025. The code is publicly available at\n  https://github.com/Aniloid2/Confidence_Elicitation_Attacks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03497v2",
                "updated": "2025-02-07T03:14:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    3,
                    14,
                    20,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-05T08:18:50Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    18,
                    50,
                    2,
                    36,
                    0
                ],
                "title": "SLCGC: A lightweight Self-supervised Low-pass Contrastive Graph\n  Clustering Network for Hyperspectral Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLCGC: A lightweight Self-supervised Low-pass Contrastive Graph\n  Clustering Network for Hyperspectral Images"
                },
                "summary": "Self-supervised hyperspectral image (HSI) clustering remains a fundamental\nyet challenging task due to the absence of labeled data and the inherent\ncomplexity of spatial-spectral interactions. While recent advancements have\nexplored innovative approaches, existing methods face critical limitations in\nclustering accuracy, feature discriminability, computational efficiency, and\nrobustness to noise, hindering their practical deployment. In this paper, a\nself-supervised efficient low-pass contrastive graph clustering (SLCGC) is\nintroduced for HSIs. Our approach begins with homogeneous region generation,\nwhich aggregates pixels into spectrally consistent regions to preserve local\nspatial-spectral coherence while drastically reducing graph complexity. We then\nconstruct a structural graph using an adjacency matrix A and introduce a\nlow-pass graph denoising mechanism to suppress high-frequency noise in the\ngraph topology, ensuring stable feature propagation. A dual-branch graph\ncontrastive learning module is developed, where Gaussian noise perturbations\ngenerate augmented views through two multilayer perceptrons (MLPs), and a\ncross-view contrastive loss enforces structural consistency between views to\nlearn noise-invariant representations. Finally, latent embeddings optimized by\nthis process are clustered via K-means. Extensive experiments and repeated\ncomparative analysis have verified that our SLCGC contains high clustering\naccuracy, low computational complexity, and strong robustness. The code source\nwill be available at https://github.com/DY-HYX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised hyperspectral image (HSI) clustering remains a fundamental\nyet challenging task due to the absence of labeled data and the inherent\ncomplexity of spatial-spectral interactions. While recent advancements have\nexplored innovative approaches, existing methods face critical limitations in\nclustering accuracy, feature discriminability, computational efficiency, and\nrobustness to noise, hindering their practical deployment. In this paper, a\nself-supervised efficient low-pass contrastive graph clustering (SLCGC) is\nintroduced for HSIs. Our approach begins with homogeneous region generation,\nwhich aggregates pixels into spectrally consistent regions to preserve local\nspatial-spectral coherence while drastically reducing graph complexity. We then\nconstruct a structural graph using an adjacency matrix A and introduce a\nlow-pass graph denoising mechanism to suppress high-frequency noise in the\ngraph topology, ensuring stable feature propagation. A dual-branch graph\ncontrastive learning module is developed, where Gaussian noise perturbations\ngenerate augmented views through two multilayer perceptrons (MLPs), and a\ncross-view contrastive loss enforces structural consistency between views to\nlearn noise-invariant representations. Finally, latent embeddings optimized by\nthis process are clustered via K-means. Extensive experiments and repeated\ncomparative analysis have verified that our SLCGC contains high clustering\naccuracy, low computational complexity, and strong robustness. The code source\nwill be available at https://github.com/DY-HYX."
                },
                "authors": [
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Zhili Zhang"
                    },
                    {
                        "name": "Aitao Yang"
                    },
                    {
                        "name": "Yaoming Cai"
                    },
                    {
                        "name": "Xiongwu Xiao"
                    },
                    {
                        "name": "Danfeng Hong"
                    },
                    {
                        "name": "Junsong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Junsong Yuan"
                },
                "author": "Junsong Yuan",
                "arxiv_comment": "12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12174v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12174v4",
                "updated": "2025-02-07T03:04:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    3,
                    4,
                    13,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-21T14:32:50Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    32,
                    50,
                    1,
                    21,
                    0
                ],
                "title": "BiMarker: Enhancing Text Watermark Detection for Large Language Models\n  with Bipolar Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiMarker: Enhancing Text Watermark Detection for Large Language Models\n  with Bipolar Watermarks"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) raises concerns about\ndistinguishing AI-generated text from human content. Existing watermarking\ntechniques, like \\kgw, struggle with low watermark strength and stringent\nfalse-positive requirements. Our analysis reveals that current methods rely on\ncoarse estimates of non-watermarked text, limiting watermark detectability. To\naddress this, we propose Bipolar Watermark (\\tool), which splits generated text\ninto positive and negative poles, enhancing detection without requiring\nadditional computational resources or knowledge of the prompt. Theoretical\nanalysis and experimental results demonstrate \\tool's effectiveness and\ncompatibility with existing optimization techniques, providing a new\noptimization dimension for watermarking in LLM-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) raises concerns about\ndistinguishing AI-generated text from human content. Existing watermarking\ntechniques, like \\kgw, struggle with low watermark strength and stringent\nfalse-positive requirements. Our analysis reveals that current methods rely on\ncoarse estimates of non-watermarked text, limiting watermark detectability. To\naddress this, we propose Bipolar Watermark (\\tool), which splits generated text\ninto positive and negative poles, enhancing detection without requiring\nadditional computational resources or knowledge of the prompt. Theoretical\nanalysis and experimental results demonstrate \\tool's effectiveness and\ncompatibility with existing optimization techniques, providing a new\noptimization dimension for watermarking in LLM-generated content."
                },
                "authors": [
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12174v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12174v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13013v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13013v3",
                "updated": "2025-02-07T02:48:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    2,
                    48,
                    19,
                    4,
                    38,
                    0
                ],
                "published": "2024-12-17T15:34:00Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    34,
                    0,
                    1,
                    352,
                    0
                ],
                "title": "The Emergence of Strategic Reasoning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Strategic Reasoning of Large Language Models"
                },
                "summary": "Although large language models (LLMs) have demonstrated strong reasoning\nabilities in structured tasks (e.g., coding and mathematics), it remains\nunexplored whether these abilities extend to strategic multi-agent\nenvironments. We investigate strategic reasoning capabilities -- the process of\nchoosing an optimal course of action by predicting and adapting to others'\nactions -- of LLMs by analyzing their performance in three classical games from\nbehavioral economics. We evaluate three standard LLMs (ChatGPT-4, Claude-2.1,\nGemini 1.5) and three specialized reasoning LLMs (GPT-o1, Claude-3.5-Sonnet,\nGemini Flash Thinking 2.0) using hierarchical models of bounded rationality.\nOur results show that reasoning LLMs exhibit superior strategic reasoning\ncompared to standard LLMs (which do not demonstrate substantial capabilities),\nand often match or exceed human performance. Since strategic reasoning is\nfundamental to future AI systems (including Agentic AI and Artificial General\nIntelligence), our findings demonstrate the importance of dedicated reasoning\ncapabilities in achieving effective strategic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have demonstrated strong reasoning\nabilities in structured tasks (e.g., coding and mathematics), it remains\nunexplored whether these abilities extend to strategic multi-agent\nenvironments. We investigate strategic reasoning capabilities -- the process of\nchoosing an optimal course of action by predicting and adapting to others'\nactions -- of LLMs by analyzing their performance in three classical games from\nbehavioral economics. We evaluate three standard LLMs (ChatGPT-4, Claude-2.1,\nGemini 1.5) and three specialized reasoning LLMs (GPT-o1, Claude-3.5-Sonnet,\nGemini Flash Thinking 2.0) using hierarchical models of bounded rationality.\nOur results show that reasoning LLMs exhibit superior strategic reasoning\ncompared to standard LLMs (which do not demonstrate substantial capabilities),\nand often match or exceed human performance. Since strategic reasoning is\nfundamental to future AI systems (including Agentic AI and Artificial General\nIntelligence), our findings demonstrate the importance of dedicated reasoning\ncapabilities in achieving effective strategic reasoning."
                },
                "authors": [
                    {
                        "name": "Dongwoo Lee"
                    },
                    {
                        "name": "Gavin Kader"
                    }
                ],
                "author_detail": {
                    "name": "Gavin Kader"
                },
                "author": "Gavin Kader",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13013v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13013v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03804v2",
                "updated": "2025-02-07T02:45:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    2,
                    45,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-06T06:27:09Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    27,
                    9,
                    3,
                    37,
                    0
                ],
                "title": "Understanding and Supporting Formal Email Exchange by Answering\n  AI-Generated Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Supporting Formal Email Exchange by Answering\n  AI-Generated Questions"
                },
                "summary": "Replying to formal emails is time-consuming and cognitively demanding, as it\nrequires crafting polite phrasing and providing an adequate response to the\nsender's demands. Although systems with Large Language Models (LLMs) were\ndesigned to simplify the email replying process, users still need to provide\ndetailed prompts to obtain the expected output. Therefore, we proposed and\nevaluated an LLM-powered question-and-answer (QA)-based approach for users to\nreply to emails by answering a set of simple and short questions generated from\nthe incoming email. We developed a prototype system, ResQ, and conducted\ncontrolled and field experiments with 12 and 8 participants. Our results\ndemonstrated that the QA-based approach improves the efficiency of replying to\nemails and reduces workload while maintaining email quality, compared to a\nconventional prompt-based approach that requires users to craft appropriate\nprompts to obtain email drafts. We discuss how the QA-based approach influences\nthe email reply process and interpersonal relationship dynamics, as well as the\nopportunities and challenges associated with using a QA-based approach in\nAI-mediated communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replying to formal emails is time-consuming and cognitively demanding, as it\nrequires crafting polite phrasing and providing an adequate response to the\nsender's demands. Although systems with Large Language Models (LLMs) were\ndesigned to simplify the email replying process, users still need to provide\ndetailed prompts to obtain the expected output. Therefore, we proposed and\nevaluated an LLM-powered question-and-answer (QA)-based approach for users to\nreply to emails by answering a set of simple and short questions generated from\nthe incoming email. We developed a prototype system, ResQ, and conducted\ncontrolled and field experiments with 12 and 8 participants. Our results\ndemonstrated that the QA-based approach improves the efficiency of replying to\nemails and reduces workload while maintaining email quality, compared to a\nconventional prompt-based approach that requires users to craft appropriate\nprompts to obtain email drafts. We discuss how the QA-based approach influences\nthe email reply process and interpersonal relationship dynamics, as well as the\nopportunities and challenges associated with using a QA-based approach in\nAI-mediated communication."
                },
                "authors": [
                    {
                        "name": "Yusuke Miura"
                    },
                    {
                        "name": "Chi-Lan Yang"
                    },
                    {
                        "name": "Masaki Kuribayashi"
                    },
                    {
                        "name": "Keigo Matsumoto"
                    },
                    {
                        "name": "Hideaki Kuzuoka"
                    },
                    {
                        "name": "Shigeo Morishima"
                    }
                ],
                "author_detail": {
                    "name": "Shigeo Morishima"
                },
                "author": "Shigeo Morishima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00902v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00902v3",
                "updated": "2025-02-07T02:29:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    2,
                    29,
                    2,
                    4,
                    38,
                    0
                ],
                "published": "2024-07-01T01:57:21Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    1,
                    57,
                    21,
                    0,
                    183,
                    0
                ],
                "title": "From Introspection to Best Practices: Principled Analysis of\n  Demonstrations in Multimodal In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Introspection to Best Practices: Principled Analysis of\n  Demonstrations in Multimodal In-Context Learning"
                },
                "summary": "Motivated by in-context learning (ICL) capabilities of Large Language Models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Guided by task-specific modality impact, we recommend\nmodality-driven demonstration strategies to boost ICL performance. We also find\nthat models may follow inductive biases from multimodal ICL even if they are\nrarely seen in or contradict semantic priors from pretraining data. Our\nprincipled analysis provides a comprehensive way of understanding the role of\ndemonstrations in multimodal in-context learning, and sheds light on\neffectively improving multimodal ICL on a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by in-context learning (ICL) capabilities of Large Language Models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Guided by task-specific modality impact, we recommend\nmodality-driven demonstration strategies to boost ICL performance. We also find\nthat models may follow inductive biases from multimodal ICL even if they are\nrarely seen in or contradict semantic priors from pretraining data. Our\nprincipled analysis provides a comprehensive way of understanding the role of\ndemonstrations in multimodal in-context learning, and sheds light on\neffectively improving multimodal ICL on a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00902v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00902v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04194v2",
                "updated": "2025-02-07T02:20:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    2,
                    20,
                    28,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-06T16:31:21Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    31,
                    21,
                    3,
                    37,
                    0
                ],
                "title": "The Best Instruction-Tuning Data are Those That Fit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Best Instruction-Tuning Data are Those That Fit"
                },
                "summary": "High-quality supervised fine-tuning (SFT) data are crucial for eliciting\nstrong capabilities from pretrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses sampled from other LLMs, which\nare often out of the distribution of the target model to be fine-tuned. This,\nat scale, can lead to diminishing returns and even hurt the models' performance\nand robustness. We propose **GRAPE**, a novel SFT framework that accounts for\nthe unique characteristics of the target model. For each instruction, it\ngathers responses from various LLMs and selects the one with the highest\nprobability measured by the target model, indicating that it aligns most\nclosely with the target model's pretrained distribution; it then proceeds with\nstandard SFT training.\n  We first evaluate GRAPE with a controlled experiment, where we sample various\nsolutions for each question in UltraInteract from multiple models and fine-tune\ncommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on\nGRAPE-selected data. GRAPE significantly outperforms strong baselines,\nincluding distilling from the strongest model with an absolute gain of up to\n13.8%, averaged across benchmarks, and training on 3x more data with a maximum\nperformance improvement of 17.3%. GRAPE's strong performance generalizes to\nrealistic settings. We experiment with the post-training data used for Tulu3\nand Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data\nby 6.1% and a state-of-the-art data selection approach by 3% on average\nperformance. Remarkably, using 1/3 of the data and half the number of epochs,\nGRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality supervised fine-tuning (SFT) data are crucial for eliciting\nstrong capabilities from pretrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses sampled from other LLMs, which\nare often out of the distribution of the target model to be fine-tuned. This,\nat scale, can lead to diminishing returns and even hurt the models' performance\nand robustness. We propose **GRAPE**, a novel SFT framework that accounts for\nthe unique characteristics of the target model. For each instruction, it\ngathers responses from various LLMs and selects the one with the highest\nprobability measured by the target model, indicating that it aligns most\nclosely with the target model's pretrained distribution; it then proceeds with\nstandard SFT training.\n  We first evaluate GRAPE with a controlled experiment, where we sample various\nsolutions for each question in UltraInteract from multiple models and fine-tune\ncommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on\nGRAPE-selected data. GRAPE significantly outperforms strong baselines,\nincluding distilling from the strongest model with an absolute gain of up to\n13.8%, averaged across benchmarks, and training on 3x more data with a maximum\nperformance improvement of 17.3%. GRAPE's strong performance generalizes to\nrealistic settings. We experiment with the post-training data used for Tulu3\nand Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data\nby 6.1% and a state-of-the-art data selection approach by 3% on average\nperformance. Remarkably, using 1/3 of the data and half the number of epochs,\nGRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%."
                },
                "authors": [
                    {
                        "name": "Dylan Zhang"
                    },
                    {
                        "name": "Qirun Dai"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]