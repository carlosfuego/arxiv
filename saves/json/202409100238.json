[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.12002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12002v2",
                "updated": "2024-09-06T17:57:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    17,
                    57,
                    52,
                    4,
                    250,
                    0
                ],
                "published": "2024-06-17T18:13:57Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    13,
                    57,
                    0,
                    169,
                    0
                ],
                "title": "Modeling, Inference, and Prediction in Mobility-Based Compartmental\n  Models for Epidemiology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling, Inference, and Prediction in Mobility-Based Compartmental\n  Models for Epidemiology"
                },
                "summary": "Classical compartmental models in epidemiology often assume a homogeneous\npopulation for simplicity, which neglects the inherent heterogeneity among\nindividuals. This assumption frequently leads to inaccurate predictions when\napplied to real-world data. For example, evidence has shown that classical\nmodels overestimate the final pandemic size in the H1N1-2009 and COVID-19\noutbreaks. To address this issue, we introduce individual mobility as a key\nfactor in disease transmission and control. We characterize disease dynamics\nusing mobility distribution functions for each compartment and propose a\nmobility-based compartmental model that incorporates population heterogeneity.\nOur results demonstrate that, for the same basic reproduction number, our\nmobility-based model predicts a smaller final pandemic size compared to the\nclassical models, effectively addressing the common overestimation problem.\nAdditionally, we infer mobility distributions from the time series of the\ninfected population. We provide sufficient conditions for uniquely identifying\nthe mobility distribution from a dataset and propose a machine-learning-based\napproach to learn mobility from both synthesized and real-world data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical compartmental models in epidemiology often assume a homogeneous\npopulation for simplicity, which neglects the inherent heterogeneity among\nindividuals. This assumption frequently leads to inaccurate predictions when\napplied to real-world data. For example, evidence has shown that classical\nmodels overestimate the final pandemic size in the H1N1-2009 and COVID-19\noutbreaks. To address this issue, we introduce individual mobility as a key\nfactor in disease transmission and control. We characterize disease dynamics\nusing mobility distribution functions for each compartment and propose a\nmobility-based compartmental model that incorporates population heterogeneity.\nOur results demonstrate that, for the same basic reproduction number, our\nmobility-based model predicts a smaller final pandemic size compared to the\nclassical models, effectively addressing the common overestimation problem.\nAdditionally, we infer mobility distributions from the time series of the\ninfected population. We provide sufficient conditions for uniquely identifying\nthe mobility distribution from a dataset and propose a machine-learning-based\napproach to learn mobility from both synthesized and real-world data."
                },
                "authors": [
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Weiqi Chu"
                    },
                    {
                        "name": "Yao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yao Li"
                },
                "author": "Yao Li",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04431v1",
                "updated": "2024-09-06T17:53:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    17,
                    53,
                    26,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T17:53:26Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    17,
                    53,
                    26,
                    4,
                    250,
                    0
                ],
                "title": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention"
                },
                "summary": "Attention is a key part of the transformer architecture. It is a\nsequence-to-sequence mapping that transforms each sequence element into a\nweighted sum of values. The weights are typically obtained as the softmax of\ndot products between keys and queries. Recent work has explored alternatives to\nsoftmax attention in transformers, such as ReLU and sigmoid activations. In\nthis work, we revisit sigmoid attention and conduct an in-depth theoretical and\nempirical analysis. Theoretically, we prove that transformers with sigmoid\nattention are universal function approximators and benefit from improved\nregularity compared to softmax attention. Through detailed empirical analysis,\nwe identify stabilization of large initial attention norms during the early\nstages of training as a crucial factor for the successful training of models\nwith sigmoid attention, outperforming prior attempts. We also introduce\nFLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid\nattention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100\nGPUs. Experiments across language, vision, and speech show that properly\nnormalized sigmoid attention matches the strong performance of softmax\nattention on a wide range of domains and scales, which previous attempts at\nsigmoid attention were unable to fully achieve. Our work unifies prior art and\nestablishes best practices for sigmoid attention as a drop-in softmax\nreplacement in transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention is a key part of the transformer architecture. It is a\nsequence-to-sequence mapping that transforms each sequence element into a\nweighted sum of values. The weights are typically obtained as the softmax of\ndot products between keys and queries. Recent work has explored alternatives to\nsoftmax attention in transformers, such as ReLU and sigmoid activations. In\nthis work, we revisit sigmoid attention and conduct an in-depth theoretical and\nempirical analysis. Theoretically, we prove that transformers with sigmoid\nattention are universal function approximators and benefit from improved\nregularity compared to softmax attention. Through detailed empirical analysis,\nwe identify stabilization of large initial attention norms during the early\nstages of training as a crucial factor for the successful training of models\nwith sigmoid attention, outperforming prior attempts. We also introduce\nFLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid\nattention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100\nGPUs. Experiments across language, vision, and speech show that properly\nnormalized sigmoid attention matches the strong performance of softmax\nattention on a wide range of domains and scales, which previous attempts at\nsigmoid attention were unable to fully achieve. Our work unifies prior art and\nestablishes best practices for sigmoid attention as a drop-in softmax\nreplacement in transformers."
                },
                "authors": [
                    {
                        "name": "Jason Ramapuram"
                    },
                    {
                        "name": "Federico Danieli"
                    },
                    {
                        "name": "Eeshan Dhekane"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Dan Busbridge"
                    },
                    {
                        "name": "Pierre Ablin"
                    },
                    {
                        "name": "Tatiana Likhomanenko"
                    },
                    {
                        "name": "Jagrit Digani"
                    },
                    {
                        "name": "Zijin Gu"
                    },
                    {
                        "name": "Amitis Shidani"
                    },
                    {
                        "name": "Russ Webb"
                    }
                ],
                "author_detail": {
                    "name": "Russ Webb"
                },
                "author": "Russ Webb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04421v1",
                "updated": "2024-09-06T17:30:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    17,
                    30,
                    45,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T17:30:45Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    17,
                    30,
                    45,
                    4,
                    250,
                    0
                ],
                "title": "RLPF: Reinforcement Learning from Prediction Feedback for User\n  Summarization with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLPF: Reinforcement Learning from Prediction Feedback for User\n  Summarization with LLMs"
                },
                "summary": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Lin Ning"
                    },
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Harrison Lee"
                    },
                    {
                        "name": "Neo Wu"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Sushant Prakash"
                    },
                    {
                        "name": "Shawn O'Banion"
                    },
                    {
                        "name": "Bradley Green"
                    },
                    {
                        "name": "Jun Xie"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xie"
                },
                "author": "Jun Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04383v1",
                "updated": "2024-09-06T16:19:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    16,
                    19,
                    35,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T16:19:35Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    16,
                    19,
                    35,
                    4,
                    250,
                    0
                ],
                "title": "Origin of yield stress and mechanical plasticity in biological tissues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Origin of yield stress and mechanical plasticity in biological tissues"
                },
                "summary": "During development and under normal physiological conditions, biological\ntissues are continuously subjected to substantial mechanical stresses. In\nresponse to large deformations cells in a tissue must undergo multicellular\nrearrangements in order to maintain integrity and robustness. However, how\nthese events are connected in time and space remains unknown. Here, using\ncomputational and theoretical modeling, we studied the mechanical plasticity of\nepithelial monolayers under large deformations. Our results demonstrate that\nthe jamming-unjamming (solid-fluid) transition in tissues can vary\nsignificantly depending on the degree of deformation, implying that tissues are\nhighly unconventional materials. Using analytical modeling, we elucidate the\norigins of this behavior. We also demonstrate how a tissue accommodates large\ndeformations through a collective series of rearrangements, which behave\nsimilarly to avalanches in non-living materials. We find that these tissue\navalanches are governed by stress redistribution and the spatial distribution\nof vulnerable spots. Finally, we propose a simple and experimentally accessible\nframework to predict avalanches and infer tissue mechanical stress based on\nstatic images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During development and under normal physiological conditions, biological\ntissues are continuously subjected to substantial mechanical stresses. In\nresponse to large deformations cells in a tissue must undergo multicellular\nrearrangements in order to maintain integrity and robustness. However, how\nthese events are connected in time and space remains unknown. Here, using\ncomputational and theoretical modeling, we studied the mechanical plasticity of\nepithelial monolayers under large deformations. Our results demonstrate that\nthe jamming-unjamming (solid-fluid) transition in tissues can vary\nsignificantly depending on the degree of deformation, implying that tissues are\nhighly unconventional materials. Using analytical modeling, we elucidate the\norigins of this behavior. We also demonstrate how a tissue accommodates large\ndeformations through a collective series of rearrangements, which behave\nsimilarly to avalanches in non-living materials. We find that these tissue\navalanches are governed by stress redistribution and the spatial distribution\nof vulnerable spots. Finally, we propose a simple and experimentally accessible\nframework to predict avalanches and infer tissue mechanical stress based on\nstatic images."
                },
                "authors": [
                    {
                        "name": "Anh Q. Nguyen"
                    },
                    {
                        "name": "Junxiang Huang"
                    },
                    {
                        "name": "Dapeng Bi"
                    }
                ],
                "author_detail": {
                    "name": "Dapeng Bi"
                },
                "author": "Dapeng Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00077v2",
                "updated": "2024-09-06T16:12:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    16,
                    12,
                    0,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-24T09:26:59Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    9,
                    26,
                    59,
                    5,
                    237,
                    0
                ],
                "title": "Are LLM-based methods good enough for detecting unfair terms of service?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM-based methods good enough for detecting unfair terms of service?"
                },
                "summary": "Countless terms of service (ToS) are being signed everyday by users all over\nthe world while interacting with all kinds of apps and websites. More often\nthan not, these online contracts spanning double-digit pages are signed blindly\nby users who simply want immediate access to the desired service. What would\nnormally require a consultation with a legal team, has now become a mundane\nactivity consisting of a few clicks where users potentially sign away their\nrights, for instance in terms of their data privacy, to countless online\nentities/companies. Large language models (LLMs) are good at parsing long\ntext-based documents, and could potentially be adopted to help users when\ndealing with dubious clauses in ToS and their underlying privacy policies. To\ninvestigate the utility of existing models for this task, we first build a\ndataset consisting of 12 questions applied individually to a set of privacy\npolicies crawled from popular websites. Thereafter, a series of open-source as\nwell as commercial chatbots such as ChatGPT, are queried over each question,\nwith the answers being compared to a given ground truth. Our results show that\nsome open-source models are able to provide a higher accuracy compared to some\ncommercial models. However, the best performance is recorded from a commercial\nchatbot (ChatGPT4). Overall, all models perform only slightly better than\nrandom at this task. Consequently, their performance needs to be significantly\nimproved before they can be adopted at large for this purpose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Countless terms of service (ToS) are being signed everyday by users all over\nthe world while interacting with all kinds of apps and websites. More often\nthan not, these online contracts spanning double-digit pages are signed blindly\nby users who simply want immediate access to the desired service. What would\nnormally require a consultation with a legal team, has now become a mundane\nactivity consisting of a few clicks where users potentially sign away their\nrights, for instance in terms of their data privacy, to countless online\nentities/companies. Large language models (LLMs) are good at parsing long\ntext-based documents, and could potentially be adopted to help users when\ndealing with dubious clauses in ToS and their underlying privacy policies. To\ninvestigate the utility of existing models for this task, we first build a\ndataset consisting of 12 questions applied individually to a set of privacy\npolicies crawled from popular websites. Thereafter, a series of open-source as\nwell as commercial chatbots such as ChatGPT, are queried over each question,\nwith the answers being compared to a given ground truth. Our results show that\nsome open-source models are able to provide a higher accuracy compared to some\ncommercial models. However, the best performance is recorded from a commercial\nchatbot (ChatGPT4). Overall, all models perform only slightly better than\nrandom at this task. Consequently, their performance needs to be significantly\nimproved before they can be adopted at large for this purpose."
                },
                "authors": [
                    {
                        "name": "Mirgita Frasheri"
                    },
                    {
                        "name": "Arian Bakhtiarnia"
                    },
                    {
                        "name": "Lukas Esterle"
                    },
                    {
                        "name": "Alexandros Iosifidis"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Iosifidis"
                },
                "author": "Alexandros Iosifidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07019v2",
                "updated": "2024-09-06T15:56:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    56,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2023-10-10T21:12:15Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    21,
                    12,
                    15,
                    1,
                    283,
                    0
                ],
                "title": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans\n  and AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans\n  and AI"
                },
                "summary": "Communities and groups often need to make decisions based on social norms and\npreferences, such as when moderating content or building AI systems that\nreflect human values. The prevailing approach has been to first create\nhigh-level guidelines -- ``constitutions'' -- and then decide on new cases\nusing the outlined criteria. However, social norms and preferences vary between\ngroups, decision-makers can interpret guidelines inconsistently, and\nexceptional situations may be under-specified.\n  In this work, we take inspiration from legal systems and introduce ``case law\ngrounding'' (CLG), a novel workflow that uses past cases and decisions\n(\\textbf{precedents}) to help ground future decisions, for both human and\nLLM-based decision-makers. We evaluate CLG against a constitution-only approach\non two tasks for both types of decision-makers, and find that decisions\nproduced with CLG were more accurately aligned to observed ground truth in all\ncases, producing a 3.3--23.3 \\%-points improvement (across different tasks and\ngroups) for humans and 9.2--30.0 \\%-points (across different tasks and groups)\nfor LLM agents. We also discuss other aspects where a case-based approach could\naugment existing ``constitutional'' approaches when it comes to aligning human\nand AI decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communities and groups often need to make decisions based on social norms and\npreferences, such as when moderating content or building AI systems that\nreflect human values. The prevailing approach has been to first create\nhigh-level guidelines -- ``constitutions'' -- and then decide on new cases\nusing the outlined criteria. However, social norms and preferences vary between\ngroups, decision-makers can interpret guidelines inconsistently, and\nexceptional situations may be under-specified.\n  In this work, we take inspiration from legal systems and introduce ``case law\ngrounding'' (CLG), a novel workflow that uses past cases and decisions\n(\\textbf{precedents}) to help ground future decisions, for both human and\nLLM-based decision-makers. We evaluate CLG against a constitution-only approach\non two tasks for both types of decision-makers, and find that decisions\nproduced with CLG were more accurately aligned to observed ground truth in all\ncases, producing a 3.3--23.3 \\%-points improvement (across different tasks and\ngroups) for humans and 9.2--30.0 \\%-points (across different tasks and groups)\nfor LLM agents. We also discuss other aspects where a case-based approach could\naugment existing ``constitutional'' approaches when it comes to aligning human\nand AI decisions."
                },
                "authors": [
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17489v2",
                "updated": "2024-09-06T15:54:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    54,
                    8,
                    4,
                    250,
                    0
                ],
                "published": "2024-03-26T08:33:44Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    8,
                    33,
                    44,
                    1,
                    86,
                    0
                ],
                "title": "Adaptive Bayesian Structure Learning of DAGs With Non-conjugate Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Bayesian Structure Learning of DAGs With Non-conjugate Prior"
                },
                "summary": "Directed Acyclic Graphs (DAGs) are solid structures used to describe and\ninfer the dependencies among variables in multivariate scenarios. Having a\nthorough comprehension of the accurate DAG-generating model is crucial for\ncausal discovery and estimation. Our work suggests utilizing a non-conjugate\nprior for Gaussian DAG structure learning to enhance the posterior probability.\nWe employ the idea of using the Bessel function to address the computational\nburden, providing faster MCMC computation compared to the use of conjugate\npriors. In addition, our proposal exhibits a greater rate of adaptation when\ncompared to the conjugate prior, specifically for the inclusion of nodes in the\nDAG-generating model. Simulation studies demonstrate the superior accuracy of\nDAG learning, and we obtain the same maximum a posteriori and median\nprobability model estimate for the AML data, using the non-conjugate prior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed Acyclic Graphs (DAGs) are solid structures used to describe and\ninfer the dependencies among variables in multivariate scenarios. Having a\nthorough comprehension of the accurate DAG-generating model is crucial for\ncausal discovery and estimation. Our work suggests utilizing a non-conjugate\nprior for Gaussian DAG structure learning to enhance the posterior probability.\nWe employ the idea of using the Bessel function to address the computational\nburden, providing faster MCMC computation compared to the use of conjugate\npriors. In addition, our proposal exhibits a greater rate of adaptation when\ncompared to the conjugate prior, specifically for the inclusion of nodes in the\nDAG-generating model. Simulation studies demonstrate the superior accuracy of\nDAG learning, and we obtain the same maximum a posteriori and median\nprobability model estimate for the AML data, using the non-conjugate prior."
                },
                "authors": [
                    {
                        "name": "S. Nazari"
                    },
                    {
                        "name": "M. Arashi"
                    },
                    {
                        "name": "A. Sadeghkhani"
                    }
                ],
                "author_detail": {
                    "name": "A. Sadeghkhani"
                },
                "author": "A. Sadeghkhani",
                "arxiv_comment": "The content is not correct and there are fundamental errors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04360v1",
                "updated": "2024-09-06T15:42:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    42,
                    10,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T15:42:10Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    42,
                    10,
                    4,
                    250,
                    0
                ],
                "title": "Connectivity-Inspired Network for Context-Aware Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connectivity-Inspired Network for Context-Aware Recognition"
                },
                "summary": "The aim of this paper is threefold. We inform the AI practitioner about the\nhuman visual system with an extensive literature review; we propose a novel\nbiologically motivated neural network for image classification; and, finally,\nwe present a new plug-and-play module to model context awareness. We focus on\nthe effect of incorporating circuit motifs found in biological brains to\naddress visual recognition. Our convolutional architecture is inspired by the\nconnectivity of human cortical and subcortical streams, and we implement\nbottom-up and top-down modulations that mimic the extensive afferent and\nefferent connections between visual and cognitive areas. Our Contextual\nAttention Block is simple and effective and can be integrated with any\nfeed-forward neural network. It infers weights that multiply the feature maps\naccording to their causal influence on the scene, modeling the co-occurrence of\ndifferent objects in the image. We place our module at different bottlenecks to\ninfuse a hierarchical context awareness into the model. We validated our\nproposals through image classification experiments on benchmark data and found\na consistent improvement in performance and the robustness of the produced\nexplanations via class activation. Our code is available at\nhttps://github.com/gianlucarloni/CoCoReco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The aim of this paper is threefold. We inform the AI practitioner about the\nhuman visual system with an extensive literature review; we propose a novel\nbiologically motivated neural network for image classification; and, finally,\nwe present a new plug-and-play module to model context awareness. We focus on\nthe effect of incorporating circuit motifs found in biological brains to\naddress visual recognition. Our convolutional architecture is inspired by the\nconnectivity of human cortical and subcortical streams, and we implement\nbottom-up and top-down modulations that mimic the extensive afferent and\nefferent connections between visual and cognitive areas. Our Contextual\nAttention Block is simple and effective and can be integrated with any\nfeed-forward neural network. It infers weights that multiply the feature maps\naccording to their causal influence on the scene, modeling the co-occurrence of\ndifferent objects in the image. We place our module at different bottlenecks to\ninfuse a hierarchical context awareness into the model. We validated our\nproposals through image classification experiments on benchmark data and found\na consistent improvement in performance and the robustness of the produced\nexplanations via class activation. Our code is available at\nhttps://github.com/gianlucarloni/CoCoReco."
                },
                "authors": [
                    {
                        "name": "Gianluca Carloni"
                    },
                    {
                        "name": "Sara Colantonio"
                    }
                ],
                "author_detail": {
                    "name": "Sara Colantonio"
                },
                "author": "Sara Colantonio",
                "arxiv_comment": "ECCV 2024 - HCV Workshop, Accepted for presentation, Submitted\n  Manuscript Version (adapted to include author names, Acknowledgements, and\n  reference DOIs): the version of the manuscript improved after peer review\n  will appear in the Proceedings later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4; I.5; J.3; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04346v1",
                "updated": "2024-09-06T15:27:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    27,
                    58,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T15:27:58Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    27,
                    58,
                    4,
                    250,
                    0
                ],
                "title": "ZTF SN Ia DR2: Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZTF SN Ia DR2: Overview"
                },
                "summary": "We present the first homogeneous release of several thousand Type Ia\nsupernovae (SNe Ia), all having spectroscopic classification, and spectroscopic\nredshifts for half the sample. This release, named the \"DR2\", contains 3628\nnearby (z < 0.3) SNe Ia discovered, followed and classified by the Zwicky\nTransient Facility survey between March 2018 and December 2020. Of these, 3000\nhave good-to-excellent sampling and 2667 pass standard cosmology light-curve\nquality cuts. This release is thus the largest SN Ia release to date,\nincreasing by an order of magnitude the number of well characterized\nlow-redshift objects. With the \"DR2\", we also provide a volume-limited (z <\n0.06) sample of nearly a thousand SNe Ia. With such a large, homogeneous and\nwell controlled dataset, we are studying key current questions on SN cosmology,\nsuch as the linearity SNe Ia standardization, the SN and host dependencies, the\ndiversity of the SN Ia population, and the accuracy of the current light-curve\nmodeling. These, and more, are studied in detail in a series of articles\nassociated with this release. Alongside the SN Ia parameters, we publish our\nforce-photometry gri-band light curves, 5138 spectra, local and global host\nproperties, observing logs, and a python tool to ease use and access of these\ndata. The photometric accuracy of the \"DR2\" is not yet suited for cosmological\nparameter inference, which will follow as \"DR2.5\" release. We nonetheless\ndemonstrate that the multi-thousand SN Ia Hubble Diagram has a typical 0.15 mag\nscatter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first homogeneous release of several thousand Type Ia\nsupernovae (SNe Ia), all having spectroscopic classification, and spectroscopic\nredshifts for half the sample. This release, named the \"DR2\", contains 3628\nnearby (z < 0.3) SNe Ia discovered, followed and classified by the Zwicky\nTransient Facility survey between March 2018 and December 2020. Of these, 3000\nhave good-to-excellent sampling and 2667 pass standard cosmology light-curve\nquality cuts. This release is thus the largest SN Ia release to date,\nincreasing by an order of magnitude the number of well characterized\nlow-redshift objects. With the \"DR2\", we also provide a volume-limited (z <\n0.06) sample of nearly a thousand SNe Ia. With such a large, homogeneous and\nwell controlled dataset, we are studying key current questions on SN cosmology,\nsuch as the linearity SNe Ia standardization, the SN and host dependencies, the\ndiversity of the SN Ia population, and the accuracy of the current light-curve\nmodeling. These, and more, are studied in detail in a series of articles\nassociated with this release. Alongside the SN Ia parameters, we publish our\nforce-photometry gri-band light curves, 5138 spectra, local and global host\nproperties, observing logs, and a python tool to ease use and access of these\ndata. The photometric accuracy of the \"DR2\" is not yet suited for cosmological\nparameter inference, which will follow as \"DR2.5\" release. We nonetheless\ndemonstrate that the multi-thousand SN Ia Hubble Diagram has a typical 0.15 mag\nscatter."
                },
                "authors": [
                    {
                        "name": "Mickael Rigault"
                    },
                    {
                        "name": "Mathew Smith"
                    },
                    {
                        "name": "Ariel Goobar"
                    },
                    {
                        "name": "Kate Maguire"
                    },
                    {
                        "name": "Georgios Dimitriadis"
                    },
                    {
                        "name": "Umut Burgaz"
                    },
                    {
                        "name": "Suhail Dhawan"
                    },
                    {
                        "name": "Jesper Sollerman"
                    },
                    {
                        "name": "Nicolas Regnault"
                    },
                    {
                        "name": "Marek Kowalski"
                    },
                    {
                        "name": "Melissa Amenouche"
                    },
                    {
                        "name": "Marie Aubert"
                    },
                    {
                        "name": "Chloé Barjou-Delayre"
                    },
                    {
                        "name": "Julian Bautista"
                    },
                    {
                        "name": "Josh S. Bloom"
                    },
                    {
                        "name": "Bastien Carreres"
                    },
                    {
                        "name": "Tracy X. Chen"
                    },
                    {
                        "name": "Yannick Copin"
                    },
                    {
                        "name": "Maxime Deckers"
                    },
                    {
                        "name": "Dominique Fouchez"
                    },
                    {
                        "name": "Christoffer Fremling"
                    },
                    {
                        "name": "Lluis Galbany"
                    },
                    {
                        "name": "Madeleine Ginolin"
                    },
                    {
                        "name": "Matthew Graham"
                    },
                    {
                        "name": "Mancy M. Kasliwal"
                    },
                    {
                        "name": "W. D'Arcy Kenworthy"
                    },
                    {
                        "name": "Young-Lo Kim"
                    },
                    {
                        "name": "Dylan Kuhn"
                    },
                    {
                        "name": "Frank F. Masci"
                    },
                    {
                        "name": "Tomas Müller-Bravo"
                    },
                    {
                        "name": "Adam Miller"
                    },
                    {
                        "name": "Joel Johansson"
                    },
                    {
                        "name": "Jakob Nordin"
                    },
                    {
                        "name": "Peter Nugent"
                    },
                    {
                        "name": "Igor Andreoni"
                    },
                    {
                        "name": "Eric Bellm"
                    },
                    {
                        "name": "Marc Betoule"
                    },
                    {
                        "name": "Mahmoud Osman"
                    },
                    {
                        "name": "Dan Perley"
                    },
                    {
                        "name": "Brodie Popovic"
                    },
                    {
                        "name": "Philippe Rosnet"
                    },
                    {
                        "name": "Damiano Rosselli"
                    },
                    {
                        "name": "Florian Ruppin"
                    },
                    {
                        "name": "Robert Senzel"
                    },
                    {
                        "name": "Ben Rusholme"
                    },
                    {
                        "name": "Tassilo Schweyer"
                    },
                    {
                        "name": "Jacco H. Terwel"
                    },
                    {
                        "name": "Alice Townsend"
                    },
                    {
                        "name": "Andy Tzanidakis"
                    },
                    {
                        "name": "Avery Wold"
                    },
                    {
                        "name": "Josiah Purdum"
                    },
                    {
                        "name": "Yu-Jing Qin"
                    },
                    {
                        "name": "Benjamin Racine"
                    },
                    {
                        "name": "Simeon Reusch"
                    },
                    {
                        "name": "Reed Riddle"
                    },
                    {
                        "name": "Lin Yan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yan"
                },
                "author": "Lin Yan",
                "arxiv_comment": "ZTF SN Ia DR2 release paper. Submitted to A&A (ZTF DR2 Special\n  Issue). Already 1 response to referee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04340v1",
                "updated": "2024-09-06T15:18:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    18,
                    12,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T15:18:12Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    18,
                    12,
                    4,
                    250,
                    0
                ],
                "title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs"
                },
                "summary": "LLMs can exhibit age biases, resulting in unequal treatment of individuals\nacross age groups. While much research has addressed racial and gender biases,\nage bias remains little explored. The scarcity of instruction-tuning and\npreference datasets for age bias hampers its detection and measurement, and\nexisting fine-tuning methods seldom address age-related fairness. In this\npaper, we construct age bias preference datasets and instruction-tuning\ndatasets for RLHF. We introduce ARG, an age fairness reward to reduce\ndifferences in the response quality of LLMs across different age groups.\nExtensive experiments demonstrate that this reward significantly improves\nresponse accuracy and reduces performance disparities across age groups. Our\nsource code and datasets are available at the anonymous\n\\href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can exhibit age biases, resulting in unequal treatment of individuals\nacross age groups. While much research has addressed racial and gender biases,\nage bias remains little explored. The scarcity of instruction-tuning and\npreference datasets for age bias hampers its detection and measurement, and\nexisting fine-tuning methods seldom address age-related fairness. In this\npaper, we construct age bias preference datasets and instruction-tuning\ndatasets for RLHF. We introduce ARG, an age fairness reward to reduce\ndifferences in the response quality of LLMs across different age groups.\nExtensive experiments demonstrate that this reward significantly improves\nresponse accuracy and reduces performance disparities across age groups. Our\nsource code and datasets are available at the anonymous\n\\href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}."
                },
                "authors": [
                    {
                        "name": "Shuirong Cao"
                    },
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Zhiqiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Wang"
                },
                "author": "Zhiqiang Wang",
                "arxiv_comment": "The first two authors contributed equally to this work. Corresponding\n  to Zhiqiang Wang. ACKNOWLEDGMENT: we would like to thank the computing\n  resources support from the State Key Laboratory of New Computer Software\n  Technologies at Nanjing University",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04332v1",
                "updated": "2024-09-06T15:09:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    9,
                    4,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T15:09:04Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    9,
                    4,
                    4,
                    250,
                    0
                ],
                "title": "Amortized Bayesian Workflow (Extended Abstract)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Bayesian Workflow (Extended Abstract)"
                },
                "summary": "Bayesian inference often faces a trade-off between computational speed and\nsampling accuracy. We propose an adaptive workflow that integrates rapid\namortized inference with gold-standard MCMC techniques to achieve both speed\nand accuracy when performing inference on many observed datasets. Our approach\nuses principled diagnostics to guide the choice of inference method for each\ndataset, moving along the Pareto front from fast amortized sampling to slower\nbut guaranteed-accurate MCMC when necessary. By reusing computations across\nsteps, our workflow creates synergies between amortized and MCMC-based\ninference. We demonstrate the effectiveness of this integrated approach on a\ngeneralized extreme value task with 1000 observed data sets, showing 90x time\nefficiency gains while maintaining high posterior quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference often faces a trade-off between computational speed and\nsampling accuracy. We propose an adaptive workflow that integrates rapid\namortized inference with gold-standard MCMC techniques to achieve both speed\nand accuracy when performing inference on many observed datasets. Our approach\nuses principled diagnostics to guide the choice of inference method for each\ndataset, moving along the Pareto front from fast amortized sampling to slower\nbut guaranteed-accurate MCMC when necessary. By reusing computations across\nsteps, our workflow creates synergies between amortized and MCMC-based\ninference. We demonstrate the effectiveness of this integrated approach on a\ngeneralized extreme value task with 1000 observed data sets, showing 90x time\nefficiency gains while maintaining high posterior quality."
                },
                "authors": [
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Chengkun Li"
                    },
                    {
                        "name": "Aki Vehtari"
                    },
                    {
                        "name": "Luigi Acerbi"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    },
                    {
                        "name": "Stefan T. Radev"
                    }
                ],
                "author_detail": {
                    "name": "Stefan T. Radev"
                },
                "author": "Stefan T. Radev",
                "arxiv_comment": "Extended Abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04888v2",
                "updated": "2024-09-06T14:55:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    55,
                    48,
                    4,
                    250,
                    0
                ],
                "published": "2024-06-07T12:33:59Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    33,
                    59,
                    4,
                    159,
                    0
                ],
                "title": "Zero-Shot Video Editing through Adaptive Sliding Score Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Video Editing through Adaptive Sliding Score Distillation"
                },
                "summary": "The rapidly evolving field of Text-to-Video generation (T2V) has catalyzed\nrenewed interest in controllable video editing research. While the application\nof editing prompts to guide diffusion model denoising has gained prominence,\nmirroring advancements in image editing, this noise-based inference process\ninherently compromises the original video's integrity, resulting in unintended\nover-editing and temporal discontinuities. To address these challenges, this\nstudy proposes a novel paradigm of video-based score distillation, facilitating\ndirect manipulation of original video content. Specifically, distinguishing it\nfrom image-based score distillation, we propose an Adaptive Sliding Score\nDistillation strategy, which incorporates both global and local video guidance\nto reduce the impact of editing errors. Combined with our proposed Image-based\nJoint Guidance mechanism, it has the ability to mitigate the inherent\ninstability of the T2V model and single-step sampling. Additionally, we design\na Weighted Attention Fusion module to further preserve the key features of the\noriginal video and avoid over-editing. Extensive experiments demonstrate that\nthese strategies effectively address existing challenges, achieving superior\nperformance compared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly evolving field of Text-to-Video generation (T2V) has catalyzed\nrenewed interest in controllable video editing research. While the application\nof editing prompts to guide diffusion model denoising has gained prominence,\nmirroring advancements in image editing, this noise-based inference process\ninherently compromises the original video's integrity, resulting in unintended\nover-editing and temporal discontinuities. To address these challenges, this\nstudy proposes a novel paradigm of video-based score distillation, facilitating\ndirect manipulation of original video content. Specifically, distinguishing it\nfrom image-based score distillation, we propose an Adaptive Sliding Score\nDistillation strategy, which incorporates both global and local video guidance\nto reduce the impact of editing errors. Combined with our proposed Image-based\nJoint Guidance mechanism, it has the ability to mitigate the inherent\ninstability of the T2V model and single-step sampling. Additionally, we design\na Weighted Attention Fusion module to further preserve the key features of the\noriginal video and avoid over-editing. Extensive experiments demonstrate that\nthese strategies effectively address existing challenges, achieving superior\nperformance compared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Lianghan Zhu"
                    },
                    {
                        "name": "Yanqi Bao"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Jing Wu"
                    },
                    {
                        "name": "Yu-Kun Lai"
                    },
                    {
                        "name": "Wenbin Li"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00884v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00884v3",
                "updated": "2024-09-06T14:49:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    49,
                    21,
                    4,
                    250,
                    0
                ],
                "published": "2024-03-01T10:01:36Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    10,
                    1,
                    36,
                    4,
                    61,
                    0
                ],
                "title": "Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for\n  Metadata Enrichment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for\n  Metadata Enrichment"
                },
                "summary": "Traditional dataset retrieval systems rely on metadata for indexing, rather\nthan on the underlying data values. However, high-quality metadata creation and\nenrichment often require manual annotations, which is a labour-intensive and\nchallenging process to automate. In this study, we propose a method to support\nmetadata enrichment using topic annotations generated by three Large Language\nModels (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses\non classifying column headers based on domain-specific topics from the\nConsortium of European Social Science Data Archives (CESSDA), a Linked Data\ncontrolled vocabulary. Our approach operates in a zero-shot setting,\nintegrating the controlled topic vocabulary directly within the input prompt.\nThis integration serves as a Large Context Windows approach, with the aim of\nimproving the results of the topic classification task.\n  We evaluated the performance of the LLMs in terms of internal consistency,\ninter-machine alignment, and agreement with human classification. Additionally,\nwe investigate the impact of contextual information (i.e., dataset description)\non the classification outcomes. Our findings suggest that ChatGPT and\nGoogleGemini outperform GoogleBard in terms of internal consistency as well as\nLLM-human-agreement. Interestingly, we found that contextual information had no\nsignificant impact on LLM performance.\n  This work proposes a novel approach that leverages LLMs for topic\nclassification of column headers using a controlled vocabulary, presenting a\npractical application of LLMs and Large Context Windows within the Semantic Web\ndomain. This approach has the potential to facilitate automated metadata\nenrichment, thereby enhancing dataset retrieval and the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) of research data on the\nWeb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional dataset retrieval systems rely on metadata for indexing, rather\nthan on the underlying data values. However, high-quality metadata creation and\nenrichment often require manual annotations, which is a labour-intensive and\nchallenging process to automate. In this study, we propose a method to support\nmetadata enrichment using topic annotations generated by three Large Language\nModels (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses\non classifying column headers based on domain-specific topics from the\nConsortium of European Social Science Data Archives (CESSDA), a Linked Data\ncontrolled vocabulary. Our approach operates in a zero-shot setting,\nintegrating the controlled topic vocabulary directly within the input prompt.\nThis integration serves as a Large Context Windows approach, with the aim of\nimproving the results of the topic classification task.\n  We evaluated the performance of the LLMs in terms of internal consistency,\ninter-machine alignment, and agreement with human classification. Additionally,\nwe investigate the impact of contextual information (i.e., dataset description)\non the classification outcomes. Our findings suggest that ChatGPT and\nGoogleGemini outperform GoogleBard in terms of internal consistency as well as\nLLM-human-agreement. Interestingly, we found that contextual information had no\nsignificant impact on LLM performance.\n  This work proposes a novel approach that leverages LLMs for topic\nclassification of column headers using a controlled vocabulary, presenting a\npractical application of LLMs and Large Context Windows within the Semantic Web\ndomain. This approach has the potential to facilitate automated metadata\nenrichment, thereby enhancing dataset retrieval and the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) of research data on the\nWeb."
                },
                "authors": [
                    {
                        "name": "Margherita Martorana"
                    },
                    {
                        "name": "Tobias Kuhn"
                    },
                    {
                        "name": "Lise Stork"
                    },
                    {
                        "name": "Jacco van Ossenbruggen"
                    }
                ],
                "author_detail": {
                    "name": "Jacco van Ossenbruggen"
                },
                "author": "Jacco van Ossenbruggen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00884v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00884v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04318v1",
                "updated": "2024-09-06T14:46:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    46,
                    37,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T14:46:37Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    46,
                    37,
                    4,
                    250,
                    0
                ],
                "title": "Learning vs Retrieval: The Role of In-Context Examples in Regression\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning vs Retrieval: The Role of In-Context Examples in Regression\n  with LLMs"
                },
                "summary": "Generative Large Language Models (LLMs) are capable of being in-context\nlearners. However, the underlying mechanism of in-context learning (ICL) is\nstill a major research question, and experimental research results about how\nmodels exploit ICL are not always consistent. In this work, we propose a\nframework for evaluating in-context learning mechanisms, which we claim are a\ncombination of retrieving internal knowledge and learning from in-context\nexamples by focusing on regression tasks. First, we show that LLMs can perform\nregression on real-world datasets and then design experiments to measure the\nextent to which the LLM retrieves its internal knowledge versus learning from\nin-context examples. We argue that this process lies on a spectrum between\nthese two extremes. We provide an in-depth analysis of the degrees to which\nthese mechanisms are triggered depending on various factors, such as prior\nknowledge about the tasks and the type and richness of the information provided\nby the in-context examples. We employ three LLMs and utilize multiple datasets\nto corroborate the robustness of our findings. Our results shed light on how to\nengineer prompts to leverage meta-learning from in-context examples and foster\nknowledge retrieval depending on the problem being addressed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) are capable of being in-context\nlearners. However, the underlying mechanism of in-context learning (ICL) is\nstill a major research question, and experimental research results about how\nmodels exploit ICL are not always consistent. In this work, we propose a\nframework for evaluating in-context learning mechanisms, which we claim are a\ncombination of retrieving internal knowledge and learning from in-context\nexamples by focusing on regression tasks. First, we show that LLMs can perform\nregression on real-world datasets and then design experiments to measure the\nextent to which the LLM retrieves its internal knowledge versus learning from\nin-context examples. We argue that this process lies on a spectrum between\nthese two extremes. We provide an in-depth analysis of the degrees to which\nthese mechanisms are triggered depending on various factors, such as prior\nknowledge about the tasks and the type and richness of the information provided\nby the in-context examples. We employ three LLMs and utilize multiple datasets\nto corroborate the robustness of our findings. Our results shed light on how to\nengineer prompts to leverage meta-learning from in-context examples and foster\nknowledge retrieval depending on the problem being addressed."
                },
                "authors": [
                    {
                        "name": "Aliakbar Nafar"
                    },
                    {
                        "name": "Kristen Brent Venable"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08061v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08061v3",
                "updated": "2024-09-06T14:35:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    35,
                    57,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-10T21:51:50Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    21,
                    51,
                    50,
                    2,
                    192,
                    0
                ],
                "title": "Geospecific View Generation -- Geometry-Context Aware High-resolution\n  Ground View Inference from Satellite Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospecific View Generation -- Geometry-Context Aware High-resolution\n  Ground View Inference from Satellite Views"
                },
                "summary": "Predicting realistic ground views from satellite imagery in urban scenes is a\nchallenging task due to the significant view gaps between satellite and\nground-view images. We propose a novel pipeline to tackle this challenge, by\ngenerating geospecifc views that maximally respect the weak geometry and\ntexture from multi-view satellite images. Different from existing approaches\nthat hallucinate images from cues such as partial semantics or geometry from\noverhead satellite images, our method directly predicts ground-view images at\ngeolocation by using a comprehensive set of information from the satellite\nimage, resulting in ground-level images with a resolution boost at a factor of\nten or more. We leverage a novel building refinement method to reduce geometric\ndistortions in satellite data at ground level, which ensures the creation of\naccurate conditions for view synthesis using diffusion networks. Moreover, we\nproposed a novel geospecific prior, which prompts distribution learning of\ndiffusion models to respect image samples that are closer to the geolocation of\nthe predicted images. We demonstrate our pipeline is the first to generate\nclose-to-real and geospecific ground views merely based on satellite images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting realistic ground views from satellite imagery in urban scenes is a\nchallenging task due to the significant view gaps between satellite and\nground-view images. We propose a novel pipeline to tackle this challenge, by\ngenerating geospecifc views that maximally respect the weak geometry and\ntexture from multi-view satellite images. Different from existing approaches\nthat hallucinate images from cues such as partial semantics or geometry from\noverhead satellite images, our method directly predicts ground-view images at\ngeolocation by using a comprehensive set of information from the satellite\nimage, resulting in ground-level images with a resolution boost at a factor of\nten or more. We leverage a novel building refinement method to reduce geometric\ndistortions in satellite data at ground level, which ensures the creation of\naccurate conditions for view synthesis using diffusion networks. Moreover, we\nproposed a novel geospecific prior, which prompts distribution learning of\ndiffusion models to respect image samples that are closer to the geolocation of\nthe predicted images. We demonstrate our pipeline is the first to generate\nclose-to-real and geospecific ground views merely based on satellite images."
                },
                "authors": [
                    {
                        "name": "Ningli Xu"
                    },
                    {
                        "name": "Rongjun Qin"
                    }
                ],
                "author_detail": {
                    "name": "Rongjun Qin"
                },
                "author": "Rongjun Qin",
                "arxiv_comment": "11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08061v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08061v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04306v1",
                "updated": "2024-09-06T14:28:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    28,
                    41,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T14:28:41Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    28,
                    41,
                    4,
                    250,
                    0
                ],
                "title": "Safe and Efficient Path Planning under Uncertainty via Deep Collision\n  Probability Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe and Efficient Path Planning under Uncertainty via Deep Collision\n  Probability Fields"
                },
                "summary": "Estimating collision probabilities between robots and environmental obstacles\nor other moving agents is crucial to ensure safety during path planning. This\nis an important building block of modern planning algorithms in many\napplication scenarios such as autonomous driving, where noisy sensors perceive\nobstacles. While many approaches exist, they either provide too conservative\nestimates of the collision probabilities or are computationally intensive due\nto their sampling-based nature. To deal with these issues, we introduce Deep\nCollision Probability Fields, a neural-based approach for computing collision\nprobabilities of arbitrary objects with arbitrary unimodal uncertainty\ndistributions. Our approach relegates the computationally intensive estimation\nof collision probabilities via sampling at the training step, allowing for fast\nneural network inference of the constraints during planning. In extensive\nexperiments, we show that Deep Collision Probability Fields can produce\nreasonably accurate collision probabilities (up to 10^{-3}) for planning and\nthat our approach can be easily plugged into standard path planning approaches\nto plan safe paths on 2-D maps containing uncertain static and dynamic\nobstacles. Additional material, code, and videos are available at\nhttps://sites.google.com/view/ral-dcpf.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating collision probabilities between robots and environmental obstacles\nor other moving agents is crucial to ensure safety during path planning. This\nis an important building block of modern planning algorithms in many\napplication scenarios such as autonomous driving, where noisy sensors perceive\nobstacles. While many approaches exist, they either provide too conservative\nestimates of the collision probabilities or are computationally intensive due\nto their sampling-based nature. To deal with these issues, we introduce Deep\nCollision Probability Fields, a neural-based approach for computing collision\nprobabilities of arbitrary objects with arbitrary unimodal uncertainty\ndistributions. Our approach relegates the computationally intensive estimation\nof collision probabilities via sampling at the training step, allowing for fast\nneural network inference of the constraints during planning. In extensive\nexperiments, we show that Deep Collision Probability Fields can produce\nreasonably accurate collision probabilities (up to 10^{-3}) for planning and\nthat our approach can be easily plugged into standard path planning approaches\nto plan safe paths on 2-D maps containing uncertain static and dynamic\nobstacles. Additional material, code, and videos are available at\nhttps://sites.google.com/view/ral-dcpf."
                },
                "authors": [
                    {
                        "name": "Felix Herrmann"
                    },
                    {
                        "name": "Sebastian Zach"
                    },
                    {
                        "name": "Jacopo Banfi"
                    },
                    {
                        "name": "Jan Peters"
                    },
                    {
                        "name": "Georgia Chalvatzaki"
                    },
                    {
                        "name": "Davide Tateo"
                    }
                ],
                "author_detail": {
                    "name": "Davide Tateo"
                },
                "author": "Davide Tateo",
                "arxiv_comment": "Preprint version of a paper accepted to the IEEE Robotics and\n  Automation Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04301v1",
                "updated": "2024-09-06T14:20:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    20,
                    38,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T14:20:38Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    20,
                    38,
                    4,
                    250,
                    0
                ],
                "title": "A Unified Approach to Inferring Chemical Compounds with the Desired\n  Aqueous Solubility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Approach to Inferring Chemical Compounds with the Desired\n  Aqueous Solubility"
                },
                "summary": "Aqueous solubility (AS) is a key physiochemical property that plays a crucial\nrole in drug discovery and material design. We report a novel unified approach\nto predict and infer chemical compounds with the desired AS based on simple\ndeterministic graph-theoretic descriptors, multiple linear regression (MLR) and\nmixed integer linear programming (MILP). Selected descriptors based on a\nforward stepwise procedure enabled the simplest regression model, MLR, to\nachieve significantly good prediction accuracy compared to the existing\napproaches, achieving the accuracy in the range [0.7191, 0.9377] for 29 diverse\ndatasets. By simulating these descriptors and learning models as MILPs, we\ninferred mathematically exact and optimal compounds with the desired AS,\nprescribed structures, and up to 50 non-hydrogen atoms in a reasonable time\nrange [6, 1204] seconds. These findings indicate a strong correlation between\nthe simple graph-theoretic descriptors and the AS of compounds, potentially\nleading to a deeper understanding of their AS without relying on widely used\ncomplicated chemical descriptors and complex machine learning models that are\ncomputationally expensive, and therefore difficult to use for inference. An\nimplementation of the proposed approach is available at\nhttps://github.com/ku-dml/mol-infer/tree/master/AqSol.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aqueous solubility (AS) is a key physiochemical property that plays a crucial\nrole in drug discovery and material design. We report a novel unified approach\nto predict and infer chemical compounds with the desired AS based on simple\ndeterministic graph-theoretic descriptors, multiple linear regression (MLR) and\nmixed integer linear programming (MILP). Selected descriptors based on a\nforward stepwise procedure enabled the simplest regression model, MLR, to\nachieve significantly good prediction accuracy compared to the existing\napproaches, achieving the accuracy in the range [0.7191, 0.9377] for 29 diverse\ndatasets. By simulating these descriptors and learning models as MILPs, we\ninferred mathematically exact and optimal compounds with the desired AS,\nprescribed structures, and up to 50 non-hydrogen atoms in a reasonable time\nrange [6, 1204] seconds. These findings indicate a strong correlation between\nthe simple graph-theoretic descriptors and the AS of compounds, potentially\nleading to a deeper understanding of their AS without relying on widely used\ncomplicated chemical descriptors and complex machine learning models that are\ncomputationally expensive, and therefore difficult to use for inference. An\nimplementation of the proposed approach is available at\nhttps://github.com/ku-dml/mol-infer/tree/master/AqSol."
                },
                "authors": [
                    {
                        "name": "Muniba Batool"
                    },
                    {
                        "name": "Naveed Ahmed Azam"
                    },
                    {
                        "name": "Jianshen Zhu"
                    },
                    {
                        "name": "Kazuya Haraguchi"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Tatsuya Akutsu"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Akutsu"
                },
                "author": "Tatsuya Akutsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03708v2",
                "updated": "2024-09-06T14:18:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    18,
                    20,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T17:14:23Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    14,
                    23,
                    3,
                    249,
                    0
                ],
                "title": "RAG based Question-Answering for Contextual Response Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG based Question-Answering for Contextual Response Prediction System"
                },
                "summary": "Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload."
                },
                "authors": [
                    {
                        "name": "Sriram Veturi"
                    },
                    {
                        "name": "Saurabh Vaichal"
                    },
                    {
                        "name": "Reshma Lal Jagadheesh"
                    },
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Nian Yan"
                    }
                ],
                "author_detail": {
                    "name": "Nian Yan"
                },
                "author": "Nian Yan",
                "arxiv_comment": "Accepted at the 1st Workshop on GenAI and RAG Systems for Enterprise,\n  CIKM'24. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09236v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09236v3",
                "updated": "2024-09-06T13:34:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    13,
                    34,
                    16,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-17T16:04:31Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    16,
                    4,
                    31,
                    5,
                    230,
                    0
                ],
                "title": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords"
                },
                "summary": "This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes."
                },
                "authors": [
                    {
                        "name": "Aman Ahluwalia"
                    },
                    {
                        "name": "Bishwajit Sutradhar"
                    },
                    {
                        "name": "Karishma Ghosh"
                    },
                    {
                        "name": "Indrapal Yadav"
                    },
                    {
                        "name": "Arpan Sheetal"
                    },
                    {
                        "name": "Prashant Patil"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Patil"
                },
                "author": "Prashant Patil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09236v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09236v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11041v2",
                "updated": "2024-09-06T13:33:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    13,
                    33,
                    34,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-06T15:03:40Z",
                "published_parsed": [
                    2024,
                    7,
                    6,
                    15,
                    3,
                    40,
                    5,
                    188,
                    0
                ],
                "title": "Integer-only Quantized Transformers for Embedded FPGA-based Time-series\n  Forecasting in AIoT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integer-only Quantized Transformers for Embedded FPGA-based Time-series\n  Forecasting in AIoT"
                },
                "summary": "This paper presents the design of a hardware accelerator for Transformers,\noptimized for on-device time-series forecasting in AIoT systems. It integrates\ninteger-only quantization and Quantization-Aware Training with optimized\nhardware designs to realize 6-bit and 4-bit quantized Transformer models, which\nachieved precision comparable to 8-bit quantized models from related research.\nUtilizing a complete implementation on an embedded FPGA (Xilinx Spartan-7\nXC7S15), we examine the feasibility of deploying Transformer models on embedded\nIoT devices. This includes a thorough analysis of achievable precision,\nresource utilization, timing, power, and energy consumption for on-device\ninference. Our results indicate that while sufficient performance can be\nattained, the optimization process is not trivial. For instance, reducing the\nquantization bitwidth does not consistently result in decreased latency or\nenergy consumption, underscoring the necessity of systematically exploring\nvarious optimization combinations. Compared to an 8-bit quantized Transformer\nmodel in related studies, our 4-bit quantized Transformer model increases test\nloss by only 0.63%, operates up to 132.33x faster, and consumes 48.19x less\nenergy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the design of a hardware accelerator for Transformers,\noptimized for on-device time-series forecasting in AIoT systems. It integrates\ninteger-only quantization and Quantization-Aware Training with optimized\nhardware designs to realize 6-bit and 4-bit quantized Transformer models, which\nachieved precision comparable to 8-bit quantized models from related research.\nUtilizing a complete implementation on an embedded FPGA (Xilinx Spartan-7\nXC7S15), we examine the feasibility of deploying Transformer models on embedded\nIoT devices. This includes a thorough analysis of achievable precision,\nresource utilization, timing, power, and energy consumption for on-device\ninference. Our results indicate that while sufficient performance can be\nattained, the optimization process is not trivial. For instance, reducing the\nquantization bitwidth does not consistently result in decreased latency or\nenergy consumption, underscoring the necessity of systematically exploring\nvarious optimization combinations. Compared to an 8-bit quantized Transformer\nmodel in related studies, our 4-bit quantized Transformer model increases test\nloss by only 0.63%, operates up to 132.33x faster, and consumes 48.19x less\nenergy."
                },
                "authors": [
                    {
                        "name": "Tianheng Ling"
                    },
                    {
                        "name": "Chao Qian"
                    },
                    {
                        "name": "Gregor Schiele"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Schiele"
                },
                "author": "Gregor Schiele",
                "arxiv_comment": "7 pages, 3 figures, 4 tables. The paper was accepted by 2024 IEEE\n  Annual Congress on Artificial Intelligence of Things (IEEE AIoT) and got best\n  paper award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04270v1",
                "updated": "2024-09-06T13:25:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    13,
                    25,
                    43,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T13:25:43Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    13,
                    25,
                    43,
                    4,
                    250,
                    0
                ],
                "title": "Advancing Automated Knowledge Transfer in Evolutionary Multitasking via\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Automated Knowledge Transfer in Evolutionary Multitasking via\n  Large Language Models"
                },
                "summary": "Evolutionary Multi-task Optimization (EMTO) is a paradigm that leverages\nknowledge transfer across simultaneously optimized tasks for enhanced search\nperformance. To facilitate EMTO's performance, various knowledge transfer\nmodels have been developed for specific optimization tasks. However, designing\nthese models often requires substantial expert knowledge. Recently, large\nlanguage models (LLMs) have achieved remarkable success in autonomous\nprogramming, aiming to produce effective solvers for specific problems. In this\nwork, a LLM-based optimization paradigm is introduced to establish an\nautonomous model factory for generating knowledge transfer models, ensuring\neffective and efficient knowledge transfer across various optimization tasks.\nTo evaluate the performance of the proposed method, we conducted comprehensive\nempirical studies comparing the knowledge transfer model generated by the LLM\nwith existing state-of-the-art knowledge transfer methods. The results\ndemonstrate that the generated model is able to achieve superior or competitive\nperformance against hand-crafted knowledge transfer models in terms of both\nefficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Multi-task Optimization (EMTO) is a paradigm that leverages\nknowledge transfer across simultaneously optimized tasks for enhanced search\nperformance. To facilitate EMTO's performance, various knowledge transfer\nmodels have been developed for specific optimization tasks. However, designing\nthese models often requires substantial expert knowledge. Recently, large\nlanguage models (LLMs) have achieved remarkable success in autonomous\nprogramming, aiming to produce effective solvers for specific problems. In this\nwork, a LLM-based optimization paradigm is introduced to establish an\nautonomous model factory for generating knowledge transfer models, ensuring\neffective and efficient knowledge transfer across various optimization tasks.\nTo evaluate the performance of the proposed method, we conducted comprehensive\nempirical studies comparing the knowledge transfer model generated by the LLM\nwith existing state-of-the-art knowledge transfer methods. The results\ndemonstrate that the generated model is able to achieve superior or competitive\nperformance against hand-crafted knowledge transfer models in terms of both\nefficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuxiao Huang"
                    },
                    {
                        "name": "Xuebin Lv"
                    },
                    {
                        "name": "Shenghao Wu"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "arxiv_comment": "10 pages, 11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04259v1",
                "updated": "2024-09-06T13:12:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    13,
                    12,
                    52,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T13:12:52Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    13,
                    12,
                    52,
                    4,
                    250,
                    0
                ],
                "title": "An \"alien\" called Oosterhoff dichotomy?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An \"alien\" called Oosterhoff dichotomy?"
                },
                "summary": "In this letter we investigate the origin of the Oosterhoff dichotomy,\nconsidering recent discoveries related to several ancient merging events of\nexternal galaxies with the Milky Way (MW). In particular, we aim to clarify if\nthe subdivision in Oosterhoff type of Galactic Globular Clusters (GGCs) and\nfield RR Lyrae (RRLs) could be traced back to one or more ancient galaxies that\nmerged with the MW in its past. To this purpose, we first explored the\nassociation of GGCs with the past merging events according to different\nliterature studies. Subsequently we compiled positions, proper motions and\nradial velocity for 10,138 field RRLs variables from the $Gaia$ Data Release 3.\nTo infer the distances, we adopted the $M_G$--[Fe/H] relation, with [Fe/H]\nvalues estimated through empirical relationships involving the individual\nperiods and Fourier parameters. We then calculated the orbits and the integrals\nof motions (IoM) using the Python library Galpy for the whole sample. By\ncomparing the location of the field RRLs in the energy-angular momentum diagram\nwith that of the GGCs we assign their likely origin. Finally, we discriminate\nfrom the $Gaia$ G-band light curves the Oosterhoff type of our sample of RRL\nstars based on their location in the Bailey diagram. The analysis of the Bailey\ndiagrams for Galactic RRLs stars and GGCs associated with \\textit{In-Situ} vs\n\\textit{Accreted} halo origin shows remarkable differences. The\n\\textit{In-Situ} sample displays a wide range of metallicities with a\ncontinuous distribution and no sign of Oosterhoff dichotomy. Conversely, the\n\\textit{Accreted} RRLs clearly shows the Oosterhoff dichotomy and a\nsignificantly smaller dispersion in metallicity. Our results suggest that the\nOosterhoff dichotomy was imported into the MW by the merging events that shaped\nthe Galaxy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter we investigate the origin of the Oosterhoff dichotomy,\nconsidering recent discoveries related to several ancient merging events of\nexternal galaxies with the Milky Way (MW). In particular, we aim to clarify if\nthe subdivision in Oosterhoff type of Galactic Globular Clusters (GGCs) and\nfield RR Lyrae (RRLs) could be traced back to one or more ancient galaxies that\nmerged with the MW in its past. To this purpose, we first explored the\nassociation of GGCs with the past merging events according to different\nliterature studies. Subsequently we compiled positions, proper motions and\nradial velocity for 10,138 field RRLs variables from the $Gaia$ Data Release 3.\nTo infer the distances, we adopted the $M_G$--[Fe/H] relation, with [Fe/H]\nvalues estimated through empirical relationships involving the individual\nperiods and Fourier parameters. We then calculated the orbits and the integrals\nof motions (IoM) using the Python library Galpy for the whole sample. By\ncomparing the location of the field RRLs in the energy-angular momentum diagram\nwith that of the GGCs we assign their likely origin. Finally, we discriminate\nfrom the $Gaia$ G-band light curves the Oosterhoff type of our sample of RRL\nstars based on their location in the Bailey diagram. The analysis of the Bailey\ndiagrams for Galactic RRLs stars and GGCs associated with \\textit{In-Situ} vs\n\\textit{Accreted} halo origin shows remarkable differences. The\n\\textit{In-Situ} sample displays a wide range of metallicities with a\ncontinuous distribution and no sign of Oosterhoff dichotomy. Conversely, the\n\\textit{Accreted} RRLs clearly shows the Oosterhoff dichotomy and a\nsignificantly smaller dispersion in metallicity. Our results suggest that the\nOosterhoff dichotomy was imported into the MW by the merging events that shaped\nthe Galaxy."
                },
                "authors": [
                    {
                        "name": "E. Luongo"
                    },
                    {
                        "name": "V. Ripepi"
                    },
                    {
                        "name": "M. Marconi"
                    },
                    {
                        "name": "Z. Prudil"
                    },
                    {
                        "name": "M. Rejkuba"
                    },
                    {
                        "name": "G. Clementini"
                    },
                    {
                        "name": "G. Longo"
                    }
                ],
                "author_detail": {
                    "name": "G. Longo"
                },
                "author": "G. Longo",
                "arxiv_comment": "6 figures, 8 tables, 2 appendices. Accepted for publication on A&A\n  letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04249v1",
                "updated": "2024-09-06T12:55:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    55,
                    49,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T12:55:49Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    55,
                    49,
                    4,
                    250,
                    0
                ],
                "title": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge\n  Devices"
                },
                "summary": "The application of Transformer-based large models has achieved numerous\nsuccess in recent years. However, the exponential growth in the parameters of\nlarge models introduces formidable memory challenge for edge deployment. Prior\nworks to address this challenge mainly focus on optimizing the model structure\nand adopting memory swapping methods. However, the former reduces the inference\naccuracy, and the latter raises the inference latency. This paper introduces\nPIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces\nmemory usage by incorporating dynamic memory management and minimizes inference\nlatency by employing parallel model loading. Based on PIPELOAD mechanism, we\npresent Hermes, a framework optimized for large model inference on edge\ndevices. We evaluate Hermes on Transformer-based models of different sizes. Our\nexperiments illustrate that Hermes achieves up to 4.24 X increase in inference\nspeed and 86.7% lower memory consumption than the state-of-the-art pipeline\nmechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3%\nlower memory consumption for GPT-style models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of Transformer-based large models has achieved numerous\nsuccess in recent years. However, the exponential growth in the parameters of\nlarge models introduces formidable memory challenge for edge deployment. Prior\nworks to address this challenge mainly focus on optimizing the model structure\nand adopting memory swapping methods. However, the former reduces the inference\naccuracy, and the latter raises the inference latency. This paper introduces\nPIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces\nmemory usage by incorporating dynamic memory management and minimizes inference\nlatency by employing parallel model loading. Based on PIPELOAD mechanism, we\npresent Hermes, a framework optimized for large model inference on edge\ndevices. We evaluate Hermes on Transformer-based models of different sizes. Our\nexperiments illustrate that Hermes achieves up to 4.24 X increase in inference\nspeed and 86.7% lower memory consumption than the state-of-the-art pipeline\nmechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3%\nlower memory consumption for GPT-style models."
                },
                "authors": [
                    {
                        "name": "Xueyuan Han"
                    },
                    {
                        "name": "Zinuo Cai"
                    },
                    {
                        "name": "Yichu Zhang"
                    },
                    {
                        "name": "Chongxin Fan"
                    },
                    {
                        "name": "Junhan Liu"
                    },
                    {
                        "name": "Ruhui Ma"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "arxiv_comment": "Accepted by the 42nd IEEE International Conference on Computer Design\n  (ICCD 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03515v2",
                "updated": "2024-09-06T12:17:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    17,
                    4,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T13:29:45Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    29,
                    45,
                    3,
                    249,
                    0
                ],
                "title": "Local Measurement Scheme of Gravitational Curvature using Atom\n  Interferometers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Measurement Scheme of Gravitational Curvature using Atom\n  Interferometers"
                },
                "summary": "Light pulse atom interferometers (AIFs) are exquisite quantum probes of\nspatial inhomogeneity and gravitational curvature. Moreover, detailed\nmeasurement and calibration are necessary prerequisites for very-long-baseline\natom interferometry (VLBAI). Here we present a method in which the differential\nsignal of two co-located interferometers singles out a phase shift proportional\nto the curvature of the gravitational potential. The scale factor depends only\non well controlled quantities, namely the photon wave number, the\ninterferometer time and the atomic recoil, which allows the curvature to be\naccurately inferred from a measured phase. As a case study, we numerically\nsimulate such a co-located gradiometric interferometer in the context of the\nHannover VLBAI facility and prove the robustness of the phase shift in\ngravitational fields with complex spatial dependence. We define an estimator of\nthe gravitational curvature for non-trivial gravitational fields and calculate\nthe trade-off between signal strength and estimation accuracy with regard to\nspatial resolution. As a perspective, we discuss the case of a time-dependent\ngravitational field and corresponding measurement strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light pulse atom interferometers (AIFs) are exquisite quantum probes of\nspatial inhomogeneity and gravitational curvature. Moreover, detailed\nmeasurement and calibration are necessary prerequisites for very-long-baseline\natom interferometry (VLBAI). Here we present a method in which the differential\nsignal of two co-located interferometers singles out a phase shift proportional\nto the curvature of the gravitational potential. The scale factor depends only\non well controlled quantities, namely the photon wave number, the\ninterferometer time and the atomic recoil, which allows the curvature to be\naccurately inferred from a measured phase. As a case study, we numerically\nsimulate such a co-located gradiometric interferometer in the context of the\nHannover VLBAI facility and prove the robustness of the phase shift in\ngravitational fields with complex spatial dependence. We define an estimator of\nthe gravitational curvature for non-trivial gravitational fields and calculate\nthe trade-off between signal strength and estimation accuracy with regard to\nspatial resolution. As a perspective, we discuss the case of a time-dependent\ngravitational field and corresponding measurement strategies."
                },
                "authors": [
                    {
                        "name": "Michael Werner"
                    },
                    {
                        "name": "Ali Lezeik"
                    },
                    {
                        "name": "Dennis Schlippert"
                    },
                    {
                        "name": "Ernst Rasel"
                    },
                    {
                        "name": "Naceur Gaaloul"
                    },
                    {
                        "name": "Klemens Hammerer"
                    }
                ],
                "author_detail": {
                    "name": "Klemens Hammerer"
                },
                "author": "Klemens Hammerer",
                "arxiv_comment": "6 pages of main text, 4 pages appendix, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04214v2",
                "updated": "2024-09-09T02:46:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    2,
                    46,
                    34,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T12:11:06Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    11,
                    6,
                    4,
                    250,
                    0
                ],
                "title": "Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver"
                },
                "summary": "Mathematical reasoning remains an ongoing challenge for AI models, especially\nfor geometry problems that require both linguistic and visual signals. As the\nvision encoders of most MLLMs are trained on natural scenes, they often\nstruggle to understand geometric diagrams, performing no better in geometry\nproblem solving than LLMs that only process text. This limitation is amplified\nby the lack of effective methods for representing geometric relationships. To\naddress these issues, we introduce the Diagram Formalization Enhanced Geometry\nProblem Solver (DFE-GPS), a new framework that integrates visual features,\ngeometric formal language, and natural language representations. We propose a\nnovel synthetic data approach and create a large-scale geometric dataset,\nSynthGeo228K, annotated with both formal and natural language captions,\ndesigned to enhance the vision encoder for a better understanding of geometric\nstructures. Our framework improves MLLMs' ability to process geometric diagrams\nand extends their application to open-ended tasks on the formalgeo7k dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning remains an ongoing challenge for AI models, especially\nfor geometry problems that require both linguistic and visual signals. As the\nvision encoders of most MLLMs are trained on natural scenes, they often\nstruggle to understand geometric diagrams, performing no better in geometry\nproblem solving than LLMs that only process text. This limitation is amplified\nby the lack of effective methods for representing geometric relationships. To\naddress these issues, we introduce the Diagram Formalization Enhanced Geometry\nProblem Solver (DFE-GPS), a new framework that integrates visual features,\ngeometric formal language, and natural language representations. We propose a\nnovel synthetic data approach and create a large-scale geometric dataset,\nSynthGeo228K, annotated with both formal and natural language captions,\ndesigned to enhance the vision encoder for a better understanding of geometric\nstructures. Our framework improves MLLMs' ability to process geometric diagrams\nand extends their application to open-ended tasks on the formalgeo7k dataset."
                },
                "authors": [
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Jo-Ku Cheng"
                    },
                    {
                        "name": "Jingyang Deng"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Jinwen Ma"
                    },
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Xiaokai Zhang"
                    },
                    {
                        "name": "Na Zhu"
                    },
                    {
                        "name": "Tuo Leng"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Leng"
                },
                "author": "Tuo Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08108v2",
                "updated": "2024-09-06T12:10:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    10,
                    50,
                    4,
                    250,
                    0
                ],
                "published": "2024-03-12T22:33:02Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    22,
                    33,
                    2,
                    1,
                    72,
                    0
                ],
                "title": "TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object\n  Detection"
                },
                "summary": "Task-oriented object detection aims to find objects suitable for\naccomplishing specific tasks. As a challenging task, it requires simultaneous\nvisual data processing and reasoning under ambiguous semantics. Recent\nsolutions are mainly all-in-one models. However, the object detection backbones\nare pre-trained without text supervision. Thus, to incorporate task\nrequirements, their intricate models undergo extensive learning on a highly\nimbalanced and scarce dataset, resulting in capped performance, laborious\ntraining, and poor generalizability. In contrast, we propose TaskCLIP, a more\nnatural two-stage design composed of general object detection and task-guided\nobject selection. Particularly for the latter, we resort to the recently\nsuccessful large Vision-Language Models (VLMs) as our backbone, which provides\nrich semantic knowledge and a uniform embedding space for images and texts.\nNevertheless, the naive application of VLMs leads to sub-optimal quality, due\nto the misalignment between embeddings of object images and their visual\nattributes, which are mainly adjective phrases. To this end, we design a\ntransformer-based aligner after the pre-trained VLMs to re-calibrate both\nembeddings. Finally, we employ a trainable score function to post-process the\nVLM matching results for object selection. Experimental results demonstrate\nthat our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by\n3.5% and only requires a single NVIDIA RTX 4090 for both training and\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented object detection aims to find objects suitable for\naccomplishing specific tasks. As a challenging task, it requires simultaneous\nvisual data processing and reasoning under ambiguous semantics. Recent\nsolutions are mainly all-in-one models. However, the object detection backbones\nare pre-trained without text supervision. Thus, to incorporate task\nrequirements, their intricate models undergo extensive learning on a highly\nimbalanced and scarce dataset, resulting in capped performance, laborious\ntraining, and poor generalizability. In contrast, we propose TaskCLIP, a more\nnatural two-stage design composed of general object detection and task-guided\nobject selection. Particularly for the latter, we resort to the recently\nsuccessful large Vision-Language Models (VLMs) as our backbone, which provides\nrich semantic knowledge and a uniform embedding space for images and texts.\nNevertheless, the naive application of VLMs leads to sub-optimal quality, due\nto the misalignment between embeddings of object images and their visual\nattributes, which are mainly adjective phrases. To this end, we design a\ntransformer-based aligner after the pre-trained VLMs to re-calibrate both\nembeddings. Finally, we employ a trainable score function to post-process the\nVLM matching results for object selection. Experimental results demonstrate\nthat our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by\n3.5% and only requires a single NVIDIA RTX 4090 for both training and\ninference."
                },
                "authors": [
                    {
                        "name": "Hanning Chen"
                    },
                    {
                        "name": "Wenjun Huang"
                    },
                    {
                        "name": "Yang Ni"
                    },
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Yezi Liu"
                    },
                    {
                        "name": "Fei Wen"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    },
                    {
                        "name": "Hugo Latapie"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02822v2",
                "updated": "2024-09-06T11:45:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    45,
                    17,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-04T15:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    29,
                    2,
                    248,
                    0
                ],
                "title": "Language Understanding as a Constraint on Consensus Size in LLM\n  Societies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Understanding as a Constraint on Consensus Size in LLM\n  Societies"
                },
                "summary": "The applications of Large Language Models (LLMs) are going towards\ncollaborative tasks where several agents interact with each other like in an\nLLM society. In such a setting, large groups of LLMs could reach consensus\nabout arbitrary norms for which there is no information supporting one option\nover another, regulating their own behavior in a self-organized way. In human\nsocieties, the ability to reach consensus without institutions has a limit in\nthe cognitive capacities of humans. To understand if a similar phenomenon\ncharacterizes also LLMs, we apply methods from complexity science and\nprinciples from behavioral sciences in a new approach of AI anthropology. We\nfind that LLMs are able to reach consensus in groups and that the opinion\ndynamics of LLMs can be understood with a function parametrized by a majority\nforce coefficient that determines whether consensus is possible. This majority\nforce is stronger for models with higher language understanding capabilities\nand decreases for larger groups, leading to a critical group size beyond which,\nfor a given LLM, consensus is unfeasible. This critical group size grows\nexponentially with the language understanding capabilities of models and for\nthe most advanced models, it can reach an order of magnitude beyond the typical\nsize of informal human groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of Large Language Models (LLMs) are going towards\ncollaborative tasks where several agents interact with each other like in an\nLLM society. In such a setting, large groups of LLMs could reach consensus\nabout arbitrary norms for which there is no information supporting one option\nover another, regulating their own behavior in a self-organized way. In human\nsocieties, the ability to reach consensus without institutions has a limit in\nthe cognitive capacities of humans. To understand if a similar phenomenon\ncharacterizes also LLMs, we apply methods from complexity science and\nprinciples from behavioral sciences in a new approach of AI anthropology. We\nfind that LLMs are able to reach consensus in groups and that the opinion\ndynamics of LLMs can be understood with a function parametrized by a majority\nforce coefficient that determines whether consensus is possible. This majority\nforce is stronger for models with higher language understanding capabilities\nand decreases for larger groups, leading to a critical group size beyond which,\nfor a given LLM, consensus is unfeasible. This critical group size grows\nexponentially with the language understanding capabilities of models and for\nthe most advanced models, it can reach an order of magnitude beyond the typical\nsize of informal human groups."
                },
                "authors": [
                    {
                        "name": "Giordano De Marzo"
                    },
                    {
                        "name": "Claudio Castellano"
                    },
                    {
                        "name": "David Garcia"
                    }
                ],
                "author_detail": {
                    "name": "David Garcia"
                },
                "author": "David Garcia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02664v3",
                "updated": "2024-09-06T11:38:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    38,
                    0,
                    4,
                    250,
                    0
                ],
                "published": "2024-05-04T13:25:06Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    13,
                    25,
                    6,
                    5,
                    125,
                    0
                ],
                "title": "MedPromptExtract (Medical Data Extraction Tool): Anonymization and\n  Hi-fidelity Automated data extraction using NLP and prompt engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedPromptExtract (Medical Data Extraction Tool): Anonymization and\n  Hi-fidelity Automated data extraction using NLP and prompt engineering"
                },
                "summary": "Introduction: The labour-intensive nature of data extraction from sources\nlike discharge summaries (DS) poses significant obstacles to the digitisation\nof medical records particularly for low- and middle-income countries (LMICs).\nIn this paper we present a completely automated method MedPromptExtract to\nefficiently extract data from DS while maintaining confidentiality. Methods:\nThe source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani\nHospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing\ntool EIGEN which leverages semi-supervised learning techniques for\nhigh-fidelity information extraction was used to anonymize the DS, Natural\nLanguage Processing (NLP) was used to extract data from regular fields. We used\nPrompt Engineering and Large Language Model(LLM) to extract custom clinical\ninformation from free flowing text describing the patients stay in the\nhospital. Twelve features associated with occurrence of AKI were extracted. The\nLLM responses were validated against clinicians annotations. Results: The\nMedPromptExtracttool first subjected DS to the anonymization pipeline which\ntook three seconds per summary. Successful anonymization was verified by\nclinicians, thereafter NLP pipeline extracted structured text from the\nanonymized pdfs at the rate of 0.2 seconds per summary with 100%\naccuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the\ntwelve features. Accuracy metrics were calculated by comparing model responses\nto clinicians annotations with seven features achieving AUCs above 0.9,\nindicating high fidelity of the extraction process. Conclusion:\nMedPromptExtract serves as an automated adaptable tool for efficient data\nextraction from medical records with a dynamic user interface. Keywords:\nDigitizing Medical Records, Automated Anonymisation, Information Retrieval,\nLarge Language Models, Prompt Engineering",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: The labour-intensive nature of data extraction from sources\nlike discharge summaries (DS) poses significant obstacles to the digitisation\nof medical records particularly for low- and middle-income countries (LMICs).\nIn this paper we present a completely automated method MedPromptExtract to\nefficiently extract data from DS while maintaining confidentiality. Methods:\nThe source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani\nHospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing\ntool EIGEN which leverages semi-supervised learning techniques for\nhigh-fidelity information extraction was used to anonymize the DS, Natural\nLanguage Processing (NLP) was used to extract data from regular fields. We used\nPrompt Engineering and Large Language Model(LLM) to extract custom clinical\ninformation from free flowing text describing the patients stay in the\nhospital. Twelve features associated with occurrence of AKI were extracted. The\nLLM responses were validated against clinicians annotations. Results: The\nMedPromptExtracttool first subjected DS to the anonymization pipeline which\ntook three seconds per summary. Successful anonymization was verified by\nclinicians, thereafter NLP pipeline extracted structured text from the\nanonymized pdfs at the rate of 0.2 seconds per summary with 100%\naccuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the\ntwelve features. Accuracy metrics were calculated by comparing model responses\nto clinicians annotations with seven features achieving AUCs above 0.9,\nindicating high fidelity of the extraction process. Conclusion:\nMedPromptExtract serves as an automated adaptable tool for efficient data\nextraction from medical records with a dynamic user interface. Keywords:\nDigitizing Medical Records, Automated Anonymisation, Information Retrieval,\nLarge Language Models, Prompt Engineering"
                },
                "authors": [
                    {
                        "name": "Roomani Srivastava"
                    },
                    {
                        "name": "Suraj Prasad"
                    },
                    {
                        "name": "Lipika Bhat"
                    },
                    {
                        "name": "Sarvesh Deshpande"
                    },
                    {
                        "name": "Barnali Das"
                    },
                    {
                        "name": "Kshitij Jadhav"
                    }
                ],
                "author_detail": {
                    "name": "Kshitij Jadhav"
                },
                "author": "Kshitij Jadhav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04196v1",
                "updated": "2024-09-06T11:34:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    34,
                    24,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T11:34:24Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    34,
                    24,
                    4,
                    250,
                    0
                ],
                "title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers"
                },
                "summary": "Reconstructing realistic 3D human models from monocular images has\nsignificant applications in creative industries, human-computer interfaces, and\nhealthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene\nrepresentation composed of a mixture of Gaussians. Predicting such mixtures for\na human from a single input image is challenging, as it is a non-uniform\ndensity (with a many-to-one relationship with input pixels) with strict\nphysical constraints. At the same time, it needs to be flexible to accommodate\na variety of clothes and poses. Our key observation is that the vertices of\nstandardized human meshes (such as SMPL) can provide an adequate density and\napproximate initial position for Gaussians. We can then train a transformer\nmodel to jointly predict comparatively small adjustments to these positions, as\nwell as the other Gaussians' attributes and the SMPL parameters. We show\nempirically that this combination (using only multi-view supervision) can\nachieve fast inference of 3D human models from a single image without test-time\noptimization, expensive diffusion models, or 3D points supervision. We also\nshow that it can improve 3D pose estimation by better fitting human models that\naccount for clothes and other variations. The code is available on the project\nwebsite https://abdullahamdi.com/gst/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing realistic 3D human models from monocular images has\nsignificant applications in creative industries, human-computer interfaces, and\nhealthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene\nrepresentation composed of a mixture of Gaussians. Predicting such mixtures for\na human from a single input image is challenging, as it is a non-uniform\ndensity (with a many-to-one relationship with input pixels) with strict\nphysical constraints. At the same time, it needs to be flexible to accommodate\na variety of clothes and poses. Our key observation is that the vertices of\nstandardized human meshes (such as SMPL) can provide an adequate density and\napproximate initial position for Gaussians. We can then train a transformer\nmodel to jointly predict comparatively small adjustments to these positions, as\nwell as the other Gaussians' attributes and the SMPL parameters. We show\nempirically that this combination (using only multi-view supervision) can\nachieve fast inference of 3D human models from a single image without test-time\noptimization, expensive diffusion models, or 3D points supervision. We also\nshow that it can improve 3D pose estimation by better fitting human models that\naccount for clothes and other variations. The code is available on the project\nwebsite https://abdullahamdi.com/gst/ ."
                },
                "authors": [
                    {
                        "name": "Lorenza Prospero"
                    },
                    {
                        "name": "Abdullah Hamdi"
                    },
                    {
                        "name": "Joao F. Henriques"
                    },
                    {
                        "name": "Christian Rupprecht"
                    }
                ],
                "author_detail": {
                    "name": "Christian Rupprecht"
                },
                "author": "Christian Rupprecht",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.06493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.06493v3",
                "updated": "2024-09-06T11:28:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    28,
                    4,
                    4,
                    250,
                    0
                ],
                "published": "2023-08-12T07:46:50Z",
                "published_parsed": [
                    2023,
                    8,
                    12,
                    7,
                    46,
                    50,
                    5,
                    224,
                    0
                ],
                "title": "EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and\n  Intermittent Observations Everywhere",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and\n  Intermittent Observations Everywhere"
                },
                "summary": "Full-body egocentric pose estimation from head and hand poses alone has\nbecome an active area of research to power articulate avatar representations on\nheadset-based platforms. However, existing methods over-rely on the indoor\nmotion-capture spaces in which datasets were recorded, while simultaneously\nassuming continuous joint motion capture and uniform body dimensions. We\npropose EgoPoser to overcome these limitations with four main contributions. 1)\nEgoPoser robustly models body pose from intermittent hand position and\norientation tracking only when inside a headset's field of view. 2) We rethink\ninput representations for headset-based ego-pose estimation and introduce a\nnovel global motion decomposition method that predicts full-body pose\nindependent of global positions. 3) We enhance pose estimation by capturing\nlonger motion time series through an efficient SlowFast module design that\nmaintains computational efficiency. 4) EgoPoser generalizes across various body\nshapes for different users. We experimentally evaluate our method and show that\nit outperforms state-of-the-art methods both qualitatively and quantitatively\nwhile maintaining a high inference speed of over 600fps. EgoPoser establishes a\nrobust baseline for future work where full-body pose estimation no longer needs\nto rely on outside-in capture and can scale to large-scale and unseen\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-body egocentric pose estimation from head and hand poses alone has\nbecome an active area of research to power articulate avatar representations on\nheadset-based platforms. However, existing methods over-rely on the indoor\nmotion-capture spaces in which datasets were recorded, while simultaneously\nassuming continuous joint motion capture and uniform body dimensions. We\npropose EgoPoser to overcome these limitations with four main contributions. 1)\nEgoPoser robustly models body pose from intermittent hand position and\norientation tracking only when inside a headset's field of view. 2) We rethink\ninput representations for headset-based ego-pose estimation and introduce a\nnovel global motion decomposition method that predicts full-body pose\nindependent of global positions. 3) We enhance pose estimation by capturing\nlonger motion time series through an efficient SlowFast module design that\nmaintains computational efficiency. 4) EgoPoser generalizes across various body\nshapes for different users. We experimentally evaluate our method and show that\nit outperforms state-of-the-art methods both qualitatively and quantitatively\nwhile maintaining a high inference speed of over 600fps. EgoPoser establishes a\nrobust baseline for future work where full-body pose estimation no longer needs\nto rely on outside-in capture and can scale to large-scale and unseen\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jiaxi Jiang"
                    },
                    {
                        "name": "Paul Streli"
                    },
                    {
                        "name": "Manuel Meier"
                    },
                    {
                        "name": "Christian Holz"
                    }
                ],
                "author_detail": {
                    "name": "Christian Holz"
                },
                "author": "Christian Holz",
                "arxiv_comment": "Accepted by ECCV 2024, Code: https://siplab.org/projects/EgoPoser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.06493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.06493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T45, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.3; I.4; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15366v2",
                "updated": "2024-09-06T11:25:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    25,
                    35,
                    4,
                    250,
                    0
                ],
                "published": "2024-02-23T14:57:51Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    14,
                    57,
                    51,
                    4,
                    54,
                    0
                ],
                "title": "Portable acceleration of CMS computing workflows with coprocessors as a\n  service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable acceleration of CMS computing workflows with coprocessors as a\n  service"
                },
                "summary": "Computing demands for large scientific experiments, such as the CMS\nexperiment at the CERN LHC, will increase dramatically in the next decades. To\ncomplement the future performance increases of software running on central\nprocessing units (CPUs), explorations of coprocessor usage in data processing\nhold great potential and interest. Coprocessors are a class of computer\nprocessors that supplement CPUs, often improving the execution of certain\nfunctions due to architectural design choices. We explore the approach of\nServices for Optimized Network Inference on Coprocessors (SONIC) and study the\ndeployment of this as-a-service approach in large-scale data processing. In the\nstudies, we take a data processing workflow of the CMS experiment and run the\nmain workflow on CPUs, while offloading several machine learning (ML) inference\ntasks onto either remote or local coprocessors, specifically graphics\nprocessing units (GPUs). With experiments performed at Google Cloud, the Purdue\nTier-2 computing center, and combinations of the two, we demonstrate the\nacceleration of these ML algorithms individually on coprocessors and the\ncorresponding throughput improvement for the entire workflow. This approach can\nbe easily generalized to different types of coprocessors and deployed on local\nCPUs without decreasing the throughput performance. We emphasize that the SONIC\napproach enables high coprocessor usage and enables the portability to run\nworkflows on different types of coprocessors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computing demands for large scientific experiments, such as the CMS\nexperiment at the CERN LHC, will increase dramatically in the next decades. To\ncomplement the future performance increases of software running on central\nprocessing units (CPUs), explorations of coprocessor usage in data processing\nhold great potential and interest. Coprocessors are a class of computer\nprocessors that supplement CPUs, often improving the execution of certain\nfunctions due to architectural design choices. We explore the approach of\nServices for Optimized Network Inference on Coprocessors (SONIC) and study the\ndeployment of this as-a-service approach in large-scale data processing. In the\nstudies, we take a data processing workflow of the CMS experiment and run the\nmain workflow on CPUs, while offloading several machine learning (ML) inference\ntasks onto either remote or local coprocessors, specifically graphics\nprocessing units (GPUs). With experiments performed at Google Cloud, the Purdue\nTier-2 computing center, and combinations of the two, we demonstrate the\nacceleration of these ML algorithms individually on coprocessors and the\ncorresponding throughput improvement for the entire workflow. This approach can\nbe easily generalized to different types of coprocessors and deployed on local\nCPUs without decreasing the throughput performance. We emphasize that the SONIC\napproach enables high coprocessor usage and enables the portability to run\nworkflows on different types of coprocessors."
                },
                "authors": [
                    {
                        "name": "CMS Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "CMS Collaboration"
                },
                "author": "CMS Collaboration",
                "arxiv_doi": "10.1007/s41781-024-00124-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s41781-024-00124-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.15366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Replaced with the published version. Added the journal reference and\n  the DOI. All the figures and tables can be found at\n  http://cms-results.web.cern.ch/cms-results/public-results/publications/MLG-23-001\n  (CMS Public Pages)",
                "arxiv_journal_ref": "Comput. Softw. Big Sci. 8 (2024) 17",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08632v2",
                "updated": "2024-09-06T11:20:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    20,
                    13,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-16T09:52:02Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    52,
                    2,
                    4,
                    229,
                    0
                ],
                "title": "A Survey on Benchmarks of Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Benchmarks of Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on\n(1)perception and understanding, (2)cognition and reasoning, (3)specific\ndomains, (4)key capabilities, and (5)other modalities. Finally, we discuss the\nlimitations of the current evaluation methods for MLLMs and explore promising\nfuture directions. Our key argument is that evaluation should be regarded as a\ncrucial discipline to support the development of MLLMs better. For more\ndetails, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on\n(1)perception and understanding, (2)cognition and reasoning, (3)specific\ndomains, (4)key capabilities, and (5)other modalities. Finally, we discuss the\nlimitations of the current evaluation methods for MLLMs and explore promising\nfuture directions. Our key argument is that evaluation should be regarded as a\ncrucial discipline to support the development of MLLMs better. For more\ndetails, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey."
                },
                "authors": [
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Weiheng Lu"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Min Xia"
                    },
                    {
                        "name": "Yizhang Jin"
                    },
                    {
                        "name": "Zhenye Gan"
                    },
                    {
                        "name": "Ding Qi"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Ying Tai"
                    },
                    {
                        "name": "Wankou Yang"
                    },
                    {
                        "name": "Yabiao Wang"
                    },
                    {
                        "name": "Chengjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chengjie Wang"
                },
                "author": "Chengjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09225v2",
                "updated": "2024-09-06T11:15:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    15,
                    23,
                    4,
                    250,
                    0
                ],
                "published": "2024-02-14T15:09:01Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    15,
                    9,
                    1,
                    2,
                    45,
                    0
                ],
                "title": "Is my Data in your AI Model? Membership Inference Test with Application\n  to Face Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is my Data in your AI Model? Membership Inference Test with Application\n  to Face Images"
                },
                "summary": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Javier Ortega-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Javier Ortega-Garcia"
                },
                "author": "Javier Ortega-Garcia",
                "arxiv_comment": "12 pages including references and authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04187v1",
                "updated": "2024-09-06T11:05:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    5,
                    12,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T11:05:12Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    5,
                    12,
                    4,
                    250,
                    0
                ],
                "title": "LITE: A Paradigm Shift in Multi-Object Tracking with Efficient ReID\n  Feature Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LITE: A Paradigm Shift in Multi-Object Tracking with Efficient ReID\n  Feature Integration"
                },
                "summary": "The Lightweight Integrated Tracking-Feature Extraction (LITE) paradigm is\nintroduced as a novel multi-object tracking (MOT) approach. It enhances\nReID-based trackers by eliminating inference, pre-processing, post-processing,\nand ReID model training costs. LITE uses real-time appearance features without\ncompromising speed. By integrating appearance feature extraction directly into\nthe tracking pipeline using standard CNN-based detectors such as YOLOv8m, LITE\ndemonstrates significant performance improvements. The simplest implementation\nof LITE on top of classic DeepSORT achieves a HOTA score of 43.03% at 28.3 FPS\non the MOT17 benchmark, making it twice as fast as DeepSORT on MOT17 and four\ntimes faster on the more crowded MOT20 dataset, while maintaining similar\naccuracy. Additionally, a new evaluation framework for tracking-by-detection\napproaches reveals that conventional trackers like DeepSORT remain competitive\nwith modern state-of-the-art trackers when evaluated under fair conditions. The\ncode will be available post-publication at https://github.com/Jumabek/LITE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lightweight Integrated Tracking-Feature Extraction (LITE) paradigm is\nintroduced as a novel multi-object tracking (MOT) approach. It enhances\nReID-based trackers by eliminating inference, pre-processing, post-processing,\nand ReID model training costs. LITE uses real-time appearance features without\ncompromising speed. By integrating appearance feature extraction directly into\nthe tracking pipeline using standard CNN-based detectors such as YOLOv8m, LITE\ndemonstrates significant performance improvements. The simplest implementation\nof LITE on top of classic DeepSORT achieves a HOTA score of 43.03% at 28.3 FPS\non the MOT17 benchmark, making it twice as fast as DeepSORT on MOT17 and four\ntimes faster on the more crowded MOT20 dataset, while maintaining similar\naccuracy. Additionally, a new evaluation framework for tracking-by-detection\napproaches reveals that conventional trackers like DeepSORT remain competitive\nwith modern state-of-the-art trackers when evaluated under fair conditions. The\ncode will be available post-publication at https://github.com/Jumabek/LITE."
                },
                "authors": [
                    {
                        "name": "Jumabek Alikhanov"
                    },
                    {
                        "name": "Dilshod Obidov"
                    },
                    {
                        "name": "Hakil Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hakil Kim"
                },
                "author": "Hakil Kim",
                "arxiv_comment": "15 pages, 6 figures, to be published in ICONIP-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04183v1",
                "updated": "2024-09-06T10:57:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    57,
                    34,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T10:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    57,
                    34,
                    4,
                    250,
                    0
                ],
                "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding"
                },
                "summary": "Programming languages possess rich semantic information such as data flow\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with four different baseline LLMs ranging in size from 350M to 8B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming languages possess rich semantic information such as data flow\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with four different baseline LLMs ranging in size from 350M to 8B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Shijie Li"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04181v1",
                "updated": "2024-09-06T10:49:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    49,
                    46,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T10:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    49,
                    46,
                    4,
                    250,
                    0
                ],
                "title": "Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question\n  Answering"
                },
                "summary": "Advancements in natural language processing have revolutionized the way we\ncan interact with digital information systems, such as databases, making them\nmore accessible. However, challenges persist, especially when accuracy is\ncritical, as in the biomedical domain. A key issue is the hallucination\nproblem, where models generate information unsupported by the underlying data,\npotentially leading to dangerous misinformation. This paper presents a novel\napproach designed to bridge this gap by combining Large Language Models (LLM)\nand Knowledge Graphs (KG) to improve the accuracy and reliability of\nquestion-answering systems, on the example of a biomedical KG. Built on the\nLangChain framework, our method incorporates a query checker that ensures the\nsyntactical and semantic validity of LLM-generated queries, which are then used\nto extract information from a Knowledge Graph, substantially reducing errors\nlike hallucinations. We evaluated the overall performance using a new benchmark\ndataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo\nand llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other\nmodels in generating accurate queries, open-source models like llama3:70b show\npromise with appropriate prompt engineering. To make this approach accessible,\na user-friendly web-based interface has been developed, allowing users to input\nnatural language queries, view generated and corrected Cypher queries, and\nverify the resulting paths for accuracy. Overall, this hybrid approach\neffectively addresses common issues such as data gaps and hallucinations,\noffering a reliable and intuitive solution for question answering systems. The\nsource code for generating the results of this paper and for the user-interface\ncan be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in natural language processing have revolutionized the way we\ncan interact with digital information systems, such as databases, making them\nmore accessible. However, challenges persist, especially when accuracy is\ncritical, as in the biomedical domain. A key issue is the hallucination\nproblem, where models generate information unsupported by the underlying data,\npotentially leading to dangerous misinformation. This paper presents a novel\napproach designed to bridge this gap by combining Large Language Models (LLM)\nand Knowledge Graphs (KG) to improve the accuracy and reliability of\nquestion-answering systems, on the example of a biomedical KG. Built on the\nLangChain framework, our method incorporates a query checker that ensures the\nsyntactical and semantic validity of LLM-generated queries, which are then used\nto extract information from a Knowledge Graph, substantially reducing errors\nlike hallucinations. We evaluated the overall performance using a new benchmark\ndataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo\nand llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other\nmodels in generating accurate queries, open-source models like llama3:70b show\npromise with appropriate prompt engineering. To make this approach accessible,\na user-friendly web-based interface has been developed, allowing users to input\nnatural language queries, view generated and corrected Cypher queries, and\nverify the resulting paths for accuracy. Overall, this hybrid approach\neffectively addresses common issues such as data gaps and hallucinations,\noffering a reliable and intuitive solution for question answering systems. The\nsource code for generating the results of this paper and for the user-interface\ncan be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui"
                },
                "authors": [
                    {
                        "name": "Larissa Pusch"
                    },
                    {
                        "name": "Tim O. F. Conrad"
                    }
                ],
                "author_detail": {
                    "name": "Tim O. F. Conrad"
                },
                "author": "Tim O. F. Conrad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04174v1",
                "updated": "2024-09-06T10:33:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    33,
                    42,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T10:33:42Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    33,
                    42,
                    4,
                    250,
                    0
                ],
                "title": "Towards Measuring Sell Side Outcomes in Buy Side Marketplace Experiments\n  using In-Experiment Bipartite Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Measuring Sell Side Outcomes in Buy Side Marketplace Experiments\n  using In-Experiment Bipartite Graph"
                },
                "summary": "In this study, we evaluate causal inference estimators for online controlled\nbipartite graph experiments in a real marketplace setting. Our novel\ncontribution is constructing a bipartite graph using in-experiment data, rather\nthan relying on prior knowledge or historical data, the common approach in the\nliterature published to date. We build the bipartite graph from various\ninteractions between buyers and sellers in the marketplace, establishing a\nnovel research direction at the intersection of bipartite experiments and\nmediation analysis. This approach is crucial for modern marketplaces aiming to\nevaluate seller-side causal effects in buyer-side experiments, or vice versa.\nWe demonstrate our method using historical buyer-side experiments conducted at\nVinted, the largest second-hand marketplace in Europe with over 80M users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we evaluate causal inference estimators for online controlled\nbipartite graph experiments in a real marketplace setting. Our novel\ncontribution is constructing a bipartite graph using in-experiment data, rather\nthan relying on prior knowledge or historical data, the common approach in the\nliterature published to date. We build the bipartite graph from various\ninteractions between buyers and sellers in the marketplace, establishing a\nnovel research direction at the intersection of bipartite experiments and\nmediation analysis. This approach is crucial for modern marketplaces aiming to\nevaluate seller-side causal effects in buyer-side experiments, or vice versa.\nWe demonstrate our method using historical buyer-side experiments conducted at\nVinted, the largest second-hand marketplace in Europe with over 80M users."
                },
                "authors": [
                    {
                        "name": "Vaiva Pilkauskaitė"
                    },
                    {
                        "name": "Jevgenij Gamper"
                    },
                    {
                        "name": "Rasa Giniūnaitė"
                    },
                    {
                        "name": "Agne Reklaitė"
                    }
                ],
                "author_detail": {
                    "name": "Agne Reklaitė"
                },
                "author": "Agne Reklaitė",
                "arxiv_comment": "5 pages, 3 figures, this work was presented at the KDD 2024\n  Conference Undergraduate Consortium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03274v2",
                "updated": "2024-09-06T10:31:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    31,
                    7,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T06:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    31,
                    37,
                    3,
                    249,
                    0
                ],
                "title": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures."
                },
                "authors": [
                    {
                        "name": "Jing Cui"
                    },
                    {
                        "name": "Yishi Xu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Shuchang Zhou"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02795v3",
                "updated": "2024-09-09T09:31:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    31,
                    30,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-04T15:11:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07747v2",
                "updated": "2024-09-06T10:19:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    19,
                    10,
                    4,
                    250,
                    0
                ],
                "published": "2024-03-12T15:32:39Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    15,
                    32,
                    39,
                    1,
                    72,
                    0
                ],
                "title": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese\n  Large Language Models"
                },
                "summary": "To thoroughly assess the mathematical reasoning abilities of Large Language\nModels (LLMs), we need to carefully curate evaluation datasets covering diverse\nmathematical concepts and mathematical problems at different difficulty levels.\nIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\nmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\nis created to cover the major key mathematical concepts taught in elementary\nschool math, which are further divided into 17 categories of math word\nproblems, enabling in-depth analysis of mathematical reasoning abilities of\nLLMs. All the 17 categories of math word problems are manually annotated with\ntheir difficulty levels according to the number of reasoning steps required to\nsolve these problems. We conduct extensive experiments on a wide range of LLMs\non FineMath and find that there is still considerable room for improvements in\nterms of mathematical reasoning capability of Chinese LLMs. We also carry out\nan in-depth analysis on the evaluation process and methods that have been\noverlooked previously. These two factors significantly influence the model\nresults and our understanding of their mathematical reasoning capabilities. The\ndataset will be publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To thoroughly assess the mathematical reasoning abilities of Large Language\nModels (LLMs), we need to carefully curate evaluation datasets covering diverse\nmathematical concepts and mathematical problems at different difficulty levels.\nIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\nmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\nis created to cover the major key mathematical concepts taught in elementary\nschool math, which are further divided into 17 categories of math word\nproblems, enabling in-depth analysis of mathematical reasoning abilities of\nLLMs. All the 17 categories of math word problems are manually annotated with\ntheir difficulty levels according to the number of reasoning steps required to\nsolve these problems. We conduct extensive experiments on a wide range of LLMs\non FineMath and find that there is still considerable room for improvements in\nterms of mathematical reasoning capability of Chinese LLMs. We also carry out\nan in-depth analysis on the evaluation process and methods that have been\noverlooked previously. These two factors significantly influence the model\nresults and our understanding of their mathematical reasoning capabilities. The\ndataset will be publicly available soon."
                },
                "authors": [
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Zheng Yao"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04168v1",
                "updated": "2024-09-06T10:09:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    9,
                    41,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T10:09:41Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    9,
                    41,
                    4,
                    250,
                    0
                ],
                "title": "From Calculation to Adjudication: Examining LLM judges on Mathematical\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Calculation to Adjudication: Examining LLM judges on Mathematical\n  Reasoning Tasks"
                },
                "summary": "To reduce the need for human annotations, large language models (LLMs) have\nbeen proposed as judges of the quality of other candidate models. LLM judges\nare typically evaluated by measuring the correlation with human judgments on\ngeneration tasks such as summarization or machine translation. In contrast, we\nstudy LLM judges on mathematical reasoning tasks. These tasks require\nmulti-step reasoning, and the correctness of their solutions is verifiable,\nenabling a more objective evaluation. We perform a detailed performance\nanalysis and find that the used judges are mostly unable to improve task\nperformance but are able to pick the better model. Our analysis uncovers a\nstrong correlation between judgment performance and the candidate model task\nperformance. We observe that judges tend to choose the model of higher quality\neven if its answer is incorrect. Further, we show that it is possible to use\nstatistics, such as the task performances of the individual models, to predict\njudgment performance. In an ablation, we either swap or mask the candidate\nanswers and observe that judges often keep the original judgment, providing\nevidence that judges incorporate writing style in their judgments. In summary,\nwe find that regularities in the judgments are quantifiable using statistical\nmeasures and provide various angles on exploiting them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce the need for human annotations, large language models (LLMs) have\nbeen proposed as judges of the quality of other candidate models. LLM judges\nare typically evaluated by measuring the correlation with human judgments on\ngeneration tasks such as summarization or machine translation. In contrast, we\nstudy LLM judges on mathematical reasoning tasks. These tasks require\nmulti-step reasoning, and the correctness of their solutions is verifiable,\nenabling a more objective evaluation. We perform a detailed performance\nanalysis and find that the used judges are mostly unable to improve task\nperformance but are able to pick the better model. Our analysis uncovers a\nstrong correlation between judgment performance and the candidate model task\nperformance. We observe that judges tend to choose the model of higher quality\neven if its answer is incorrect. Further, we show that it is possible to use\nstatistics, such as the task performances of the individual models, to predict\njudgment performance. In an ablation, we either swap or mask the candidate\nanswers and observe that judges often keep the original judgment, providing\nevidence that judges incorporate writing style in their judgments. In summary,\nwe find that regularities in the judgments are quantifiable using statistical\nmeasures and provide various angles on exploiting them."
                },
                "authors": [
                    {
                        "name": "Andreas Stephan"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Matthias Aßenmacher"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Benjamin Roth"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Roth"
                },
                "author": "Benjamin Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04164v1",
                "updated": "2024-09-06T10:03:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    3,
                    49,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T10:03:49Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    3,
                    49,
                    4,
                    250,
                    0
                ],
                "title": "Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language\n  Models for Text-to-Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language\n  Models for Text-to-Code Generation"
                },
                "summary": "In recent years, large language models (LLMs) have emerged as powerful tools\nwith potential applications in various fields, including software engineering.\nWithin the scope of this research, we evaluate five different state-of-the-art\nLLMs - Bard, BingChat, ChatGPT, Llama2, and Code Llama - concerning their\ncapabilities for text-to-code generation. In an empirical study, we feed\nprompts with textual descriptions of coding problems sourced from the\nprogramming website LeetCode to the models with the task of creating solutions\nin Python. Subsequently, the quality of the generated outputs is assessed using\nthe testing functionalities of LeetCode. The results indicate large differences\nin performance between the investigated models. ChatGPT can handle these\ntypical programming challenges by far the most effectively, surpassing even\ncode-specialized models like Code Llama. To gain further insights, we measure\nthe runtime as well as the memory usage of the generated outputs and compared\nthem to the other code submissions on Leetcode. A detailed error analysis,\nencompassing a comparison of the differences concerning correct indentation and\nform of the generated code as well as an assignment of the incorrectly solved\ntasks to certain error categories allows us to obtain a more nuanced picture of\nthe results and potential for improvement. The results also show a clear\npattern of increasingly incorrect produced code when the models are facing a\nlot of context in the form of longer prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have emerged as powerful tools\nwith potential applications in various fields, including software engineering.\nWithin the scope of this research, we evaluate five different state-of-the-art\nLLMs - Bard, BingChat, ChatGPT, Llama2, and Code Llama - concerning their\ncapabilities for text-to-code generation. In an empirical study, we feed\nprompts with textual descriptions of coding problems sourced from the\nprogramming website LeetCode to the models with the task of creating solutions\nin Python. Subsequently, the quality of the generated outputs is assessed using\nthe testing functionalities of LeetCode. The results indicate large differences\nin performance between the investigated models. ChatGPT can handle these\ntypical programming challenges by far the most effectively, surpassing even\ncode-specialized models like Code Llama. To gain further insights, we measure\nthe runtime as well as the memory usage of the generated outputs and compared\nthem to the other code submissions on Leetcode. A detailed error analysis,\nencompassing a comparison of the differences concerning correct indentation and\nform of the generated code as well as an assignment of the incorrectly solved\ntasks to certain error categories allows us to obtain a more nuanced picture of\nthe results and potential for improvement. The results also show a clear\npattern of increasingly incorrect produced code when the models are facing a\nlot of context in the form of longer prompts."
                },
                "authors": [
                    {
                        "name": "Luis Mayer"
                    },
                    {
                        "name": "Christian Heumann"
                    },
                    {
                        "name": "Matthias Aßenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Aßenmacher"
                },
                "author": "Matthias Aßenmacher",
                "arxiv_comment": "Conference Paper accepted at the 9th SwissText Conference (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03381v2",
                "updated": "2024-09-06T09:37:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    37,
                    36,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T09:33:24Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "title": "CogniDual Framework: Self-Training Large Language Models within a\n  Dual-System Theoretical Framework for Improving Cognitive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniDual Framework: Self-Training Large Language Models within a\n  Dual-System Theoretical Framework for Improving Cognitive Tasks"
                },
                "summary": "Cognitive psychology investigates perception, attention, memory, language,\nproblem-solving, decision-making, and reasoning. Kahneman's dual-system theory\nelucidates the human decision-making process, distinguishing between the rapid,\nintuitive System 1 and the deliberative, rational System 2. Recent advancements\nhave positioned large language Models (LLMs) as formidable tools nearing\nhuman-level proficiency in various cognitive tasks. Nonetheless, the presence\nof a dual-system framework analogous to human cognition in LLMs remains\nunexplored. This study introduces the \\textbf{CogniDual Framework for LLMs}\n(CFLLMs), designed to assess whether LLMs can, through self-training, evolve\nfrom deliberate deduction to intuitive responses, thereby emulating the human\nprocess of acquiring and mastering new information. Our findings reveal the\ncognitive mechanisms behind LLMs' response generation, enhancing our\nunderstanding of their capabilities in cognitive psychology. Practically,\nself-trained models can provide faster responses to certain queries, reducing\ncomputational demands during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive psychology investigates perception, attention, memory, language,\nproblem-solving, decision-making, and reasoning. Kahneman's dual-system theory\nelucidates the human decision-making process, distinguishing between the rapid,\nintuitive System 1 and the deliberative, rational System 2. Recent advancements\nhave positioned large language Models (LLMs) as formidable tools nearing\nhuman-level proficiency in various cognitive tasks. Nonetheless, the presence\nof a dual-system framework analogous to human cognition in LLMs remains\nunexplored. This study introduces the \\textbf{CogniDual Framework for LLMs}\n(CFLLMs), designed to assess whether LLMs can, through self-training, evolve\nfrom deliberate deduction to intuitive responses, thereby emulating the human\nprocess of acquiring and mastering new information. Our findings reveal the\ncognitive mechanisms behind LLMs' response generation, enhancing our\nunderstanding of their capabilities in cognitive psychology. Practically,\nself-trained models can provide faster responses to certain queries, reducing\ncomputational demands during inference."
                },
                "authors": [
                    {
                        "name": "Yongxin Deng"
                    },
                    {
                        "name": "Xihe Qiu"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Jing Pan"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Wei Chu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chu"
                },
                "arxiv_affiliation": "INF Technology",
                "author": "Wei Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04153v1",
                "updated": "2024-09-06T09:31:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    31,
                    14,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T09:31:14Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    31,
                    14,
                    4,
                    250,
                    0
                ],
                "title": "A Stackelberg Game based on the Secretary Problem: Optimal Response is\n  History Dependent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stackelberg Game based on the Secretary Problem: Optimal Response is\n  History Dependent"
                },
                "summary": "This article considers a problem arising from a two-player game based on the\nclassical secretary problem. First, Player 1 selects one object from a sequence\nas in the secretary problem. All of the other objects are then presented to\nPlayer 2 in the same order as in the original sequence. The goal of both\nplayers is to select the best object. The optimal response of Player 2 is\nadapted to the optimal strategy in the secretary problem. This means that when\nPlayer 2 observes an object that is the best seen so far, it can be inferred\nwhether Player 1 selected one of the earlier objects in the original sequence.\nHowever, Player 2 cannot compare the current object with the one selected by\nPlayer 1. Hence, this game defines an auxiliary problem in which Player 2 has\nincomplete information on the relative rank of an object. It is shown that the\noptimal strategy of Player 2 is based on both the number of objects to have\nappeared and the probability that the current object is better than the object\nchosen by Player 1 (if Player 1 chose an earlier object in the sequence).\nHowever, this probability is dependent on the previously observed objects. A\nlower bound on the optimal expected reward in the auxiliary problem is defined\nby limiting the memory of Player 2. An upper bound is derived by giving Player\n2 additional information at appropriate times. The methods used illustrate\napproaches that can be used to approximate the optimal reward in a stopping\nproblem when there is incomplete information on the ranks of objects and/or the\noptimal strategy is history dependent, as in the Robbins' problem",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article considers a problem arising from a two-player game based on the\nclassical secretary problem. First, Player 1 selects one object from a sequence\nas in the secretary problem. All of the other objects are then presented to\nPlayer 2 in the same order as in the original sequence. The goal of both\nplayers is to select the best object. The optimal response of Player 2 is\nadapted to the optimal strategy in the secretary problem. This means that when\nPlayer 2 observes an object that is the best seen so far, it can be inferred\nwhether Player 1 selected one of the earlier objects in the original sequence.\nHowever, Player 2 cannot compare the current object with the one selected by\nPlayer 1. Hence, this game defines an auxiliary problem in which Player 2 has\nincomplete information on the relative rank of an object. It is shown that the\noptimal strategy of Player 2 is based on both the number of objects to have\nappeared and the probability that the current object is better than the object\nchosen by Player 1 (if Player 1 chose an earlier object in the sequence).\nHowever, this probability is dependent on the previously observed objects. A\nlower bound on the optimal expected reward in the auxiliary problem is defined\nby limiting the memory of Player 2. An upper bound is derived by giving Player\n2 additional information at appropriate times. The methods used illustrate\napproaches that can be used to approximate the optimal reward in a stopping\nproblem when there is incomplete information on the ranks of objects and/or the\noptimal strategy is history dependent, as in the Robbins' problem"
                },
                "authors": [
                    {
                        "name": "David Ramsey"
                    }
                ],
                "author_detail": {
                    "name": "David Ramsey"
                },
                "author": "David Ramsey",
                "arxiv_comment": "52 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60G40, 91A20, 91A27, 49N30, 05A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; G.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04140v1",
                "updated": "2024-09-06T09:11:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    11,
                    15,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T09:11:15Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    11,
                    15,
                    4,
                    250,
                    0
                ],
                "title": "Half-VAE: An Encoder-Free VAE to Bypass Explicit Inverse Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Half-VAE: An Encoder-Free VAE to Bypass Explicit Inverse Mapping"
                },
                "summary": "Inference and inverse problems are closely related concepts, both\nfundamentally involving the deduction of unknown causes or parameters from\nobserved data. Bayesian inference, a powerful class of methods, is often\nemployed to solve a variety of problems, including those related to causal\ninference. Variational inference, a subset of Bayesian inference, is primarily\nused to efficiently approximate complex posterior distributions. Variational\nAutoencoders (VAEs), which combine variational inference with deep learning,\nhave become widely applied across various domains. This study explores the\npotential of VAEs for solving inverse problems, such as Independent Component\nAnalysis (ICA), without relying on an explicit inverse mapping process. Unlike\nother VAE-based ICA methods, this approach discards the encoder in the VAE\narchitecture, directly setting the latent variables as trainable parameters. In\nother words, the latent variables are no longer outputs of the encoder but are\ninstead optimized directly through the objective function to converge to\nappropriate values. We find that, with a suitable prior setup, the latent\nvariables, represented by trainable parameters, can exhibit mutually\nindependent properties as the parameters converge, all without the need for an\nencoding process. This approach, referred to as the Half-VAE, bypasses the\ninverse mapping process by eliminating the encoder. This study demonstrates the\nfeasibility of using the Half-VAE to solve ICA without the need for an explicit\ninverse mapping process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference and inverse problems are closely related concepts, both\nfundamentally involving the deduction of unknown causes or parameters from\nobserved data. Bayesian inference, a powerful class of methods, is often\nemployed to solve a variety of problems, including those related to causal\ninference. Variational inference, a subset of Bayesian inference, is primarily\nused to efficiently approximate complex posterior distributions. Variational\nAutoencoders (VAEs), which combine variational inference with deep learning,\nhave become widely applied across various domains. This study explores the\npotential of VAEs for solving inverse problems, such as Independent Component\nAnalysis (ICA), without relying on an explicit inverse mapping process. Unlike\nother VAE-based ICA methods, this approach discards the encoder in the VAE\narchitecture, directly setting the latent variables as trainable parameters. In\nother words, the latent variables are no longer outputs of the encoder but are\ninstead optimized directly through the objective function to converge to\nappropriate values. We find that, with a suitable prior setup, the latent\nvariables, represented by trainable parameters, can exhibit mutually\nindependent properties as the parameters converge, all without the need for an\nencoding process. This approach, referred to as the Half-VAE, bypasses the\ninverse mapping process by eliminating the encoder. This study demonstrates the\nfeasibility of using the Half-VAE to solve ICA without the need for an explicit\ninverse mapping process."
                },
                "authors": [
                    {
                        "name": "Yuan-Hao Wei"
                    },
                    {
                        "name": "Yan-Jie Sun"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2111.02019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2111.02019v2",
                "updated": "2024-09-06T09:06:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    6,
                    25,
                    4,
                    250,
                    0
                ],
                "published": "2021-11-03T04:47:37Z",
                "published_parsed": [
                    2021,
                    11,
                    3,
                    4,
                    47,
                    37,
                    2,
                    307,
                    0
                ],
                "title": "Scalable mixed-domain Gaussian process modeling and model reduction for\n  longitudinal data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable mixed-domain Gaussian process modeling and model reduction for\n  longitudinal data"
                },
                "summary": "Gaussian process (GP) models that combine both categorical and continuous\ninput variables have found use in longitudinal data analysis of and computer\nexperiments. However, standard inference for these models has the typical cubic\nscaling, and common scalable approximation schemes for GPs cannot be applied\nsince the covariance function is non-continuous. In this work, we derive a\nbasis function approximation scheme for mixed-domain covariance functions,\nwhich scales linearly with respect to the number of observations and total\nnumber of basis functions. The proposed approach is naturally applicable to\nalso Bayesian GP regression with discrete observation models. We demonstrate\nthe scalability of the approach and compare model reduction techniques for\nadditive GP models in a longitudinal data context. We confirm that we can\napproximate the exact GP model accurately in a fraction of the runtime compared\nto fitting the corresponding exact model. In addition, we demonstrate a\nscalable model reduction workflow for obtaining smaller and more interpretable\nmodels when dealing with a large number of candidate predictors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process (GP) models that combine both categorical and continuous\ninput variables have found use in longitudinal data analysis of and computer\nexperiments. However, standard inference for these models has the typical cubic\nscaling, and common scalable approximation schemes for GPs cannot be applied\nsince the covariance function is non-continuous. In this work, we derive a\nbasis function approximation scheme for mixed-domain covariance functions,\nwhich scales linearly with respect to the number of observations and total\nnumber of basis functions. The proposed approach is naturally applicable to\nalso Bayesian GP regression with discrete observation models. We demonstrate\nthe scalability of the approach and compare model reduction techniques for\nadditive GP models in a longitudinal data context. We confirm that we can\napproximate the exact GP model accurately in a fraction of the runtime compared\nto fitting the corresponding exact model. In addition, we demonstrate a\nscalable model reduction workflow for obtaining smaller and more interpretable\nmodels when dealing with a large number of candidate predictors."
                },
                "authors": [
                    {
                        "name": "Juho Timonen"
                    },
                    {
                        "name": "Harri Lähdesmäki"
                    }
                ],
                "author_detail": {
                    "name": "Harri Lähdesmäki"
                },
                "author": "Harri Lähdesmäki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2111.02019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2111.02019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04126v1",
                "updated": "2024-09-06T08:52:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    52,
                    13,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:52:13Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    52,
                    13,
                    4,
                    250,
                    0
                ],
                "title": "Incorporating external data for analyzing randomized clinical trials: A\n  transfer learning approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external data for analyzing randomized clinical trials: A\n  transfer learning approach"
                },
                "summary": "Randomized clinical trials are the gold standard for analyzing treatment\neffects, but high costs and ethical concerns can limit recruitment, potentially\nleading to invalid inferences. Incorporating external trial data with similar\ncharacteristics into the analysis using transfer learning appears promising for\naddressing these issues. In this paper, we present a formal framework for\napplying transfer learning to the analysis of clinical trials, considering\nthree key perspectives: transfer algorithm, theoretical foundation, and\ninference method. For the algorithm, we adopt a parameter-based transfer\nlearning approach to enhance the lasso-adjusted stratum-specific estimator\ndeveloped for estimating treatment effects. A key component in constructing the\ntransfer learning estimator is deriving the regression coefficient estimates\nwithin each stratum, accounting for the bias between source and target data. To\nprovide a theoretical foundation, we derive the $l_1$ convergence rate for the\nestimated regression coefficients and establish the asymptotic normality of the\ntransfer learning estimator. Our results show that when external trial data\nresembles current trial data, the sample size requirements can be reduced\ncompared to using only the current trial data. Finally, we propose a consistent\nnonparametric variance estimator to facilitate inference. Numerical studies\ndemonstrate the effectiveness and robustness of our proposed estimator across\nvarious scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized clinical trials are the gold standard for analyzing treatment\neffects, but high costs and ethical concerns can limit recruitment, potentially\nleading to invalid inferences. Incorporating external trial data with similar\ncharacteristics into the analysis using transfer learning appears promising for\naddressing these issues. In this paper, we present a formal framework for\napplying transfer learning to the analysis of clinical trials, considering\nthree key perspectives: transfer algorithm, theoretical foundation, and\ninference method. For the algorithm, we adopt a parameter-based transfer\nlearning approach to enhance the lasso-adjusted stratum-specific estimator\ndeveloped for estimating treatment effects. A key component in constructing the\ntransfer learning estimator is deriving the regression coefficient estimates\nwithin each stratum, accounting for the bias between source and target data. To\nprovide a theoretical foundation, we derive the $l_1$ convergence rate for the\nestimated regression coefficients and establish the asymptotic normality of the\ntransfer learning estimator. Our results show that when external trial data\nresembles current trial data, the sample size requirements can be reduced\ncompared to using only the current trial data. Finally, we propose a consistent\nnonparametric variance estimator to facilitate inference. Numerical studies\ndemonstrate the effectiveness and robustness of our proposed estimator across\nvarious scenarios."
                },
                "authors": [
                    {
                        "name": "Yujia Gu"
                    },
                    {
                        "name": "Hanzhong Liu"
                    },
                    {
                        "name": "Wei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ma"
                },
                "author": "Wei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04122v1",
                "updated": "2024-09-06T08:43:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    43,
                    10,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:43:10Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    43,
                    10,
                    4,
                    250,
                    0
                ],
                "title": "Prompt-based Personality Profiling: Reinforcement Learning for Relevance\n  Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based Personality Profiling: Reinforcement Learning for Relevance\n  Filtering"
                },
                "summary": "Author profiling is the task of inferring characteristics about individuals\nby analyzing content they share. Supervised machine learning still dominates\nautomatic systems that perform this task, despite the popularity of prompting\nlarge language models to address natural language understanding tasks. One\nreason is that the classification instances consist of large amounts of posts,\npotentially a whole user profile, which may exceed the input length of\nTransformers. Even if a model can use a large context window, the entirety of\nposts makes the application of API-accessed black box systems costly and slow,\nnext to issues which come with such \"needle-in-the-haystack\" tasks. To mitigate\nthis limitation, we propose a new method for author profiling which aims at\ndistinguishing relevant from irrelevant content first, followed by the actual\nuser profiling only with relevant data. To circumvent the need for\nrelevance-annotated data, we optimize this relevance filter via reinforcement\nlearning with a reward function that utilizes the zero-shot capabilities of\nlarge language models. We evaluate our method for Big Five personality trait\nprediction on two Twitter corpora. On publicly available real-world data with a\nskewed label distribution, our method shows similar efficacy to using all posts\nin a user profile, but with a substantially shorter context. An evaluation on a\nversion of these data balanced with artificial posts shows that the filtering\nto relevant posts leads to a significantly improved accuracy of the\npredictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Author profiling is the task of inferring characteristics about individuals\nby analyzing content they share. Supervised machine learning still dominates\nautomatic systems that perform this task, despite the popularity of prompting\nlarge language models to address natural language understanding tasks. One\nreason is that the classification instances consist of large amounts of posts,\npotentially a whole user profile, which may exceed the input length of\nTransformers. Even if a model can use a large context window, the entirety of\nposts makes the application of API-accessed black box systems costly and slow,\nnext to issues which come with such \"needle-in-the-haystack\" tasks. To mitigate\nthis limitation, we propose a new method for author profiling which aims at\ndistinguishing relevant from irrelevant content first, followed by the actual\nuser profiling only with relevant data. To circumvent the need for\nrelevance-annotated data, we optimize this relevance filter via reinforcement\nlearning with a reward function that utilizes the zero-shot capabilities of\nlarge language models. We evaluate our method for Big Five personality trait\nprediction on two Twitter corpora. On publicly available real-world data with a\nskewed label distribution, our method shows similar efficacy to using all posts\nin a user profile, but with a substantially shorter context. An evaluation on a\nversion of these data balanced with artificial posts shows that the filtering\nto relevant posts leads to a significantly improved accuracy of the\npredictions."
                },
                "authors": [
                    {
                        "name": "Jan Hofmann"
                    },
                    {
                        "name": "Cornelia Sindermann"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "arxiv_comment": "preprint, under review, supplementary material will be made available\n  upon acceptance of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04114v1",
                "updated": "2024-09-06T08:31:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    31,
                    18,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:31:18Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    31,
                    18,
                    4,
                    250,
                    0
                ],
                "title": "Multi-Programming Language Ensemble for Code Generation in Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Programming Language Ensemble for Code Generation in Large\n  Language Model"
                },
                "summary": "Large language models (LLMs) have significantly improved code generation,\nparticularly in one-pass code generation. However, most existing approaches\nfocus solely on generating code in a single programming language, overlooking\nthe potential of leveraging the multi-language capabilities of LLMs. LLMs have\nvarying patterns of errors across different languages, suggesting that a more\nrobust approach could be developed by leveraging these multi-language outputs.\nIn this study, we propose Multi-Programming Language Ensemble (MPLE), a novel\nensemble-based method that utilizes code generation across multiple programming\nlanguages to enhance overall performance. By treating each language-specific\ncode generation process as an individual \"weak expert\" and effectively\nintegrating their outputs, our method mitigates language-specific errors and\nbiases. This multi-language ensemble strategy leverages the complementary\nstrengths of different programming languages, enabling the model to produce\nmore accurate and robust code. Our approach can be seamlessly integrated with\ncommonly used techniques such as the reflection algorithm and Monte Carlo tree\nsearch to improve code generation quality further. Experimental results show\nthat our framework consistently enhances baseline performance by up to 17.92%\non existing benchmarks (HumanEval and HumanEval-plus), with a standout result\nof 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art\nresults across various LLM models. The code will be released at\nhttps://github.com/NinjaTech-AI/MPLE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly improved code generation,\nparticularly in one-pass code generation. However, most existing approaches\nfocus solely on generating code in a single programming language, overlooking\nthe potential of leveraging the multi-language capabilities of LLMs. LLMs have\nvarying patterns of errors across different languages, suggesting that a more\nrobust approach could be developed by leveraging these multi-language outputs.\nIn this study, we propose Multi-Programming Language Ensemble (MPLE), a novel\nensemble-based method that utilizes code generation across multiple programming\nlanguages to enhance overall performance. By treating each language-specific\ncode generation process as an individual \"weak expert\" and effectively\nintegrating their outputs, our method mitigates language-specific errors and\nbiases. This multi-language ensemble strategy leverages the complementary\nstrengths of different programming languages, enabling the model to produce\nmore accurate and robust code. Our approach can be seamlessly integrated with\ncommonly used techniques such as the reflection algorithm and Monte Carlo tree\nsearch to improve code generation quality further. Experimental results show\nthat our framework consistently enhances baseline performance by up to 17.92%\non existing benchmarks (HumanEval and HumanEval-plus), with a standout result\nof 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art\nresults across various LLM models. The code will be released at\nhttps://github.com/NinjaTech-AI/MPLE"
                },
                "authors": [
                    {
                        "name": "Tengfei Xue"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Tahir Azim"
                    },
                    {
                        "name": "Roman Smirnov"
                    },
                    {
                        "name": "Jianhui Yu"
                    },
                    {
                        "name": "Arash Sadrieh"
                    },
                    {
                        "name": "Babak Pahlavan"
                    }
                ],
                "author_detail": {
                    "name": "Babak Pahlavan"
                },
                "author": "Babak Pahlavan",
                "arxiv_comment": "Code available at https://github.com/NinjaTech-AI/MPLE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04111v1",
                "updated": "2024-09-06T08:28:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    35,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:28:35Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    35,
                    4,
                    250,
                    0
                ],
                "title": "Active-Passive Federated Learning for Vertically Partitioned Multi-view\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active-Passive Federated Learning for Vertically Partitioned Multi-view\n  Data"
                },
                "summary": "Vertical federated learning is a natural and elegant approach to integrate\nmulti-view data vertically partitioned across devices (clients) while\npreserving their privacies. Apart from the model training, existing methods\nrequires the collaboration of all clients in the model inference. However, the\nmodel inference is probably maintained for service in a long time, while the\ncollaboration, especially when the clients belong to different organizations,\nis unpredictable in real-world scenarios, such as concellation of contract,\nnetwork unavailablity, etc., resulting in the failure of them. To address this\nissue, we, at the first attempt, propose a flexible Active-Passive Federated\nlearning (APFed) framework. Specifically, the active client is the initiator of\na learning task and responsible to build the complete model, while the passive\nclients only serve as assistants. Once the model built, the active client can\nmake inference independently. In addition, we instance the APFed framework into\ntwo classification methods with employing the reconstruction loss and the\ncontrastive loss on passive clients, respectively. Meanwhile, the two methods\nare tested in a set of experiments and achieves desired results, validating\ntheir effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical federated learning is a natural and elegant approach to integrate\nmulti-view data vertically partitioned across devices (clients) while\npreserving their privacies. Apart from the model training, existing methods\nrequires the collaboration of all clients in the model inference. However, the\nmodel inference is probably maintained for service in a long time, while the\ncollaboration, especially when the clients belong to different organizations,\nis unpredictable in real-world scenarios, such as concellation of contract,\nnetwork unavailablity, etc., resulting in the failure of them. To address this\nissue, we, at the first attempt, propose a flexible Active-Passive Federated\nlearning (APFed) framework. Specifically, the active client is the initiator of\na learning task and responsible to build the complete model, while the passive\nclients only serve as assistants. Once the model built, the active client can\nmake inference independently. In addition, we instance the APFed framework into\ntwo classification methods with employing the reconstruction loss and the\ncontrastive loss on passive clients, respectively. Meanwhile, the two methods\nare tested in a set of experiments and achieves desired results, validating\ntheir effectiveness."
                },
                "authors": [
                    {
                        "name": "Jiyuan Liu"
                    },
                    {
                        "name": "Xinwang Liu"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Xingchen Hu"
                    },
                    {
                        "name": "Qing Liao"
                    },
                    {
                        "name": "Xinhang Wan"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Kunlun He"
                    }
                ],
                "author_detail": {
                    "name": "Kunlun He"
                },
                "author": "Kunlun He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04109v1",
                "updated": "2024-09-06T08:25:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    25,
                    3,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:25:03Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    25,
                    3,
                    4,
                    250,
                    0
                ],
                "title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with\n  100+ NLP Researchers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with\n  100+ NLP Researchers"
                },
                "summary": "Recent advancements in large language models (LLMs) have sparked optimism\nabout their potential to accelerate scientific discovery, with a growing number\nof works proposing research agents that autonomously generate and validate new\nideas. Despite this, no evaluations have shown that LLM systems can take the\nvery first step of producing novel, expert-level ideas, let alone perform the\nentire research process. We address this by establishing an experimental design\nthat evaluates research idea generation while controlling for confounders and\nperforms the first head-to-head comparison between expert NLP researchers and\nan LLM ideation agent. By recruiting over 100 NLP researchers to write novel\nideas and blind reviews of both LLM and human ideas, we obtain the first\nstatistically significant conclusion on current LLM capabilities for research\nideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than\nhuman expert ideas while being judged slightly weaker on feasibility. Studying\nour agent baselines closely, we identify open problems in building and\nevaluating research agents, including failures of LLM self-evaluation and their\nlack of diversity in generation. Finally, we acknowledge that human judgements\nof novelty can be difficult, even by experts, and propose an end-to-end study\ndesign which recruits researchers to execute these ideas into full projects,\nenabling us to study whether these novelty and feasibility judgements result in\nmeaningful differences in research outcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have sparked optimism\nabout their potential to accelerate scientific discovery, with a growing number\nof works proposing research agents that autonomously generate and validate new\nideas. Despite this, no evaluations have shown that LLM systems can take the\nvery first step of producing novel, expert-level ideas, let alone perform the\nentire research process. We address this by establishing an experimental design\nthat evaluates research idea generation while controlling for confounders and\nperforms the first head-to-head comparison between expert NLP researchers and\nan LLM ideation agent. By recruiting over 100 NLP researchers to write novel\nideas and blind reviews of both LLM and human ideas, we obtain the first\nstatistically significant conclusion on current LLM capabilities for research\nideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than\nhuman expert ideas while being judged slightly weaker on feasibility. Studying\nour agent baselines closely, we identify open problems in building and\nevaluating research agents, including failures of LLM self-evaluation and their\nlack of diversity in generation. Finally, we acknowledge that human judgements\nof novelty can be difficult, even by experts, and propose an end-to-end study\ndesign which recruits researchers to execute these ideas into full projects,\nenabling us to study whether these novelty and feasibility judgements result in\nmeaningful differences in research outcome."
                },
                "authors": [
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Diyi Yang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "main paper is 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04108v1",
                "updated": "2024-09-06T08:24:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    24,
                    56,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:24:56Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    24,
                    56,
                    4,
                    250,
                    0
                ],
                "title": "A Generalization of Axiomatic Approach to Information Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generalization of Axiomatic Approach to Information Leakage"
                },
                "summary": "In this paper, we extend the framework of quantitative information flow (QIF)\nto include adversaries that use Kolmogorov-Nagumo $f$-mean to infer secrets of\na private system. Specifically, in our setting, an adversary uses\nKolmogorov-Nagumo $f$-mean to compute its best actions before and after\nobserving the system's randomized outputs. This leads to generalized notions of\nprior and posterior vulnerability and generalized axiomatic relations that we\nwill derive to elucidate how these $f$-mean based vulnerabilities interact with\neach other. We demonstrate usefulness of this framework by showing how some\nnotions of leakage that had been derived outside of the QIF framework and so\nfar seemed incompatible with it are indeed explainable via such extension of\nQIF. These leakage measures include $\\alpha$-leakage, which is the same as\nArimoto mutual information of order $\\alpha$, maximal $\\alpha$-leakage which is\nthe $\\alpha$-leakage capacity, and $(\\alpha,\\beta)$ leakage, which is a\ngeneralization of the above and captures local differential privacy as a\nspecial case. We also propose a new pointwise notion of gain function, which we\ncoin pointwise information gain. We show that this pointwise information gain\ncan explain R\\'eyni divergence and Sibson mutual information of order $\\alpha\n\\in [0,\\infty]$ as the Kolmogorov-Nagumo average of the gain with a proper\nchoice of function $f$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we extend the framework of quantitative information flow (QIF)\nto include adversaries that use Kolmogorov-Nagumo $f$-mean to infer secrets of\na private system. Specifically, in our setting, an adversary uses\nKolmogorov-Nagumo $f$-mean to compute its best actions before and after\nobserving the system's randomized outputs. This leads to generalized notions of\nprior and posterior vulnerability and generalized axiomatic relations that we\nwill derive to elucidate how these $f$-mean based vulnerabilities interact with\neach other. We demonstrate usefulness of this framework by showing how some\nnotions of leakage that had been derived outside of the QIF framework and so\nfar seemed incompatible with it are indeed explainable via such extension of\nQIF. These leakage measures include $\\alpha$-leakage, which is the same as\nArimoto mutual information of order $\\alpha$, maximal $\\alpha$-leakage which is\nthe $\\alpha$-leakage capacity, and $(\\alpha,\\beta)$ leakage, which is a\ngeneralization of the above and captures local differential privacy as a\nspecial case. We also propose a new pointwise notion of gain function, which we\ncoin pointwise information gain. We show that this pointwise information gain\ncan explain R\\'eyni divergence and Sibson mutual information of order $\\alpha\n\\in [0,\\infty]$ as the Kolmogorov-Nagumo average of the gain with a proper\nchoice of function $f$."
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Zarrabian"
                    },
                    {
                        "name": "Parastoo Sadeghi"
                    }
                ],
                "author_detail": {
                    "name": "Parastoo Sadeghi"
                },
                "author": "Parastoo Sadeghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04106v1",
                "updated": "2024-09-06T08:20:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    20,
                    2,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:20:02Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    20,
                    2,
                    4,
                    250,
                    0
                ],
                "title": "CryptoAnalytics: Cryptocoins Price Forecasting with Machine Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoAnalytics: Cryptocoins Price Forecasting with Machine Learning\n  Techniques"
                },
                "summary": "This paper introduces CryptoAnalytics, a software toolkit for cryptocoins\nprice forecasting with machine learning (ML) techniques. Cryptocoins are\ntradable digital assets exchanged for specific trading prices. While history\nhas shown the extreme volatility of such trading prices, the ability to\nefficiently model and forecast the time series resulting from the exchange\nprice volatility remains an open research challenge. Good results can been\nachieved with state-of-the-art ML techniques, including Gradient-Boosting\nMachines (GBMs) and Recurrent Neural Networks (RNNs). CryptoAnalytics is a\nsoftware toolkit to easily train these models and make inference on up-to-date\ncryptocoin trading price data, with facilities to fetch datasets from one of\nthe main leading aggregator websites, i.e., CoinMarketCap, train models and\ninfer the future trends. This software is implemented in Python. It relies on\nPyTorch for the implementation of RNNs (LSTM and GRU), while for GBMs, it\nleverages on XgBoost, LightGBM and CatBoost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces CryptoAnalytics, a software toolkit for cryptocoins\nprice forecasting with machine learning (ML) techniques. Cryptocoins are\ntradable digital assets exchanged for specific trading prices. While history\nhas shown the extreme volatility of such trading prices, the ability to\nefficiently model and forecast the time series resulting from the exchange\nprice volatility remains an open research challenge. Good results can been\nachieved with state-of-the-art ML techniques, including Gradient-Boosting\nMachines (GBMs) and Recurrent Neural Networks (RNNs). CryptoAnalytics is a\nsoftware toolkit to easily train these models and make inference on up-to-date\ncryptocoin trading price data, with facilities to fetch datasets from one of\nthe main leading aggregator websites, i.e., CoinMarketCap, train models and\ninfer the future trends. This software is implemented in Python. It relies on\nPyTorch for the implementation of RNNs (LSTM and GRU), while for GBMs, it\nleverages on XgBoost, LightGBM and CatBoost."
                },
                "authors": [
                    {
                        "name": "Pasquale De Rosa"
                    },
                    {
                        "name": "Pascal Felber"
                    },
                    {
                        "name": "Valerio Schiavoni"
                    }
                ],
                "author_detail": {
                    "name": "Valerio Schiavoni"
                },
                "author": "Valerio Schiavoni",
                "arxiv_doi": "10.1016/j.softx.2024.101663",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.softx.2024.101663",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.04106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "SoftwareX, Volume 26, May 2024",
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01588v2",
                "updated": "2024-09-06T08:16:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    16,
                    2,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-03T04:04:40Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    4,
                    4,
                    40,
                    1,
                    247,
                    0
                ],
                "title": "Large-scale Urban Facility Location Selection with Knowledge-informed\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Urban Facility Location Selection with Knowledge-informed\n  Reinforcement Learning"
                },
                "summary": "The facility location problem (FLP) is a classical combinatorial optimization\nchallenge aimed at strategically laying out facilities to maximize their\naccessibility. In this paper, we propose a reinforcement learning method\ntailored to solve large-scale urban FLP, capable of producing near-optimal\nsolutions at superfast inference speed. We distill the essential swap operation\nfrom local search, and simulate it by intelligently selecting edges on a graph\nof urban regions, guided by a knowledge-informed graph neural network, thus\nsidestepping the need for heavy computation of local search. Extensive\nexperiments on four US cities with different geospatial conditions demonstrate\nthat our approach can achieve comparable performance to commercial solvers with\nless than 5\\% accessibility loss, while displaying up to 1000 times speedup. We\ndeploy our model as an online geospatial application at\nhttps://huggingface.co/spaces/randommmm/MFLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The facility location problem (FLP) is a classical combinatorial optimization\nchallenge aimed at strategically laying out facilities to maximize their\naccessibility. In this paper, we propose a reinforcement learning method\ntailored to solve large-scale urban FLP, capable of producing near-optimal\nsolutions at superfast inference speed. We distill the essential swap operation\nfrom local search, and simulate it by intelligently selecting edges on a graph\nof urban regions, guided by a knowledge-informed graph neural network, thus\nsidestepping the need for heavy computation of local search. Extensive\nexperiments on four US cities with different geospatial conditions demonstrate\nthat our approach can achieve comparable performance to commercial solvers with\nless than 5\\% accessibility loss, while displaying up to 1000 times speedup. We\ndeploy our model as an online geospatial application at\nhttps://huggingface.co/spaces/randommmm/MFLP."
                },
                "authors": [
                    {
                        "name": "Hongyuan Su"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Jingtao Ding"
                    },
                    {
                        "name": "Depeng Jin"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_doi": "10.1145/3678717.3691254",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678717.3691254",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.01588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Sigspatial2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04102v2",
                "updated": "2024-09-09T08:55:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    55,
                    22,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T08:08:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    8,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "Intelligent tutoring systems by Bayesian nets with noisy gates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring systems by Bayesian nets with noisy gates"
                },
                "summary": "Directed graphical models such as Bayesian nets are often used to implement\nintelligent tutoring systems able to interact in real-time with learners in a\npurely automatic way. When coping with such models, keeping a bound on the\nnumber of parameters might be important for multiple reasons. First, as these\nmodels are typically based on expert knowledge, a huge number of parameters to\nelicit might discourage practitioners from adopting them. Moreover, the number\nof model parameters affects the complexity of the inferences, while a fast\ncomputation of the queries is needed for real-time feedback. We advocate\nlogical gates with uncertainty for a compact parametrization of the conditional\nprobability tables in the underlying Bayesian net used by tutoring systems. We\ndiscuss the semantics of the model parameters to elicit and the assumptions\nrequired to apply such approach in this domain. We also derive a dedicated\ninference scheme to speed up computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed graphical models such as Bayesian nets are often used to implement\nintelligent tutoring systems able to interact in real-time with learners in a\npurely automatic way. When coping with such models, keeping a bound on the\nnumber of parameters might be important for multiple reasons. First, as these\nmodels are typically based on expert knowledge, a huge number of parameters to\nelicit might discourage practitioners from adopting them. Moreover, the number\nof model parameters affects the complexity of the inferences, while a fast\ncomputation of the queries is needed for real-time feedback. We advocate\nlogical gates with uncertainty for a compact parametrization of the conditional\nprobability tables in the underlying Bayesian net used by tutoring systems. We\ndiscuss the semantics of the model parameters to elicit and the assumptions\nrequired to apply such approach in this domain. We also derive a dedicated\ninference scheme to speed up computations."
                },
                "authors": [
                    {
                        "name": "Alessandro Antonucci"
                    },
                    {
                        "name": "Francesca Mangili"
                    },
                    {
                        "name": "Claudio Bonesana"
                    },
                    {
                        "name": "Giorgia Adorni"
                    }
                ],
                "author_detail": {
                    "name": "Giorgia Adorni"
                },
                "author": "Giorgia Adorni",
                "arxiv_doi": "10.32473/flairs.v35i.130692",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.32473/flairs.v35i.130692",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.04102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The International FLAIRS Conference Proceedings 35 (2022)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04095v1",
                "updated": "2024-09-06T08:02:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    2,
                    43,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:02:43Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    2,
                    43,
                    4,
                    250,
                    0
                ],
                "title": "UNIT: Unifying Image and Text Recognition in One Vision Encoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNIT: Unifying Image and Text Recognition in One Vision Encoder"
                },
                "summary": "Currently, vision encoder models like Vision Transformers (ViTs) typically\nexcel at image recognition tasks but cannot simultaneously support text\nrecognition like human visual recognition. To address this limitation, we\npropose UNIT, a novel training framework aimed at UNifying Image and Text\nrecognition within a single model. Starting with a vision encoder pre-trained\nwith image recognition tasks, UNIT introduces a lightweight language decoder\nfor predicting text outputs and a lightweight vision decoder to prevent\ncatastrophic forgetting of the original image encoding capabilities. The\ntraining process comprises two stages: intra-scale pretraining and inter-scale\nfinetuning. During intra-scale pretraining, UNIT learns unified representations\nfrom multi-scale inputs, where images and documents are at their commonly used\nresolution, to enable fundamental recognition capability. In the inter-scale\nfinetuning stage, the model introduces scale-exchanged data, featuring images\nand documents at resolutions different from the most commonly used ones, to\nenhance its scale robustness. Notably, UNIT retains the original vision encoder\narchitecture, making it cost-free in terms of inference and deployment.\nExperiments across multiple benchmarks confirm that our method significantly\noutperforms existing methods on document-related tasks (e.g., OCR and DocQA)\nwhile maintaining the performances on natural images, demonstrating its ability\nto substantially enhance text recognition without compromising its core image\nrecognition capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, vision encoder models like Vision Transformers (ViTs) typically\nexcel at image recognition tasks but cannot simultaneously support text\nrecognition like human visual recognition. To address this limitation, we\npropose UNIT, a novel training framework aimed at UNifying Image and Text\nrecognition within a single model. Starting with a vision encoder pre-trained\nwith image recognition tasks, UNIT introduces a lightweight language decoder\nfor predicting text outputs and a lightweight vision decoder to prevent\ncatastrophic forgetting of the original image encoding capabilities. The\ntraining process comprises two stages: intra-scale pretraining and inter-scale\nfinetuning. During intra-scale pretraining, UNIT learns unified representations\nfrom multi-scale inputs, where images and documents are at their commonly used\nresolution, to enable fundamental recognition capability. In the inter-scale\nfinetuning stage, the model introduces scale-exchanged data, featuring images\nand documents at resolutions different from the most commonly used ones, to\nenhance its scale robustness. Notably, UNIT retains the original vision encoder\narchitecture, making it cost-free in terms of inference and deployment.\nExperiments across multiple benchmarks confirm that our method significantly\noutperforms existing methods on document-related tasks (e.g., OCR and DocQA)\nwhile maintaining the performances on natural images, demonstrating its ability\nto substantially enhance text recognition without compromising its core image\nrecognition capabilities."
                },
                "authors": [
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Yanpeng Zhou"
                    },
                    {
                        "name": "Chunwei Wang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Hang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Xu"
                },
                "author": "Hang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04081v1",
                "updated": "2024-09-06T07:44:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    44,
                    44,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T07:44:44Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    44,
                    44,
                    4,
                    250,
                    0
                ],
                "title": "UI-JEPA: Towards Active Perception of User Intent through Onscreen User\n  Activity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-JEPA: Towards Active Perception of User Intent through Onscreen User\n  Activity"
                },
                "summary": "Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding."
                },
                "authors": [
                    {
                        "name": "Yicheng Fu"
                    },
                    {
                        "name": "Raviteja Anantha"
                    },
                    {
                        "name": "Prabal Vashisht"
                    },
                    {
                        "name": "Jianpeng Cheng"
                    },
                    {
                        "name": "Etai Littwin"
                    }
                ],
                "author_detail": {
                    "name": "Etai Littwin"
                },
                "author": "Etai Littwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04073v2",
                "updated": "2024-09-09T11:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    33,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T07:29:01Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    29,
                    1,
                    4,
                    250,
                    0
                ],
                "title": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language\n  Model"
                },
                "summary": "Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens)."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Paul Groth"
                    },
                    {
                        "name": "Iacer Calixto"
                    },
                    {
                        "name": "Sebastian Schelter"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Schelter"
                },
                "author": "Sebastian Schelter",
                "arxiv_comment": "12 pages excluding references, 3 figures, and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.04226v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.04226v3",
                "updated": "2024-09-06T07:10:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    10,
                    3,
                    4,
                    250,
                    0
                ],
                "published": "2023-07-09T16:37:47Z",
                "published_parsed": [
                    2023,
                    7,
                    9,
                    16,
                    37,
                    47,
                    6,
                    190,
                    0
                ],
                "title": "Seismic Data Interpolation via Denoising Diffusion Implicit Models with\n  Coherence-corrected Resampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seismic Data Interpolation via Denoising Diffusion Implicit Models with\n  Coherence-corrected Resampling"
                },
                "summary": "Accurate interpolation of seismic data is crucial for improving the quality\nof imaging and interpretation. In recent years, deep learning models such as\nU-Net and generative adversarial networks have been widely applied to seismic\ndata interpolation. However, they often underperform when the training and test\nmissing patterns do not match. To alleviate this issue, here we propose a novel\nframework that is built upon the multi-modal adaptable diffusion models. In the\ntraining phase, following the common wisdom, we use the denoising diffusion\nprobabilistic model with a cosine noise schedule. This cosine global noise\nconfiguration improves the use of seismic data by reducing the involvement of\nexcessive noise stages. In the inference phase, we introduce the denoising\ndiffusion implicit model to reduce the number of sampling steps. Different from\nthe conventional unconditional generation, we incorporate the known trace\ninformation into each reverse sampling step for achieving conditional\ninterpolation. To enhance the coherence and continuity between the revealed\ntraces and the missing traces, we further propose two strategies, including\nsuccessive coherence correction and resampling. Coherence correction penalizes\nthe mismatches in the revealed traces, while resampling conducts cyclic\ninterpolation between adjacent reverse steps. Extensive experiments on\nsynthetic and field seismic data validate our model's superiority and\ndemonstrate its generalization capability to various missing patterns and\ndifferent noise levels with just one training session. In addition, uncertainty\nquantification and ablation studies are also investigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate interpolation of seismic data is crucial for improving the quality\nof imaging and interpretation. In recent years, deep learning models such as\nU-Net and generative adversarial networks have been widely applied to seismic\ndata interpolation. However, they often underperform when the training and test\nmissing patterns do not match. To alleviate this issue, here we propose a novel\nframework that is built upon the multi-modal adaptable diffusion models. In the\ntraining phase, following the common wisdom, we use the denoising diffusion\nprobabilistic model with a cosine noise schedule. This cosine global noise\nconfiguration improves the use of seismic data by reducing the involvement of\nexcessive noise stages. In the inference phase, we introduce the denoising\ndiffusion implicit model to reduce the number of sampling steps. Different from\nthe conventional unconditional generation, we incorporate the known trace\ninformation into each reverse sampling step for achieving conditional\ninterpolation. To enhance the coherence and continuity between the revealed\ntraces and the missing traces, we further propose two strategies, including\nsuccessive coherence correction and resampling. Coherence correction penalizes\nthe mismatches in the revealed traces, while resampling conducts cyclic\ninterpolation between adjacent reverse steps. Extensive experiments on\nsynthetic and field seismic data validate our model's superiority and\ndemonstrate its generalization capability to various missing patterns and\ndifferent noise levels with just one training session. In addition, uncertainty\nquantification and ablation studies are also investigated."
                },
                "authors": [
                    {
                        "name": "Xiaoli Wei"
                    },
                    {
                        "name": "Chunxia Zhang"
                    },
                    {
                        "name": "Hongtao Wang"
                    },
                    {
                        "name": "Chengli Tan"
                    },
                    {
                        "name": "Deng Xiong"
                    },
                    {
                        "name": "Baisong Jiang"
                    },
                    {
                        "name": "Jiangshe Zhang"
                    },
                    {
                        "name": "Sang-Woon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Sang-Woon Kim"
                },
                "author": "Sang-Woon Kim",
                "arxiv_comment": "14 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.04226v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.04226v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03431v3",
                "updated": "2024-09-09T03:22:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    3,
                    22,
                    43,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T11:23:41Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    23,
                    41,
                    3,
                    249,
                    0
                ],
                "title": "UV-Mamba: A DCN-Enhanced State Space Model for Urban Village Boundary\n  Identification in High-Resolution Remote Sensing Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UV-Mamba: A DCN-Enhanced State Space Model for Urban Village Boundary\n  Identification in High-Resolution Remote Sensing Images"
                },
                "summary": "Due to the diverse geographical environments, intricate landscapes, and\nhigh-density settlements, the automatic identification of urban village\nboundaries using remote sensing images remains a highly challenging task. This\npaper proposes a novel and efficient neural network model called UV-Mamba for\naccurate boundary detection in high-resolution remote sensing images. UV-Mamba\nmitigates the memory loss problem in lengthy sequence modeling, which arises in\nstate space models with increasing image size, by incorporating deformable\nconvolutions. Its architecture utilizes an encoder-decoder framework and\nincludes an encoder with four deformable state space augmentation blocks for\nefficient multi-level semantic extraction and a decoder to integrate the\nextracted semantic information. We conducted experiments on two large datasets\nshowing that UV-Mamba achieves state-of-the-art performance. Specifically, our\nmodel achieves 73.3% and 78.1% IoU on the Beijing and Xi'an datasets,\nrespectively, representing improvements of 1.2% and 3.4% IoU over the previous\nbest model while also being 6x faster in inference speed and 40x smaller in\nparameter count. Source code and pre-trained models are available at\nhttps://github.com/Devin-Egber/UV-Mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the diverse geographical environments, intricate landscapes, and\nhigh-density settlements, the automatic identification of urban village\nboundaries using remote sensing images remains a highly challenging task. This\npaper proposes a novel and efficient neural network model called UV-Mamba for\naccurate boundary detection in high-resolution remote sensing images. UV-Mamba\nmitigates the memory loss problem in lengthy sequence modeling, which arises in\nstate space models with increasing image size, by incorporating deformable\nconvolutions. Its architecture utilizes an encoder-decoder framework and\nincludes an encoder with four deformable state space augmentation blocks for\nefficient multi-level semantic extraction and a decoder to integrate the\nextracted semantic information. We conducted experiments on two large datasets\nshowing that UV-Mamba achieves state-of-the-art performance. Specifically, our\nmodel achieves 73.3% and 78.1% IoU on the Beijing and Xi'an datasets,\nrespectively, representing improvements of 1.2% and 3.4% IoU over the previous\nbest model while also being 6x faster in inference speed and 40x smaller in\nparameter count. Source code and pre-trained models are available at\nhttps://github.com/Devin-Egber/UV-Mamba."
                },
                "authors": [
                    {
                        "name": "Lulin Li"
                    },
                    {
                        "name": "Ben Chen"
                    },
                    {
                        "name": "Xuechao Zou"
                    },
                    {
                        "name": "Junliang Xing"
                    },
                    {
                        "name": "Pin Tao"
                    }
                ],
                "author_detail": {
                    "name": "Pin Tao"
                },
                "author": "Pin Tao",
                "arxiv_comment": "5 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04056v1",
                "updated": "2024-09-06T06:53:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    53,
                    45,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:53:45Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    53,
                    45,
                    4,
                    250,
                    0
                ],
                "title": "Refining Wikidata Taxonomy using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Wikidata Taxonomy using Large Language Models"
                },
                "summary": "Due to its collaborative nature, Wikidata is known to have a complex\ntaxonomy, with recurrent issues like the ambiguity between instances and\nclasses, the inaccuracy of some taxonomic paths, the presence of cycles, and\nthe high level of redundancy across classes. Manual efforts to clean up this\ntaxonomy are time-consuming and prone to errors or subjective decisions. We\npresent WiKC, a new version of Wikidata taxonomy cleaned automatically using a\ncombination of Large Language Models (LLMs) and graph mining techniques.\nOperations on the taxonomy, such as cutting links or merging classes, are\nperformed with the help of zero-shot prompting on an open-source LLM. The\nquality of the refined taxonomy is evaluated from both intrinsic and extrinsic\nperspectives, on a task of entity typing for the latter, showing the practical\ninterest of WiKC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to its collaborative nature, Wikidata is known to have a complex\ntaxonomy, with recurrent issues like the ambiguity between instances and\nclasses, the inaccuracy of some taxonomic paths, the presence of cycles, and\nthe high level of redundancy across classes. Manual efforts to clean up this\ntaxonomy are time-consuming and prone to errors or subjective decisions. We\npresent WiKC, a new version of Wikidata taxonomy cleaned automatically using a\ncombination of Large Language Models (LLMs) and graph mining techniques.\nOperations on the taxonomy, such as cutting links or merging classes, are\nperformed with the help of zero-shot prompting on an open-source LLM. The\nquality of the refined taxonomy is evaluated from both intrinsic and extrinsic\nperspectives, on a task of entity typing for the latter, showing the practical\ninterest of WiKC."
                },
                "authors": [
                    {
                        "name": "Yiwen Peng"
                    },
                    {
                        "name": "Thomas Bonald"
                    },
                    {
                        "name": "Mehwish Alam"
                    }
                ],
                "author_detail": {
                    "name": "Mehwish Alam"
                },
                "arxiv_affiliation": "IP Paris",
                "author": "Mehwish Alam",
                "arxiv_doi": "10.1145/3627673.3679156",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679156",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.04056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM International Conference on Information and Knowledge Management,\n  Oct 2024, Boise, Idaho, United States",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04054v1",
                "updated": "2024-09-06T06:51:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    51,
                    11,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:51:11Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    51,
                    11,
                    4,
                    250,
                    0
                ],
                "title": "The carbon footprint of astronomical observatories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The carbon footprint of astronomical observatories"
                },
                "summary": "The carbon footprint of astronomical research is an increasingly topical\nissue. From a comparison of existing literature, we infer an annual per capita\ncarbon footprint of several tens of tonnes of CO$_2$ equivalents for an average\nperson working in astronomy. Astronomical observatories contribute\nsignificantly to the carbon footprint of astronomy, and we examine the related\nsources of greenhouse gas emissions as well as lever arms for their reduction.\nComparison with other scientific domains illustrates that astronomy is not the\nonly field that needs to accomplish significant carbon footprint reductions of\ntheir research facilities. We show that limiting global warming to 1.5{\\deg}C\nor 2{\\deg}C implies greenhouse gas emission reductions that can only be reached\nby a systemic change of astronomical research activities, and we argue that a\nnew narrative for doing astronomical research is needed if we want to keep our\nplanet habitable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The carbon footprint of astronomical research is an increasingly topical\nissue. From a comparison of existing literature, we infer an annual per capita\ncarbon footprint of several tens of tonnes of CO$_2$ equivalents for an average\nperson working in astronomy. Astronomical observatories contribute\nsignificantly to the carbon footprint of astronomy, and we examine the related\nsources of greenhouse gas emissions as well as lever arms for their reduction.\nComparison with other scientific domains illustrates that astronomy is not the\nonly field that needs to accomplish significant carbon footprint reductions of\ntheir research facilities. We show that limiting global warming to 1.5{\\deg}C\nor 2{\\deg}C implies greenhouse gas emission reductions that can only be reached\nby a systemic change of astronomical research activities, and we argue that a\nnew narrative for doing astronomical research is needed if we want to keep our\nplanet habitable."
                },
                "authors": [
                    {
                        "name": "Jürgen Knödlseder"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Knödlseder"
                },
                "author": "Jürgen Knödlseder",
                "arxiv_comment": "10 pages, 3 figures, ADASS XXXII (2022) conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03659v2",
                "updated": "2024-09-06T06:50:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    50,
                    32,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T16:12:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    12,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "LLM-based multi-agent poetry generation in non-cooperative environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent poetry generation in non-cooperative environments"
                },
                "summary": "Despite substantial progress of large language models (LLMs) for automatic\npoetry generation, the generated poetry lacks diversity while the training\nprocess differs greatly from human learning. Under the rationale that the\nlearning process of the poetry generation systems should be more human-like and\ntheir output more diverse and novel, we introduce a framework based on social\nlearning where we emphasize non-cooperative interactions besides cooperative\ninteractions to encourage diversity. Our experiments are the first attempt at\nLLM-based multi-agent systems in non-cooperative environments for poetry\ngeneration employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED\nagents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows\nthat our framework benefits the poetry generation process for TRAINING-BASED\nagents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity\nand a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.\nThe generated poetry from TRAINING-BASED agents also exhibits group divergence\nin terms of lexicons, styles and semantics. PROMPTING-BASED agents in our\nframework also benefit from non-cooperative environments and a more diverse\nensemble of models with non-homogeneous agents has the potential to further\nenhance diversity, with an increase of 7.0-17.5 pp according to our\nexperiments. However, PROMPTING-BASED agents show a decrease in lexical\ndiversity over time and do not exhibit the group-based divergence intended in\nthe social network. Our paper argues for a paradigm shift in creative tasks\nsuch as automatic poetry generation to include social learning processes (via\nLLM-based agent modeling) similar to human interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial progress of large language models (LLMs) for automatic\npoetry generation, the generated poetry lacks diversity while the training\nprocess differs greatly from human learning. Under the rationale that the\nlearning process of the poetry generation systems should be more human-like and\ntheir output more diverse and novel, we introduce a framework based on social\nlearning where we emphasize non-cooperative interactions besides cooperative\ninteractions to encourage diversity. Our experiments are the first attempt at\nLLM-based multi-agent systems in non-cooperative environments for poetry\ngeneration employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED\nagents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows\nthat our framework benefits the poetry generation process for TRAINING-BASED\nagents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity\nand a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.\nThe generated poetry from TRAINING-BASED agents also exhibits group divergence\nin terms of lexicons, styles and semantics. PROMPTING-BASED agents in our\nframework also benefit from non-cooperative environments and a more diverse\nensemble of models with non-homogeneous agents has the potential to further\nenhance diversity, with an increase of 7.0-17.5 pp according to our\nexperiments. However, PROMPTING-BASED agents show a decrease in lexical\ndiversity over time and do not exhibit the group-based divergence intended in\nthe social network. Our paper argues for a paradigm shift in creative tasks\nsuch as automatic poetry generation to include social learning processes (via\nLLM-based agent modeling) similar to human interaction."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18521v2",
                "updated": "2024-09-06T06:49:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    49,
                    31,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-26T05:34:34Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    5,
                    34,
                    34,
                    4,
                    208,
                    0
                ],
                "title": "Patched MOA: optimizing inference for diverse software development tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patched MOA: optimizing inference for diverse software development tasks"
                },
                "summary": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm."
                },
                "authors": [
                    {
                        "name": "Asankhaya Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Asankhaya Sharma"
                },
                "author": "Asankhaya Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04050v1",
                "updated": "2024-09-06T06:46:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    46,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:46:01Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    46,
                    1,
                    4,
                    250,
                    0
                ],
                "title": "EigenSR: Eigenimage-Bridged Pre-Trained RGB Learners for Single\n  Hyperspectral Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EigenSR: Eigenimage-Bridged Pre-Trained RGB Learners for Single\n  Hyperspectral Image Super-Resolution"
                },
                "summary": "Single hyperspectral image super-resolution (single-HSI-SR) aims to improve\nthe resolution of a single input low-resolution HSI. Due to the bottleneck of\ndata scarcity, the development of single-HSI-SR lags far behind that of RGB\nnatural images. In recent years, research on RGB SR has shown that models\npre-trained on large-scale benchmark datasets can greatly improve performance\non unseen data, which may stand as a remedy for HSI. But how can we transfer\nthe pre-trained RGB model to HSI, to overcome the data-scarcity bottleneck?\nBecause of the significant difference in the channels between the pre-trained\nRGB model and the HSI, the model cannot focus on the correlation along the\nspectral dimension, thus limiting its ability to utilize on HSI. Inspired by\nthe HSI spatial-spectral decoupling, we propose a new framework that first\nfine-tunes the pre-trained model with the spatial components (known as\neigenimages), and then infers on unseen HSI using an iterative spectral\nregularization (ISR) to maintain the spectral correlation. The advantages of\nour method lie in: 1) we effectively inject the spatial texture processing\ncapabilities of the pre-trained RGB model into HSI while keeping spectral\nfidelity, 2) learning in the spectral-decorrelated domain can improve the\ngeneralizability to spectral-agnostic data, and 3) our inference in the\neigenimage domain naturally exploits the spectral low-rank property of HSI,\nthereby reducing the complexity. This work bridges the gap between pre-trained\nRGB models and HSI via eigenimages, addressing the issue of limited HSI\ntraining data, hence the name EigenSR. Extensive experiments show that EigenSR\noutperforms the state-of-the-art (SOTA) methods in both spatial and spectral\nmetrics. Our code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single hyperspectral image super-resolution (single-HSI-SR) aims to improve\nthe resolution of a single input low-resolution HSI. Due to the bottleneck of\ndata scarcity, the development of single-HSI-SR lags far behind that of RGB\nnatural images. In recent years, research on RGB SR has shown that models\npre-trained on large-scale benchmark datasets can greatly improve performance\non unseen data, which may stand as a remedy for HSI. But how can we transfer\nthe pre-trained RGB model to HSI, to overcome the data-scarcity bottleneck?\nBecause of the significant difference in the channels between the pre-trained\nRGB model and the HSI, the model cannot focus on the correlation along the\nspectral dimension, thus limiting its ability to utilize on HSI. Inspired by\nthe HSI spatial-spectral decoupling, we propose a new framework that first\nfine-tunes the pre-trained model with the spatial components (known as\neigenimages), and then infers on unseen HSI using an iterative spectral\nregularization (ISR) to maintain the spectral correlation. The advantages of\nour method lie in: 1) we effectively inject the spatial texture processing\ncapabilities of the pre-trained RGB model into HSI while keeping spectral\nfidelity, 2) learning in the spectral-decorrelated domain can improve the\ngeneralizability to spectral-agnostic data, and 3) our inference in the\neigenimage domain naturally exploits the spectral low-rank property of HSI,\nthereby reducing the complexity. This work bridges the gap between pre-trained\nRGB models and HSI via eigenimages, addressing the issue of limited HSI\ntraining data, hence the name EigenSR. Extensive experiments show that EigenSR\noutperforms the state-of-the-art (SOTA) methods in both spatial and spectral\nmetrics. Our code will be released."
                },
                "authors": [
                    {
                        "name": "Xi Su"
                    },
                    {
                        "name": "Xiangfei Shen"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Jing Nie"
                    },
                    {
                        "name": "Lihui Chen"
                    },
                    {
                        "name": "Haijun Liu"
                    },
                    {
                        "name": "Xichuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xichuan Zhou"
                },
                "author": "Xichuan Zhou",
                "arxiv_comment": "Submitted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04043v1",
                "updated": "2024-09-06T06:27:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    27,
                    35,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:27:35Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    27,
                    35,
                    4,
                    250,
                    0
                ],
                "title": "Towards Safer Online Spaces: Simulating and Assessing Intervention\n  Strategies for Eating Disorder Discussions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safer Online Spaces: Simulating and Assessing Intervention\n  Strategies for Eating Disorder Discussions"
                },
                "summary": "Eating disorders are complex mental health conditions that affect millions of\npeople around the world. Effective interventions on social media platforms are\ncrucial, yet testing strategies in situ can be risky. We present a novel\nLLM-driven experimental testbed for simulating and assessing intervention\nstrategies in ED-related discussions. Our framework generates synthetic\nconversations across multiple platforms, models, and ED-related topics,\nallowing for controlled experimentation with diverse intervention approaches.\nWe analyze the impact of various intervention strategies on conversation\ndynamics across four dimensions: intervention type, generative model, social\nmedia platform, and ED-related community/topic. We employ cognitive domain\nanalysis metrics, including sentiment, emotions, etc., to evaluate the\neffectiveness of interventions. Our findings reveal that civility-focused\ninterventions consistently improve positive sentiment and emotional tone across\nall dimensions, while insight-resetting approaches tend to increase negative\nemotions. We also uncover significant biases in LLM-generated conversations,\nwith cognitive metrics varying notably between models (Claude-3 Haiku $>$\nMistral $>$ GPT-3.5-turbo $>$ LLaMA3) and even between versions of the same\nmodel. These variations highlight the importance of model selection in\nsimulating realistic discussions related to ED. Our work provides valuable\ninformation on the complex dynamics of ED-related discussions and the\neffectiveness of various intervention strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eating disorders are complex mental health conditions that affect millions of\npeople around the world. Effective interventions on social media platforms are\ncrucial, yet testing strategies in situ can be risky. We present a novel\nLLM-driven experimental testbed for simulating and assessing intervention\nstrategies in ED-related discussions. Our framework generates synthetic\nconversations across multiple platforms, models, and ED-related topics,\nallowing for controlled experimentation with diverse intervention approaches.\nWe analyze the impact of various intervention strategies on conversation\ndynamics across four dimensions: intervention type, generative model, social\nmedia platform, and ED-related community/topic. We employ cognitive domain\nanalysis metrics, including sentiment, emotions, etc., to evaluate the\neffectiveness of interventions. Our findings reveal that civility-focused\ninterventions consistently improve positive sentiment and emotional tone across\nall dimensions, while insight-resetting approaches tend to increase negative\nemotions. We also uncover significant biases in LLM-generated conversations,\nwith cognitive metrics varying notably between models (Claude-3 Haiku $>$\nMistral $>$ GPT-3.5-turbo $>$ LLaMA3) and even between versions of the same\nmodel. These variations highlight the importance of model selection in\nsimulating realistic discussions related to ED. Our work provides valuable\ninformation on the complex dynamics of ED-related discussions and the\neffectiveness of various intervention strategies."
                },
                "authors": [
                    {
                        "name": "Louis Penafiel"
                    },
                    {
                        "name": "Hsien-Te Kao"
                    },
                    {
                        "name": "Isabel Erickson"
                    },
                    {
                        "name": "David Chu"
                    },
                    {
                        "name": "Robert McCormack"
                    },
                    {
                        "name": "Kristina Lerman"
                    },
                    {
                        "name": "Svitlana Volkova"
                    }
                ],
                "author_detail": {
                    "name": "Svitlana Volkova"
                },
                "author": "Svitlana Volkova",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04041v1",
                "updated": "2024-09-06T06:20:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    20,
                    11,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:20:11Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    20,
                    11,
                    4,
                    250,
                    0
                ],
                "title": "On Evaluation of Vision Datasets and Models using Human Competency\n  Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Evaluation of Vision Datasets and Models using Human Competency\n  Frameworks"
                },
                "summary": "Evaluating models and datasets in computer vision remains a challenging task,\nwith most leaderboards relying solely on accuracy. While accuracy is a popular\nmetric for model evaluation, it provides only a coarse assessment by\nconsidering a single model's score on all dataset items. This paper explores\nItem Response Theory (IRT), a framework that infers interpretable latent\nparameters for an ensemble of models and each dataset item, enabling richer\nevaluation and analysis beyond the single accuracy number. Leveraging IRT, we\nassess model calibration, select informative data subsets, and demonstrate the\nusefulness of its latent parameters for analyzing and comparing models and\ndatasets in computer vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating models and datasets in computer vision remains a challenging task,\nwith most leaderboards relying solely on accuracy. While accuracy is a popular\nmetric for model evaluation, it provides only a coarse assessment by\nconsidering a single model's score on all dataset items. This paper explores\nItem Response Theory (IRT), a framework that infers interpretable latent\nparameters for an ensemble of models and each dataset item, enabling richer\nevaluation and analysis beyond the single accuracy number. Leveraging IRT, we\nassess model calibration, select informative data subsets, and demonstrate the\nusefulness of its latent parameters for analyzing and comparing models and\ndatasets in computer vision."
                },
                "authors": [
                    {
                        "name": "Rahul Ramachandran"
                    },
                    {
                        "name": "Tejal Kulkarni"
                    },
                    {
                        "name": "Charchit Sharma"
                    },
                    {
                        "name": "Deepak Vijaykeerthy"
                    },
                    {
                        "name": "Vineeth N Balasubramanian"
                    }
                ],
                "author_detail": {
                    "name": "Vineeth N Balasubramanian"
                },
                "author": "Vineeth N Balasubramanian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11633v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11633v3",
                "updated": "2024-09-09T02:17:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    2,
                    17,
                    12,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-16T11:55:23Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    11,
                    55,
                    23,
                    1,
                    198,
                    0
                ],
                "title": "Scaling Diffusion Transformers to 16 Billion Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Diffusion Transformers to 16 Billion Parameters"
                },
                "summary": "In this paper, we present DiT-MoE, a sparse version of the diffusion\nTransformer, that is scalable and competitive with dense networks while\nexhibiting highly optimized inference. The DiT-MoE includes two simple designs:\nshared expert routing and expert-level balance loss, thereby capturing common\nknowledge and reducing redundancy among the different routed experts. When\napplied to conditional image generation, a deep analysis of experts\nspecialization gains some interesting observations: (i) Expert selection shows\npreference with spatial position and denoising time step, while insensitive\nwith different class-conditional information; (ii) As the MoE layers go deeper,\nthe selection of experts gradually shifts from specific spacial position to\ndispersion and balance. (iii) Expert specialization tends to be more\nconcentrated at the early time step and then gradually uniform after half. We\nattribute it to the diffusion process that first models the low-frequency\nspatial information and then high-frequency complex information. Based on the\nabove guidance, a series of DiT-MoE experimentally achieves performance on par\nwith dense networks yet requires much less computational load during inference.\nMore encouragingly, we demonstrate the potential of DiT-MoE with synthesized\nimage data, scaling diffusion model at a 16.5B parameter that attains a new\nSoTA FID-50K score of 1.80 in 512$\\times$512 resolution settings. The project\npage: https://github.com/feizc/DiT-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present DiT-MoE, a sparse version of the diffusion\nTransformer, that is scalable and competitive with dense networks while\nexhibiting highly optimized inference. The DiT-MoE includes two simple designs:\nshared expert routing and expert-level balance loss, thereby capturing common\nknowledge and reducing redundancy among the different routed experts. When\napplied to conditional image generation, a deep analysis of experts\nspecialization gains some interesting observations: (i) Expert selection shows\npreference with spatial position and denoising time step, while insensitive\nwith different class-conditional information; (ii) As the MoE layers go deeper,\nthe selection of experts gradually shifts from specific spacial position to\ndispersion and balance. (iii) Expert specialization tends to be more\nconcentrated at the early time step and then gradually uniform after half. We\nattribute it to the diffusion process that first models the low-frequency\nspatial information and then high-frequency complex information. Based on the\nabove guidance, a series of DiT-MoE experimentally achieves performance on par\nwith dense networks yet requires much less computational load during inference.\nMore encouragingly, we demonstrate the potential of DiT-MoE with synthesized\nimage data, scaling diffusion model at a 16.5B parameter that attains a new\nSoTA FID-50K score of 1.80 in 512$\\times$512 resolution settings. The project\npage: https://github.com/feizc/DiT-MoE."
                },
                "authors": [
                    {
                        "name": "Zhengcong Fei"
                    },
                    {
                        "name": "Mingyuan Fan"
                    },
                    {
                        "name": "Changqian Yu"
                    },
                    {
                        "name": "Debang Li"
                    },
                    {
                        "name": "Junshi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Junshi Huang"
                },
                "author": "Junshi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11633v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11633v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09562v2",
                "updated": "2024-09-06T05:19:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    5,
                    19,
                    5,
                    4,
                    250,
                    0
                ],
                "published": "2024-03-14T16:54:17Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    16,
                    54,
                    17,
                    3,
                    74,
                    0
                ],
                "title": "PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy\n  Traps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy\n  Traps"
                },
                "summary": "The pre-training and fine-tuning paradigm has demonstrated its effectiveness\nand has become the standard approach for tailoring language models to various\ntasks. Currently, community-based platforms offer easy access to various\npre-trained models, as anyone can publish without strict validation processes.\nHowever, a released pre-trained model can be a privacy trap for fine-tuning\ndatasets if it is carefully designed. In this work, we propose PreCurious\nframework to reveal the new attack surface where the attacker releases the\npre-trained model and gets a black-box access to the final fine-tuned model.\nPreCurious aims to escalate the general privacy risk of both membership\ninference and data extraction on the fine-tuning dataset. The key intuition\nbehind PreCurious is to manipulate the memorization stage of the pre-trained\nmodel and guide fine-tuning with a seemingly legitimate configuration. While\nempirical and theoretical evidence suggests that parameter-efficient and\ndifferentially private fine-tuning techniques can defend against privacy\nattacks on a fine-tuned model, PreCurious demonstrates the possibility of\nbreaking up this invulnerability in a stealthy manner compared to fine-tuning\non a benign pre-trained model. While DP provides some mitigation for membership\ninference attack, by further leveraging a sanitized dataset, PreCurious\ndemonstrates potential vulnerabilities for targeted data extraction even under\ndifferentially private tuning with a strict privacy budget e.g.\n$\\epsilon=0.05$. Thus, PreCurious raises warnings for users on the potential\nrisks of downloading pre-trained models from unknown sources, relying solely on\ntutorials or common-sense defenses, and releasing sanitized datasets even after\nperfect scrubbing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pre-training and fine-tuning paradigm has demonstrated its effectiveness\nand has become the standard approach for tailoring language models to various\ntasks. Currently, community-based platforms offer easy access to various\npre-trained models, as anyone can publish without strict validation processes.\nHowever, a released pre-trained model can be a privacy trap for fine-tuning\ndatasets if it is carefully designed. In this work, we propose PreCurious\nframework to reveal the new attack surface where the attacker releases the\npre-trained model and gets a black-box access to the final fine-tuned model.\nPreCurious aims to escalate the general privacy risk of both membership\ninference and data extraction on the fine-tuning dataset. The key intuition\nbehind PreCurious is to manipulate the memorization stage of the pre-trained\nmodel and guide fine-tuning with a seemingly legitimate configuration. While\nempirical and theoretical evidence suggests that parameter-efficient and\ndifferentially private fine-tuning techniques can defend against privacy\nattacks on a fine-tuned model, PreCurious demonstrates the possibility of\nbreaking up this invulnerability in a stealthy manner compared to fine-tuning\non a benign pre-trained model. While DP provides some mitigation for membership\ninference attack, by further leveraging a sanitized dataset, PreCurious\ndemonstrates potential vulnerabilities for targeted data extraction even under\ndifferentially private tuning with a strict privacy budget e.g.\n$\\epsilon=0.05$. Thus, PreCurious raises warnings for users on the potential\nrisks of downloading pre-trained models from unknown sources, relying solely on\ntutorials or common-sense defenses, and releasing sanitized datasets even after\nperfect scrubbing."
                },
                "authors": [
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Tianhao Wang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.04939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.04939v2",
                "updated": "2024-09-06T05:06:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    5,
                    6,
                    51,
                    4,
                    250,
                    0
                ],
                "published": "2023-11-08T01:45:37Z",
                "published_parsed": [
                    2023,
                    11,
                    8,
                    1,
                    45,
                    37,
                    2,
                    312,
                    0
                ],
                "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LooGLE: Can Long-Context Language Models Understand Long Contexts?"
                },
                "summary": "Large language models (LLMs), despite their impressive performance in various\nlanguage tasks, are typically limited to processing texts within context-window\nsize. This limitation has spurred significant research efforts to enhance LLMs'\nlong-context understanding with high-quality long-sequence benchmarks. However,\nprior datasets in this regard suffer from shortcomings, such as short context\nlength compared to the context window of modern LLMs; outdated documents that\nhave data leakage problems; and an emphasis on short dependency tasks rather\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\nGeneric Language Evaluation benchmark for LLMs' long context understanding.\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\ndocument and 6,000 newly generated questions spanning diverse domains. Human\nannotators meticulously crafted more than 1,100 high-quality question-answer\npairs to meet the long dependency requirements. These pairs underwent thorough\ncross-validation, yielding the most precise assessment of LLMs' long dependency\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\nexcelled in short dependency tasks like short question-answering and cloze\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\nlearning and chaining thoughts offered only marginal improvements; (iv)\nretrieval-based techniques demonstrated substantial benefits for short\nquestion-answering, while strategies for extending context window length had\nlimited impact on long context understanding. As such, LooGLE not only provides\na systematic and comprehensive evaluation schema on long-context LLMs, but also\nsheds light on future development of enhanced models towards \"true long-context\nunderstanding\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), despite their impressive performance in various\nlanguage tasks, are typically limited to processing texts within context-window\nsize. This limitation has spurred significant research efforts to enhance LLMs'\nlong-context understanding with high-quality long-sequence benchmarks. However,\nprior datasets in this regard suffer from shortcomings, such as short context\nlength compared to the context window of modern LLMs; outdated documents that\nhave data leakage problems; and an emphasis on short dependency tasks rather\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\nGeneric Language Evaluation benchmark for LLMs' long context understanding.\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\ndocument and 6,000 newly generated questions spanning diverse domains. Human\nannotators meticulously crafted more than 1,100 high-quality question-answer\npairs to meet the long dependency requirements. These pairs underwent thorough\ncross-validation, yielding the most precise assessment of LLMs' long dependency\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\nexcelled in short dependency tasks like short question-answering and cloze\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\nlearning and chaining thoughts offered only marginal improvements; (iv)\nretrieval-based techniques demonstrated substantial benefits for short\nquestion-answering, while strategies for extending context window length had\nlimited impact on long context understanding. As such, LooGLE not only provides\na systematic and comprehensive evaluation schema on long-context LLMs, but also\nsheds light on future development of enhanced models towards \"true long-context\nunderstanding\"."
                },
                "authors": [
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.04939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.04939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02834v2",
                "updated": "2024-09-06T05:06:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    5,
                    6,
                    27,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-04T16:00:21Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    0,
                    21,
                    2,
                    248,
                    0
                ],
                "title": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models"
                },
                "summary": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets."
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Ji Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02102v2",
                "updated": "2024-09-06T04:34:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    4,
                    34,
                    30,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-03T17:57:09Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    57,
                    9,
                    1,
                    247,
                    0
                ],
                "title": "Dimensionality Reduction Techniques for Statistical Inference in\n  Cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dimensionality Reduction Techniques for Statistical Inference in\n  Cosmology"
                },
                "summary": "We explore linear and non-linear dimensionality reduction techniques for\nstatistical inference of parameters in cosmology. Given the importance of\ncompressing the increasingly complex data vectors used in cosmology, we address\nquestions that impact the constraining power achieved, such as: Are currently\nused methods effectively lossless? Under what conditions do nonlinear methods,\ntypically based on neural nets, outperform linear methods? Through theoretical\nanalysis and experiments with simulated weak lensing data vectors we compare\nthree standard linear methods and neural network based methods. We propose two\nlinear methods that outperform all others while using less computational\nresources: a variation of the MOPED algorithm we call e-MOPED and an adaptation\nof Canonical Correlation Analysis (CCA), which is a method new to cosmology but\nwell known in statistics. Both e-MOPED and CCA utilize simulations spanning the\nfull parameter space, and rely on the sensitivity of the data vector to the\nparameters of interest. The gains we obtain are significant compared to\ncompression methods used in the literature: up to 30% in the Figure of Merit\nfor $\\Omega_m$ and $S_8$ in a realistic Simulation Based Inference analysis\nthat includes statistical and systematic errors. We also recommend two\nmodifications that improve the performance of all methods: First, include\ncomponents in the compressed data vector that may not target the key parameters\nbut still enhance the constraints on due to their correlations. The gain is\nsignificant, above 20% in the Figure of Merit. Second, compress Gaussian and\nnon-Gaussian statistics separately -- we include two summary statistics of each\ntype in our analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore linear and non-linear dimensionality reduction techniques for\nstatistical inference of parameters in cosmology. Given the importance of\ncompressing the increasingly complex data vectors used in cosmology, we address\nquestions that impact the constraining power achieved, such as: Are currently\nused methods effectively lossless? Under what conditions do nonlinear methods,\ntypically based on neural nets, outperform linear methods? Through theoretical\nanalysis and experiments with simulated weak lensing data vectors we compare\nthree standard linear methods and neural network based methods. We propose two\nlinear methods that outperform all others while using less computational\nresources: a variation of the MOPED algorithm we call e-MOPED and an adaptation\nof Canonical Correlation Analysis (CCA), which is a method new to cosmology but\nwell known in statistics. Both e-MOPED and CCA utilize simulations spanning the\nfull parameter space, and rely on the sensitivity of the data vector to the\nparameters of interest. The gains we obtain are significant compared to\ncompression methods used in the literature: up to 30% in the Figure of Merit\nfor $\\Omega_m$ and $S_8$ in a realistic Simulation Based Inference analysis\nthat includes statistical and systematic errors. We also recommend two\nmodifications that improve the performance of all methods: First, include\ncomponents in the compressed data vector that may not target the key parameters\nbut still enhance the constraints on due to their correlations. The gain is\nsignificant, above 20% in the Figure of Merit. Second, compress Gaussian and\nnon-Gaussian statistics separately -- we include two summary statistics of each\ntype in our analysis."
                },
                "authors": [
                    {
                        "name": "Minsu Park"
                    },
                    {
                        "name": "Marco Gatti"
                    },
                    {
                        "name": "Bhuvnesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvnesh Jain"
                },
                "author": "Bhuvnesh Jain",
                "arxiv_comment": "23 pages, 9 figures. Comments welcome. To be submitted to PRD.\n  Implementation examples in https://github.com/98minsu/CosmoCompression/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12787v2",
                "updated": "2024-09-06T04:30:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    4,
                    30,
                    50,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-23T01:37:29Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    37,
                    29,
                    4,
                    236,
                    0
                ],
                "title": "LLM-PBE: Assessing Data Privacy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PBE: Assessing Data Privacy in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment."
                },
                "authors": [
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Junyuan Hong"
                    },
                    {
                        "name": "Chulin Xie"
                    },
                    {
                        "name": "Jeffrey Tan"
                    },
                    {
                        "name": "Rachel Xin"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Xavier Yin"
                    },
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06429v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06429v2",
                "updated": "2024-09-06T04:01:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    4,
                    1,
                    37,
                    4,
                    250,
                    0
                ],
                "published": "2024-05-10T12:20:12Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    12,
                    20,
                    12,
                    4,
                    131,
                    0
                ],
                "title": "Probing orbits of stellar mass objects deep in galactic nuclei with\n  quasi-periodic eruptions -- II: population analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing orbits of stellar mass objects deep in galactic nuclei with\n  quasi-periodic eruptions -- II: population analysis"
                },
                "summary": "Quasi-periodic eruptions (QPEs) are intense repeating soft X-ray bursts with\nrecurrence times about a few hours to a few weeks from galactic nuclei. Though\nthe debates on the origin of QPEs have not completely settled down, more and\nmore analyses favor the interpretation that QPEs are the result of collisions\nbetween a stellar mass object (a stellar mass black hole or a main sequence\nstar) and an accretion disk around a supermassive black hole (SMBH) in galactic\nnuclei. If this interpretation is correct, QPEs will be invaluable in probing\nthe orbits of stellar mass objects in the vicinity of SMBHs, and further\ninferring the formation of extreme mass ratio inspirals (EMRIs), one of the\nmajor targets of spaceborne gravitational wave missions. In this work, we\nextended the EMRI orbital analysis in Paper I arXiv:2401.11190 to all the known\nQPE sources with more than $6$ flares observed. Among all the analyzed 5 QPE\nsources, two distinct EMRI populations are identified: 4 EMRIs are of low\norbital eccentricity (consistent with 0) which should be born in the wet EMRI\nformation channel, and 1 mildly eccentric EMRI (with $e= 0.25^{+0.18}_{-0.20}$\nat 2-$\\sigma$ confidence level) is consistent with the predictions of both the\ndry loss-cone formation channel and the Hills mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi-periodic eruptions (QPEs) are intense repeating soft X-ray bursts with\nrecurrence times about a few hours to a few weeks from galactic nuclei. Though\nthe debates on the origin of QPEs have not completely settled down, more and\nmore analyses favor the interpretation that QPEs are the result of collisions\nbetween a stellar mass object (a stellar mass black hole or a main sequence\nstar) and an accretion disk around a supermassive black hole (SMBH) in galactic\nnuclei. If this interpretation is correct, QPEs will be invaluable in probing\nthe orbits of stellar mass objects in the vicinity of SMBHs, and further\ninferring the formation of extreme mass ratio inspirals (EMRIs), one of the\nmajor targets of spaceborne gravitational wave missions. In this work, we\nextended the EMRI orbital analysis in Paper I arXiv:2401.11190 to all the known\nQPE sources with more than $6$ flares observed. Among all the analyzed 5 QPE\nsources, two distinct EMRI populations are identified: 4 EMRIs are of low\norbital eccentricity (consistent with 0) which should be born in the wet EMRI\nformation channel, and 1 mildly eccentric EMRI (with $e= 0.25^{+0.18}_{-0.20}$\nat 2-$\\sigma$ confidence level) is consistent with the predictions of both the\ndry loss-cone formation channel and the Hills mechanism."
                },
                "authors": [
                    {
                        "name": "Cong Zhou"
                    },
                    {
                        "name": "Binyu Zhong"
                    },
                    {
                        "name": "Yuhe Zeng"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Zhen Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Pan"
                },
                "author": "Zhen Pan",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06429v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06429v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14418v2",
                "updated": "2024-09-06T03:05:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    3,
                    5,
                    29,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-26T17:04:00Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues"
                },
                "summary": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization."
                },
                "authors": [
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Abhinav Ramesh Kashyap"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Andy T. Liu"
                    },
                    {
                        "name": "Vijay Prakash Dwivedi"
                    },
                    {
                        "name": "Thanh-Tung Nguyen"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02432v2",
                "updated": "2024-09-06T03:00:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    3,
                    0,
                    9,
                    4,
                    250,
                    0
                ],
                "published": "2023-12-05T02:17:48Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    2,
                    17,
                    48,
                    1,
                    339,
                    0
                ],
                "title": "Orthogonal Adaptation for Modular Customization of Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orthogonal Adaptation for Modular Customization of Diffusion Models"
                },
                "summary": "Customization techniques for text-to-image models have paved the way for a\nwide range of previously unattainable applications, enabling the generation of\nspecific concepts across diverse contexts and styles. While existing methods\nfacilitate high-fidelity customization for individual concepts or a limited,\npre-defined set of them, they fall short of achieving scalability, where a\nsingle model can seamlessly render countless concepts. In this paper, we\naddress a new problem called Modular Customization, with the goal of\nefficiently merging customized models that were fine-tuned independently for\nindividual concepts. This allows the merged model to jointly synthesize\nconcepts in one image without compromising fidelity or incurring any additional\ncomputational costs. To address this problem, we introduce Orthogonal\nAdaptation, a method designed to encourage the customized models, which do not\nhave access to each other during fine-tuning, to have orthogonal residual\nweights. This ensures that during inference time, the customized models can be\nsummed with minimal interference. Our proposed method is both simple and\nversatile, applicable to nearly all optimizable weights in the model\narchitecture. Through an extensive set of quantitative and qualitative\nevaluations, our method consistently outperforms relevant baselines in terms of\nefficiency and identity preservation, demonstrating a significant leap toward\nscalable customization of diffusion models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customization techniques for text-to-image models have paved the way for a\nwide range of previously unattainable applications, enabling the generation of\nspecific concepts across diverse contexts and styles. While existing methods\nfacilitate high-fidelity customization for individual concepts or a limited,\npre-defined set of them, they fall short of achieving scalability, where a\nsingle model can seamlessly render countless concepts. In this paper, we\naddress a new problem called Modular Customization, with the goal of\nefficiently merging customized models that were fine-tuned independently for\nindividual concepts. This allows the merged model to jointly synthesize\nconcepts in one image without compromising fidelity or incurring any additional\ncomputational costs. To address this problem, we introduce Orthogonal\nAdaptation, a method designed to encourage the customized models, which do not\nhave access to each other during fine-tuning, to have orthogonal residual\nweights. This ensures that during inference time, the customized models can be\nsummed with minimal interference. Our proposed method is both simple and\nversatile, applicable to nearly all optimizable weights in the model\narchitecture. Through an extensive set of quantitative and qualitative\nevaluations, our method consistently outperforms relevant baselines in terms of\nefficiency and identity preservation, demonstrating a significant leap toward\nscalable customization of diffusion models."
                },
                "authors": [
                    {
                        "name": "Ryan Po"
                    },
                    {
                        "name": "Guandao Yang"
                    },
                    {
                        "name": "Kfir Aberman"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Wetzstein"
                },
                "author": "Gordon Wetzstein",
                "arxiv_comment": "Project page: https://ryanpo.com/ortha/; Hugging Face Demo:\n  https://huggingface.co/spaces/ujin-song/ortha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03992v1",
                "updated": "2024-09-06T02:44:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    44,
                    27,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T02:44:27Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    44,
                    27,
                    4,
                    250,
                    0
                ],
                "title": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study"
                },
                "summary": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on NVIDIA H100 GPUs for large language model (LLM) inference\ntasks. We benchmark the overhead introduced by TEE mode across various models\nand token lengths, focusing on the bottleneck caused by CPU-GPU data transfers\nvia PCIe. Our results show that while there is minimal computational overhead\nwithin the GPU, the overall performance penalty is primarily due to data\ntransfer. For most typical LLM queries, the overhead remains below 5%, with\nlarger models and longer sequences experiencing near-zero overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on NVIDIA H100 GPUs for large language model (LLM) inference\ntasks. We benchmark the overhead introduced by TEE mode across various models\nand token lengths, focusing on the bottleneck caused by CPU-GPU data transfers\nvia PCIe. Our results show that while there is minimal computational overhead\nwithin the GPU, the overall performance penalty is primarily due to data\ntransfer. For most typical LLM queries, the overhead remains below 5%, with\nlarger models and longer sequences experiencing near-zero overhead."
                },
                "authors": [
                    {
                        "name": "Jianwei Zhu"
                    },
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Shunfan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shunfan Zhou"
                },
                "author": "Shunfan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20234v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20234v3",
                "updated": "2024-09-06T02:41:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    41,
                    35,
                    4,
                    250,
                    0
                ],
                "published": "2024-05-30T16:36:47Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    16,
                    36,
                    47,
                    3,
                    151,
                    0
                ],
                "title": "Hidden in Plain Sight: Exploring Chat History Tampering in Interactive\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden in Plain Sight: Exploring Chat History Tampering in Interactive\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent\nin real-world applications, exhibiting impressive text generation performance.\nLLMs are fundamentally developed from a scenario where the input data remains\nstatic and unstructured. To behave interactively, LLM-based chat systems must\nintegrate prior chat history as context into their inputs, following a\npre-defined structure. However, LLMs cannot separate user inputs from context,\nenabling chat history tampering. This paper introduces a systematic methodology\nto inject user-supplied history into LLM conversations without any prior\nknowledge of the target model. The key is to utilize prompt templates that can\nwell organize the messages to be injected, leading the target LLM to interpret\nthem as genuine chat history. To automatically search for effective templates\nin a WebUI black-box setting, we propose the LLM-Guided Genetic Algorithm\n(LLMGA) that leverages an LLM to generate and iteratively optimize the\ntemplates. We apply the proposed method to popular real-world LLMs including\nChatGPT and Llama-2/3. The results show that chat history tampering can enhance\nthe malleability of the model's behavior over time and greatly influence the\nmodel output. For example, it can improve the success rate of disallowed\nresponse elicitation up to 97% on ChatGPT. Our findings provide insights into\nthe challenges associated with the real-world deployment of interactive LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent\nin real-world applications, exhibiting impressive text generation performance.\nLLMs are fundamentally developed from a scenario where the input data remains\nstatic and unstructured. To behave interactively, LLM-based chat systems must\nintegrate prior chat history as context into their inputs, following a\npre-defined structure. However, LLMs cannot separate user inputs from context,\nenabling chat history tampering. This paper introduces a systematic methodology\nto inject user-supplied history into LLM conversations without any prior\nknowledge of the target model. The key is to utilize prompt templates that can\nwell organize the messages to be injected, leading the target LLM to interpret\nthem as genuine chat history. To automatically search for effective templates\nin a WebUI black-box setting, we propose the LLM-Guided Genetic Algorithm\n(LLMGA) that leverages an LLM to generate and iteratively optimize the\ntemplates. We apply the proposed method to popular real-world LLMs including\nChatGPT and Llama-2/3. The results show that chat history tampering can enhance\nthe malleability of the model's behavior over time and greatly influence the\nmodel output. For example, it can improve the success rate of disallowed\nresponse elicitation up to 97% on ChatGPT. Our findings provide insights into\nthe challenges associated with the real-world deployment of interactive LLMs."
                },
                "authors": [
                    {
                        "name": "Cheng'an Wei"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yujia Gong"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Lu Xiang"
                    },
                    {
                        "name": "Shenchen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Shenchen Zhu"
                },
                "author": "Shenchen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20234v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20234v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07482v2",
                "updated": "2024-09-06T02:38:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    38,
                    40,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-14T11:55:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    55,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice."
                },
                "authors": [
                    {
                        "name": "Ning Lu"
                    },
                    {
                        "name": "Qian Xie"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wenyi Fang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zheng Hu"
                    },
                    {
                        "name": "Jiantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Ma"
                },
                "author": "Jiantao Ma",
                "arxiv_comment": "To be published in: IEEE International Symposium on Software\n  Reliability Engineering (ISSRE2024) workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05572v2",
                "updated": "2024-09-06T02:13:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    13,
                    51,
                    4,
                    250,
                    0
                ],
                "published": "2024-06-08T20:56:14Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    20,
                    56,
                    14,
                    5,
                    160,
                    0
                ],
                "title": "Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and\n  Constraint Satisfaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and\n  Constraint Satisfaction"
                },
                "summary": "Recent developments in pretrained large language models (LLMs) applied to\nrobotics have demonstrated their capacity for sequencing a set of discrete\nskills to achieve open-ended goals in simple robotic tasks. In this paper, we\nexamine the topic of LLM planning for a set of continuously parameterized\nskills whose execution must avoid violations of a set of kinematic, geometric,\nand physical constraints. We prompt the LLM to output code for a function with\nopen parameters, which, together with environmental constraints, can be viewed\nas a Continuous Constraint Satisfaction Problem (CCSP). This CCSP can be solved\nthrough sampling or optimization to find a skill sequence and continuous\nparameter settings that achieve the goal while avoiding constraint violations.\nAdditionally, we consider cases where the LLM proposes unsatisfiable CCSPs,\nsuch as those that are kinematically infeasible, dynamically unstable, or lead\nto collisions, and re-prompt the LLM to form a new CCSP accordingly.\nExperiments across three different simulated 3D domains demonstrate that our\nproposed strategy, PRoC3S, is capable of solving a wide range of complex\nmanipulation tasks with realistic constraints on continuous parameters much\nmore efficiently and effectively than existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in pretrained large language models (LLMs) applied to\nrobotics have demonstrated their capacity for sequencing a set of discrete\nskills to achieve open-ended goals in simple robotic tasks. In this paper, we\nexamine the topic of LLM planning for a set of continuously parameterized\nskills whose execution must avoid violations of a set of kinematic, geometric,\nand physical constraints. We prompt the LLM to output code for a function with\nopen parameters, which, together with environmental constraints, can be viewed\nas a Continuous Constraint Satisfaction Problem (CCSP). This CCSP can be solved\nthrough sampling or optimization to find a skill sequence and continuous\nparameter settings that achieve the goal while avoiding constraint violations.\nAdditionally, we consider cases where the LLM proposes unsatisfiable CCSPs,\nsuch as those that are kinematically infeasible, dynamically unstable, or lead\nto collisions, and re-prompt the LLM to form a new CCSP accordingly.\nExperiments across three different simulated 3D domains demonstrate that our\nproposed strategy, PRoC3S, is capable of solving a wide range of complex\nmanipulation tasks with realistic constraints on continuous parameters much\nmore efficiently and effectively than existing baselines."
                },
                "authors": [
                    {
                        "name": "Aidan Curtis"
                    },
                    {
                        "name": "Nishanth Kumar"
                    },
                    {
                        "name": "Jing Cao"
                    },
                    {
                        "name": "Tomás Lozano-Pérez"
                    },
                    {
                        "name": "Leslie Pack Kaelbling"
                    }
                ],
                "author_detail": {
                    "name": "Leslie Pack Kaelbling"
                },
                "author": "Leslie Pack Kaelbling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03980v1",
                "updated": "2024-09-06T02:01:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    1,
                    3,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T02:01:03Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    1,
                    3,
                    4,
                    250,
                    0
                ],
                "title": "Entry-Specific Matrix Estimation under Arbitrary Sampling Patterns\n  through the Lens of Network Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entry-Specific Matrix Estimation under Arbitrary Sampling Patterns\n  through the Lens of Network Flows"
                },
                "summary": "Matrix completion tackles the task of predicting missing values in a low-rank\nmatrix based on a sparse set of observed entries. It is often assumed that the\nobservation pattern is generated uniformly at random or has a very specific\nstructure tuned to a given algorithm. There is still a gap in our understanding\nwhen it comes to arbitrary sampling patterns. Given an arbitrary sampling\npattern, we introduce a matrix completion algorithm based on network flows in\nthe bipartite graph induced by the observation pattern. For additive matrices,\nthe particular flow we used is the electrical flow and we establish error upper\nbounds customized to each entry as a function of the observation set, along\nwith matching minimax lower bounds. Our results show that the minimax squared\nerror for recovery of a particular entry in the matrix is proportional to the\neffective resistance of the corresponding edge in the graph. Furthermore, we\nshow that our estimator is equivalent to the least squares estimator. We apply\nour estimator to the two-way fixed effects model and show that it enables us to\naccurately infer individual causal effects and the unit-specific and\ntime-specific confounders. For rank-$1$ matrices, we use edge-disjoint paths to\nform an estimator that achieves minimax optimal estimation when the sampling is\nsufficiently dense. Our discovery introduces a new family of estimators\nparametrized by network flows, which provide a fine-grained and intuitive\nunderstanding of the impact of the given sampling pattern on the relative\ndifficulty of estimation at an entry-specific level. This graph-based approach\nallows us to quantify the inherent complexity of matrix completion for\nindividual entries, rather than relying solely on global measures of\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix completion tackles the task of predicting missing values in a low-rank\nmatrix based on a sparse set of observed entries. It is often assumed that the\nobservation pattern is generated uniformly at random or has a very specific\nstructure tuned to a given algorithm. There is still a gap in our understanding\nwhen it comes to arbitrary sampling patterns. Given an arbitrary sampling\npattern, we introduce a matrix completion algorithm based on network flows in\nthe bipartite graph induced by the observation pattern. For additive matrices,\nthe particular flow we used is the electrical flow and we establish error upper\nbounds customized to each entry as a function of the observation set, along\nwith matching minimax lower bounds. Our results show that the minimax squared\nerror for recovery of a particular entry in the matrix is proportional to the\neffective resistance of the corresponding edge in the graph. Furthermore, we\nshow that our estimator is equivalent to the least squares estimator. We apply\nour estimator to the two-way fixed effects model and show that it enables us to\naccurately infer individual causal effects and the unit-specific and\ntime-specific confounders. For rank-$1$ matrices, we use edge-disjoint paths to\nform an estimator that achieves minimax optimal estimation when the sampling is\nsufficiently dense. Our discovery introduces a new family of estimators\nparametrized by network flows, which provide a fine-grained and intuitive\nunderstanding of the impact of the given sampling pattern on the relative\ndifficulty of estimation at an entry-specific level. This graph-based approach\nallows us to quantify the inherent complexity of matrix completion for\nindividual entries, rather than relying solely on global measures of\nperformance."
                },
                "authors": [
                    {
                        "name": "Yudong Chen"
                    },
                    {
                        "name": "Xumei Xi"
                    },
                    {
                        "name": "Christina Lee Yu"
                    }
                ],
                "author_detail": {
                    "name": "Christina Lee Yu"
                },
                "author": "Christina Lee Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03979v1",
                "updated": "2024-09-06T01:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    59,
                    29,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T01:59:29Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    59,
                    29,
                    4,
                    250,
                    0
                ],
                "title": "Extreme Quantile Treatment Effects under Endogeneity: Evaluating Policy\n  Effects for the Most Vulnerable Individuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Quantile Treatment Effects under Endogeneity: Evaluating Policy\n  Effects for the Most Vulnerable Individuals"
                },
                "summary": "We introduce a novel method for estimating and conducting inference about\nextreme quantile treatment effects (QTEs) in the presence of endogeneity. Our\napproach is applicable to a broad range of empirical research designs,\nincluding instrumental variables design and regression discontinuity design,\namong others. By leveraging regular variation and subsampling, the method\nensures robust performance even in extreme tails, where data may be sparse or\nentirely absent. Simulation studies confirm the theoretical robustness of our\napproach. Applying our method to assess the impact of job training provided by\nthe Job Training Partnership Act (JTPA), we find significantly negative QTEs\nfor the lowest quantiles (i.e., the most disadvantaged individuals),\ncontrasting with previous literature that emphasizes positive QTEs for\nintermediate quantiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel method for estimating and conducting inference about\nextreme quantile treatment effects (QTEs) in the presence of endogeneity. Our\napproach is applicable to a broad range of empirical research designs,\nincluding instrumental variables design and regression discontinuity design,\namong others. By leveraging regular variation and subsampling, the method\nensures robust performance even in extreme tails, where data may be sparse or\nentirely absent. Simulation studies confirm the theoretical robustness of our\napproach. Applying our method to assess the impact of job training provided by\nthe Job Training Partnership Act (JTPA), we find significantly negative QTEs\nfor the lowest quantiles (i.e., the most disadvantaged individuals),\ncontrasting with previous literature that emphasizes positive QTEs for\nintermediate quantiles."
                },
                "authors": [
                    {
                        "name": "Yuya Sasaki"
                    },
                    {
                        "name": "Yulong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Wang"
                },
                "author": "Yulong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04298v3",
                "updated": "2024-09-06T01:14:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    14,
                    26,
                    4,
                    250,
                    0
                ],
                "published": "2024-04-04T20:27:37Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    20,
                    27,
                    37,
                    3,
                    95,
                    0
                ],
                "title": "SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated\n  Responses"
                },
                "summary": "Can LLMs consistently improve their previous outputs for better results? For\nthis to be true, LLMs would need to be better at discriminating among\npreviously-generated alternatives, than generating initial responses. We\nexplore the validity of this hypothesis in practice. We first formulate a\nunified framework that allows us to compare the generative and discriminative\ncapability of any model on any task. In our resulting experimental analysis of\nseveral open-source and industrial LLMs, we observe that models are not\nreliably better at discriminating among previously-generated alternatives than\ngenerating initial responses. This finding challenges the notion that LLMs may\nbe able to enhance their performance only through their own judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs consistently improve their previous outputs for better results? For\nthis to be true, LLMs would need to be better at discriminating among\npreviously-generated alternatives, than generating initial responses. We\nexplore the validity of this hypothesis in practice. We first formulate a\nunified framework that allows us to compare the generative and discriminative\ncapability of any model on any task. In our resulting experimental analysis of\nseveral open-source and industrial LLMs, we observe that models are not\nreliably better at discriminating among previously-generated alternatives than\ngenerating initial responses. This finding challenges the notion that LLMs may\nbe able to enhance their performance only through their own judgment."
                },
                "authors": [
                    {
                        "name": "Dongwei Jiang"
                    },
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Nathaniel Weir"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03963v1",
                "updated": "2024-09-06T01:08:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    8,
                    58,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T01:08:58Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    8,
                    58,
                    4,
                    250,
                    0
                ],
                "title": "The Arizona Molecular ISM Survey with the SMT: Variations in the\n  CO(2-1)/CO(1-0) Line Ratio Across the Galaxy Population",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Arizona Molecular ISM Survey with the SMT: Variations in the\n  CO(2-1)/CO(1-0) Line Ratio Across the Galaxy Population"
                },
                "summary": "The J=1$\\rightarrow$0 spectral line of carbon monoxide (CO(1-0)) is the\ncanonical tracer of molecular gas. However, CO(2-1) is frequently used in its\nplace, following the assumption that the higher energy line can be used to\ninfer the CO(1-0) luminosity and molecular gas mass. The use of CO(2-1) depends\non a knowledge of the ratio between CO(2-1) and CO(1-0) luminosities, r21. Here\nwe present galaxy-integrated r21 measurements for 122 galaxies spanning stellar\nmasses from 10$^9$ to 10$^{11.5}$ M$_\\odot$ and star formation rates (SFRs)\nfrom 0.08 to 35 M$_\\odot$/yr. We find strong trends between r21 and SFR, SFR\nsurface density, star formation efficiency, and distance from the star\nformation main sequence (SFMS). We show that the assumption of a constant r21\ncan introduce biases into the molecular gas trends in galaxy population studies\nand demonstrate how this affects the recovery of important galaxy scaling\nrelations, including the Kennicutt-Schmidt law and the relation between SFMS\noffset and star formation efficiency. We provide a prescription which accounts\nfor variations in r21 as a function of SFR and can be used to convert between\nCO(2-1) and CO(1-0) when only one line is available. Our prescription matches\nvariations in r21 for both AMISS and literature samples and can be used to\nderive more accurate gas masses from CO(2-1) observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The J=1$\\rightarrow$0 spectral line of carbon monoxide (CO(1-0)) is the\ncanonical tracer of molecular gas. However, CO(2-1) is frequently used in its\nplace, following the assumption that the higher energy line can be used to\ninfer the CO(1-0) luminosity and molecular gas mass. The use of CO(2-1) depends\non a knowledge of the ratio between CO(2-1) and CO(1-0) luminosities, r21. Here\nwe present galaxy-integrated r21 measurements for 122 galaxies spanning stellar\nmasses from 10$^9$ to 10$^{11.5}$ M$_\\odot$ and star formation rates (SFRs)\nfrom 0.08 to 35 M$_\\odot$/yr. We find strong trends between r21 and SFR, SFR\nsurface density, star formation efficiency, and distance from the star\nformation main sequence (SFMS). We show that the assumption of a constant r21\ncan introduce biases into the molecular gas trends in galaxy population studies\nand demonstrate how this affects the recovery of important galaxy scaling\nrelations, including the Kennicutt-Schmidt law and the relation between SFMS\noffset and star formation efficiency. We provide a prescription which accounts\nfor variations in r21 as a function of SFR and can be used to convert between\nCO(2-1) and CO(1-0) when only one line is available. Our prescription matches\nvariations in r21 for both AMISS and literature samples and can be used to\nderive more accurate gas masses from CO(2-1) observations."
                },
                "authors": [
                    {
                        "name": "Ryan P. Keenan"
                    },
                    {
                        "name": "Daniel P. Marrone"
                    },
                    {
                        "name": "Garrett K. Keating"
                    }
                ],
                "author_detail": {
                    "name": "Garrett K. Keating"
                },
                "author": "Garrett K. Keating",
                "arxiv_comment": "Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03962v1",
                "updated": "2024-09-06T01:07:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    7,
                    29,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T01:07:29Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    7,
                    29,
                    4,
                    250,
                    0
                ],
                "title": "Average Causal Effect Estimation in DAGs with Hidden Variables:\n  Extensions of Back-Door and Front-Door Criteria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Average Causal Effect Estimation in DAGs with Hidden Variables:\n  Extensions of Back-Door and Front-Door Criteria"
                },
                "summary": "The identification theory for causal effects in directed acyclic graphs\n(DAGs) with hidden variables is well-developed, but methods for estimating and\ninferring functionals beyond the g-formula remain limited. Previous studies\nhave proposed semiparametric estimators for identifiable functionals in a broad\nclass of DAGs with hidden variables. While demonstrating double robustness in\nsome models, existing estimators face challenges, particularly with density\nestimation and numerical integration for continuous variables, and their\nestimates may fall outside the parameter space of the target estimand. Their\nasymptotic properties are also underexplored, especially when using flexible\nstatistical and machine learning models for nuisance estimation. This study\naddresses these challenges by introducing novel one-step corrected plug-in and\ntargeted minimum loss-based estimators of causal effects for a class of DAGs\nthat extend classical back-door and front-door criteria (known as the treatment\nprimal fixability criterion in prior literature). These estimators leverage\nmachine learning to minimize modeling assumptions while ensuring key\nstatistical properties such as asymptotic linearity, double robustness,\nefficiency, and staying within the bounds of the target parameter space. We\nestablish conditions for nuisance functional estimates in terms of L2(P)-norms\nto achieve root-n consistent causal effect estimates. To facilitate practical\napplication, we have developed the flexCausal package in R.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The identification theory for causal effects in directed acyclic graphs\n(DAGs) with hidden variables is well-developed, but methods for estimating and\ninferring functionals beyond the g-formula remain limited. Previous studies\nhave proposed semiparametric estimators for identifiable functionals in a broad\nclass of DAGs with hidden variables. While demonstrating double robustness in\nsome models, existing estimators face challenges, particularly with density\nestimation and numerical integration for continuous variables, and their\nestimates may fall outside the parameter space of the target estimand. Their\nasymptotic properties are also underexplored, especially when using flexible\nstatistical and machine learning models for nuisance estimation. This study\naddresses these challenges by introducing novel one-step corrected plug-in and\ntargeted minimum loss-based estimators of causal effects for a class of DAGs\nthat extend classical back-door and front-door criteria (known as the treatment\nprimal fixability criterion in prior literature). These estimators leverage\nmachine learning to minimize modeling assumptions while ensuring key\nstatistical properties such as asymptotic linearity, double robustness,\nefficiency, and staying within the bounds of the target parameter space. We\nestablish conditions for nuisance functional estimates in terms of L2(P)-norms\nto achieve root-n consistent causal effect estimates. To facilitate practical\napplication, we have developed the flexCausal package in R."
                },
                "authors": [
                    {
                        "name": "Anna Guo"
                    },
                    {
                        "name": "Razieh Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Razieh Nabi"
                },
                "author": "Razieh Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09932v2",
                "updated": "2024-09-06T00:46:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    46,
                    40,
                    4,
                    250,
                    0
                ],
                "published": "2024-04-15T16:58:28Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    16,
                    58,
                    28,
                    0,
                    106,
                    0
                ],
                "title": "Foundational Challenges in Assuring Alignment and Safety of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Challenges in Assuring Alignment and Safety of Large\n  Language Models"
                },
                "summary": "This work identifies 18 foundational challenges in assuring the alignment and\nsafety of large language models (LLMs). These challenges are organized into\nthree different categories: scientific understanding of LLMs, development and\ndeployment methods, and sociotechnical challenges. Based on the identified\nchallenges, we pose $200+$ concrete research questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies 18 foundational challenges in assuring the alignment and\nsafety of large language models (LLMs). These challenges are organized into\nthree different categories: scientific understanding of LLMs, development and\ndeployment methods, and sociotechnical challenges. Based on the identified\nchallenges, we pose $200+$ concrete research questions."
                },
                "authors": [
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Abulhair Saparov"
                    },
                    {
                        "name": "Javier Rando"
                    },
                    {
                        "name": "Daniel Paleka"
                    },
                    {
                        "name": "Miles Turpin"
                    },
                    {
                        "name": "Peter Hase"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Erik Jenner"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Oliver Sourbut"
                    },
                    {
                        "name": "Benjamin L. Edelman"
                    },
                    {
                        "name": "Zhaowei Zhang"
                    },
                    {
                        "name": "Mario Günther"
                    },
                    {
                        "name": "Anton Korinek"
                    },
                    {
                        "name": "Jose Hernandez-Orallo"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Eric Bigelow"
                    },
                    {
                        "name": "Alexander Pan"
                    },
                    {
                        "name": "Lauro Langosco"
                    },
                    {
                        "name": "Tomasz Korbak"
                    },
                    {
                        "name": "Heidi Zhang"
                    },
                    {
                        "name": "Ruiqi Zhong"
                    },
                    {
                        "name": "Seán Ó hÉigeartaigh"
                    },
                    {
                        "name": "Gabriel Recchia"
                    },
                    {
                        "name": "Giulio Corsi"
                    },
                    {
                        "name": "Alan Chan"
                    },
                    {
                        "name": "Markus Anderljung"
                    },
                    {
                        "name": "Lilian Edwards"
                    },
                    {
                        "name": "Aleksandar Petrov"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Sumeet Ramesh Motwan"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Tegan Maharaj"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Florian Tramer"
                    },
                    {
                        "name": "He He"
                    },
                    {
                        "name": "Atoosa Kasirzadeh"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "David Krueger"
                    }
                ],
                "author_detail": {
                    "name": "David Krueger"
                },
                "author": "David Krueger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18148v2",
                "updated": "2024-09-06T00:02:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    2,
                    52,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-25T15:58:56Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    58,
                    56,
                    3,
                    207,
                    0
                ],
                "title": "StraightLine: An End-to-End Resource-Aware Scheduler for Machine\n  Learning Application Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StraightLine: An End-to-End Resource-Aware Scheduler for Machine\n  Learning Application Requests"
                },
                "summary": "The life cycle of machine learning (ML) applications consists of two stages:\nmodel development and model deployment. However, traditional ML systems (e.g.,\ntraining-specific or inference-specific systems) focus on one particular stage\nor phase of the life cycle of ML applications. These systems often aim at\noptimizing model training or accelerating model inference, and they frequently\nassume homogeneous infrastructure, which may not always reflect real-world\nscenarios that include cloud data centers, local servers, containers, and\nserverless platforms. We present StraightLine, an end-to-end resource-aware\nscheduler that schedules the optimal resources (e.g., container, virtual\nmachine, or serverless) for different ML application requests in a hybrid\ninfrastructure. The key innovation is an empirical dynamic placing algorithm\nthat intelligently places requests based on their unique characteristics (e.g.,\nrequest frequency, input data size, and data distribution). In contrast to\nexisting ML systems, StraightLine offers end-to-end resource-aware placement,\nthereby it can significantly reduce response time and failure rate for model\ndeployment when facing different computing resources in the hybrid\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The life cycle of machine learning (ML) applications consists of two stages:\nmodel development and model deployment. However, traditional ML systems (e.g.,\ntraining-specific or inference-specific systems) focus on one particular stage\nor phase of the life cycle of ML applications. These systems often aim at\noptimizing model training or accelerating model inference, and they frequently\nassume homogeneous infrastructure, which may not always reflect real-world\nscenarios that include cloud data centers, local servers, containers, and\nserverless platforms. We present StraightLine, an end-to-end resource-aware\nscheduler that schedules the optimal resources (e.g., container, virtual\nmachine, or serverless) for different ML application requests in a hybrid\ninfrastructure. The key innovation is an empirical dynamic placing algorithm\nthat intelligently places requests based on their unique characteristics (e.g.,\nrequest frequency, input data size, and data distribution). In contrast to\nexisting ML systems, StraightLine offers end-to-end resource-aware placement,\nthereby it can significantly reduce response time and failure rate for model\ndeployment when facing different computing resources in the hybrid\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Cheng-Wei Ching"
                    },
                    {
                        "name": "Boyuan Guan"
                    },
                    {
                        "name": "Hailu Xu"
                    },
                    {
                        "name": "Liting Hu"
                    }
                ],
                "author_detail": {
                    "name": "Liting Hu"
                },
                "author": "Liting Hu",
                "arxiv_comment": "6 pages, 8 figures, to appear in AIoTC'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03946v1",
                "updated": "2024-09-06T00:02:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    2,
                    9,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T00:02:09Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    2,
                    9,
                    4,
                    250,
                    0
                ],
                "title": "On The Role of Prompt Construction In Enhancing Efficacy and Efficiency\n  of LLM-Based Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Role of Prompt Construction In Enhancing Efficacy and Efficiency\n  of LLM-Based Tabular Data Generation"
                },
                "summary": "LLM-based data generation for real-world tabular data can be challenged by\nthe lack of sufficient semantic context in feature names used to describe\ncolumns. We hypothesize that enriching prompts with domain-specific insights\ncan improve both the quality and efficiency of data generation. To test this\nhypothesis, we explore three prompt construction protocols: Expert-guided,\nLLM-guided, and Novel-Mapping. Through empirical studies with the recently\nproposed GReaT framework, we find that context-enriched prompts lead to\nsignificantly improved data generation quality and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based data generation for real-world tabular data can be challenged by\nthe lack of sufficient semantic context in feature names used to describe\ncolumns. We hypothesize that enriching prompts with domain-specific insights\ncan improve both the quality and efficiency of data generation. To test this\nhypothesis, we explore three prompt construction protocols: Expert-guided,\nLLM-guided, and Novel-Mapping. Through empirical studies with the recently\nproposed GReaT framework, we find that context-enriched prompts lead to\nsignificantly improved data generation quality and training efficiency."
                },
                "authors": [
                    {
                        "name": "Banooqa Banday"
                    },
                    {
                        "name": "Kowshik Thopalli"
                    },
                    {
                        "name": "Tanzima Z. Islam"
                    },
                    {
                        "name": "Jayaraman J. Thiagarajan"
                    }
                ],
                "author_detail": {
                    "name": "Jayaraman J. Thiagarajan"
                },
                "author": "Jayaraman J. Thiagarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16966v2",
                "updated": "2024-09-05T23:18:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    18,
                    0,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-30T01:56:57Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    56,
                    57,
                    4,
                    243,
                    0
                ],
                "title": "UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Neo Wu"
                    },
                    {
                        "name": "Lin Ning"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Shawn O'Banion"
                    },
                    {
                        "name": "Bradley Green"
                    }
                ],
                "author_detail": {
                    "name": "Bradley Green"
                },
                "author": "Bradley Green",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03939v1",
                "updated": "2024-09-05T23:17:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    17,
                    18,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T23:17:18Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    17,
                    18,
                    3,
                    249,
                    0
                ],
                "title": "Experimentation in Content Moderation using RWKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimentation in Content Moderation using RWKV"
                },
                "summary": "This paper investigates the RWKV model's efficacy in content moderation\nthrough targeted experimentation. We introduce a novel dataset specifically\ndesigned for distillation into smaller models, enhancing content moderation\npractices. This comprehensive dataset encompasses images, videos, sounds, and\ntext data that present societal challenges. Leveraging advanced Large Language\nModels (LLMs), we generated an extensive set of responses -- 558,958 for text\nand 83,625 for images -- to train and refine content moderation systems. Our\ncore experimentation involved fine-tuning the RWKV model, capitalizing on its\nCPU-efficient architecture to address large-scale content moderation tasks. By\nhighlighting the dataset's potential for knowledge distillation, this study not\nonly demonstrates RWKV's capability in improving the accuracy and efficiency of\ncontent moderation systems but also paves the way for developing more compact,\nresource-efficient models in this domain. Datasets and models can be found in\nHuggingFace: https://huggingface.co/modrwkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the RWKV model's efficacy in content moderation\nthrough targeted experimentation. We introduce a novel dataset specifically\ndesigned for distillation into smaller models, enhancing content moderation\npractices. This comprehensive dataset encompasses images, videos, sounds, and\ntext data that present societal challenges. Leveraging advanced Large Language\nModels (LLMs), we generated an extensive set of responses -- 558,958 for text\nand 83,625 for images -- to train and refine content moderation systems. Our\ncore experimentation involved fine-tuning the RWKV model, capitalizing on its\nCPU-efficient architecture to address large-scale content moderation tasks. By\nhighlighting the dataset's potential for knowledge distillation, this study not\nonly demonstrates RWKV's capability in improving the accuracy and efficiency of\ncontent moderation systems but also paves the way for developing more compact,\nresource-efficient models in this domain. Datasets and models can be found in\nHuggingFace: https://huggingface.co/modrwkv"
                },
                "authors": [
                    {
                        "name": "Umut Yildirim"
                    },
                    {
                        "name": "Rohan Dutta"
                    },
                    {
                        "name": "Burak Yildirim"
                    },
                    {
                        "name": "Atharva Vaidya"
                    }
                ],
                "author_detail": {
                    "name": "Atharva Vaidya"
                },
                "author": "Atharva Vaidya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03938v1",
                "updated": "2024-09-05T23:07:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    7,
                    21,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T23:07:21Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    7,
                    21,
                    3,
                    249,
                    0
                ],
                "title": "Deep Clustering of Remote Sensing Scenes through Heterogeneous Transfer\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Clustering of Remote Sensing Scenes through Heterogeneous Transfer\n  Learning"
                },
                "summary": "This paper proposes a method for unsupervised whole-image clustering of a\ntarget dataset of remote sensing scenes with no labels. The method consists of\nthree main steps: (1) finetuning a pretrained deep neural network (DINOv2) on a\nlabelled source remote sensing imagery dataset and using it to extract a\nfeature vector from each image in the target dataset, (2) reducing the\ndimension of these deep features via manifold projection into a low-dimensional\nEuclidean space, and (3) clustering the embedded features using a Bayesian\nnonparametric technique to infer the number and membership of clusters\nsimultaneously. The method takes advantage of heterogeneous transfer learning\nto cluster unseen data with different feature and label distributions. We\ndemonstrate the performance of this approach outperforming state-of-the-art\nzero-shot classification methods on several remote sensing scene classification\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a method for unsupervised whole-image clustering of a\ntarget dataset of remote sensing scenes with no labels. The method consists of\nthree main steps: (1) finetuning a pretrained deep neural network (DINOv2) on a\nlabelled source remote sensing imagery dataset and using it to extract a\nfeature vector from each image in the target dataset, (2) reducing the\ndimension of these deep features via manifold projection into a low-dimensional\nEuclidean space, and (3) clustering the embedded features using a Bayesian\nnonparametric technique to infer the number and membership of clusters\nsimultaneously. The method takes advantage of heterogeneous transfer learning\nto cluster unseen data with different feature and label distributions. We\ndemonstrate the performance of this approach outperforming state-of-the-art\nzero-shot classification methods on several remote sensing scene classification\ndatasets."
                },
                "authors": [
                    {
                        "name": "Isaac Ray"
                    },
                    {
                        "name": "Alexei Skurikhin"
                    }
                ],
                "author_detail": {
                    "name": "Alexei Skurikhin"
                },
                "author": "Alexei Skurikhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03937v1",
                "updated": "2024-09-05T23:04:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    4,
                    28,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T23:04:28Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    4,
                    28,
                    3,
                    249,
                    0
                ],
                "title": "Harnessing LLMs for Cross-City OD Flow Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing LLMs for Cross-City OD Flow Prediction"
                },
                "summary": "Understanding and predicting Origin-Destination (OD) flows is crucial for\nurban planning and transportation management. Traditional OD prediction models,\nwhile effective within single cities, often face limitations when applied\nacross different cities due to varied traffic conditions, urban layouts, and\nsocio-economic factors. In this paper, by employing Large Language Models\n(LLMs), we introduce a new method for cross-city OD flow prediction. Our\napproach leverages the advanced semantic understanding and contextual learning\ncapabilities of LLMs to bridge the gap between cities with different\ncharacteristics, providing a robust and adaptable solution for accurate OD flow\nprediction that can be transferred from one city to another. Our novel\nframework involves four major components: collecting OD training datasets from\na source city, instruction-tuning the LLMs, predicting destination POIs in a\ntarget city, and identifying the locations that best match the predicted\ndestination POIs. We introduce a new loss function that integrates POI\nsemantics and trip distance during training. By extracting high-quality\nsemantic features from human mobility and POI data, the model understands\nspatial and functional relationships within urban spaces and captures\ninteractions between individuals and various POIs. Extensive experimental\nresults demonstrate the superiority of our approach over the state-of-the-art\nlearning-based methods in cross-city OD flow prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting Origin-Destination (OD) flows is crucial for\nurban planning and transportation management. Traditional OD prediction models,\nwhile effective within single cities, often face limitations when applied\nacross different cities due to varied traffic conditions, urban layouts, and\nsocio-economic factors. In this paper, by employing Large Language Models\n(LLMs), we introduce a new method for cross-city OD flow prediction. Our\napproach leverages the advanced semantic understanding and contextual learning\ncapabilities of LLMs to bridge the gap between cities with different\ncharacteristics, providing a robust and adaptable solution for accurate OD flow\nprediction that can be transferred from one city to another. Our novel\nframework involves four major components: collecting OD training datasets from\na source city, instruction-tuning the LLMs, predicting destination POIs in a\ntarget city, and identifying the locations that best match the predicted\ndestination POIs. We introduce a new loss function that integrates POI\nsemantics and trip distance during training. By extracting high-quality\nsemantic features from human mobility and POI data, the model understands\nspatial and functional relationships within urban spaces and captures\ninteractions between individuals and various POIs. Extensive experimental\nresults demonstrate the superiority of our approach over the state-of-the-art\nlearning-based methods in cross-city OD flow prediction."
                },
                "authors": [
                    {
                        "name": "Chenyang Yu"
                    },
                    {
                        "name": "Xinpeng Xie"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Chenxi Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Qiu"
                },
                "author": "Chenxi Qiu",
                "arxiv_comment": "12 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03935v1",
                "updated": "2024-09-05T23:01:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    1,
                    31,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T23:01:31Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    1,
                    31,
                    3,
                    249,
                    0
                ],
                "title": "Galled Perfect Transfer Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galled Perfect Transfer Networks"
                },
                "summary": "Predicting horizontal gene transfers often requires comparative sequence\ndata, but recent work has shown that character-based approaches could also be\nuseful for this task. Notably, perfect transfer networks (PTN) explain the\ncharacter diversity of a set of taxa for traits that are gained once, rarely\nlost, but that can be transferred laterally. Characterizing the structure of\nsuch characters is an important step towards understanding more complex\ncharacters. Although efficient algorithms can infer such networks from\ncharacter data, they can sometimes predict overly complicated transfer\nhistories. With the goal of recovering the simplest possible scenarios in this\nmodel, we introduce galled perfect transfer networks, which are PTNs that are\ngalled trees. Such networks are useful for characters that are incompatible in\nterms of tree-like evolution, but that do fit in an almost-tree scenario. We\nprovide polynomial-time algorithms for two problems: deciding whether one can\nadd transfer edges to a tree to transform it into a galled PTN, and deciding\nwhether a set of characters are galled-compatible, that is, they can be\nexplained by some galled PTN. We also analyze a real dataset comprising of a\nbacterial species trees and KEGG functions as characters, and derive several\nconclusions on the difficulty of explaining characters in a galled tree, which\nprovide several directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting horizontal gene transfers often requires comparative sequence\ndata, but recent work has shown that character-based approaches could also be\nuseful for this task. Notably, perfect transfer networks (PTN) explain the\ncharacter diversity of a set of taxa for traits that are gained once, rarely\nlost, but that can be transferred laterally. Characterizing the structure of\nsuch characters is an important step towards understanding more complex\ncharacters. Although efficient algorithms can infer such networks from\ncharacter data, they can sometimes predict overly complicated transfer\nhistories. With the goal of recovering the simplest possible scenarios in this\nmodel, we introduce galled perfect transfer networks, which are PTNs that are\ngalled trees. Such networks are useful for characters that are incompatible in\nterms of tree-like evolution, but that do fit in an almost-tree scenario. We\nprovide polynomial-time algorithms for two problems: deciding whether one can\nadd transfer edges to a tree to transform it into a galled PTN, and deciding\nwhether a set of characters are galled-compatible, that is, they can be\nexplained by some galled PTN. We also analyze a real dataset comprising of a\nbacterial species trees and KEGG functions as characters, and derive several\nconclusions on the difficulty of explaining characters in a galled tree, which\nprovide several directions for future research."
                },
                "authors": [
                    {
                        "name": "Alitzel López Sánchez"
                    },
                    {
                        "name": "Manuel Lafond"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Lafond"
                },
                "author": "Manuel Lafond",
                "arxiv_comment": "extended article based on previously accepted manuscript at RECOMB-CG\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03928v1",
                "updated": "2024-09-05T22:22:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    22,
                    22,
                    57,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T22:22:57Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    22,
                    22,
                    57,
                    3,
                    249,
                    0
                ],
                "title": "RETAIN: Interactive Tool for Regression Testing Guided LLM Migration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RETAIN: Interactive Tool for Regression Testing Guided LLM Migration"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into diverse\napplications. The rapid evolution of LLMs presents opportunities for developers\nto enhance applications continuously. However, this constant adaptation can\nalso lead to performance regressions during model migrations. While several\ninteractive tools have been proposed to streamline the complexity of prompt\nengineering, few address the specific requirements of regression testing for\nLLM Migrations. To bridge this gap, we introduce RETAIN (REgression Testing\nguided LLM migrAtIoN), a tool designed explicitly for regression testing in LLM\nMigrations. RETAIN comprises two key components: an interactive interface\ntailored to regression testing needs during LLM migrations, and an error\ndiscovery module that facilitates understanding of differences in model\nbehaviors. The error discovery module generates textual descriptions of various\nerrors or differences between model outputs, providing actionable insights for\nprompt refinement. Our automatic evaluation and empirical user studies\ndemonstrate that RETAIN, when compared to manual evaluation, enabled\nparticipants to identify twice as many errors, facilitated experimentation with\n75% more prompts, and achieves 12% higher metric scores in a given time frame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into diverse\napplications. The rapid evolution of LLMs presents opportunities for developers\nto enhance applications continuously. However, this constant adaptation can\nalso lead to performance regressions during model migrations. While several\ninteractive tools have been proposed to streamline the complexity of prompt\nengineering, few address the specific requirements of regression testing for\nLLM Migrations. To bridge this gap, we introduce RETAIN (REgression Testing\nguided LLM migrAtIoN), a tool designed explicitly for regression testing in LLM\nMigrations. RETAIN comprises two key components: an interactive interface\ntailored to regression testing needs during LLM migrations, and an error\ndiscovery module that facilitates understanding of differences in model\nbehaviors. The error discovery module generates textual descriptions of various\nerrors or differences between model outputs, providing actionable insights for\nprompt refinement. Our automatic evaluation and empirical user studies\ndemonstrate that RETAIN, when compared to manual evaluation, enabled\nparticipants to identify twice as many errors, facilitated experimentation with\n75% more prompts, and achieves 12% higher metric scores in a given time frame."
                },
                "authors": [
                    {
                        "name": "Tanay Dixit"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Sally Fang"
                    },
                    {
                        "name": "Sai Sree Harsha"
                    },
                    {
                        "name": "Anirudh Sureshan"
                    },
                    {
                        "name": "Akash Maharaj"
                    },
                    {
                        "name": "Yunyao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yunyao Li"
                },
                "author": "Yunyao Li",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12580v2",
                "updated": "2024-09-05T21:29:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    21,
                    29,
                    32,
                    3,
                    249,
                    0
                ],
                "published": "2024-06-18T13:06:58Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    6,
                    58,
                    1,
                    170,
                    0
                ],
                "title": "Behavior-Dependent Linear Recurrent Units for Efficient Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior-Dependent Linear Recurrent Units for Efficient Sequential\n  Recommendation"
                },
                "summary": "Sequential recommender systems aims to predict the users' next interaction\nthrough user behavior modeling with various operators like RNNs and attentions.\nHowever, existing models generally fail to achieve the three golden principles\nfor sequential recommendation simultaneously, i.e., training efficiency,\nlow-cost inference, and strong performance. To this end, we propose RecBLR, an\nEfficient Sequential Recommendation Model based on Behavior-Dependent Linear\nRecurrent Units to accomplish the impossible triangle of the three principles.\nBy incorporating gating mechanisms and behavior-dependent designs into linear\nrecurrent units, our model significantly enhances user behavior modeling and\nrecommendation performance. Furthermore, we unlock the parallelizable training\nas well as inference efficiency for our model by designing a hardware-aware\nscanning acceleration algorithm with a customized CUDA kernel. Extensive\nexperiments on real-world datasets with varying lengths of user behavior\nsequences demonstrate RecBLR's remarkable effectiveness in simultaneously\nachieving all three golden principles - strong recommendation performance,\ntraining efficiency, and low-cost inference, while exhibiting excellent\nscalability to datasets with long user interaction histories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommender systems aims to predict the users' next interaction\nthrough user behavior modeling with various operators like RNNs and attentions.\nHowever, existing models generally fail to achieve the three golden principles\nfor sequential recommendation simultaneously, i.e., training efficiency,\nlow-cost inference, and strong performance. To this end, we propose RecBLR, an\nEfficient Sequential Recommendation Model based on Behavior-Dependent Linear\nRecurrent Units to accomplish the impossible triangle of the three principles.\nBy incorporating gating mechanisms and behavior-dependent designs into linear\nrecurrent units, our model significantly enhances user behavior modeling and\nrecommendation performance. Furthermore, we unlock the parallelizable training\nas well as inference efficiency for our model by designing a hardware-aware\nscanning acceleration algorithm with a customized CUDA kernel. Extensive\nexperiments on real-world datasets with varying lengths of user behavior\nsequences demonstrate RecBLR's remarkable effectiveness in simultaneously\nachieving all three golden principles - strong recommendation performance,\ntraining efficiency, and low-cost inference, while exhibiting excellent\nscalability to datasets with long user interaction histories."
                },
                "authors": [
                    {
                        "name": "Chengkai Liu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Hanzhou Liu"
                    },
                    {
                        "name": "Jianling Wang"
                    },
                    {
                        "name": "James Caverlee"
                    }
                ],
                "author_detail": {
                    "name": "James Caverlee"
                },
                "author": "James Caverlee",
                "arxiv_comment": "Accepted to CIKM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03918v1",
                "updated": "2024-09-05T21:26:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    21,
                    26,
                    25,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T21:26:25Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    21,
                    26,
                    25,
                    3,
                    249,
                    0
                ],
                "title": "PoTo: A Hybrid Andersen's Points-to Analysis for Python",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoTo: A Hybrid Andersen's Points-to Analysis for Python"
                },
                "summary": "As Python is increasingly being adopted for large and complex programs, the\nimportance of static analysis for Python (such as type inference) grows.\nUnfortunately, static analysis for Python remains a challenging task due to its\ndynamic language features and its abundant external libraries. To help fill\nthis gap, this paper presents PoTo, an Andersen-style context-insensitive and\nflow-insensitive points-to analysis for Python. PoTo addresses Python-specific\nchallenges and works for large programs via a novel hybrid evaluation,\nintegrating traditional static points-to analysis with concrete evaluation in\nthe Python interpreter for external library calls. Next, this paper presents\nPoTo+, a static type inference for Python built on the points-to analysis. We\nevaluate PoTo+ and compare it to two state-of-the-art Python type inference\ntechniques: (1) the static rule-based Pytype and (2) the deep-learning based\nDLInfer. Our results show that PoTo+ outperforms both Pytype and DLInfer on\nexisting Python packages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Python is increasingly being adopted for large and complex programs, the\nimportance of static analysis for Python (such as type inference) grows.\nUnfortunately, static analysis for Python remains a challenging task due to its\ndynamic language features and its abundant external libraries. To help fill\nthis gap, this paper presents PoTo, an Andersen-style context-insensitive and\nflow-insensitive points-to analysis for Python. PoTo addresses Python-specific\nchallenges and works for large programs via a novel hybrid evaluation,\nintegrating traditional static points-to analysis with concrete evaluation in\nthe Python interpreter for external library calls. Next, this paper presents\nPoTo+, a static type inference for Python built on the points-to analysis. We\nevaluate PoTo+ and compare it to two state-of-the-art Python type inference\ntechniques: (1) the static rule-based Pytype and (2) the deep-learning based\nDLInfer. Our results show that PoTo+ outperforms both Pytype and DLInfer on\nexisting Python packages."
                },
                "authors": [
                    {
                        "name": "Ingkarat Rak-amnouykit"
                    },
                    {
                        "name": "Ana Milanova"
                    },
                    {
                        "name": "Guillaume Baudart"
                    },
                    {
                        "name": "Martin Hirzel"
                    },
                    {
                        "name": "Julian Dolby"
                    }
                ],
                "author_detail": {
                    "name": "Julian Dolby"
                },
                "author": "Julian Dolby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03912v1",
                "updated": "2024-09-05T21:08:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    21,
                    8,
                    55,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T21:08:55Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    21,
                    8,
                    55,
                    3,
                    249,
                    0
                ],
                "title": "The anti-aligned spin of GW191109: glitch mitigation and its\n  implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The anti-aligned spin of GW191109: glitch mitigation and its\n  implications"
                },
                "summary": "With a high total mass and an inferred effective spin anti-aligned with the\norbital axis at the 99.9% level, GW191109 is one of the most promising\ncandidates for a dynamical formation origin among gravitational wave events\nobserved so far. However, the data containing GW191109 are afflicted with\nterrestrial noise transients, i.e., detector glitches, generated by the\nscattering of laser light in both LIGO detectors. We study the implications of\nthe glitch(es) on the inferred properties and astrophysical interpretation of\nGW191109. Using time- and frequency-domain analysis methods, we isolate the\ncritical data for spin inference to 35 - 40 Hz and 0.1 - 0.04 s before the\nmerger in LIGO Livingston, directly coincident with the glitch. Using two\nmodels of glitch behavior, one tailored to slow scattered light and one more\ngeneric, we perform joint inference of the glitch and binary parameters. When\nthe glitch is modeled as slow scattered light, the binary parameters favor\nanti-aligned spins, in agreement with existing interpretations. When more\nflexible glitch modeling based on sine-Gaussian wavelets is used instead, a\nbimodal aligned/anti-aligned solution emerges. The anti-aligned spin mode is\ncorrelated with a weaker inferred glitch and preferred by ~ 70 : 30 compared to\nthe aligned spin mode and a stronger inferred glitch. We conclude that if we\nassume that the data are only impacted by slow scattering noise, then the\nanti-aligned spin inference is robust. However, the data alone cannot validate\nthis assumption and resolve the anti-aligned spin and potentially dynamical\nformation history of GW191109.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a high total mass and an inferred effective spin anti-aligned with the\norbital axis at the 99.9% level, GW191109 is one of the most promising\ncandidates for a dynamical formation origin among gravitational wave events\nobserved so far. However, the data containing GW191109 are afflicted with\nterrestrial noise transients, i.e., detector glitches, generated by the\nscattering of laser light in both LIGO detectors. We study the implications of\nthe glitch(es) on the inferred properties and astrophysical interpretation of\nGW191109. Using time- and frequency-domain analysis methods, we isolate the\ncritical data for spin inference to 35 - 40 Hz and 0.1 - 0.04 s before the\nmerger in LIGO Livingston, directly coincident with the glitch. Using two\nmodels of glitch behavior, one tailored to slow scattered light and one more\ngeneric, we perform joint inference of the glitch and binary parameters. When\nthe glitch is modeled as slow scattered light, the binary parameters favor\nanti-aligned spins, in agreement with existing interpretations. When more\nflexible glitch modeling based on sine-Gaussian wavelets is used instead, a\nbimodal aligned/anti-aligned solution emerges. The anti-aligned spin mode is\ncorrelated with a weaker inferred glitch and preferred by ~ 70 : 30 compared to\nthe aligned spin mode and a stronger inferred glitch. We conclude that if we\nassume that the data are only impacted by slow scattering noise, then the\nanti-aligned spin inference is robust. However, the data alone cannot validate\nthis assumption and resolve the anti-aligned spin and potentially dynamical\nformation history of GW191109."
                },
                "authors": [
                    {
                        "name": "Rhiannon Udall"
                    },
                    {
                        "name": "Sophie Hourihane"
                    },
                    {
                        "name": "Simona Miller"
                    },
                    {
                        "name": "Derek Davis"
                    },
                    {
                        "name": "Katerina Chatziioannou"
                    },
                    {
                        "name": "Max Isi"
                    },
                    {
                        "name": "Howard Deshong"
                    }
                ],
                "author_detail": {
                    "name": "Howard Deshong"
                },
                "author": "Howard Deshong",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03905v1",
                "updated": "2024-09-05T20:42:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    42,
                    35,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T20:42:35Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    42,
                    35,
                    3,
                    249,
                    0
                ],
                "title": "CACER: Clinical Concept Annotations for Cancer Events and Relations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CACER: Clinical Concept Annotations for Cancer Events and Relations"
                },
                "summary": "Clinical notes contain unstructured representations of patient histories,\nincluding the relationships between medical problems and prescription drugs. To\ninvestigate the relationship between cancer drugs and their associated symptom\nburden, we extract structured, semantic representations of medical problem and\ndrug information from the clinical narratives of oncology notes. We present\nClinical Concept Annotations for Cancer Events and Relations (CACER), a novel\ncorpus with fine-grained annotations for over 48,000 medical problems and drug\nevents and 10,000 drug-problem and problem-problem relations. Leveraging CACER,\nwe develop and evaluate transformer-based information extraction (IE) models\nsuch as BERT, Flan-T5, Llama3, and GPT-4 using fine-tuning and in-context\nlearning (ICL). In event extraction, the fine-tuned BERT and Llama3 models\nachieved the highest performance at 88.2-88.0 F1, which is comparable to the\ninter-annotator agreement (IAA) of 88.4 F1. In relation extraction, the\nfine-tuned BERT, Flan-T5, and Llama3 achieved the highest performance at\n61.8-65.3 F1. GPT-4 with ICL achieved the worst performance across both tasks.\nThe fine-tuned models significantly outperformed GPT-4 in ICL, highlighting the\nimportance of annotated training data and model optimization. Furthermore, the\nBERT models performed similarly to Llama3. For our task, LLMs offer no\nperformance advantage over the smaller BERT models. The results emphasize the\nneed for annotated training data to optimize models. Multiple fine-tuned\ntransformer models achieved performance comparable to IAA for several\nextraction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical notes contain unstructured representations of patient histories,\nincluding the relationships between medical problems and prescription drugs. To\ninvestigate the relationship between cancer drugs and their associated symptom\nburden, we extract structured, semantic representations of medical problem and\ndrug information from the clinical narratives of oncology notes. We present\nClinical Concept Annotations for Cancer Events and Relations (CACER), a novel\ncorpus with fine-grained annotations for over 48,000 medical problems and drug\nevents and 10,000 drug-problem and problem-problem relations. Leveraging CACER,\nwe develop and evaluate transformer-based information extraction (IE) models\nsuch as BERT, Flan-T5, Llama3, and GPT-4 using fine-tuning and in-context\nlearning (ICL). In event extraction, the fine-tuned BERT and Llama3 models\nachieved the highest performance at 88.2-88.0 F1, which is comparable to the\ninter-annotator agreement (IAA) of 88.4 F1. In relation extraction, the\nfine-tuned BERT, Flan-T5, and Llama3 achieved the highest performance at\n61.8-65.3 F1. GPT-4 with ICL achieved the worst performance across both tasks.\nThe fine-tuned models significantly outperformed GPT-4 in ICL, highlighting the\nimportance of annotated training data and model optimization. Furthermore, the\nBERT models performed similarly to Llama3. For our task, LLMs offer no\nperformance advantage over the smaller BERT models. The results emphasize the\nneed for annotated training data to optimize models. Multiple fine-tuned\ntransformer models achieved performance comparable to IAA for several\nextraction tasks."
                },
                "authors": [
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Giridhar Kaushik Ramachandran"
                    },
                    {
                        "name": "Ahmad Halwani"
                    },
                    {
                        "name": "Bridget T. McInnes"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Kevin Lybarger"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Özlem Uzuner"
                    }
                ],
                "author_detail": {
                    "name": "Özlem Uzuner"
                },
                "author": "Özlem Uzuner",
                "arxiv_doi": "10.1093/jamia/ocae231",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/jamia/ocae231",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is a pre-copy-editing, author-produced PDF of an article\n  accepted for publication in JAMIA following peer review. The definitive\n  publisher-authenticated version is available online at\n  https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae231/7748302",
                "arxiv_journal_ref": "Journal of the American Medical Informatics Association (2024):\n  ocae231",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04421v1",
                "updated": "2024-09-06T17:30:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    17,
                    30,
                    45,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T17:30:45Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    17,
                    30,
                    45,
                    4,
                    250,
                    0
                ],
                "title": "RLPF: Reinforcement Learning from Prediction Feedback for User\n  Summarization with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLPF: Reinforcement Learning from Prediction Feedback for User\n  Summarization with LLMs"
                },
                "summary": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Lin Ning"
                    },
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Harrison Lee"
                    },
                    {
                        "name": "Neo Wu"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Sushant Prakash"
                    },
                    {
                        "name": "Shawn O'Banion"
                    },
                    {
                        "name": "Bradley Green"
                    },
                    {
                        "name": "Jun Xie"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xie"
                },
                "author": "Jun Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17057v2",
                "updated": "2024-09-06T17:15:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    17,
                    15,
                    49,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-30T07:32:19Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    32,
                    19,
                    4,
                    243,
                    0
                ],
                "title": "LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality\n  Assessment Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality\n  Assessment Model"
                },
                "summary": "Recent advancements in the field of No-Reference Image Quality Assessment\n(NR-IQA) using deep learning techniques demonstrate high performance across\nmultiple open-source datasets. However, such models are typically very large\nand complex making them not so suitable for real-world deployment, especially\non resource- and battery-constrained mobile devices. To address this\nlimitation, we propose a compact, lightweight NR-IQA model that achieves\nstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation\nand test datasets while being also nearly 5.7 times faster than the fastest\nSOTA model. Our model features a dual-branch architecture, with each branch\nseparately trained on synthetically and authentically distorted images which\nenhances the model's generalizability across different distortion types. To\nimprove robustness under diverse real-world visual conditions, we additionally\nincorporate multiple color spaces during the training process. We also\ndemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks\n(KANs) for final quality regression as compared to the conventional Multi-Layer\nPerceptrons (MLPs). Our evaluation considering various open-source datasets\nhighlights the practical, high-accuracy, and robust performance of our proposed\nlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the field of No-Reference Image Quality Assessment\n(NR-IQA) using deep learning techniques demonstrate high performance across\nmultiple open-source datasets. However, such models are typically very large\nand complex making them not so suitable for real-world deployment, especially\non resource- and battery-constrained mobile devices. To address this\nlimitation, we propose a compact, lightweight NR-IQA model that achieves\nstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation\nand test datasets while being also nearly 5.7 times faster than the fastest\nSOTA model. Our model features a dual-branch architecture, with each branch\nseparately trained on synthetically and authentically distorted images which\nenhances the model's generalizability across different distortion types. To\nimprove robustness under diverse real-world visual conditions, we additionally\nincorporate multiple color spaces during the training process. We also\ndemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks\n(KANs) for final quality regression as compared to the conventional Multi-Layer\nPerceptrons (MLPs). Our evaluation considering various open-source datasets\nhighlights the practical, high-accuracy, and robust performance of our proposed\nlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA."
                },
                "authors": [
                    {
                        "name": "Nasim Jamshidi Avanaki"
                    },
                    {
                        "name": "Abhijay Ghildyal"
                    },
                    {
                        "name": "Nabajeet Barman"
                    },
                    {
                        "name": "Saman Zadtootaghaj"
                    }
                ],
                "author_detail": {
                    "name": "Saman Zadtootaghaj"
                },
                "author": "Saman Zadtootaghaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00077v2",
                "updated": "2024-09-06T16:12:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    16,
                    12,
                    0,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-24T09:26:59Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    9,
                    26,
                    59,
                    5,
                    237,
                    0
                ],
                "title": "Are LLM-based methods good enough for detecting unfair terms of service?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM-based methods good enough for detecting unfair terms of service?"
                },
                "summary": "Countless terms of service (ToS) are being signed everyday by users all over\nthe world while interacting with all kinds of apps and websites. More often\nthan not, these online contracts spanning double-digit pages are signed blindly\nby users who simply want immediate access to the desired service. What would\nnormally require a consultation with a legal team, has now become a mundane\nactivity consisting of a few clicks where users potentially sign away their\nrights, for instance in terms of their data privacy, to countless online\nentities/companies. Large language models (LLMs) are good at parsing long\ntext-based documents, and could potentially be adopted to help users when\ndealing with dubious clauses in ToS and their underlying privacy policies. To\ninvestigate the utility of existing models for this task, we first build a\ndataset consisting of 12 questions applied individually to a set of privacy\npolicies crawled from popular websites. Thereafter, a series of open-source as\nwell as commercial chatbots such as ChatGPT, are queried over each question,\nwith the answers being compared to a given ground truth. Our results show that\nsome open-source models are able to provide a higher accuracy compared to some\ncommercial models. However, the best performance is recorded from a commercial\nchatbot (ChatGPT4). Overall, all models perform only slightly better than\nrandom at this task. Consequently, their performance needs to be significantly\nimproved before they can be adopted at large for this purpose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Countless terms of service (ToS) are being signed everyday by users all over\nthe world while interacting with all kinds of apps and websites. More often\nthan not, these online contracts spanning double-digit pages are signed blindly\nby users who simply want immediate access to the desired service. What would\nnormally require a consultation with a legal team, has now become a mundane\nactivity consisting of a few clicks where users potentially sign away their\nrights, for instance in terms of their data privacy, to countless online\nentities/companies. Large language models (LLMs) are good at parsing long\ntext-based documents, and could potentially be adopted to help users when\ndealing with dubious clauses in ToS and their underlying privacy policies. To\ninvestigate the utility of existing models for this task, we first build a\ndataset consisting of 12 questions applied individually to a set of privacy\npolicies crawled from popular websites. Thereafter, a series of open-source as\nwell as commercial chatbots such as ChatGPT, are queried over each question,\nwith the answers being compared to a given ground truth. Our results show that\nsome open-source models are able to provide a higher accuracy compared to some\ncommercial models. However, the best performance is recorded from a commercial\nchatbot (ChatGPT4). Overall, all models perform only slightly better than\nrandom at this task. Consequently, their performance needs to be significantly\nimproved before they can be adopted at large for this purpose."
                },
                "authors": [
                    {
                        "name": "Mirgita Frasheri"
                    },
                    {
                        "name": "Arian Bakhtiarnia"
                    },
                    {
                        "name": "Lukas Esterle"
                    },
                    {
                        "name": "Alexandros Iosifidis"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Iosifidis"
                },
                "author": "Alexandros Iosifidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07019v2",
                "updated": "2024-09-06T15:56:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    56,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2023-10-10T21:12:15Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    21,
                    12,
                    15,
                    1,
                    283,
                    0
                ],
                "title": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans\n  and AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case Law Grounding: Using Precedents to Align Decision-Making for Humans\n  and AI"
                },
                "summary": "Communities and groups often need to make decisions based on social norms and\npreferences, such as when moderating content or building AI systems that\nreflect human values. The prevailing approach has been to first create\nhigh-level guidelines -- ``constitutions'' -- and then decide on new cases\nusing the outlined criteria. However, social norms and preferences vary between\ngroups, decision-makers can interpret guidelines inconsistently, and\nexceptional situations may be under-specified.\n  In this work, we take inspiration from legal systems and introduce ``case law\ngrounding'' (CLG), a novel workflow that uses past cases and decisions\n(\\textbf{precedents}) to help ground future decisions, for both human and\nLLM-based decision-makers. We evaluate CLG against a constitution-only approach\non two tasks for both types of decision-makers, and find that decisions\nproduced with CLG were more accurately aligned to observed ground truth in all\ncases, producing a 3.3--23.3 \\%-points improvement (across different tasks and\ngroups) for humans and 9.2--30.0 \\%-points (across different tasks and groups)\nfor LLM agents. We also discuss other aspects where a case-based approach could\naugment existing ``constitutional'' approaches when it comes to aligning human\nand AI decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communities and groups often need to make decisions based on social norms and\npreferences, such as when moderating content or building AI systems that\nreflect human values. The prevailing approach has been to first create\nhigh-level guidelines -- ``constitutions'' -- and then decide on new cases\nusing the outlined criteria. However, social norms and preferences vary between\ngroups, decision-makers can interpret guidelines inconsistently, and\nexceptional situations may be under-specified.\n  In this work, we take inspiration from legal systems and introduce ``case law\ngrounding'' (CLG), a novel workflow that uses past cases and decisions\n(\\textbf{precedents}) to help ground future decisions, for both human and\nLLM-based decision-makers. We evaluate CLG against a constitution-only approach\non two tasks for both types of decision-makers, and find that decisions\nproduced with CLG were more accurately aligned to observed ground truth in all\ncases, producing a 3.3--23.3 \\%-points improvement (across different tasks and\ngroups) for humans and 9.2--30.0 \\%-points (across different tasks and groups)\nfor LLM agents. We also discuss other aspects where a case-based approach could\naugment existing ``constitutional'' approaches when it comes to aligning human\nand AI decisions."
                },
                "authors": [
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04340v1",
                "updated": "2024-09-06T15:18:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    18,
                    12,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T15:18:12Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    15,
                    18,
                    12,
                    4,
                    250,
                    0
                ],
                "title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs"
                },
                "summary": "LLMs can exhibit age biases, resulting in unequal treatment of individuals\nacross age groups. While much research has addressed racial and gender biases,\nage bias remains little explored. The scarcity of instruction-tuning and\npreference datasets for age bias hampers its detection and measurement, and\nexisting fine-tuning methods seldom address age-related fairness. In this\npaper, we construct age bias preference datasets and instruction-tuning\ndatasets for RLHF. We introduce ARG, an age fairness reward to reduce\ndifferences in the response quality of LLMs across different age groups.\nExtensive experiments demonstrate that this reward significantly improves\nresponse accuracy and reduces performance disparities across age groups. Our\nsource code and datasets are available at the anonymous\n\\href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can exhibit age biases, resulting in unequal treatment of individuals\nacross age groups. While much research has addressed racial and gender biases,\nage bias remains little explored. The scarcity of instruction-tuning and\npreference datasets for age bias hampers its detection and measurement, and\nexisting fine-tuning methods seldom address age-related fairness. In this\npaper, we construct age bias preference datasets and instruction-tuning\ndatasets for RLHF. We introduce ARG, an age fairness reward to reduce\ndifferences in the response quality of LLMs across different age groups.\nExtensive experiments demonstrate that this reward significantly improves\nresponse accuracy and reduces performance disparities across age groups. Our\nsource code and datasets are available at the anonymous\n\\href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}."
                },
                "authors": [
                    {
                        "name": "Shuirong Cao"
                    },
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Zhiqiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Wang"
                },
                "author": "Zhiqiang Wang",
                "arxiv_comment": "The first two authors contributed equally to this work. Corresponding\n  to Zhiqiang Wang. ACKNOWLEDGMENT: we would like to thank the computing\n  resources support from the State Key Laboratory of New Computer Software\n  Technologies at Nanjing University",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00884v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00884v3",
                "updated": "2024-09-06T14:49:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    49,
                    21,
                    4,
                    250,
                    0
                ],
                "published": "2024-03-01T10:01:36Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    10,
                    1,
                    36,
                    4,
                    61,
                    0
                ],
                "title": "Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for\n  Metadata Enrichment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for\n  Metadata Enrichment"
                },
                "summary": "Traditional dataset retrieval systems rely on metadata for indexing, rather\nthan on the underlying data values. However, high-quality metadata creation and\nenrichment often require manual annotations, which is a labour-intensive and\nchallenging process to automate. In this study, we propose a method to support\nmetadata enrichment using topic annotations generated by three Large Language\nModels (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses\non classifying column headers based on domain-specific topics from the\nConsortium of European Social Science Data Archives (CESSDA), a Linked Data\ncontrolled vocabulary. Our approach operates in a zero-shot setting,\nintegrating the controlled topic vocabulary directly within the input prompt.\nThis integration serves as a Large Context Windows approach, with the aim of\nimproving the results of the topic classification task.\n  We evaluated the performance of the LLMs in terms of internal consistency,\ninter-machine alignment, and agreement with human classification. Additionally,\nwe investigate the impact of contextual information (i.e., dataset description)\non the classification outcomes. Our findings suggest that ChatGPT and\nGoogleGemini outperform GoogleBard in terms of internal consistency as well as\nLLM-human-agreement. Interestingly, we found that contextual information had no\nsignificant impact on LLM performance.\n  This work proposes a novel approach that leverages LLMs for topic\nclassification of column headers using a controlled vocabulary, presenting a\npractical application of LLMs and Large Context Windows within the Semantic Web\ndomain. This approach has the potential to facilitate automated metadata\nenrichment, thereby enhancing dataset retrieval and the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) of research data on the\nWeb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional dataset retrieval systems rely on metadata for indexing, rather\nthan on the underlying data values. However, high-quality metadata creation and\nenrichment often require manual annotations, which is a labour-intensive and\nchallenging process to automate. In this study, we propose a method to support\nmetadata enrichment using topic annotations generated by three Large Language\nModels (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses\non classifying column headers based on domain-specific topics from the\nConsortium of European Social Science Data Archives (CESSDA), a Linked Data\ncontrolled vocabulary. Our approach operates in a zero-shot setting,\nintegrating the controlled topic vocabulary directly within the input prompt.\nThis integration serves as a Large Context Windows approach, with the aim of\nimproving the results of the topic classification task.\n  We evaluated the performance of the LLMs in terms of internal consistency,\ninter-machine alignment, and agreement with human classification. Additionally,\nwe investigate the impact of contextual information (i.e., dataset description)\non the classification outcomes. Our findings suggest that ChatGPT and\nGoogleGemini outperform GoogleBard in terms of internal consistency as well as\nLLM-human-agreement. Interestingly, we found that contextual information had no\nsignificant impact on LLM performance.\n  This work proposes a novel approach that leverages LLMs for topic\nclassification of column headers using a controlled vocabulary, presenting a\npractical application of LLMs and Large Context Windows within the Semantic Web\ndomain. This approach has the potential to facilitate automated metadata\nenrichment, thereby enhancing dataset retrieval and the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) of research data on the\nWeb."
                },
                "authors": [
                    {
                        "name": "Margherita Martorana"
                    },
                    {
                        "name": "Tobias Kuhn"
                    },
                    {
                        "name": "Lise Stork"
                    },
                    {
                        "name": "Jacco van Ossenbruggen"
                    }
                ],
                "author_detail": {
                    "name": "Jacco van Ossenbruggen"
                },
                "author": "Jacco van Ossenbruggen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00884v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00884v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04318v1",
                "updated": "2024-09-06T14:46:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    46,
                    37,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T14:46:37Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    46,
                    37,
                    4,
                    250,
                    0
                ],
                "title": "Learning vs Retrieval: The Role of In-Context Examples in Regression\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning vs Retrieval: The Role of In-Context Examples in Regression\n  with LLMs"
                },
                "summary": "Generative Large Language Models (LLMs) are capable of being in-context\nlearners. However, the underlying mechanism of in-context learning (ICL) is\nstill a major research question, and experimental research results about how\nmodels exploit ICL are not always consistent. In this work, we propose a\nframework for evaluating in-context learning mechanisms, which we claim are a\ncombination of retrieving internal knowledge and learning from in-context\nexamples by focusing on regression tasks. First, we show that LLMs can perform\nregression on real-world datasets and then design experiments to measure the\nextent to which the LLM retrieves its internal knowledge versus learning from\nin-context examples. We argue that this process lies on a spectrum between\nthese two extremes. We provide an in-depth analysis of the degrees to which\nthese mechanisms are triggered depending on various factors, such as prior\nknowledge about the tasks and the type and richness of the information provided\nby the in-context examples. We employ three LLMs and utilize multiple datasets\nto corroborate the robustness of our findings. Our results shed light on how to\nengineer prompts to leverage meta-learning from in-context examples and foster\nknowledge retrieval depending on the problem being addressed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) are capable of being in-context\nlearners. However, the underlying mechanism of in-context learning (ICL) is\nstill a major research question, and experimental research results about how\nmodels exploit ICL are not always consistent. In this work, we propose a\nframework for evaluating in-context learning mechanisms, which we claim are a\ncombination of retrieving internal knowledge and learning from in-context\nexamples by focusing on regression tasks. First, we show that LLMs can perform\nregression on real-world datasets and then design experiments to measure the\nextent to which the LLM retrieves its internal knowledge versus learning from\nin-context examples. We argue that this process lies on a spectrum between\nthese two extremes. We provide an in-depth analysis of the degrees to which\nthese mechanisms are triggered depending on various factors, such as prior\nknowledge about the tasks and the type and richness of the information provided\nby the in-context examples. We employ three LLMs and utilize multiple datasets\nto corroborate the robustness of our findings. Our results shed light on how to\nengineer prompts to leverage meta-learning from in-context examples and foster\nknowledge retrieval depending on the problem being addressed."
                },
                "authors": [
                    {
                        "name": "Aliakbar Nafar"
                    },
                    {
                        "name": "Kristen Brent Venable"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04302v1",
                "updated": "2024-09-06T14:20:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    20,
                    40,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T14:20:40Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    20,
                    40,
                    4,
                    250,
                    0
                ],
                "title": "Fast Adaptation for Deep Learning-based Wireless Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Adaptation for Deep Learning-based Wireless Communications"
                },
                "summary": "The integration with artificial intelligence (AI) is recognized as one of the\nsix usage scenarios in next-generation wireless communications. However,\nseveral critical challenges hinder the widespread application of deep learning\n(DL) techniques in wireless communications. In particular, existing DL-based\nwireless communications struggle to adapt to the rapidly changing wireless\nenvironments. In this paper, we discuss fast adaptation for DL-based wireless\ncommunications by using few-shot learning (FSL) techniques. We first identify\nthe differences between fast adaptation in wireless communications and\ntraditional AI tasks by highlighting two distinct FSL design requirements for\nwireless communications. To establish a wide perspective, we present a\ncomprehensive review of the existing FSL techniques in wireless communications\nthat satisfy these two design requirements. In particular, we emphasize the\nimportance of applying domain knowledge in achieving fast adaptation. We\nspecifically focus on multiuser multiple-input multiple-output (MU-MIMO)\nprecoding as an examples to demonstrate the advantages of the FSL to achieve\nfast adaptation in wireless communications. Finally, we highlight several open\nresearch issues for achieving broadscope future deployment of fast adaptive DL\nin wireless communication applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration with artificial intelligence (AI) is recognized as one of the\nsix usage scenarios in next-generation wireless communications. However,\nseveral critical challenges hinder the widespread application of deep learning\n(DL) techniques in wireless communications. In particular, existing DL-based\nwireless communications struggle to adapt to the rapidly changing wireless\nenvironments. In this paper, we discuss fast adaptation for DL-based wireless\ncommunications by using few-shot learning (FSL) techniques. We first identify\nthe differences between fast adaptation in wireless communications and\ntraditional AI tasks by highlighting two distinct FSL design requirements for\nwireless communications. To establish a wide perspective, we present a\ncomprehensive review of the existing FSL techniques in wireless communications\nthat satisfy these two design requirements. In particular, we emphasize the\nimportance of applying domain knowledge in achieving fast adaptation. We\nspecifically focus on multiuser multiple-input multiple-output (MU-MIMO)\nprecoding as an examples to demonstrate the advantages of the FSL to achieve\nfast adaptation in wireless communications. Finally, we highlight several open\nresearch issues for achieving broadscope future deployment of fast adaptive DL\nin wireless communication applications."
                },
                "authors": [
                    {
                        "name": "Ouya Wang"
                    },
                    {
                        "name": "Hengtao He"
                    },
                    {
                        "name": "Shenglong Zhou"
                    },
                    {
                        "name": "Zhi Ding"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    },
                    {
                        "name": "Geoffrey Ye Li"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Ye Li"
                },
                "author": "Geoffrey Ye Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03708v2",
                "updated": "2024-09-06T14:18:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    18,
                    20,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T17:14:23Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    14,
                    23,
                    3,
                    249,
                    0
                ],
                "title": "RAG based Question-Answering for Contextual Response Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG based Question-Answering for Contextual Response Prediction System"
                },
                "summary": "Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload."
                },
                "authors": [
                    {
                        "name": "Sriram Veturi"
                    },
                    {
                        "name": "Saurabh Vaichal"
                    },
                    {
                        "name": "Reshma Lal Jagadheesh"
                    },
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Nian Yan"
                    }
                ],
                "author_detail": {
                    "name": "Nian Yan"
                },
                "author": "Nian Yan",
                "arxiv_comment": "Accepted at the 1st Workshop on GenAI and RAG Systems for Enterprise,\n  CIKM'24. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09236v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09236v3",
                "updated": "2024-09-06T13:34:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    13,
                    34,
                    16,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-17T16:04:31Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    16,
                    4,
                    31,
                    5,
                    230,
                    0
                ],
                "title": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords"
                },
                "summary": "This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes."
                },
                "authors": [
                    {
                        "name": "Aman Ahluwalia"
                    },
                    {
                        "name": "Bishwajit Sutradhar"
                    },
                    {
                        "name": "Karishma Ghosh"
                    },
                    {
                        "name": "Indrapal Yadav"
                    },
                    {
                        "name": "Arpan Sheetal"
                    },
                    {
                        "name": "Prashant Patil"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Patil"
                },
                "author": "Prashant Patil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09236v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09236v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04270v1",
                "updated": "2024-09-06T13:25:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    13,
                    25,
                    43,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T13:25:43Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    13,
                    25,
                    43,
                    4,
                    250,
                    0
                ],
                "title": "Advancing Automated Knowledge Transfer in Evolutionary Multitasking via\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Automated Knowledge Transfer in Evolutionary Multitasking via\n  Large Language Models"
                },
                "summary": "Evolutionary Multi-task Optimization (EMTO) is a paradigm that leverages\nknowledge transfer across simultaneously optimized tasks for enhanced search\nperformance. To facilitate EMTO's performance, various knowledge transfer\nmodels have been developed for specific optimization tasks. However, designing\nthese models often requires substantial expert knowledge. Recently, large\nlanguage models (LLMs) have achieved remarkable success in autonomous\nprogramming, aiming to produce effective solvers for specific problems. In this\nwork, a LLM-based optimization paradigm is introduced to establish an\nautonomous model factory for generating knowledge transfer models, ensuring\neffective and efficient knowledge transfer across various optimization tasks.\nTo evaluate the performance of the proposed method, we conducted comprehensive\nempirical studies comparing the knowledge transfer model generated by the LLM\nwith existing state-of-the-art knowledge transfer methods. The results\ndemonstrate that the generated model is able to achieve superior or competitive\nperformance against hand-crafted knowledge transfer models in terms of both\nefficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Multi-task Optimization (EMTO) is a paradigm that leverages\nknowledge transfer across simultaneously optimized tasks for enhanced search\nperformance. To facilitate EMTO's performance, various knowledge transfer\nmodels have been developed for specific optimization tasks. However, designing\nthese models often requires substantial expert knowledge. Recently, large\nlanguage models (LLMs) have achieved remarkable success in autonomous\nprogramming, aiming to produce effective solvers for specific problems. In this\nwork, a LLM-based optimization paradigm is introduced to establish an\nautonomous model factory for generating knowledge transfer models, ensuring\neffective and efficient knowledge transfer across various optimization tasks.\nTo evaluate the performance of the proposed method, we conducted comprehensive\nempirical studies comparing the knowledge transfer model generated by the LLM\nwith existing state-of-the-art knowledge transfer methods. The results\ndemonstrate that the generated model is able to achieve superior or competitive\nperformance against hand-crafted knowledge transfer models in terms of both\nefficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuxiao Huang"
                    },
                    {
                        "name": "Xuebin Lv"
                    },
                    {
                        "name": "Shenghao Wu"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "arxiv_comment": "10 pages, 11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04249v1",
                "updated": "2024-09-06T12:55:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    55,
                    49,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T12:55:49Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    55,
                    49,
                    4,
                    250,
                    0
                ],
                "title": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge\n  Devices"
                },
                "summary": "The application of Transformer-based large models has achieved numerous\nsuccess in recent years. However, the exponential growth in the parameters of\nlarge models introduces formidable memory challenge for edge deployment. Prior\nworks to address this challenge mainly focus on optimizing the model structure\nand adopting memory swapping methods. However, the former reduces the inference\naccuracy, and the latter raises the inference latency. This paper introduces\nPIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces\nmemory usage by incorporating dynamic memory management and minimizes inference\nlatency by employing parallel model loading. Based on PIPELOAD mechanism, we\npresent Hermes, a framework optimized for large model inference on edge\ndevices. We evaluate Hermes on Transformer-based models of different sizes. Our\nexperiments illustrate that Hermes achieves up to 4.24 X increase in inference\nspeed and 86.7% lower memory consumption than the state-of-the-art pipeline\nmechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3%\nlower memory consumption for GPT-style models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of Transformer-based large models has achieved numerous\nsuccess in recent years. However, the exponential growth in the parameters of\nlarge models introduces formidable memory challenge for edge deployment. Prior\nworks to address this challenge mainly focus on optimizing the model structure\nand adopting memory swapping methods. However, the former reduces the inference\naccuracy, and the latter raises the inference latency. This paper introduces\nPIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces\nmemory usage by incorporating dynamic memory management and minimizes inference\nlatency by employing parallel model loading. Based on PIPELOAD mechanism, we\npresent Hermes, a framework optimized for large model inference on edge\ndevices. We evaluate Hermes on Transformer-based models of different sizes. Our\nexperiments illustrate that Hermes achieves up to 4.24 X increase in inference\nspeed and 86.7% lower memory consumption than the state-of-the-art pipeline\nmechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3%\nlower memory consumption for GPT-style models."
                },
                "authors": [
                    {
                        "name": "Xueyuan Han"
                    },
                    {
                        "name": "Zinuo Cai"
                    },
                    {
                        "name": "Yichu Zhang"
                    },
                    {
                        "name": "Chongxin Fan"
                    },
                    {
                        "name": "Junhan Liu"
                    },
                    {
                        "name": "Ruhui Ma"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "arxiv_comment": "Accepted by the 42nd IEEE International Conference on Computer Design\n  (ICCD 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04218v1",
                "updated": "2024-09-06T12:17:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    17,
                    23,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T12:17:23Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    17,
                    23,
                    4,
                    250,
                    0
                ],
                "title": "MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox\n  Detection"
                },
                "summary": "Due to the lack of effective mpox detection tools, the mpox virus continues\nto spread worldwide and has once again been declared a public health emergency\nof international concern by the World Health Organization. Deep learning-based\nmpox detection tools are crucial to alleviate mpox outbreak. However, existing\nmethods have difficulty in achieving a good trade-off between detection\nperformance, parameter size, and model complexity, which is crucial for\npractical applications and widespread deployment, especially in\nresource-limited scenarios. Given that the success of Mamba in modeling\nlong-range dependencies and its linear complexity, we proposed a lightweight\nhybrid architecture called MpoxMamba. MpoxMamba utilizes deep separable\nconvolutions to extract local feature representations in mpox skin lesions, and\ngreatly enhances the model's ability to model the global contextual information\nby grouped Mamba modules. Experimental results on two widely recognized mpox\ndatasets demonstrate that MpoxMamba outperforms existing mpox detection methods\nand state-of-the-art lightweight models. We also developed a web-based online\napplication to provide free mpox detection services to the public in the\nepidemic areas (http://5227i971s5.goho.co:30290). The source codes of MpoxMamba\nare available at https://github.com/YubiaoYue/MpoxMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the lack of effective mpox detection tools, the mpox virus continues\nto spread worldwide and has once again been declared a public health emergency\nof international concern by the World Health Organization. Deep learning-based\nmpox detection tools are crucial to alleviate mpox outbreak. However, existing\nmethods have difficulty in achieving a good trade-off between detection\nperformance, parameter size, and model complexity, which is crucial for\npractical applications and widespread deployment, especially in\nresource-limited scenarios. Given that the success of Mamba in modeling\nlong-range dependencies and its linear complexity, we proposed a lightweight\nhybrid architecture called MpoxMamba. MpoxMamba utilizes deep separable\nconvolutions to extract local feature representations in mpox skin lesions, and\ngreatly enhances the model's ability to model the global contextual information\nby grouped Mamba modules. Experimental results on two widely recognized mpox\ndatasets demonstrate that MpoxMamba outperforms existing mpox detection methods\nand state-of-the-art lightweight models. We also developed a web-based online\napplication to provide free mpox detection services to the public in the\nepidemic areas (http://5227i971s5.goho.co:30290). The source codes of MpoxMamba\nare available at https://github.com/YubiaoYue/MpoxMamba."
                },
                "authors": [
                    {
                        "name": "Yubiao Yue"
                    },
                    {
                        "name": "Jun Xue"
                    },
                    {
                        "name": "Haihuang Liang"
                    },
                    {
                        "name": "Zhenzhang Li"
                    },
                    {
                        "name": "Yufeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yufeng Wang"
                },
                "author": "Yufeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04214v2",
                "updated": "2024-09-09T02:46:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    2,
                    46,
                    34,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T12:11:06Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    11,
                    6,
                    4,
                    250,
                    0
                ],
                "title": "Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver"
                },
                "summary": "Mathematical reasoning remains an ongoing challenge for AI models, especially\nfor geometry problems that require both linguistic and visual signals. As the\nvision encoders of most MLLMs are trained on natural scenes, they often\nstruggle to understand geometric diagrams, performing no better in geometry\nproblem solving than LLMs that only process text. This limitation is amplified\nby the lack of effective methods for representing geometric relationships. To\naddress these issues, we introduce the Diagram Formalization Enhanced Geometry\nProblem Solver (DFE-GPS), a new framework that integrates visual features,\ngeometric formal language, and natural language representations. We propose a\nnovel synthetic data approach and create a large-scale geometric dataset,\nSynthGeo228K, annotated with both formal and natural language captions,\ndesigned to enhance the vision encoder for a better understanding of geometric\nstructures. Our framework improves MLLMs' ability to process geometric diagrams\nand extends their application to open-ended tasks on the formalgeo7k dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning remains an ongoing challenge for AI models, especially\nfor geometry problems that require both linguistic and visual signals. As the\nvision encoders of most MLLMs are trained on natural scenes, they often\nstruggle to understand geometric diagrams, performing no better in geometry\nproblem solving than LLMs that only process text. This limitation is amplified\nby the lack of effective methods for representing geometric relationships. To\naddress these issues, we introduce the Diagram Formalization Enhanced Geometry\nProblem Solver (DFE-GPS), a new framework that integrates visual features,\ngeometric formal language, and natural language representations. We propose a\nnovel synthetic data approach and create a large-scale geometric dataset,\nSynthGeo228K, annotated with both formal and natural language captions,\ndesigned to enhance the vision encoder for a better understanding of geometric\nstructures. Our framework improves MLLMs' ability to process geometric diagrams\nand extends their application to open-ended tasks on the formalgeo7k dataset."
                },
                "authors": [
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Jo-Ku Cheng"
                    },
                    {
                        "name": "Jingyang Deng"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Jinwen Ma"
                    },
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Xiaokai Zhang"
                    },
                    {
                        "name": "Na Zhu"
                    },
                    {
                        "name": "Tuo Leng"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Leng"
                },
                "author": "Tuo Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02822v2",
                "updated": "2024-09-06T11:45:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    45,
                    17,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-04T15:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    42,
                    29,
                    2,
                    248,
                    0
                ],
                "title": "Language Understanding as a Constraint on Consensus Size in LLM\n  Societies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Understanding as a Constraint on Consensus Size in LLM\n  Societies"
                },
                "summary": "The applications of Large Language Models (LLMs) are going towards\ncollaborative tasks where several agents interact with each other like in an\nLLM society. In such a setting, large groups of LLMs could reach consensus\nabout arbitrary norms for which there is no information supporting one option\nover another, regulating their own behavior in a self-organized way. In human\nsocieties, the ability to reach consensus without institutions has a limit in\nthe cognitive capacities of humans. To understand if a similar phenomenon\ncharacterizes also LLMs, we apply methods from complexity science and\nprinciples from behavioral sciences in a new approach of AI anthropology. We\nfind that LLMs are able to reach consensus in groups and that the opinion\ndynamics of LLMs can be understood with a function parametrized by a majority\nforce coefficient that determines whether consensus is possible. This majority\nforce is stronger for models with higher language understanding capabilities\nand decreases for larger groups, leading to a critical group size beyond which,\nfor a given LLM, consensus is unfeasible. This critical group size grows\nexponentially with the language understanding capabilities of models and for\nthe most advanced models, it can reach an order of magnitude beyond the typical\nsize of informal human groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of Large Language Models (LLMs) are going towards\ncollaborative tasks where several agents interact with each other like in an\nLLM society. In such a setting, large groups of LLMs could reach consensus\nabout arbitrary norms for which there is no information supporting one option\nover another, regulating their own behavior in a self-organized way. In human\nsocieties, the ability to reach consensus without institutions has a limit in\nthe cognitive capacities of humans. To understand if a similar phenomenon\ncharacterizes also LLMs, we apply methods from complexity science and\nprinciples from behavioral sciences in a new approach of AI anthropology. We\nfind that LLMs are able to reach consensus in groups and that the opinion\ndynamics of LLMs can be understood with a function parametrized by a majority\nforce coefficient that determines whether consensus is possible. This majority\nforce is stronger for models with higher language understanding capabilities\nand decreases for larger groups, leading to a critical group size beyond which,\nfor a given LLM, consensus is unfeasible. This critical group size grows\nexponentially with the language understanding capabilities of models and for\nthe most advanced models, it can reach an order of magnitude beyond the typical\nsize of informal human groups."
                },
                "authors": [
                    {
                        "name": "Giordano De Marzo"
                    },
                    {
                        "name": "Claudio Castellano"
                    },
                    {
                        "name": "David Garcia"
                    }
                ],
                "author_detail": {
                    "name": "David Garcia"
                },
                "author": "David Garcia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02664v3",
                "updated": "2024-09-06T11:38:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    38,
                    0,
                    4,
                    250,
                    0
                ],
                "published": "2024-05-04T13:25:06Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    13,
                    25,
                    6,
                    5,
                    125,
                    0
                ],
                "title": "MedPromptExtract (Medical Data Extraction Tool): Anonymization and\n  Hi-fidelity Automated data extraction using NLP and prompt engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedPromptExtract (Medical Data Extraction Tool): Anonymization and\n  Hi-fidelity Automated data extraction using NLP and prompt engineering"
                },
                "summary": "Introduction: The labour-intensive nature of data extraction from sources\nlike discharge summaries (DS) poses significant obstacles to the digitisation\nof medical records particularly for low- and middle-income countries (LMICs).\nIn this paper we present a completely automated method MedPromptExtract to\nefficiently extract data from DS while maintaining confidentiality. Methods:\nThe source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani\nHospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing\ntool EIGEN which leverages semi-supervised learning techniques for\nhigh-fidelity information extraction was used to anonymize the DS, Natural\nLanguage Processing (NLP) was used to extract data from regular fields. We used\nPrompt Engineering and Large Language Model(LLM) to extract custom clinical\ninformation from free flowing text describing the patients stay in the\nhospital. Twelve features associated with occurrence of AKI were extracted. The\nLLM responses were validated against clinicians annotations. Results: The\nMedPromptExtracttool first subjected DS to the anonymization pipeline which\ntook three seconds per summary. Successful anonymization was verified by\nclinicians, thereafter NLP pipeline extracted structured text from the\nanonymized pdfs at the rate of 0.2 seconds per summary with 100%\naccuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the\ntwelve features. Accuracy metrics were calculated by comparing model responses\nto clinicians annotations with seven features achieving AUCs above 0.9,\nindicating high fidelity of the extraction process. Conclusion:\nMedPromptExtract serves as an automated adaptable tool for efficient data\nextraction from medical records with a dynamic user interface. Keywords:\nDigitizing Medical Records, Automated Anonymisation, Information Retrieval,\nLarge Language Models, Prompt Engineering",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: The labour-intensive nature of data extraction from sources\nlike discharge summaries (DS) poses significant obstacles to the digitisation\nof medical records particularly for low- and middle-income countries (LMICs).\nIn this paper we present a completely automated method MedPromptExtract to\nefficiently extract data from DS while maintaining confidentiality. Methods:\nThe source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani\nHospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing\ntool EIGEN which leverages semi-supervised learning techniques for\nhigh-fidelity information extraction was used to anonymize the DS, Natural\nLanguage Processing (NLP) was used to extract data from regular fields. We used\nPrompt Engineering and Large Language Model(LLM) to extract custom clinical\ninformation from free flowing text describing the patients stay in the\nhospital. Twelve features associated with occurrence of AKI were extracted. The\nLLM responses were validated against clinicians annotations. Results: The\nMedPromptExtracttool first subjected DS to the anonymization pipeline which\ntook three seconds per summary. Successful anonymization was verified by\nclinicians, thereafter NLP pipeline extracted structured text from the\nanonymized pdfs at the rate of 0.2 seconds per summary with 100%\naccuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the\ntwelve features. Accuracy metrics were calculated by comparing model responses\nto clinicians annotations with seven features achieving AUCs above 0.9,\nindicating high fidelity of the extraction process. Conclusion:\nMedPromptExtract serves as an automated adaptable tool for efficient data\nextraction from medical records with a dynamic user interface. Keywords:\nDigitizing Medical Records, Automated Anonymisation, Information Retrieval,\nLarge Language Models, Prompt Engineering"
                },
                "authors": [
                    {
                        "name": "Roomani Srivastava"
                    },
                    {
                        "name": "Suraj Prasad"
                    },
                    {
                        "name": "Lipika Bhat"
                    },
                    {
                        "name": "Sarvesh Deshpande"
                    },
                    {
                        "name": "Barnali Das"
                    },
                    {
                        "name": "Kshitij Jadhav"
                    }
                ],
                "author_detail": {
                    "name": "Kshitij Jadhav"
                },
                "author": "Kshitij Jadhav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15366v2",
                "updated": "2024-09-06T11:25:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    25,
                    35,
                    4,
                    250,
                    0
                ],
                "published": "2024-02-23T14:57:51Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    14,
                    57,
                    51,
                    4,
                    54,
                    0
                ],
                "title": "Portable acceleration of CMS computing workflows with coprocessors as a\n  service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable acceleration of CMS computing workflows with coprocessors as a\n  service"
                },
                "summary": "Computing demands for large scientific experiments, such as the CMS\nexperiment at the CERN LHC, will increase dramatically in the next decades. To\ncomplement the future performance increases of software running on central\nprocessing units (CPUs), explorations of coprocessor usage in data processing\nhold great potential and interest. Coprocessors are a class of computer\nprocessors that supplement CPUs, often improving the execution of certain\nfunctions due to architectural design choices. We explore the approach of\nServices for Optimized Network Inference on Coprocessors (SONIC) and study the\ndeployment of this as-a-service approach in large-scale data processing. In the\nstudies, we take a data processing workflow of the CMS experiment and run the\nmain workflow on CPUs, while offloading several machine learning (ML) inference\ntasks onto either remote or local coprocessors, specifically graphics\nprocessing units (GPUs). With experiments performed at Google Cloud, the Purdue\nTier-2 computing center, and combinations of the two, we demonstrate the\nacceleration of these ML algorithms individually on coprocessors and the\ncorresponding throughput improvement for the entire workflow. This approach can\nbe easily generalized to different types of coprocessors and deployed on local\nCPUs without decreasing the throughput performance. We emphasize that the SONIC\napproach enables high coprocessor usage and enables the portability to run\nworkflows on different types of coprocessors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computing demands for large scientific experiments, such as the CMS\nexperiment at the CERN LHC, will increase dramatically in the next decades. To\ncomplement the future performance increases of software running on central\nprocessing units (CPUs), explorations of coprocessor usage in data processing\nhold great potential and interest. Coprocessors are a class of computer\nprocessors that supplement CPUs, often improving the execution of certain\nfunctions due to architectural design choices. We explore the approach of\nServices for Optimized Network Inference on Coprocessors (SONIC) and study the\ndeployment of this as-a-service approach in large-scale data processing. In the\nstudies, we take a data processing workflow of the CMS experiment and run the\nmain workflow on CPUs, while offloading several machine learning (ML) inference\ntasks onto either remote or local coprocessors, specifically graphics\nprocessing units (GPUs). With experiments performed at Google Cloud, the Purdue\nTier-2 computing center, and combinations of the two, we demonstrate the\nacceleration of these ML algorithms individually on coprocessors and the\ncorresponding throughput improvement for the entire workflow. This approach can\nbe easily generalized to different types of coprocessors and deployed on local\nCPUs without decreasing the throughput performance. We emphasize that the SONIC\napproach enables high coprocessor usage and enables the portability to run\nworkflows on different types of coprocessors."
                },
                "authors": [
                    {
                        "name": "CMS Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "CMS Collaboration"
                },
                "author": "CMS Collaboration",
                "arxiv_doi": "10.1007/s41781-024-00124-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s41781-024-00124-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.15366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Replaced with the published version. Added the journal reference and\n  the DOI. All the figures and tables can be found at\n  http://cms-results.web.cern.ch/cms-results/public-results/publications/MLG-23-001\n  (CMS Public Pages)",
                "arxiv_journal_ref": "Comput. Softw. Big Sci. 8 (2024) 17",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08632v2",
                "updated": "2024-09-06T11:20:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    20,
                    13,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-16T09:52:02Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    9,
                    52,
                    2,
                    4,
                    229,
                    0
                ],
                "title": "A Survey on Benchmarks of Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Benchmarks of Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on\n(1)perception and understanding, (2)cognition and reasoning, (3)specific\ndomains, (4)key capabilities, and (5)other modalities. Finally, we discuss the\nlimitations of the current evaluation methods for MLLMs and explore promising\nfuture directions. Our key argument is that evaluation should be regarded as a\ncrucial discipline to support the development of MLLMs better. For more\ndetails, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on\n(1)perception and understanding, (2)cognition and reasoning, (3)specific\ndomains, (4)key capabilities, and (5)other modalities. Finally, we discuss the\nlimitations of the current evaluation methods for MLLMs and explore promising\nfuture directions. Our key argument is that evaluation should be regarded as a\ncrucial discipline to support the development of MLLMs better. For more\ndetails, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey."
                },
                "authors": [
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Weiheng Lu"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Min Xia"
                    },
                    {
                        "name": "Yizhang Jin"
                    },
                    {
                        "name": "Zhenye Gan"
                    },
                    {
                        "name": "Ding Qi"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Ying Tai"
                    },
                    {
                        "name": "Wankou Yang"
                    },
                    {
                        "name": "Yabiao Wang"
                    },
                    {
                        "name": "Chengjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chengjie Wang"
                },
                "author": "Chengjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09225v2",
                "updated": "2024-09-06T11:15:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    11,
                    15,
                    23,
                    4,
                    250,
                    0
                ],
                "published": "2024-02-14T15:09:01Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    15,
                    9,
                    1,
                    2,
                    45,
                    0
                ],
                "title": "Is my Data in your AI Model? Membership Inference Test with Application\n  to Face Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is my Data in your AI Model? Membership Inference Test with Application\n  to Face Images"
                },
                "summary": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Javier Ortega-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Javier Ortega-Garcia"
                },
                "author": "Javier Ortega-Garcia",
                "arxiv_comment": "12 pages including references and authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04183v1",
                "updated": "2024-09-06T10:57:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    57,
                    34,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T10:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    57,
                    34,
                    4,
                    250,
                    0
                ],
                "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding"
                },
                "summary": "Programming languages possess rich semantic information such as data flow\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with four different baseline LLMs ranging in size from 350M to 8B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming languages possess rich semantic information such as data flow\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with four different baseline LLMs ranging in size from 350M to 8B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Shijie Li"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04181v1",
                "updated": "2024-09-06T10:49:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    49,
                    46,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T10:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    49,
                    46,
                    4,
                    250,
                    0
                ],
                "title": "Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question\n  Answering"
                },
                "summary": "Advancements in natural language processing have revolutionized the way we\ncan interact with digital information systems, such as databases, making them\nmore accessible. However, challenges persist, especially when accuracy is\ncritical, as in the biomedical domain. A key issue is the hallucination\nproblem, where models generate information unsupported by the underlying data,\npotentially leading to dangerous misinformation. This paper presents a novel\napproach designed to bridge this gap by combining Large Language Models (LLM)\nand Knowledge Graphs (KG) to improve the accuracy and reliability of\nquestion-answering systems, on the example of a biomedical KG. Built on the\nLangChain framework, our method incorporates a query checker that ensures the\nsyntactical and semantic validity of LLM-generated queries, which are then used\nto extract information from a Knowledge Graph, substantially reducing errors\nlike hallucinations. We evaluated the overall performance using a new benchmark\ndataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo\nand llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other\nmodels in generating accurate queries, open-source models like llama3:70b show\npromise with appropriate prompt engineering. To make this approach accessible,\na user-friendly web-based interface has been developed, allowing users to input\nnatural language queries, view generated and corrected Cypher queries, and\nverify the resulting paths for accuracy. Overall, this hybrid approach\neffectively addresses common issues such as data gaps and hallucinations,\noffering a reliable and intuitive solution for question answering systems. The\nsource code for generating the results of this paper and for the user-interface\ncan be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in natural language processing have revolutionized the way we\ncan interact with digital information systems, such as databases, making them\nmore accessible. However, challenges persist, especially when accuracy is\ncritical, as in the biomedical domain. A key issue is the hallucination\nproblem, where models generate information unsupported by the underlying data,\npotentially leading to dangerous misinformation. This paper presents a novel\napproach designed to bridge this gap by combining Large Language Models (LLM)\nand Knowledge Graphs (KG) to improve the accuracy and reliability of\nquestion-answering systems, on the example of a biomedical KG. Built on the\nLangChain framework, our method incorporates a query checker that ensures the\nsyntactical and semantic validity of LLM-generated queries, which are then used\nto extract information from a Knowledge Graph, substantially reducing errors\nlike hallucinations. We evaluated the overall performance using a new benchmark\ndataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo\nand llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other\nmodels in generating accurate queries, open-source models like llama3:70b show\npromise with appropriate prompt engineering. To make this approach accessible,\na user-friendly web-based interface has been developed, allowing users to input\nnatural language queries, view generated and corrected Cypher queries, and\nverify the resulting paths for accuracy. Overall, this hybrid approach\neffectively addresses common issues such as data gaps and hallucinations,\noffering a reliable and intuitive solution for question answering systems. The\nsource code for generating the results of this paper and for the user-interface\ncan be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui"
                },
                "authors": [
                    {
                        "name": "Larissa Pusch"
                    },
                    {
                        "name": "Tim O. F. Conrad"
                    }
                ],
                "author_detail": {
                    "name": "Tim O. F. Conrad"
                },
                "author": "Tim O. F. Conrad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03274v2",
                "updated": "2024-09-06T10:31:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    31,
                    7,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T06:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    31,
                    37,
                    3,
                    249,
                    0
                ],
                "title": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures."
                },
                "authors": [
                    {
                        "name": "Jing Cui"
                    },
                    {
                        "name": "Yishi Xu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Shuchang Zhou"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02795v3",
                "updated": "2024-09-09T09:31:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    31,
                    30,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-04T15:11:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07747v2",
                "updated": "2024-09-06T10:19:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    19,
                    10,
                    4,
                    250,
                    0
                ],
                "published": "2024-03-12T15:32:39Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    15,
                    32,
                    39,
                    1,
                    72,
                    0
                ],
                "title": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese\n  Large Language Models"
                },
                "summary": "To thoroughly assess the mathematical reasoning abilities of Large Language\nModels (LLMs), we need to carefully curate evaluation datasets covering diverse\nmathematical concepts and mathematical problems at different difficulty levels.\nIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\nmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\nis created to cover the major key mathematical concepts taught in elementary\nschool math, which are further divided into 17 categories of math word\nproblems, enabling in-depth analysis of mathematical reasoning abilities of\nLLMs. All the 17 categories of math word problems are manually annotated with\ntheir difficulty levels according to the number of reasoning steps required to\nsolve these problems. We conduct extensive experiments on a wide range of LLMs\non FineMath and find that there is still considerable room for improvements in\nterms of mathematical reasoning capability of Chinese LLMs. We also carry out\nan in-depth analysis on the evaluation process and methods that have been\noverlooked previously. These two factors significantly influence the model\nresults and our understanding of their mathematical reasoning capabilities. The\ndataset will be publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To thoroughly assess the mathematical reasoning abilities of Large Language\nModels (LLMs), we need to carefully curate evaluation datasets covering diverse\nmathematical concepts and mathematical problems at different difficulty levels.\nIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\nmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\nis created to cover the major key mathematical concepts taught in elementary\nschool math, which are further divided into 17 categories of math word\nproblems, enabling in-depth analysis of mathematical reasoning abilities of\nLLMs. All the 17 categories of math word problems are manually annotated with\ntheir difficulty levels according to the number of reasoning steps required to\nsolve these problems. We conduct extensive experiments on a wide range of LLMs\non FineMath and find that there is still considerable room for improvements in\nterms of mathematical reasoning capability of Chinese LLMs. We also carry out\nan in-depth analysis on the evaluation process and methods that have been\noverlooked previously. These two factors significantly influence the model\nresults and our understanding of their mathematical reasoning capabilities. The\ndataset will be publicly available soon."
                },
                "authors": [
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Zheng Yao"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04168v1",
                "updated": "2024-09-06T10:09:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    9,
                    41,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T10:09:41Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    9,
                    41,
                    4,
                    250,
                    0
                ],
                "title": "From Calculation to Adjudication: Examining LLM judges on Mathematical\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Calculation to Adjudication: Examining LLM judges on Mathematical\n  Reasoning Tasks"
                },
                "summary": "To reduce the need for human annotations, large language models (LLMs) have\nbeen proposed as judges of the quality of other candidate models. LLM judges\nare typically evaluated by measuring the correlation with human judgments on\ngeneration tasks such as summarization or machine translation. In contrast, we\nstudy LLM judges on mathematical reasoning tasks. These tasks require\nmulti-step reasoning, and the correctness of their solutions is verifiable,\nenabling a more objective evaluation. We perform a detailed performance\nanalysis and find that the used judges are mostly unable to improve task\nperformance but are able to pick the better model. Our analysis uncovers a\nstrong correlation between judgment performance and the candidate model task\nperformance. We observe that judges tend to choose the model of higher quality\neven if its answer is incorrect. Further, we show that it is possible to use\nstatistics, such as the task performances of the individual models, to predict\njudgment performance. In an ablation, we either swap or mask the candidate\nanswers and observe that judges often keep the original judgment, providing\nevidence that judges incorporate writing style in their judgments. In summary,\nwe find that regularities in the judgments are quantifiable using statistical\nmeasures and provide various angles on exploiting them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce the need for human annotations, large language models (LLMs) have\nbeen proposed as judges of the quality of other candidate models. LLM judges\nare typically evaluated by measuring the correlation with human judgments on\ngeneration tasks such as summarization or machine translation. In contrast, we\nstudy LLM judges on mathematical reasoning tasks. These tasks require\nmulti-step reasoning, and the correctness of their solutions is verifiable,\nenabling a more objective evaluation. We perform a detailed performance\nanalysis and find that the used judges are mostly unable to improve task\nperformance but are able to pick the better model. Our analysis uncovers a\nstrong correlation between judgment performance and the candidate model task\nperformance. We observe that judges tend to choose the model of higher quality\neven if its answer is incorrect. Further, we show that it is possible to use\nstatistics, such as the task performances of the individual models, to predict\njudgment performance. In an ablation, we either swap or mask the candidate\nanswers and observe that judges often keep the original judgment, providing\nevidence that judges incorporate writing style in their judgments. In summary,\nwe find that regularities in the judgments are quantifiable using statistical\nmeasures and provide various angles on exploiting them."
                },
                "authors": [
                    {
                        "name": "Andreas Stephan"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Matthias Aßenmacher"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Benjamin Roth"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Roth"
                },
                "author": "Benjamin Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04164v1",
                "updated": "2024-09-06T10:03:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    3,
                    49,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T10:03:49Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    3,
                    49,
                    4,
                    250,
                    0
                ],
                "title": "Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language\n  Models for Text-to-Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language\n  Models for Text-to-Code Generation"
                },
                "summary": "In recent years, large language models (LLMs) have emerged as powerful tools\nwith potential applications in various fields, including software engineering.\nWithin the scope of this research, we evaluate five different state-of-the-art\nLLMs - Bard, BingChat, ChatGPT, Llama2, and Code Llama - concerning their\ncapabilities for text-to-code generation. In an empirical study, we feed\nprompts with textual descriptions of coding problems sourced from the\nprogramming website LeetCode to the models with the task of creating solutions\nin Python. Subsequently, the quality of the generated outputs is assessed using\nthe testing functionalities of LeetCode. The results indicate large differences\nin performance between the investigated models. ChatGPT can handle these\ntypical programming challenges by far the most effectively, surpassing even\ncode-specialized models like Code Llama. To gain further insights, we measure\nthe runtime as well as the memory usage of the generated outputs and compared\nthem to the other code submissions on Leetcode. A detailed error analysis,\nencompassing a comparison of the differences concerning correct indentation and\nform of the generated code as well as an assignment of the incorrectly solved\ntasks to certain error categories allows us to obtain a more nuanced picture of\nthe results and potential for improvement. The results also show a clear\npattern of increasingly incorrect produced code when the models are facing a\nlot of context in the form of longer prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have emerged as powerful tools\nwith potential applications in various fields, including software engineering.\nWithin the scope of this research, we evaluate five different state-of-the-art\nLLMs - Bard, BingChat, ChatGPT, Llama2, and Code Llama - concerning their\ncapabilities for text-to-code generation. In an empirical study, we feed\nprompts with textual descriptions of coding problems sourced from the\nprogramming website LeetCode to the models with the task of creating solutions\nin Python. Subsequently, the quality of the generated outputs is assessed using\nthe testing functionalities of LeetCode. The results indicate large differences\nin performance between the investigated models. ChatGPT can handle these\ntypical programming challenges by far the most effectively, surpassing even\ncode-specialized models like Code Llama. To gain further insights, we measure\nthe runtime as well as the memory usage of the generated outputs and compared\nthem to the other code submissions on Leetcode. A detailed error analysis,\nencompassing a comparison of the differences concerning correct indentation and\nform of the generated code as well as an assignment of the incorrectly solved\ntasks to certain error categories allows us to obtain a more nuanced picture of\nthe results and potential for improvement. The results also show a clear\npattern of increasingly incorrect produced code when the models are facing a\nlot of context in the form of longer prompts."
                },
                "authors": [
                    {
                        "name": "Luis Mayer"
                    },
                    {
                        "name": "Christian Heumann"
                    },
                    {
                        "name": "Matthias Aßenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Aßenmacher"
                },
                "author": "Matthias Aßenmacher",
                "arxiv_comment": "Conference Paper accepted at the 9th SwissText Conference (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03381v2",
                "updated": "2024-09-06T09:37:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    37,
                    36,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T09:33:24Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "title": "CogniDual Framework: Self-Training Large Language Models within a\n  Dual-System Theoretical Framework for Improving Cognitive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniDual Framework: Self-Training Large Language Models within a\n  Dual-System Theoretical Framework for Improving Cognitive Tasks"
                },
                "summary": "Cognitive psychology investigates perception, attention, memory, language,\nproblem-solving, decision-making, and reasoning. Kahneman's dual-system theory\nelucidates the human decision-making process, distinguishing between the rapid,\nintuitive System 1 and the deliberative, rational System 2. Recent advancements\nhave positioned large language Models (LLMs) as formidable tools nearing\nhuman-level proficiency in various cognitive tasks. Nonetheless, the presence\nof a dual-system framework analogous to human cognition in LLMs remains\nunexplored. This study introduces the \\textbf{CogniDual Framework for LLMs}\n(CFLLMs), designed to assess whether LLMs can, through self-training, evolve\nfrom deliberate deduction to intuitive responses, thereby emulating the human\nprocess of acquiring and mastering new information. Our findings reveal the\ncognitive mechanisms behind LLMs' response generation, enhancing our\nunderstanding of their capabilities in cognitive psychology. Practically,\nself-trained models can provide faster responses to certain queries, reducing\ncomputational demands during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive psychology investigates perception, attention, memory, language,\nproblem-solving, decision-making, and reasoning. Kahneman's dual-system theory\nelucidates the human decision-making process, distinguishing between the rapid,\nintuitive System 1 and the deliberative, rational System 2. Recent advancements\nhave positioned large language Models (LLMs) as formidable tools nearing\nhuman-level proficiency in various cognitive tasks. Nonetheless, the presence\nof a dual-system framework analogous to human cognition in LLMs remains\nunexplored. This study introduces the \\textbf{CogniDual Framework for LLMs}\n(CFLLMs), designed to assess whether LLMs can, through self-training, evolve\nfrom deliberate deduction to intuitive responses, thereby emulating the human\nprocess of acquiring and mastering new information. Our findings reveal the\ncognitive mechanisms behind LLMs' response generation, enhancing our\nunderstanding of their capabilities in cognitive psychology. Practically,\nself-trained models can provide faster responses to certain queries, reducing\ncomputational demands during inference."
                },
                "authors": [
                    {
                        "name": "Yongxin Deng"
                    },
                    {
                        "name": "Xihe Qiu"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Jing Pan"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Wei Chu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chu"
                },
                "arxiv_affiliation": "INF Technology",
                "author": "Wei Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18262v2",
                "updated": "2024-09-06T09:34:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    34,
                    39,
                    4,
                    250,
                    0
                ],
                "published": "2024-06-26T11:20:15Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    11,
                    20,
                    15,
                    2,
                    178,
                    0
                ],
                "title": "GlucOS: Security, correctness, and simplicity for automated insulin\n  delivery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GlucOS: Security, correctness, and simplicity for automated insulin\n  delivery"
                },
                "summary": "We present GlucOS, a novel system for trustworthy automated insulin delivery.\nFundamentally, this paper is about a system we designed, implemented, and\ndeployed on real humans and the lessons learned from our experiences. GlucOS\ncombines algorithmic security, driver security, and end-to-end verification to\nprotect against malicious ML models, vulnerable pump drivers, and drastic\nchanges in human physiology. We use formal methods to prove correctness of\ncritical components and incorporate humans as part of our defensive strategy.\nOur evaluation includes both a real-world deployment with seven individuals and\nresults from simulation to show that our techniques generalize. Our results\nshow that GlucOS maintains safety and improves glucose control even under\nattack conditions. This work demonstrates the potential for secure,\npersonalized, automated healthcare systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GlucOS, a novel system for trustworthy automated insulin delivery.\nFundamentally, this paper is about a system we designed, implemented, and\ndeployed on real humans and the lessons learned from our experiences. GlucOS\ncombines algorithmic security, driver security, and end-to-end verification to\nprotect against malicious ML models, vulnerable pump drivers, and drastic\nchanges in human physiology. We use formal methods to prove correctness of\ncritical components and incorporate humans as part of our defensive strategy.\nOur evaluation includes both a real-world deployment with seven individuals and\nresults from simulation to show that our techniques generalize. Our results\nshow that GlucOS maintains safety and improves glucose control even under\nattack conditions. This work demonstrates the potential for secure,\npersonalized, automated healthcare systems."
                },
                "authors": [
                    {
                        "name": "Hari Venugopalan"
                    },
                    {
                        "name": "Shreyas Madhav Ambattur Vijayanand"
                    },
                    {
                        "name": "Caleb Stanford"
                    },
                    {
                        "name": "Stephanie Crossen"
                    },
                    {
                        "name": "Samuel T. King"
                    }
                ],
                "author_detail": {
                    "name": "Samuel T. King"
                },
                "author": "Samuel T. King",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04114v1",
                "updated": "2024-09-06T08:31:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    31,
                    18,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:31:18Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    31,
                    18,
                    4,
                    250,
                    0
                ],
                "title": "Multi-Programming Language Ensemble for Code Generation in Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Programming Language Ensemble for Code Generation in Large\n  Language Model"
                },
                "summary": "Large language models (LLMs) have significantly improved code generation,\nparticularly in one-pass code generation. However, most existing approaches\nfocus solely on generating code in a single programming language, overlooking\nthe potential of leveraging the multi-language capabilities of LLMs. LLMs have\nvarying patterns of errors across different languages, suggesting that a more\nrobust approach could be developed by leveraging these multi-language outputs.\nIn this study, we propose Multi-Programming Language Ensemble (MPLE), a novel\nensemble-based method that utilizes code generation across multiple programming\nlanguages to enhance overall performance. By treating each language-specific\ncode generation process as an individual \"weak expert\" and effectively\nintegrating their outputs, our method mitigates language-specific errors and\nbiases. This multi-language ensemble strategy leverages the complementary\nstrengths of different programming languages, enabling the model to produce\nmore accurate and robust code. Our approach can be seamlessly integrated with\ncommonly used techniques such as the reflection algorithm and Monte Carlo tree\nsearch to improve code generation quality further. Experimental results show\nthat our framework consistently enhances baseline performance by up to 17.92%\non existing benchmarks (HumanEval and HumanEval-plus), with a standout result\nof 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art\nresults across various LLM models. The code will be released at\nhttps://github.com/NinjaTech-AI/MPLE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly improved code generation,\nparticularly in one-pass code generation. However, most existing approaches\nfocus solely on generating code in a single programming language, overlooking\nthe potential of leveraging the multi-language capabilities of LLMs. LLMs have\nvarying patterns of errors across different languages, suggesting that a more\nrobust approach could be developed by leveraging these multi-language outputs.\nIn this study, we propose Multi-Programming Language Ensemble (MPLE), a novel\nensemble-based method that utilizes code generation across multiple programming\nlanguages to enhance overall performance. By treating each language-specific\ncode generation process as an individual \"weak expert\" and effectively\nintegrating their outputs, our method mitigates language-specific errors and\nbiases. This multi-language ensemble strategy leverages the complementary\nstrengths of different programming languages, enabling the model to produce\nmore accurate and robust code. Our approach can be seamlessly integrated with\ncommonly used techniques such as the reflection algorithm and Monte Carlo tree\nsearch to improve code generation quality further. Experimental results show\nthat our framework consistently enhances baseline performance by up to 17.92%\non existing benchmarks (HumanEval and HumanEval-plus), with a standout result\nof 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art\nresults across various LLM models. The code will be released at\nhttps://github.com/NinjaTech-AI/MPLE"
                },
                "authors": [
                    {
                        "name": "Tengfei Xue"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Tahir Azim"
                    },
                    {
                        "name": "Roman Smirnov"
                    },
                    {
                        "name": "Jianhui Yu"
                    },
                    {
                        "name": "Arash Sadrieh"
                    },
                    {
                        "name": "Babak Pahlavan"
                    }
                ],
                "author_detail": {
                    "name": "Babak Pahlavan"
                },
                "author": "Babak Pahlavan",
                "arxiv_comment": "Code available at https://github.com/NinjaTech-AI/MPLE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04109v1",
                "updated": "2024-09-06T08:25:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    25,
                    3,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:25:03Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    25,
                    3,
                    4,
                    250,
                    0
                ],
                "title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with\n  100+ NLP Researchers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with\n  100+ NLP Researchers"
                },
                "summary": "Recent advancements in large language models (LLMs) have sparked optimism\nabout their potential to accelerate scientific discovery, with a growing number\nof works proposing research agents that autonomously generate and validate new\nideas. Despite this, no evaluations have shown that LLM systems can take the\nvery first step of producing novel, expert-level ideas, let alone perform the\nentire research process. We address this by establishing an experimental design\nthat evaluates research idea generation while controlling for confounders and\nperforms the first head-to-head comparison between expert NLP researchers and\nan LLM ideation agent. By recruiting over 100 NLP researchers to write novel\nideas and blind reviews of both LLM and human ideas, we obtain the first\nstatistically significant conclusion on current LLM capabilities for research\nideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than\nhuman expert ideas while being judged slightly weaker on feasibility. Studying\nour agent baselines closely, we identify open problems in building and\nevaluating research agents, including failures of LLM self-evaluation and their\nlack of diversity in generation. Finally, we acknowledge that human judgements\nof novelty can be difficult, even by experts, and propose an end-to-end study\ndesign which recruits researchers to execute these ideas into full projects,\nenabling us to study whether these novelty and feasibility judgements result in\nmeaningful differences in research outcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have sparked optimism\nabout their potential to accelerate scientific discovery, with a growing number\nof works proposing research agents that autonomously generate and validate new\nideas. Despite this, no evaluations have shown that LLM systems can take the\nvery first step of producing novel, expert-level ideas, let alone perform the\nentire research process. We address this by establishing an experimental design\nthat evaluates research idea generation while controlling for confounders and\nperforms the first head-to-head comparison between expert NLP researchers and\nan LLM ideation agent. By recruiting over 100 NLP researchers to write novel\nideas and blind reviews of both LLM and human ideas, we obtain the first\nstatistically significant conclusion on current LLM capabilities for research\nideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than\nhuman expert ideas while being judged slightly weaker on feasibility. Studying\nour agent baselines closely, we identify open problems in building and\nevaluating research agents, including failures of LLM self-evaluation and their\nlack of diversity in generation. Finally, we acknowledge that human judgements\nof novelty can be difficult, even by experts, and propose an end-to-end study\ndesign which recruits researchers to execute these ideas into full projects,\nenabling us to study whether these novelty and feasibility judgements result in\nmeaningful differences in research outcome."
                },
                "authors": [
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Diyi Yang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "main paper is 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04095v1",
                "updated": "2024-09-06T08:02:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    2,
                    43,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T08:02:43Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    2,
                    43,
                    4,
                    250,
                    0
                ],
                "title": "UNIT: Unifying Image and Text Recognition in One Vision Encoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNIT: Unifying Image and Text Recognition in One Vision Encoder"
                },
                "summary": "Currently, vision encoder models like Vision Transformers (ViTs) typically\nexcel at image recognition tasks but cannot simultaneously support text\nrecognition like human visual recognition. To address this limitation, we\npropose UNIT, a novel training framework aimed at UNifying Image and Text\nrecognition within a single model. Starting with a vision encoder pre-trained\nwith image recognition tasks, UNIT introduces a lightweight language decoder\nfor predicting text outputs and a lightweight vision decoder to prevent\ncatastrophic forgetting of the original image encoding capabilities. The\ntraining process comprises two stages: intra-scale pretraining and inter-scale\nfinetuning. During intra-scale pretraining, UNIT learns unified representations\nfrom multi-scale inputs, where images and documents are at their commonly used\nresolution, to enable fundamental recognition capability. In the inter-scale\nfinetuning stage, the model introduces scale-exchanged data, featuring images\nand documents at resolutions different from the most commonly used ones, to\nenhance its scale robustness. Notably, UNIT retains the original vision encoder\narchitecture, making it cost-free in terms of inference and deployment.\nExperiments across multiple benchmarks confirm that our method significantly\noutperforms existing methods on document-related tasks (e.g., OCR and DocQA)\nwhile maintaining the performances on natural images, demonstrating its ability\nto substantially enhance text recognition without compromising its core image\nrecognition capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, vision encoder models like Vision Transformers (ViTs) typically\nexcel at image recognition tasks but cannot simultaneously support text\nrecognition like human visual recognition. To address this limitation, we\npropose UNIT, a novel training framework aimed at UNifying Image and Text\nrecognition within a single model. Starting with a vision encoder pre-trained\nwith image recognition tasks, UNIT introduces a lightweight language decoder\nfor predicting text outputs and a lightweight vision decoder to prevent\ncatastrophic forgetting of the original image encoding capabilities. The\ntraining process comprises two stages: intra-scale pretraining and inter-scale\nfinetuning. During intra-scale pretraining, UNIT learns unified representations\nfrom multi-scale inputs, where images and documents are at their commonly used\nresolution, to enable fundamental recognition capability. In the inter-scale\nfinetuning stage, the model introduces scale-exchanged data, featuring images\nand documents at resolutions different from the most commonly used ones, to\nenhance its scale robustness. Notably, UNIT retains the original vision encoder\narchitecture, making it cost-free in terms of inference and deployment.\nExperiments across multiple benchmarks confirm that our method significantly\noutperforms existing methods on document-related tasks (e.g., OCR and DocQA)\nwhile maintaining the performances on natural images, demonstrating its ability\nto substantially enhance text recognition without compromising its core image\nrecognition capabilities."
                },
                "authors": [
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Yanpeng Zhou"
                    },
                    {
                        "name": "Chunwei Wang"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Hang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Xu"
                },
                "author": "Hang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.06615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.06615v2",
                "updated": "2024-09-06T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    44,
                    47,
                    4,
                    250,
                    0
                ],
                "published": "2022-11-12T09:32:31Z",
                "published_parsed": [
                    2022,
                    11,
                    12,
                    9,
                    32,
                    31,
                    5,
                    316,
                    0
                ],
                "title": "A Shared Cluster-based Stochastic Channel Model for Integrated Sensing\n  and Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Shared Cluster-based Stochastic Channel Model for Integrated Sensing\n  and Communication Systems"
                },
                "summary": "Integrated Sensing And Communication (ISAC) has been recognized as a\npromising technology in the 6G communication. A realistic channel model is a\nprerequisite for designing ISAC systems. Most existing channel models\nindependently generate the communication and sensing channels under the same\nframework. However, due to the multiplexing of hardware resources and the same\nenvironment, signals enabled for communication and sensing may experience\nshared propagation scatterers. This practical sharing feature necessities the\njoint generation of communication and sensing channels for realistic modeling,\nwhere the shared clusters (contributed by the shared scatterers) should be\nreconstructed.In this paper, we first conduct communication and sensing channel\nmeasurements for an indoor scenario at 28 GHz. The power-angular-delay profiles\nof multipath components are obtained, and the shared scatterers by\ncommunication and sensing channels are intuitively observed. Then, a stochastic\nISAC channel model is proposed to capture the sharing feature, where shared and\nnon-shared clusters by the two channels are dfined and superimposed. To extract\nthose clusters from measured ISAC channels, a KPowerMeans-based joint\nclustering algorithm is novelly introduced. Finally, stochastic channel\ncharacteristics are analyzed, and empirical simulations validate that the\nchannel Sharing Degree (SD) increases with more shared clusters. The proposed\nmodel can realistically capture the sharing feature of ISAC channels and is\nable to evaluate and simulate the channel SD values, which is valuable for the\ndesign and deployment of ISAC systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Sensing And Communication (ISAC) has been recognized as a\npromising technology in the 6G communication. A realistic channel model is a\nprerequisite for designing ISAC systems. Most existing channel models\nindependently generate the communication and sensing channels under the same\nframework. However, due to the multiplexing of hardware resources and the same\nenvironment, signals enabled for communication and sensing may experience\nshared propagation scatterers. This practical sharing feature necessities the\njoint generation of communication and sensing channels for realistic modeling,\nwhere the shared clusters (contributed by the shared scatterers) should be\nreconstructed.In this paper, we first conduct communication and sensing channel\nmeasurements for an indoor scenario at 28 GHz. The power-angular-delay profiles\nof multipath components are obtained, and the shared scatterers by\ncommunication and sensing channels are intuitively observed. Then, a stochastic\nISAC channel model is proposed to capture the sharing feature, where shared and\nnon-shared clusters by the two channels are dfined and superimposed. To extract\nthose clusters from measured ISAC channels, a KPowerMeans-based joint\nclustering algorithm is novelly introduced. Finally, stochastic channel\ncharacteristics are analyzed, and empirical simulations validate that the\nchannel Sharing Degree (SD) increases with more shared clusters. The proposed\nmodel can realistically capture the sharing feature of ISAC channels and is\nable to evaluate and simulate the channel SD values, which is valuable for the\ndesign and deployment of ISAC systems."
                },
                "authors": [
                    {
                        "name": "Yameng Liu"
                    },
                    {
                        "name": "Jianhua Zhang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Zhiqiang Yuan"
                    },
                    {
                        "name": "Guangyi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guangyi Liu"
                },
                "author": "Guangyi Liu",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.06615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.06615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04081v1",
                "updated": "2024-09-06T07:44:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    44,
                    44,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T07:44:44Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    44,
                    44,
                    4,
                    250,
                    0
                ],
                "title": "UI-JEPA: Towards Active Perception of User Intent through Onscreen User\n  Activity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-JEPA: Towards Active Perception of User Intent through Onscreen User\n  Activity"
                },
                "summary": "Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding."
                },
                "authors": [
                    {
                        "name": "Yicheng Fu"
                    },
                    {
                        "name": "Raviteja Anantha"
                    },
                    {
                        "name": "Prabal Vashisht"
                    },
                    {
                        "name": "Jianpeng Cheng"
                    },
                    {
                        "name": "Etai Littwin"
                    }
                ],
                "author_detail": {
                    "name": "Etai Littwin"
                },
                "author": "Etai Littwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04073v2",
                "updated": "2024-09-09T11:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    33,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T07:29:01Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    29,
                    1,
                    4,
                    250,
                    0
                ],
                "title": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language\n  Model"
                },
                "summary": "Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens)."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Paul Groth"
                    },
                    {
                        "name": "Iacer Calixto"
                    },
                    {
                        "name": "Sebastian Schelter"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Schelter"
                },
                "author": "Sebastian Schelter",
                "arxiv_comment": "12 pages excluding references, 3 figures, and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03351v2",
                "updated": "2024-09-06T07:29:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    29,
                    0,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T08:53:23Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    8,
                    53,
                    23,
                    3,
                    249,
                    0
                ],
                "title": "Digital Ecosystem for FAIR Time Series Data Management in Environmental\n  System Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Ecosystem for FAIR Time Series Data Management in Environmental\n  System Science"
                },
                "summary": "Addressing the challenges posed by climate change, biodiversity loss, and\nenvironmental pollution requires comprehensive monitoring and effective data\nmanagement strategies that are applicable across various scales in\nenvironmental system science. This paper introduces a versatile and\ntransferable digital ecosystem for managing time series data, designed to\nadhere to the FAIR principles (Findable, Accessible, Interoperable, and\nReusable). The system is highly adaptable, cloud-ready, and suitable for\ndeployment in a wide range of settings, from small-scale projects to\nlarge-scale monitoring initiatives. The ecosystem comprises three core\ncomponents: the Sensor Management System (SMS) for detailed metadata\nregistration and management; time.IO, a platform for efficient time series data\nstorage, transfer, and real-time visualization; and the System for Automated\nQuality Control (SaQC), which ensures data integrity through real-time analysis\nand quality assurance. The modular architecture, combined with standardized\nprotocols and interfaces, ensures that the ecosystem can be easily transferred\nand deployed across different environments and institutions. This approach\nenhances data accessibility for a broad spectrum of stakeholders, including\nresearchers, policymakers, and the public, while fostering collaboration and\nadvancing scientific research in environmental monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the challenges posed by climate change, biodiversity loss, and\nenvironmental pollution requires comprehensive monitoring and effective data\nmanagement strategies that are applicable across various scales in\nenvironmental system science. This paper introduces a versatile and\ntransferable digital ecosystem for managing time series data, designed to\nadhere to the FAIR principles (Findable, Accessible, Interoperable, and\nReusable). The system is highly adaptable, cloud-ready, and suitable for\ndeployment in a wide range of settings, from small-scale projects to\nlarge-scale monitoring initiatives. The ecosystem comprises three core\ncomponents: the Sensor Management System (SMS) for detailed metadata\nregistration and management; time.IO, a platform for efficient time series data\nstorage, transfer, and real-time visualization; and the System for Automated\nQuality Control (SaQC), which ensures data integrity through real-time analysis\nand quality assurance. The modular architecture, combined with standardized\nprotocols and interfaces, ensures that the ecosystem can be easily transferred\nand deployed across different environments and institutions. This approach\nenhances data accessibility for a broad spectrum of stakeholders, including\nresearchers, policymakers, and the public, while fostering collaboration and\nadvancing scientific research in environmental monitoring."
                },
                "authors": [
                    {
                        "name": "J. Bumberger"
                    },
                    {
                        "name": "M. Abbrent"
                    },
                    {
                        "name": "N. Brinckmann"
                    },
                    {
                        "name": "J. Hemmen"
                    },
                    {
                        "name": "R. Kunkel"
                    },
                    {
                        "name": "C. Lorenz"
                    },
                    {
                        "name": "P. Lünenschloß"
                    },
                    {
                        "name": "B. Palm"
                    },
                    {
                        "name": "T. Schnicke"
                    },
                    {
                        "name": "C. Schulz"
                    },
                    {
                        "name": "H. van der Schaaf"
                    },
                    {
                        "name": "D. Schäfer"
                    }
                ],
                "author_detail": {
                    "name": "D. Schäfer"
                },
                "author": "D. Schäfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04056v1",
                "updated": "2024-09-06T06:53:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    53,
                    45,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:53:45Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    53,
                    45,
                    4,
                    250,
                    0
                ],
                "title": "Refining Wikidata Taxonomy using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Wikidata Taxonomy using Large Language Models"
                },
                "summary": "Due to its collaborative nature, Wikidata is known to have a complex\ntaxonomy, with recurrent issues like the ambiguity between instances and\nclasses, the inaccuracy of some taxonomic paths, the presence of cycles, and\nthe high level of redundancy across classes. Manual efforts to clean up this\ntaxonomy are time-consuming and prone to errors or subjective decisions. We\npresent WiKC, a new version of Wikidata taxonomy cleaned automatically using a\ncombination of Large Language Models (LLMs) and graph mining techniques.\nOperations on the taxonomy, such as cutting links or merging classes, are\nperformed with the help of zero-shot prompting on an open-source LLM. The\nquality of the refined taxonomy is evaluated from both intrinsic and extrinsic\nperspectives, on a task of entity typing for the latter, showing the practical\ninterest of WiKC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to its collaborative nature, Wikidata is known to have a complex\ntaxonomy, with recurrent issues like the ambiguity between instances and\nclasses, the inaccuracy of some taxonomic paths, the presence of cycles, and\nthe high level of redundancy across classes. Manual efforts to clean up this\ntaxonomy are time-consuming and prone to errors or subjective decisions. We\npresent WiKC, a new version of Wikidata taxonomy cleaned automatically using a\ncombination of Large Language Models (LLMs) and graph mining techniques.\nOperations on the taxonomy, such as cutting links or merging classes, are\nperformed with the help of zero-shot prompting on an open-source LLM. The\nquality of the refined taxonomy is evaluated from both intrinsic and extrinsic\nperspectives, on a task of entity typing for the latter, showing the practical\ninterest of WiKC."
                },
                "authors": [
                    {
                        "name": "Yiwen Peng"
                    },
                    {
                        "name": "Thomas Bonald"
                    },
                    {
                        "name": "Mehwish Alam"
                    }
                ],
                "author_detail": {
                    "name": "Mehwish Alam"
                },
                "arxiv_affiliation": "IP Paris",
                "author": "Mehwish Alam",
                "arxiv_doi": "10.1145/3627673.3679156",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679156",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.04056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM International Conference on Information and Knowledge Management,\n  Oct 2024, Boise, Idaho, United States",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03659v2",
                "updated": "2024-09-06T06:50:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    50,
                    32,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T16:12:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    12,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "LLM-based multi-agent poetry generation in non-cooperative environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent poetry generation in non-cooperative environments"
                },
                "summary": "Despite substantial progress of large language models (LLMs) for automatic\npoetry generation, the generated poetry lacks diversity while the training\nprocess differs greatly from human learning. Under the rationale that the\nlearning process of the poetry generation systems should be more human-like and\ntheir output more diverse and novel, we introduce a framework based on social\nlearning where we emphasize non-cooperative interactions besides cooperative\ninteractions to encourage diversity. Our experiments are the first attempt at\nLLM-based multi-agent systems in non-cooperative environments for poetry\ngeneration employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED\nagents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows\nthat our framework benefits the poetry generation process for TRAINING-BASED\nagents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity\nand a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.\nThe generated poetry from TRAINING-BASED agents also exhibits group divergence\nin terms of lexicons, styles and semantics. PROMPTING-BASED agents in our\nframework also benefit from non-cooperative environments and a more diverse\nensemble of models with non-homogeneous agents has the potential to further\nenhance diversity, with an increase of 7.0-17.5 pp according to our\nexperiments. However, PROMPTING-BASED agents show a decrease in lexical\ndiversity over time and do not exhibit the group-based divergence intended in\nthe social network. Our paper argues for a paradigm shift in creative tasks\nsuch as automatic poetry generation to include social learning processes (via\nLLM-based agent modeling) similar to human interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial progress of large language models (LLMs) for automatic\npoetry generation, the generated poetry lacks diversity while the training\nprocess differs greatly from human learning. Under the rationale that the\nlearning process of the poetry generation systems should be more human-like and\ntheir output more diverse and novel, we introduce a framework based on social\nlearning where we emphasize non-cooperative interactions besides cooperative\ninteractions to encourage diversity. Our experiments are the first attempt at\nLLM-based multi-agent systems in non-cooperative environments for poetry\ngeneration employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED\nagents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows\nthat our framework benefits the poetry generation process for TRAINING-BASED\nagents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity\nand a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.\nThe generated poetry from TRAINING-BASED agents also exhibits group divergence\nin terms of lexicons, styles and semantics. PROMPTING-BASED agents in our\nframework also benefit from non-cooperative environments and a more diverse\nensemble of models with non-homogeneous agents has the potential to further\nenhance diversity, with an increase of 7.0-17.5 pp according to our\nexperiments. However, PROMPTING-BASED agents show a decrease in lexical\ndiversity over time and do not exhibit the group-based divergence intended in\nthe social network. Our paper argues for a paradigm shift in creative tasks\nsuch as automatic poetry generation to include social learning processes (via\nLLM-based agent modeling) similar to human interaction."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18521v2",
                "updated": "2024-09-06T06:49:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    49,
                    31,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-26T05:34:34Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    5,
                    34,
                    34,
                    4,
                    208,
                    0
                ],
                "title": "Patched MOA: optimizing inference for diverse software development tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patched MOA: optimizing inference for diverse software development tasks"
                },
                "summary": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm."
                },
                "authors": [
                    {
                        "name": "Asankhaya Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Asankhaya Sharma"
                },
                "author": "Asankhaya Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04043v1",
                "updated": "2024-09-06T06:27:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    27,
                    35,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:27:35Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    27,
                    35,
                    4,
                    250,
                    0
                ],
                "title": "Towards Safer Online Spaces: Simulating and Assessing Intervention\n  Strategies for Eating Disorder Discussions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safer Online Spaces: Simulating and Assessing Intervention\n  Strategies for Eating Disorder Discussions"
                },
                "summary": "Eating disorders are complex mental health conditions that affect millions of\npeople around the world. Effective interventions on social media platforms are\ncrucial, yet testing strategies in situ can be risky. We present a novel\nLLM-driven experimental testbed for simulating and assessing intervention\nstrategies in ED-related discussions. Our framework generates synthetic\nconversations across multiple platforms, models, and ED-related topics,\nallowing for controlled experimentation with diverse intervention approaches.\nWe analyze the impact of various intervention strategies on conversation\ndynamics across four dimensions: intervention type, generative model, social\nmedia platform, and ED-related community/topic. We employ cognitive domain\nanalysis metrics, including sentiment, emotions, etc., to evaluate the\neffectiveness of interventions. Our findings reveal that civility-focused\ninterventions consistently improve positive sentiment and emotional tone across\nall dimensions, while insight-resetting approaches tend to increase negative\nemotions. We also uncover significant biases in LLM-generated conversations,\nwith cognitive metrics varying notably between models (Claude-3 Haiku $>$\nMistral $>$ GPT-3.5-turbo $>$ LLaMA3) and even between versions of the same\nmodel. These variations highlight the importance of model selection in\nsimulating realistic discussions related to ED. Our work provides valuable\ninformation on the complex dynamics of ED-related discussions and the\neffectiveness of various intervention strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eating disorders are complex mental health conditions that affect millions of\npeople around the world. Effective interventions on social media platforms are\ncrucial, yet testing strategies in situ can be risky. We present a novel\nLLM-driven experimental testbed for simulating and assessing intervention\nstrategies in ED-related discussions. Our framework generates synthetic\nconversations across multiple platforms, models, and ED-related topics,\nallowing for controlled experimentation with diverse intervention approaches.\nWe analyze the impact of various intervention strategies on conversation\ndynamics across four dimensions: intervention type, generative model, social\nmedia platform, and ED-related community/topic. We employ cognitive domain\nanalysis metrics, including sentiment, emotions, etc., to evaluate the\neffectiveness of interventions. Our findings reveal that civility-focused\ninterventions consistently improve positive sentiment and emotional tone across\nall dimensions, while insight-resetting approaches tend to increase negative\nemotions. We also uncover significant biases in LLM-generated conversations,\nwith cognitive metrics varying notably between models (Claude-3 Haiku $>$\nMistral $>$ GPT-3.5-turbo $>$ LLaMA3) and even between versions of the same\nmodel. These variations highlight the importance of model selection in\nsimulating realistic discussions related to ED. Our work provides valuable\ninformation on the complex dynamics of ED-related discussions and the\neffectiveness of various intervention strategies."
                },
                "authors": [
                    {
                        "name": "Louis Penafiel"
                    },
                    {
                        "name": "Hsien-Te Kao"
                    },
                    {
                        "name": "Isabel Erickson"
                    },
                    {
                        "name": "David Chu"
                    },
                    {
                        "name": "Robert McCormack"
                    },
                    {
                        "name": "Kristina Lerman"
                    },
                    {
                        "name": "Svitlana Volkova"
                    }
                ],
                "author_detail": {
                    "name": "Svitlana Volkova"
                },
                "author": "Svitlana Volkova",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.04939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.04939v2",
                "updated": "2024-09-06T05:06:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    5,
                    6,
                    51,
                    4,
                    250,
                    0
                ],
                "published": "2023-11-08T01:45:37Z",
                "published_parsed": [
                    2023,
                    11,
                    8,
                    1,
                    45,
                    37,
                    2,
                    312,
                    0
                ],
                "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LooGLE: Can Long-Context Language Models Understand Long Contexts?"
                },
                "summary": "Large language models (LLMs), despite their impressive performance in various\nlanguage tasks, are typically limited to processing texts within context-window\nsize. This limitation has spurred significant research efforts to enhance LLMs'\nlong-context understanding with high-quality long-sequence benchmarks. However,\nprior datasets in this regard suffer from shortcomings, such as short context\nlength compared to the context window of modern LLMs; outdated documents that\nhave data leakage problems; and an emphasis on short dependency tasks rather\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\nGeneric Language Evaluation benchmark for LLMs' long context understanding.\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\ndocument and 6,000 newly generated questions spanning diverse domains. Human\nannotators meticulously crafted more than 1,100 high-quality question-answer\npairs to meet the long dependency requirements. These pairs underwent thorough\ncross-validation, yielding the most precise assessment of LLMs' long dependency\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\nexcelled in short dependency tasks like short question-answering and cloze\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\nlearning and chaining thoughts offered only marginal improvements; (iv)\nretrieval-based techniques demonstrated substantial benefits for short\nquestion-answering, while strategies for extending context window length had\nlimited impact on long context understanding. As such, LooGLE not only provides\na systematic and comprehensive evaluation schema on long-context LLMs, but also\nsheds light on future development of enhanced models towards \"true long-context\nunderstanding\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), despite their impressive performance in various\nlanguage tasks, are typically limited to processing texts within context-window\nsize. This limitation has spurred significant research efforts to enhance LLMs'\nlong-context understanding with high-quality long-sequence benchmarks. However,\nprior datasets in this regard suffer from shortcomings, such as short context\nlength compared to the context window of modern LLMs; outdated documents that\nhave data leakage problems; and an emphasis on short dependency tasks rather\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\nGeneric Language Evaluation benchmark for LLMs' long context understanding.\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\ndocument and 6,000 newly generated questions spanning diverse domains. Human\nannotators meticulously crafted more than 1,100 high-quality question-answer\npairs to meet the long dependency requirements. These pairs underwent thorough\ncross-validation, yielding the most precise assessment of LLMs' long dependency\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\nexcelled in short dependency tasks like short question-answering and cloze\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\nlearning and chaining thoughts offered only marginal improvements; (iv)\nretrieval-based techniques demonstrated substantial benefits for short\nquestion-answering, while strategies for extending context window length had\nlimited impact on long context understanding. As such, LooGLE not only provides\na systematic and comprehensive evaluation schema on long-context LLMs, but also\nsheds light on future development of enhanced models towards \"true long-context\nunderstanding\"."
                },
                "authors": [
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.04939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.04939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02834v2",
                "updated": "2024-09-06T05:06:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    5,
                    6,
                    27,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-04T16:00:21Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    0,
                    21,
                    2,
                    248,
                    0
                ],
                "title": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the\n  Mathematics Reasoning of Large Multimodal Models"
                },
                "summary": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have obtained promising results in mathematical\nreasoning, which is a foundational skill for human intelligence. Most previous\nstudies focus on improving and measuring the performance of LLMs based on\ntextual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few\nresearchers have released English multimodal math datasets (e.g., MATHVISTA and\nMATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In\nthis paper, we release a Chinese multimodal math (CMM-Math) dataset, including\nbenchmark and training parts, to evaluate and enhance the mathematical\nreasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,\nfeaturing a variety of problem types (e.g., multiple-choice, fill-in-the-blank,\nand so on) with detailed solutions across 12 grade levels from elementary to\nhigh school in China. Specifically, the visual context may be present in the\nquestions or opinions, which makes this dataset more challenging. Through\ncomprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math\ndataset face challenges, emphasizing the necessity for further improvements in\nLMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to\nhandle the problems with mixed input of multiple images and text segments. We\ntrain our model using three stages, including foundational pre-training,\nfoundational fine-tuning, and mathematical fine-tuning. The extensive\nexperiments indicate that our model effectively improves math reasoning\nperformance by comparing it with the SOTA LMMs over three multimodal\nmathematical datasets."
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Ji Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12787v2",
                "updated": "2024-09-06T04:30:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    4,
                    30,
                    50,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-23T01:37:29Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    1,
                    37,
                    29,
                    4,
                    236,
                    0
                ],
                "title": "LLM-PBE: Assessing Data Privacy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PBE: Assessing Data Privacy in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment."
                },
                "authors": [
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Junyuan Hong"
                    },
                    {
                        "name": "Chulin Xie"
                    },
                    {
                        "name": "Jeffrey Tan"
                    },
                    {
                        "name": "Rachel Xin"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Xavier Yin"
                    },
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04002v1",
                "updated": "2024-09-06T03:07:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    3,
                    7,
                    54,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T03:07:54Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    3,
                    7,
                    54,
                    4,
                    250,
                    0
                ],
                "title": "Low-Earth Orbit Satellite Network Analysis: Coverage under\n  Distance-Dependent Shadowing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Earth Orbit Satellite Network Analysis: Coverage under\n  Distance-Dependent Shadowing"
                },
                "summary": "This paper offers a thorough analysis of the coverage performance of Low\nEarth Orbit (LEO) satellite networks using a strongest satellite association\napproach, with a particular emphasis on shadowing effects modeled through a\nPoisson point process (PPP)-based network framework. We derive an analytical\nexpression for the coverage probability, which incorporates key system\nparameters and a distance-dependent shadowing probability function, explicitly\naccounting for both line-of-sight and non-line-of-sight propagation channels.\nTo enhance the practical relevance of our findings, we provide both lower and\nupper bounds for the coverage probability and introduce a closed-form solution\nbased on a simplified shadowing model. Our analysis reveals several important\nnetwork design insights, including the enhancement of coverage probability by\ndistance-dependent shadowing effects and the identification of an optimal\nsatellite altitude that balances beam gain benefits with interference\ndrawbacks. Notably, our PPP-based network model shows strong alignment with\nother established models, confirming its accuracy and applicability across a\nvariety of satellite network configurations. The insights gained from our\nanalysis are valuable for optimizing LEO satellite deployment strategies and\nimproving network performance in diverse scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper offers a thorough analysis of the coverage performance of Low\nEarth Orbit (LEO) satellite networks using a strongest satellite association\napproach, with a particular emphasis on shadowing effects modeled through a\nPoisson point process (PPP)-based network framework. We derive an analytical\nexpression for the coverage probability, which incorporates key system\nparameters and a distance-dependent shadowing probability function, explicitly\naccounting for both line-of-sight and non-line-of-sight propagation channels.\nTo enhance the practical relevance of our findings, we provide both lower and\nupper bounds for the coverage probability and introduce a closed-form solution\nbased on a simplified shadowing model. Our analysis reveals several important\nnetwork design insights, including the enhancement of coverage probability by\ndistance-dependent shadowing effects and the identification of an optimal\nsatellite altitude that balances beam gain benefits with interference\ndrawbacks. Notably, our PPP-based network model shows strong alignment with\nother established models, confirming its accuracy and applicability across a\nvariety of satellite network configurations. The insights gained from our\nanalysis are valuable for optimizing LEO satellite deployment strategies and\nimproving network performance in diverse scenarios."
                },
                "authors": [
                    {
                        "name": "Jinseok Choi"
                    },
                    {
                        "name": "Jeonghun Park"
                    },
                    {
                        "name": "Junse Lee"
                    },
                    {
                        "name": "Namyoon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Namyoon Lee"
                },
                "author": "Namyoon Lee",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14418v2",
                "updated": "2024-09-06T03:05:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    3,
                    5,
                    29,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-26T17:04:00Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues"
                },
                "summary": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization."
                },
                "authors": [
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Abhinav Ramesh Kashyap"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Andy T. Liu"
                    },
                    {
                        "name": "Vijay Prakash Dwivedi"
                    },
                    {
                        "name": "Thanh-Tung Nguyen"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03992v1",
                "updated": "2024-09-06T02:44:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    44,
                    27,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T02:44:27Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    44,
                    27,
                    4,
                    250,
                    0
                ],
                "title": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study"
                },
                "summary": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on NVIDIA H100 GPUs for large language model (LLM) inference\ntasks. We benchmark the overhead introduced by TEE mode across various models\nand token lengths, focusing on the bottleneck caused by CPU-GPU data transfers\nvia PCIe. Our results show that while there is minimal computational overhead\nwithin the GPU, the overall performance penalty is primarily due to data\ntransfer. For most typical LLM queries, the overhead remains below 5%, with\nlarger models and longer sequences experiencing near-zero overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on NVIDIA H100 GPUs for large language model (LLM) inference\ntasks. We benchmark the overhead introduced by TEE mode across various models\nand token lengths, focusing on the bottleneck caused by CPU-GPU data transfers\nvia PCIe. Our results show that while there is minimal computational overhead\nwithin the GPU, the overall performance penalty is primarily due to data\ntransfer. For most typical LLM queries, the overhead remains below 5%, with\nlarger models and longer sequences experiencing near-zero overhead."
                },
                "authors": [
                    {
                        "name": "Jianwei Zhu"
                    },
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Shunfan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shunfan Zhou"
                },
                "author": "Shunfan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20234v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20234v3",
                "updated": "2024-09-06T02:41:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    41,
                    35,
                    4,
                    250,
                    0
                ],
                "published": "2024-05-30T16:36:47Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    16,
                    36,
                    47,
                    3,
                    151,
                    0
                ],
                "title": "Hidden in Plain Sight: Exploring Chat History Tampering in Interactive\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden in Plain Sight: Exploring Chat History Tampering in Interactive\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent\nin real-world applications, exhibiting impressive text generation performance.\nLLMs are fundamentally developed from a scenario where the input data remains\nstatic and unstructured. To behave interactively, LLM-based chat systems must\nintegrate prior chat history as context into their inputs, following a\npre-defined structure. However, LLMs cannot separate user inputs from context,\nenabling chat history tampering. This paper introduces a systematic methodology\nto inject user-supplied history into LLM conversations without any prior\nknowledge of the target model. The key is to utilize prompt templates that can\nwell organize the messages to be injected, leading the target LLM to interpret\nthem as genuine chat history. To automatically search for effective templates\nin a WebUI black-box setting, we propose the LLM-Guided Genetic Algorithm\n(LLMGA) that leverages an LLM to generate and iteratively optimize the\ntemplates. We apply the proposed method to popular real-world LLMs including\nChatGPT and Llama-2/3. The results show that chat history tampering can enhance\nthe malleability of the model's behavior over time and greatly influence the\nmodel output. For example, it can improve the success rate of disallowed\nresponse elicitation up to 97% on ChatGPT. Our findings provide insights into\nthe challenges associated with the real-world deployment of interactive LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent\nin real-world applications, exhibiting impressive text generation performance.\nLLMs are fundamentally developed from a scenario where the input data remains\nstatic and unstructured. To behave interactively, LLM-based chat systems must\nintegrate prior chat history as context into their inputs, following a\npre-defined structure. However, LLMs cannot separate user inputs from context,\nenabling chat history tampering. This paper introduces a systematic methodology\nto inject user-supplied history into LLM conversations without any prior\nknowledge of the target model. The key is to utilize prompt templates that can\nwell organize the messages to be injected, leading the target LLM to interpret\nthem as genuine chat history. To automatically search for effective templates\nin a WebUI black-box setting, we propose the LLM-Guided Genetic Algorithm\n(LLMGA) that leverages an LLM to generate and iteratively optimize the\ntemplates. We apply the proposed method to popular real-world LLMs including\nChatGPT and Llama-2/3. The results show that chat history tampering can enhance\nthe malleability of the model's behavior over time and greatly influence the\nmodel output. For example, it can improve the success rate of disallowed\nresponse elicitation up to 97% on ChatGPT. Our findings provide insights into\nthe challenges associated with the real-world deployment of interactive LLMs."
                },
                "authors": [
                    {
                        "name": "Cheng'an Wei"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yujia Gong"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Lu Xiang"
                    },
                    {
                        "name": "Shenchen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Shenchen Zhu"
                },
                "author": "Shenchen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20234v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20234v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07482v2",
                "updated": "2024-09-06T02:38:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    38,
                    40,
                    4,
                    250,
                    0
                ],
                "published": "2024-08-14T11:55:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    55,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice."
                },
                "authors": [
                    {
                        "name": "Ning Lu"
                    },
                    {
                        "name": "Qian Xie"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wenyi Fang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zheng Hu"
                    },
                    {
                        "name": "Jiantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Ma"
                },
                "author": "Jiantao Ma",
                "arxiv_comment": "To be published in: IEEE International Symposium on Software\n  Reliability Engineering (ISSRE2024) workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05572v2",
                "updated": "2024-09-06T02:13:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    13,
                    51,
                    4,
                    250,
                    0
                ],
                "published": "2024-06-08T20:56:14Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    20,
                    56,
                    14,
                    5,
                    160,
                    0
                ],
                "title": "Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and\n  Constraint Satisfaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust the PRoC3S: Solving Long-Horizon Robotics Problems with LLMs and\n  Constraint Satisfaction"
                },
                "summary": "Recent developments in pretrained large language models (LLMs) applied to\nrobotics have demonstrated their capacity for sequencing a set of discrete\nskills to achieve open-ended goals in simple robotic tasks. In this paper, we\nexamine the topic of LLM planning for a set of continuously parameterized\nskills whose execution must avoid violations of a set of kinematic, geometric,\nand physical constraints. We prompt the LLM to output code for a function with\nopen parameters, which, together with environmental constraints, can be viewed\nas a Continuous Constraint Satisfaction Problem (CCSP). This CCSP can be solved\nthrough sampling or optimization to find a skill sequence and continuous\nparameter settings that achieve the goal while avoiding constraint violations.\nAdditionally, we consider cases where the LLM proposes unsatisfiable CCSPs,\nsuch as those that are kinematically infeasible, dynamically unstable, or lead\nto collisions, and re-prompt the LLM to form a new CCSP accordingly.\nExperiments across three different simulated 3D domains demonstrate that our\nproposed strategy, PRoC3S, is capable of solving a wide range of complex\nmanipulation tasks with realistic constraints on continuous parameters much\nmore efficiently and effectively than existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in pretrained large language models (LLMs) applied to\nrobotics have demonstrated their capacity for sequencing a set of discrete\nskills to achieve open-ended goals in simple robotic tasks. In this paper, we\nexamine the topic of LLM planning for a set of continuously parameterized\nskills whose execution must avoid violations of a set of kinematic, geometric,\nand physical constraints. We prompt the LLM to output code for a function with\nopen parameters, which, together with environmental constraints, can be viewed\nas a Continuous Constraint Satisfaction Problem (CCSP). This CCSP can be solved\nthrough sampling or optimization to find a skill sequence and continuous\nparameter settings that achieve the goal while avoiding constraint violations.\nAdditionally, we consider cases where the LLM proposes unsatisfiable CCSPs,\nsuch as those that are kinematically infeasible, dynamically unstable, or lead\nto collisions, and re-prompt the LLM to form a new CCSP accordingly.\nExperiments across three different simulated 3D domains demonstrate that our\nproposed strategy, PRoC3S, is capable of solving a wide range of complex\nmanipulation tasks with realistic constraints on continuous parameters much\nmore efficiently and effectively than existing baselines."
                },
                "authors": [
                    {
                        "name": "Aidan Curtis"
                    },
                    {
                        "name": "Nishanth Kumar"
                    },
                    {
                        "name": "Jing Cao"
                    },
                    {
                        "name": "Tomás Lozano-Pérez"
                    },
                    {
                        "name": "Leslie Pack Kaelbling"
                    }
                ],
                "author_detail": {
                    "name": "Leslie Pack Kaelbling"
                },
                "author": "Leslie Pack Kaelbling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03666v3",
                "updated": "2024-09-06T02:02:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    2,
                    41,
                    4,
                    250,
                    0
                ],
                "published": "2024-02-06T03:39:44Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    3,
                    39,
                    44,
                    1,
                    37,
                    0
                ],
                "title": "QuEST: Low-bit Diffusion Model Quantization via Efficient Selective\n  Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuEST: Low-bit Diffusion Model Quantization via Efficient Selective\n  Finetuning"
                },
                "summary": "The practical deployment of diffusion models still suffers from the high\nmemory and time overhead. While quantization paves a way for compression and\nacceleration, existing methods unfortunately fail when the models are quantized\nto low-bits. In this paper, we empirically unravel three properties in\nquantized diffusion models that compromise the efficacy of current methods:\nimbalanced activation distributions, imprecise temporal information, and\nvulnerability to perturbations of specific modules. To alleviate the\nintensified low-bit quantization difficulty stemming from the distribution\nimbalance, we propose finetuning the quantized model to better adapt to the\nactivation distribution. Building on this idea, we identify two critical types\nof quantized layers: those holding vital temporal information and those\nsensitive to reduced bit-width, and finetune them to mitigate performance\ndegradation with efficiency. We empirically verify that our approach modifies\nthe activation distribution and provides meaningful temporal information,\nfacilitating easier and more accurate quantization. Our method is evaluated\nover three high-resolution image generation tasks and achieves state-of-the-art\nperformance under various bit-width settings, as well as being the first method\nto generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion. Code is\navailable \\href{https://github.com/hatchetProject/QuEST}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The practical deployment of diffusion models still suffers from the high\nmemory and time overhead. While quantization paves a way for compression and\nacceleration, existing methods unfortunately fail when the models are quantized\nto low-bits. In this paper, we empirically unravel three properties in\nquantized diffusion models that compromise the efficacy of current methods:\nimbalanced activation distributions, imprecise temporal information, and\nvulnerability to perturbations of specific modules. To alleviate the\nintensified low-bit quantization difficulty stemming from the distribution\nimbalance, we propose finetuning the quantized model to better adapt to the\nactivation distribution. Building on this idea, we identify two critical types\nof quantized layers: those holding vital temporal information and those\nsensitive to reduced bit-width, and finetune them to mitigate performance\ndegradation with efficiency. We empirically verify that our approach modifies\nthe activation distribution and provides meaningful temporal information,\nfacilitating easier and more accurate quantization. Our method is evaluated\nover three high-resolution image generation tasks and achieves state-of-the-art\nperformance under various bit-width settings, as well as being the first method\nto generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion. Code is\navailable \\href{https://github.com/hatchetProject/QuEST}{here}."
                },
                "authors": [
                    {
                        "name": "Haoxuan Wang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Yan Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yan Yan"
                },
                "author": "Yan Yan",
                "arxiv_comment": "Code available at https://github.com/hatchetProject/QuEST",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04298v3",
                "updated": "2024-09-06T01:14:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    1,
                    14,
                    26,
                    4,
                    250,
                    0
                ],
                "published": "2024-04-04T20:27:37Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    20,
                    27,
                    37,
                    3,
                    95,
                    0
                ],
                "title": "SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated\n  Responses"
                },
                "summary": "Can LLMs consistently improve their previous outputs for better results? For\nthis to be true, LLMs would need to be better at discriminating among\npreviously-generated alternatives, than generating initial responses. We\nexplore the validity of this hypothesis in practice. We first formulate a\nunified framework that allows us to compare the generative and discriminative\ncapability of any model on any task. In our resulting experimental analysis of\nseveral open-source and industrial LLMs, we observe that models are not\nreliably better at discriminating among previously-generated alternatives than\ngenerating initial responses. This finding challenges the notion that LLMs may\nbe able to enhance their performance only through their own judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs consistently improve their previous outputs for better results? For\nthis to be true, LLMs would need to be better at discriminating among\npreviously-generated alternatives, than generating initial responses. We\nexplore the validity of this hypothesis in practice. We first formulate a\nunified framework that allows us to compare the generative and discriminative\ncapability of any model on any task. In our resulting experimental analysis of\nseveral open-source and industrial LLMs, we observe that models are not\nreliably better at discriminating among previously-generated alternatives than\ngenerating initial responses. This finding challenges the notion that LLMs may\nbe able to enhance their performance only through their own judgment."
                },
                "authors": [
                    {
                        "name": "Dongwei Jiang"
                    },
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Nathaniel Weir"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09932v2",
                "updated": "2024-09-06T00:46:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    46,
                    40,
                    4,
                    250,
                    0
                ],
                "published": "2024-04-15T16:58:28Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    16,
                    58,
                    28,
                    0,
                    106,
                    0
                ],
                "title": "Foundational Challenges in Assuring Alignment and Safety of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Challenges in Assuring Alignment and Safety of Large\n  Language Models"
                },
                "summary": "This work identifies 18 foundational challenges in assuring the alignment and\nsafety of large language models (LLMs). These challenges are organized into\nthree different categories: scientific understanding of LLMs, development and\ndeployment methods, and sociotechnical challenges. Based on the identified\nchallenges, we pose $200+$ concrete research questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies 18 foundational challenges in assuring the alignment and\nsafety of large language models (LLMs). These challenges are organized into\nthree different categories: scientific understanding of LLMs, development and\ndeployment methods, and sociotechnical challenges. Based on the identified\nchallenges, we pose $200+$ concrete research questions."
                },
                "authors": [
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Abulhair Saparov"
                    },
                    {
                        "name": "Javier Rando"
                    },
                    {
                        "name": "Daniel Paleka"
                    },
                    {
                        "name": "Miles Turpin"
                    },
                    {
                        "name": "Peter Hase"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Erik Jenner"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Oliver Sourbut"
                    },
                    {
                        "name": "Benjamin L. Edelman"
                    },
                    {
                        "name": "Zhaowei Zhang"
                    },
                    {
                        "name": "Mario Günther"
                    },
                    {
                        "name": "Anton Korinek"
                    },
                    {
                        "name": "Jose Hernandez-Orallo"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Eric Bigelow"
                    },
                    {
                        "name": "Alexander Pan"
                    },
                    {
                        "name": "Lauro Langosco"
                    },
                    {
                        "name": "Tomasz Korbak"
                    },
                    {
                        "name": "Heidi Zhang"
                    },
                    {
                        "name": "Ruiqi Zhong"
                    },
                    {
                        "name": "Seán Ó hÉigeartaigh"
                    },
                    {
                        "name": "Gabriel Recchia"
                    },
                    {
                        "name": "Giulio Corsi"
                    },
                    {
                        "name": "Alan Chan"
                    },
                    {
                        "name": "Markus Anderljung"
                    },
                    {
                        "name": "Lilian Edwards"
                    },
                    {
                        "name": "Aleksandar Petrov"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Sumeet Ramesh Motwan"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Tegan Maharaj"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Florian Tramer"
                    },
                    {
                        "name": "He He"
                    },
                    {
                        "name": "Atoosa Kasirzadeh"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "David Krueger"
                    }
                ],
                "author_detail": {
                    "name": "David Krueger"
                },
                "author": "David Krueger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18148v2",
                "updated": "2024-09-06T00:02:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    2,
                    52,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-25T15:58:56Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    58,
                    56,
                    3,
                    207,
                    0
                ],
                "title": "StraightLine: An End-to-End Resource-Aware Scheduler for Machine\n  Learning Application Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StraightLine: An End-to-End Resource-Aware Scheduler for Machine\n  Learning Application Requests"
                },
                "summary": "The life cycle of machine learning (ML) applications consists of two stages:\nmodel development and model deployment. However, traditional ML systems (e.g.,\ntraining-specific or inference-specific systems) focus on one particular stage\nor phase of the life cycle of ML applications. These systems often aim at\noptimizing model training or accelerating model inference, and they frequently\nassume homogeneous infrastructure, which may not always reflect real-world\nscenarios that include cloud data centers, local servers, containers, and\nserverless platforms. We present StraightLine, an end-to-end resource-aware\nscheduler that schedules the optimal resources (e.g., container, virtual\nmachine, or serverless) for different ML application requests in a hybrid\ninfrastructure. The key innovation is an empirical dynamic placing algorithm\nthat intelligently places requests based on their unique characteristics (e.g.,\nrequest frequency, input data size, and data distribution). In contrast to\nexisting ML systems, StraightLine offers end-to-end resource-aware placement,\nthereby it can significantly reduce response time and failure rate for model\ndeployment when facing different computing resources in the hybrid\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The life cycle of machine learning (ML) applications consists of two stages:\nmodel development and model deployment. However, traditional ML systems (e.g.,\ntraining-specific or inference-specific systems) focus on one particular stage\nor phase of the life cycle of ML applications. These systems often aim at\noptimizing model training or accelerating model inference, and they frequently\nassume homogeneous infrastructure, which may not always reflect real-world\nscenarios that include cloud data centers, local servers, containers, and\nserverless platforms. We present StraightLine, an end-to-end resource-aware\nscheduler that schedules the optimal resources (e.g., container, virtual\nmachine, or serverless) for different ML application requests in a hybrid\ninfrastructure. The key innovation is an empirical dynamic placing algorithm\nthat intelligently places requests based on their unique characteristics (e.g.,\nrequest frequency, input data size, and data distribution). In contrast to\nexisting ML systems, StraightLine offers end-to-end resource-aware placement,\nthereby it can significantly reduce response time and failure rate for model\ndeployment when facing different computing resources in the hybrid\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Cheng-Wei Ching"
                    },
                    {
                        "name": "Boyuan Guan"
                    },
                    {
                        "name": "Hailu Xu"
                    },
                    {
                        "name": "Liting Hu"
                    }
                ],
                "author_detail": {
                    "name": "Liting Hu"
                },
                "author": "Liting Hu",
                "arxiv_comment": "6 pages, 8 figures, to appear in AIoTC'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03946v1",
                "updated": "2024-09-06T00:02:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    2,
                    9,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T00:02:09Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    0,
                    2,
                    9,
                    4,
                    250,
                    0
                ],
                "title": "On The Role of Prompt Construction In Enhancing Efficacy and Efficiency\n  of LLM-Based Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Role of Prompt Construction In Enhancing Efficacy and Efficiency\n  of LLM-Based Tabular Data Generation"
                },
                "summary": "LLM-based data generation for real-world tabular data can be challenged by\nthe lack of sufficient semantic context in feature names used to describe\ncolumns. We hypothesize that enriching prompts with domain-specific insights\ncan improve both the quality and efficiency of data generation. To test this\nhypothesis, we explore three prompt construction protocols: Expert-guided,\nLLM-guided, and Novel-Mapping. Through empirical studies with the recently\nproposed GReaT framework, we find that context-enriched prompts lead to\nsignificantly improved data generation quality and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based data generation for real-world tabular data can be challenged by\nthe lack of sufficient semantic context in feature names used to describe\ncolumns. We hypothesize that enriching prompts with domain-specific insights\ncan improve both the quality and efficiency of data generation. To test this\nhypothesis, we explore three prompt construction protocols: Expert-guided,\nLLM-guided, and Novel-Mapping. Through empirical studies with the recently\nproposed GReaT framework, we find that context-enriched prompts lead to\nsignificantly improved data generation quality and training efficiency."
                },
                "authors": [
                    {
                        "name": "Banooqa Banday"
                    },
                    {
                        "name": "Kowshik Thopalli"
                    },
                    {
                        "name": "Tanzima Z. Islam"
                    },
                    {
                        "name": "Jayaraman J. Thiagarajan"
                    }
                ],
                "author_detail": {
                    "name": "Jayaraman J. Thiagarajan"
                },
                "author": "Jayaraman J. Thiagarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16966v2",
                "updated": "2024-09-05T23:18:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    18,
                    0,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-30T01:56:57Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    1,
                    56,
                    57,
                    4,
                    243,
                    0
                ],
                "title": "UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UserSumBench: A Benchmark Framework for Evaluating User Summarization\n  Approaches"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Neo Wu"
                    },
                    {
                        "name": "Lin Ning"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Shawn O'Banion"
                    },
                    {
                        "name": "Bradley Green"
                    }
                ],
                "author_detail": {
                    "name": "Bradley Green"
                },
                "author": "Bradley Green",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03939v1",
                "updated": "2024-09-05T23:17:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    17,
                    18,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T23:17:18Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    17,
                    18,
                    3,
                    249,
                    0
                ],
                "title": "Experimentation in Content Moderation using RWKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimentation in Content Moderation using RWKV"
                },
                "summary": "This paper investigates the RWKV model's efficacy in content moderation\nthrough targeted experimentation. We introduce a novel dataset specifically\ndesigned for distillation into smaller models, enhancing content moderation\npractices. This comprehensive dataset encompasses images, videos, sounds, and\ntext data that present societal challenges. Leveraging advanced Large Language\nModels (LLMs), we generated an extensive set of responses -- 558,958 for text\nand 83,625 for images -- to train and refine content moderation systems. Our\ncore experimentation involved fine-tuning the RWKV model, capitalizing on its\nCPU-efficient architecture to address large-scale content moderation tasks. By\nhighlighting the dataset's potential for knowledge distillation, this study not\nonly demonstrates RWKV's capability in improving the accuracy and efficiency of\ncontent moderation systems but also paves the way for developing more compact,\nresource-efficient models in this domain. Datasets and models can be found in\nHuggingFace: https://huggingface.co/modrwkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the RWKV model's efficacy in content moderation\nthrough targeted experimentation. We introduce a novel dataset specifically\ndesigned for distillation into smaller models, enhancing content moderation\npractices. This comprehensive dataset encompasses images, videos, sounds, and\ntext data that present societal challenges. Leveraging advanced Large Language\nModels (LLMs), we generated an extensive set of responses -- 558,958 for text\nand 83,625 for images -- to train and refine content moderation systems. Our\ncore experimentation involved fine-tuning the RWKV model, capitalizing on its\nCPU-efficient architecture to address large-scale content moderation tasks. By\nhighlighting the dataset's potential for knowledge distillation, this study not\nonly demonstrates RWKV's capability in improving the accuracy and efficiency of\ncontent moderation systems but also paves the way for developing more compact,\nresource-efficient models in this domain. Datasets and models can be found in\nHuggingFace: https://huggingface.co/modrwkv"
                },
                "authors": [
                    {
                        "name": "Umut Yildirim"
                    },
                    {
                        "name": "Rohan Dutta"
                    },
                    {
                        "name": "Burak Yildirim"
                    },
                    {
                        "name": "Atharva Vaidya"
                    }
                ],
                "author_detail": {
                    "name": "Atharva Vaidya"
                },
                "author": "Atharva Vaidya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03937v1",
                "updated": "2024-09-05T23:04:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    4,
                    28,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T23:04:28Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    4,
                    28,
                    3,
                    249,
                    0
                ],
                "title": "Harnessing LLMs for Cross-City OD Flow Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing LLMs for Cross-City OD Flow Prediction"
                },
                "summary": "Understanding and predicting Origin-Destination (OD) flows is crucial for\nurban planning and transportation management. Traditional OD prediction models,\nwhile effective within single cities, often face limitations when applied\nacross different cities due to varied traffic conditions, urban layouts, and\nsocio-economic factors. In this paper, by employing Large Language Models\n(LLMs), we introduce a new method for cross-city OD flow prediction. Our\napproach leverages the advanced semantic understanding and contextual learning\ncapabilities of LLMs to bridge the gap between cities with different\ncharacteristics, providing a robust and adaptable solution for accurate OD flow\nprediction that can be transferred from one city to another. Our novel\nframework involves four major components: collecting OD training datasets from\na source city, instruction-tuning the LLMs, predicting destination POIs in a\ntarget city, and identifying the locations that best match the predicted\ndestination POIs. We introduce a new loss function that integrates POI\nsemantics and trip distance during training. By extracting high-quality\nsemantic features from human mobility and POI data, the model understands\nspatial and functional relationships within urban spaces and captures\ninteractions between individuals and various POIs. Extensive experimental\nresults demonstrate the superiority of our approach over the state-of-the-art\nlearning-based methods in cross-city OD flow prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting Origin-Destination (OD) flows is crucial for\nurban planning and transportation management. Traditional OD prediction models,\nwhile effective within single cities, often face limitations when applied\nacross different cities due to varied traffic conditions, urban layouts, and\nsocio-economic factors. In this paper, by employing Large Language Models\n(LLMs), we introduce a new method for cross-city OD flow prediction. Our\napproach leverages the advanced semantic understanding and contextual learning\ncapabilities of LLMs to bridge the gap between cities with different\ncharacteristics, providing a robust and adaptable solution for accurate OD flow\nprediction that can be transferred from one city to another. Our novel\nframework involves four major components: collecting OD training datasets from\na source city, instruction-tuning the LLMs, predicting destination POIs in a\ntarget city, and identifying the locations that best match the predicted\ndestination POIs. We introduce a new loss function that integrates POI\nsemantics and trip distance during training. By extracting high-quality\nsemantic features from human mobility and POI data, the model understands\nspatial and functional relationships within urban spaces and captures\ninteractions between individuals and various POIs. Extensive experimental\nresults demonstrate the superiority of our approach over the state-of-the-art\nlearning-based methods in cross-city OD flow prediction."
                },
                "authors": [
                    {
                        "name": "Chenyang Yu"
                    },
                    {
                        "name": "Xinpeng Xie"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Chenxi Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Qiu"
                },
                "author": "Chenxi Qiu",
                "arxiv_comment": "12 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03931v1",
                "updated": "2024-09-05T22:55:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    22,
                    55,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T22:55:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    22,
                    55,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Machine Learning for Reducing Noise in RF Control Signals at Industrial\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning for Reducing Noise in RF Control Signals at Industrial\n  Accelerators"
                },
                "summary": "Industrial particle accelerators typically operate in dirtier environments\nthan research accelerators, leading to increased noise in RF and electronic\nsystems. Furthermore, given that industrial accelerators are mass produced,\nless attention is given to optimizing the performance of individual systems. As\na result, industrial accelerators tend to underperform their own hardware\ncapabilities. Improving signal processing for these machines will improve cost\nand time margins for deployment, helping to meet the growing demand for\naccelerators for medical sterilization, food irradiation, cancer treatment, and\nimaging. Our work focuses on using machine learning techniques to reduce noise\nin RF signals used for pulse-to-pulse feedback in industrial accelerators. Here\nwe review our algorithms and observed results for simulated RF systems, and\ndiscuss next steps with the ultimate goal of deployment on industrial systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial particle accelerators typically operate in dirtier environments\nthan research accelerators, leading to increased noise in RF and electronic\nsystems. Furthermore, given that industrial accelerators are mass produced,\nless attention is given to optimizing the performance of individual systems. As\na result, industrial accelerators tend to underperform their own hardware\ncapabilities. Improving signal processing for these machines will improve cost\nand time margins for deployment, helping to meet the growing demand for\naccelerators for medical sterilization, food irradiation, cancer treatment, and\nimaging. Our work focuses on using machine learning techniques to reduce noise\nin RF signals used for pulse-to-pulse feedback in industrial accelerators. Here\nwe review our algorithms and observed results for simulated RF systems, and\ndiscuss next steps with the ultimate goal of deployment on industrial systems."
                },
                "authors": [
                    {
                        "name": "M. Henderson"
                    },
                    {
                        "name": "J. P. Edelen"
                    },
                    {
                        "name": "J. Einstein-Curtis"
                    },
                    {
                        "name": "C. C. Hall"
                    },
                    {
                        "name": "J. A. Diaz Cruz"
                    },
                    {
                        "name": "A. L. Edelen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. Edelen"
                },
                "author": "A. L. Edelen",
                "arxiv_comment": "9 pages, 6 figures, accepted for publication in Journal of\n  Instrumentation (J. Inst., pending)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03928v1",
                "updated": "2024-09-05T22:22:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    22,
                    22,
                    57,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T22:22:57Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    22,
                    22,
                    57,
                    3,
                    249,
                    0
                ],
                "title": "RETAIN: Interactive Tool for Regression Testing Guided LLM Migration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RETAIN: Interactive Tool for Regression Testing Guided LLM Migration"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into diverse\napplications. The rapid evolution of LLMs presents opportunities for developers\nto enhance applications continuously. However, this constant adaptation can\nalso lead to performance regressions during model migrations. While several\ninteractive tools have been proposed to streamline the complexity of prompt\nengineering, few address the specific requirements of regression testing for\nLLM Migrations. To bridge this gap, we introduce RETAIN (REgression Testing\nguided LLM migrAtIoN), a tool designed explicitly for regression testing in LLM\nMigrations. RETAIN comprises two key components: an interactive interface\ntailored to regression testing needs during LLM migrations, and an error\ndiscovery module that facilitates understanding of differences in model\nbehaviors. The error discovery module generates textual descriptions of various\nerrors or differences between model outputs, providing actionable insights for\nprompt refinement. Our automatic evaluation and empirical user studies\ndemonstrate that RETAIN, when compared to manual evaluation, enabled\nparticipants to identify twice as many errors, facilitated experimentation with\n75% more prompts, and achieves 12% higher metric scores in a given time frame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into diverse\napplications. The rapid evolution of LLMs presents opportunities for developers\nto enhance applications continuously. However, this constant adaptation can\nalso lead to performance regressions during model migrations. While several\ninteractive tools have been proposed to streamline the complexity of prompt\nengineering, few address the specific requirements of regression testing for\nLLM Migrations. To bridge this gap, we introduce RETAIN (REgression Testing\nguided LLM migrAtIoN), a tool designed explicitly for regression testing in LLM\nMigrations. RETAIN comprises two key components: an interactive interface\ntailored to regression testing needs during LLM migrations, and an error\ndiscovery module that facilitates understanding of differences in model\nbehaviors. The error discovery module generates textual descriptions of various\nerrors or differences between model outputs, providing actionable insights for\nprompt refinement. Our automatic evaluation and empirical user studies\ndemonstrate that RETAIN, when compared to manual evaluation, enabled\nparticipants to identify twice as many errors, facilitated experimentation with\n75% more prompts, and achieves 12% higher metric scores in a given time frame."
                },
                "authors": [
                    {
                        "name": "Tanay Dixit"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Sally Fang"
                    },
                    {
                        "name": "Sai Sree Harsha"
                    },
                    {
                        "name": "Anirudh Sureshan"
                    },
                    {
                        "name": "Akash Maharaj"
                    },
                    {
                        "name": "Yunyao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yunyao Li"
                },
                "author": "Yunyao Li",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03906v1",
                "updated": "2024-09-05T20:49:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    49,
                    35,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T20:49:35Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    49,
                    35,
                    3,
                    249,
                    0
                ],
                "title": "Analytical Optimized Traffic Flow Recovery for Large-scale Urban\n  Transportation Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical Optimized Traffic Flow Recovery for Large-scale Urban\n  Transportation Network"
                },
                "summary": "The implementation of intelligent transportation systems (ITS) has enhanced\ndata collection in urban transportation through advanced traffic sensing\ndevices. However, the high costs associated with installation and maintenance\nresult in sparse traffic data coverage. To obtain complete, accurate, and\nhigh-resolution network-wide traffic flow data, this study introduces the\nAnalytical Optimized Recovery (AOR) approach that leverages abundant GPS speed\ndata alongside sparse flow data to estimate traffic flow in large-scale urban\nnetworks. The method formulates a constrained optimization framework that\nutilizes a quadratic objective function with l2 norm regularization terms to\naddress the traffic flow recovery problem effectively and incorporates a\nLagrangian relaxation technique to maintain non-negativity constraints. The\neffectiveness of this approach was validated in a large urban network in\nShenzhen's Futian District using the Simulation of Urban MObility (SUMO)\nplatform. Analytical results indicate that the method achieves low estimation\nerrors, affirming its suitability for comprehensive traffic analysis in urban\nsettings with limited sensor deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of intelligent transportation systems (ITS) has enhanced\ndata collection in urban transportation through advanced traffic sensing\ndevices. However, the high costs associated with installation and maintenance\nresult in sparse traffic data coverage. To obtain complete, accurate, and\nhigh-resolution network-wide traffic flow data, this study introduces the\nAnalytical Optimized Recovery (AOR) approach that leverages abundant GPS speed\ndata alongside sparse flow data to estimate traffic flow in large-scale urban\nnetworks. The method formulates a constrained optimization framework that\nutilizes a quadratic objective function with l2 norm regularization terms to\naddress the traffic flow recovery problem effectively and incorporates a\nLagrangian relaxation technique to maintain non-negativity constraints. The\neffectiveness of this approach was validated in a large urban network in\nShenzhen's Futian District using the Simulation of Urban MObility (SUMO)\nplatform. Analytical results indicate that the method achieves low estimation\nerrors, affirming its suitability for comprehensive traffic analysis in urban\nsettings with limited sensor deployment."
                },
                "authors": [
                    {
                        "name": "Sicheng Fu"
                    },
                    {
                        "name": "Haotian Shi"
                    },
                    {
                        "name": "Shixiao Liang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Ran Bin"
                    }
                ],
                "author_detail": {
                    "name": "Ran Bin"
                },
                "author": "Ran Bin",
                "arxiv_comment": "27 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03905v1",
                "updated": "2024-09-05T20:42:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    42,
                    35,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T20:42:35Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    42,
                    35,
                    3,
                    249,
                    0
                ],
                "title": "CACER: Clinical Concept Annotations for Cancer Events and Relations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CACER: Clinical Concept Annotations for Cancer Events and Relations"
                },
                "summary": "Clinical notes contain unstructured representations of patient histories,\nincluding the relationships between medical problems and prescription drugs. To\ninvestigate the relationship between cancer drugs and their associated symptom\nburden, we extract structured, semantic representations of medical problem and\ndrug information from the clinical narratives of oncology notes. We present\nClinical Concept Annotations for Cancer Events and Relations (CACER), a novel\ncorpus with fine-grained annotations for over 48,000 medical problems and drug\nevents and 10,000 drug-problem and problem-problem relations. Leveraging CACER,\nwe develop and evaluate transformer-based information extraction (IE) models\nsuch as BERT, Flan-T5, Llama3, and GPT-4 using fine-tuning and in-context\nlearning (ICL). In event extraction, the fine-tuned BERT and Llama3 models\nachieved the highest performance at 88.2-88.0 F1, which is comparable to the\ninter-annotator agreement (IAA) of 88.4 F1. In relation extraction, the\nfine-tuned BERT, Flan-T5, and Llama3 achieved the highest performance at\n61.8-65.3 F1. GPT-4 with ICL achieved the worst performance across both tasks.\nThe fine-tuned models significantly outperformed GPT-4 in ICL, highlighting the\nimportance of annotated training data and model optimization. Furthermore, the\nBERT models performed similarly to Llama3. For our task, LLMs offer no\nperformance advantage over the smaller BERT models. The results emphasize the\nneed for annotated training data to optimize models. Multiple fine-tuned\ntransformer models achieved performance comparable to IAA for several\nextraction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical notes contain unstructured representations of patient histories,\nincluding the relationships between medical problems and prescription drugs. To\ninvestigate the relationship between cancer drugs and their associated symptom\nburden, we extract structured, semantic representations of medical problem and\ndrug information from the clinical narratives of oncology notes. We present\nClinical Concept Annotations for Cancer Events and Relations (CACER), a novel\ncorpus with fine-grained annotations for over 48,000 medical problems and drug\nevents and 10,000 drug-problem and problem-problem relations. Leveraging CACER,\nwe develop and evaluate transformer-based information extraction (IE) models\nsuch as BERT, Flan-T5, Llama3, and GPT-4 using fine-tuning and in-context\nlearning (ICL). In event extraction, the fine-tuned BERT and Llama3 models\nachieved the highest performance at 88.2-88.0 F1, which is comparable to the\ninter-annotator agreement (IAA) of 88.4 F1. In relation extraction, the\nfine-tuned BERT, Flan-T5, and Llama3 achieved the highest performance at\n61.8-65.3 F1. GPT-4 with ICL achieved the worst performance across both tasks.\nThe fine-tuned models significantly outperformed GPT-4 in ICL, highlighting the\nimportance of annotated training data and model optimization. Furthermore, the\nBERT models performed similarly to Llama3. For our task, LLMs offer no\nperformance advantage over the smaller BERT models. The results emphasize the\nneed for annotated training data to optimize models. Multiple fine-tuned\ntransformer models achieved performance comparable to IAA for several\nextraction tasks."
                },
                "authors": [
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Giridhar Kaushik Ramachandran"
                    },
                    {
                        "name": "Ahmad Halwani"
                    },
                    {
                        "name": "Bridget T. McInnes"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Kevin Lybarger"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Özlem Uzuner"
                    }
                ],
                "author_detail": {
                    "name": "Özlem Uzuner"
                },
                "author": "Özlem Uzuner",
                "arxiv_doi": "10.1093/jamia/ocae231",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/jamia/ocae231",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is a pre-copy-editing, author-produced PDF of an article\n  accepted for publication in JAMIA following peer review. The definitive\n  publisher-authenticated version is available online at\n  https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae231/7748302",
                "arxiv_journal_ref": "Journal of the American Medical Informatics Association (2024):\n  ocae231",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02486v2",
                "updated": "2024-09-05T20:17:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    17,
                    12,
                    3,
                    249,
                    0
                ],
                "published": "2024-03-04T21:08:25Z",
                "published_parsed": [
                    2024,
                    3,
                    4,
                    21,
                    8,
                    25,
                    0,
                    64,
                    0
                ],
                "title": "Demonstrating a Robust Walking Algorithm for Underactuated Bipedal\n  Robots in Non-flat, Non-stationary Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstrating a Robust Walking Algorithm for Underactuated Bipedal\n  Robots in Non-flat, Non-stationary Environments"
                },
                "summary": "This work explores an innovative algorithm designed to enhance the mobility\nof underactuated bipedal robots across challenging terrains, especially when\nnavigating through spaces with constrained opportunities for foot support, like\nsteps or stairs. By combining ankle torque with a refined angular\nmomentum-based linear inverted pendulum model (ALIP), our method allows\nvariability in the robot's center of mass height. We employ a dual-strategy\ncontroller that merges virtual constraints for precise motion regulation across\nessential degrees of freedom with an ALIP-centric model predictive control\n(MPC) framework, aimed at enforcing gait stability. The effectiveness of our\nfeedback design is demonstrated through its application on the Cassie bipedal\nrobot, which features 20 degrees of freedom. Key to our implementation is the\ndevelopment of tailored nominal trajectories and an optimized MPC that reduces\nthe execution time to under 500 microseconds--and, hence, is compatible with\nCassie's controller update frequency. This paper not only showcases the\nsuccessful hardware deployment but also demonstrates a new capability, a\nbipedal robot using a moving walkway.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores an innovative algorithm designed to enhance the mobility\nof underactuated bipedal robots across challenging terrains, especially when\nnavigating through spaces with constrained opportunities for foot support, like\nsteps or stairs. By combining ankle torque with a refined angular\nmomentum-based linear inverted pendulum model (ALIP), our method allows\nvariability in the robot's center of mass height. We employ a dual-strategy\ncontroller that merges virtual constraints for precise motion regulation across\nessential degrees of freedom with an ALIP-centric model predictive control\n(MPC) framework, aimed at enforcing gait stability. The effectiveness of our\nfeedback design is demonstrated through its application on the Cassie bipedal\nrobot, which features 20 degrees of freedom. Key to our implementation is the\ndevelopment of tailored nominal trajectories and an optimized MPC that reduces\nthe execution time to under 500 microseconds--and, hence, is compatible with\nCassie's controller update frequency. This paper not only showcases the\nsuccessful hardware deployment but also demonstrates a new capability, a\nbipedal robot using a moving walkway."
                },
                "authors": [
                    {
                        "name": "Oluwami Dosunmu-Ogunbi"
                    },
                    {
                        "name": "Aayushi Shrivastava"
                    },
                    {
                        "name": "Jessy W Grizzle"
                    }
                ],
                "author_detail": {
                    "name": "Jessy W Grizzle"
                },
                "author": "Jessy W Grizzle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03868v1",
                "updated": "2024-09-05T19:10:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    19,
                    10,
                    29,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T19:10:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    19,
                    10,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "Few-shot Adaptation of Medical Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Adaptation of Medical Vision-Language Models"
                },
                "summary": "Integrating image and text data through multi-modal learning has emerged as a\nnew approach in medical imaging research, following its successful deployment\nin computer vision. While considerable efforts have been dedicated to\nestablishing medical foundation models and their zero-shot transfer to\ndownstream tasks, the popular few-shot setting remains relatively unexplored.\nFollowing on from the currently strong emergence of this setting in computer\nvision, we introduce the first structured benchmark for adapting medical\nvision-language models (VLMs) in a strict few-shot regime and investigate\nvarious adaptation strategies commonly used in the context of natural images.\nFurthermore, we evaluate a simple generalization of the linear-probe adaptation\nbaseline, which seeks an optimal blending of the visual prototypes and text\nembeddings via learnable class-wise multipliers. Surprisingly, such a\ntext-informed linear probe yields competitive performances in comparison to\nconvoluted prompt-learning and adapter-based strategies, while running\nconsiderably faster and accommodating the black-box setting. Our extensive\nexperiments span three different medical modalities and specialized foundation\nmodels, nine downstream tasks, and several state-of-the-art few-shot adaptation\nmethods. We made our benchmark and code publicly available to trigger further\ndevelopments in this emergent subject:\n\\url{https://github.com/FereshteShakeri/few-shot-MedVLMs}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating image and text data through multi-modal learning has emerged as a\nnew approach in medical imaging research, following its successful deployment\nin computer vision. While considerable efforts have been dedicated to\nestablishing medical foundation models and their zero-shot transfer to\ndownstream tasks, the popular few-shot setting remains relatively unexplored.\nFollowing on from the currently strong emergence of this setting in computer\nvision, we introduce the first structured benchmark for adapting medical\nvision-language models (VLMs) in a strict few-shot regime and investigate\nvarious adaptation strategies commonly used in the context of natural images.\nFurthermore, we evaluate a simple generalization of the linear-probe adaptation\nbaseline, which seeks an optimal blending of the visual prototypes and text\nembeddings via learnable class-wise multipliers. Surprisingly, such a\ntext-informed linear probe yields competitive performances in comparison to\nconvoluted prompt-learning and adapter-based strategies, while running\nconsiderably faster and accommodating the black-box setting. Our extensive\nexperiments span three different medical modalities and specialized foundation\nmodels, nine downstream tasks, and several state-of-the-art few-shot adaptation\nmethods. We made our benchmark and code publicly available to trigger further\ndevelopments in this emergent subject:\n\\url{https://github.com/FereshteShakeri/few-shot-MedVLMs}."
                },
                "authors": [
                    {
                        "name": "Fereshteh Shakeri"
                    },
                    {
                        "name": "Yunshi Huang"
                    },
                    {
                        "name": "Julio Silva-Rodríguez"
                    },
                    {
                        "name": "Houda Bahig"
                    },
                    {
                        "name": "An Tang"
                    },
                    {
                        "name": "Jose Dolz"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Ismail Ben Ayed"
                },
                "author": "Ismail Ben Ayed",
                "arxiv_comment": "MICCAI 2024 (Spotlight) - Code is available at\n  https://github.com/FereshteShakeri/few-shot-MedVLMs.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03856v1",
                "updated": "2024-09-05T18:38:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    18,
                    38,
                    7,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T18:38:07Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    18,
                    38,
                    7,
                    3,
                    249,
                    0
                ],
                "title": "Sirius: Contextual Sparsity with Correction for Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sirius: Contextual Sparsity with Correction for Efficient LLMs"
                },
                "summary": "With the blossom of large language models (LLMs), inference efficiency\nbecomes increasingly important. Various approximation methods are proposed to\nreduce the cost at inference time. Contextual Sparsity (CS) is appealing for\nits training-free nature and its ability to reach a higher compression ratio\nseemingly without quality degradation. However, after a comprehensive\nevaluation of contextual sparsity methods on various complex generation tasks,\nwe find that although CS succeeds in prompt-understanding tasks, CS\nsignificantly degrades the model performance for reasoning, deduction, and\nknowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that\nsparse models often share general problem-solving logic and require only a few\ntoken corrections to recover the original model performance. This paper\nintroduces Sirius, an efficient correction mechanism, which significantly\nrecovers CS models quality on reasoning tasks while maintaining its efficiency\ngain. Sirius is evaluated on 6 models with 8 difficult generation tasks in\nreasoning, math, and coding and shows consistent effectiveness and efficiency.\nAlso, we carefully develop a system implementation for Sirius and show that\nSirius achieves roughly 20% reduction in latency for 8B model on-chip and 35%\nreduction for 70B model offloading. We open-source our implementation of Sirius\nat https://github.com/Infini-AI-Lab/Sirius.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the blossom of large language models (LLMs), inference efficiency\nbecomes increasingly important. Various approximation methods are proposed to\nreduce the cost at inference time. Contextual Sparsity (CS) is appealing for\nits training-free nature and its ability to reach a higher compression ratio\nseemingly without quality degradation. However, after a comprehensive\nevaluation of contextual sparsity methods on various complex generation tasks,\nwe find that although CS succeeds in prompt-understanding tasks, CS\nsignificantly degrades the model performance for reasoning, deduction, and\nknowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that\nsparse models often share general problem-solving logic and require only a few\ntoken corrections to recover the original model performance. This paper\nintroduces Sirius, an efficient correction mechanism, which significantly\nrecovers CS models quality on reasoning tasks while maintaining its efficiency\ngain. Sirius is evaluated on 6 models with 8 difficult generation tasks in\nreasoning, math, and coding and shows consistent effectiveness and efficiency.\nAlso, we carefully develop a system implementation for Sirius and show that\nSirius achieves roughly 20% reduction in latency for 8B model on-chip and 35%\nreduction for 70B model offloading. We open-source our implementation of Sirius\nat https://github.com/Infini-AI-Lab/Sirius.git."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Victoria Lin"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07088v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07088v6",
                "updated": "2024-09-05T18:26:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    18,
                    26,
                    56,
                    3,
                    249,
                    0
                ],
                "published": "2024-03-11T18:26:02Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    18,
                    26,
                    2,
                    0,
                    71,
                    0
                ],
                "title": "SPA: Towards A Computational Friendly Cloud-Base and On-Devices\n  Collaboration Seq2seq Personalized Generation with Casual Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPA: Towards A Computational Friendly Cloud-Base and On-Devices\n  Collaboration Seq2seq Personalized Generation with Casual Inference"
                },
                "summary": "Large language models(LLMs) have shown its outperforming ability on various\ntasks and question answering. However, LLMs require substantial memory storage\non low-resource devices. More critically, the computational speed on these\ndevices is also severely limited. In this paper, we propose SPA(Side Plugin\nAdaption), a lightweight architecture for fast on-devices inference on the\nconstraints of strict on-devices computation and memory constraints. Compared\nwith other on-devices seq2seq generation, SPA could make a fast and stable\ninference on low-resource constraints, allowing it to obtain cost effiency. Our\nmethod establish an interaction between a pretrained LLMs on-cloud and additive\nparameters on-devices, which could provide the knowledge on both pretrained\nLLMs and featured personal feature. Further more, SPA provides a framework to\nkeep feature-base parameters on low computational devices while leave the\nparameters containing general information on the high computational devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) have shown its outperforming ability on various\ntasks and question answering. However, LLMs require substantial memory storage\non low-resource devices. More critically, the computational speed on these\ndevices is also severely limited. In this paper, we propose SPA(Side Plugin\nAdaption), a lightweight architecture for fast on-devices inference on the\nconstraints of strict on-devices computation and memory constraints. Compared\nwith other on-devices seq2seq generation, SPA could make a fast and stable\ninference on low-resource constraints, allowing it to obtain cost effiency. Our\nmethod establish an interaction between a pretrained LLMs on-cloud and additive\nparameters on-devices, which could provide the knowledge on both pretrained\nLLMs and featured personal feature. Further more, SPA provides a framework to\nkeep feature-base parameters on low computational devices while leave the\nparameters containing general information on the high computational devices."
                },
                "authors": [
                    {
                        "name": "Yanming Liu"
                    },
                    {
                        "name": "Xinyue Peng"
                    },
                    {
                        "name": "Shi Bo"
                    },
                    {
                        "name": "Ningjing Sang"
                    },
                    {
                        "name": "Yafeng Yan"
                    },
                    {
                        "name": "Xiaolan Ke"
                    },
                    {
                        "name": "Zhiting Zheng"
                    },
                    {
                        "name": "Shaobo Liu"
                    },
                    {
                        "name": "Songhang Deng"
                    },
                    {
                        "name": "Jiannan Cao"
                    },
                    {
                        "name": "Le Dai"
                    },
                    {
                        "name": "Xingzu Liu"
                    },
                    {
                        "name": "Ruilin Nong"
                    },
                    {
                        "name": "Weihao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weihao Liu"
                },
                "author": "Weihao Liu",
                "arxiv_comment": "12 pages, third version of SPA(Side Plugin Adaption)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07088v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07088v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05399v2",
                "updated": "2024-09-05T18:13:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    18,
                    13,
                    0,
                    3,
                    249,
                    0
                ],
                "published": "2024-02-08T04:27:14Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    27,
                    14,
                    3,
                    39,
                    0
                ],
                "title": "CURE: Simulation-Augmented Auto-Tuning in Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURE: Simulation-Augmented Auto-Tuning in Robotics"
                },
                "summary": "Robotic systems are typically composed of various subsystems, such as\nlocalization and navigation, each encompassing numerous configurable components\n(e.g., selecting different planning algorithms). Once an algorithm has been\nselected for a component, its associated configuration options must be set to\nthe appropriate values. Configuration options across the system stack interact\nnon-trivially. Finding optimal configurations for highly configurable robots to\nachieve desired performance poses a significant challenge due to the\ninteractions between configuration options across software and hardware that\nresult in an exponentially large and complex configuration space. These\nchallenges are further compounded by the need for transferability between\ndifferent environments and robotic platforms. Data efficient optimization\nalgorithms (e.g., Bayesian optimization) have been increasingly employed to\nautomate the tuning of configurable parameters in cyber-physical systems.\nHowever, such optimization algorithms converge at later stages, often after\nexhausting the allocated budget (e.g., optimization steps, allotted time) and\nlacking transferability. This paper proposes CURE -- a method that identifies\ncausally relevant configuration options, enabling the optimization process to\noperate in a reduced search space, thereby enabling faster optimization of\nrobot performance. CURE abstracts the causal relationships between various\nconfiguration options and robot performance objectives by learning a causal\nmodel in the source (a low-cost environment such as the Gazebo simulator) and\napplying the learned knowledge to perform optimization in the target (e.g.,\nTurtlebot 3 physical robot). We demonstrate the effectiveness and\ntransferability of CURE by conducting experiments that involve varying degrees\nof deployment changes in both physical robots and simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic systems are typically composed of various subsystems, such as\nlocalization and navigation, each encompassing numerous configurable components\n(e.g., selecting different planning algorithms). Once an algorithm has been\nselected for a component, its associated configuration options must be set to\nthe appropriate values. Configuration options across the system stack interact\nnon-trivially. Finding optimal configurations for highly configurable robots to\nachieve desired performance poses a significant challenge due to the\ninteractions between configuration options across software and hardware that\nresult in an exponentially large and complex configuration space. These\nchallenges are further compounded by the need for transferability between\ndifferent environments and robotic platforms. Data efficient optimization\nalgorithms (e.g., Bayesian optimization) have been increasingly employed to\nautomate the tuning of configurable parameters in cyber-physical systems.\nHowever, such optimization algorithms converge at later stages, often after\nexhausting the allocated budget (e.g., optimization steps, allotted time) and\nlacking transferability. This paper proposes CURE -- a method that identifies\ncausally relevant configuration options, enabling the optimization process to\noperate in a reduced search space, thereby enabling faster optimization of\nrobot performance. CURE abstracts the causal relationships between various\nconfiguration options and robot performance objectives by learning a causal\nmodel in the source (a low-cost environment such as the Gazebo simulator) and\napplying the learned knowledge to perform optimization in the target (e.g.,\nTurtlebot 3 physical robot). We demonstrate the effectiveness and\ntransferability of CURE by conducting experiments that involve varying degrees\nof deployment changes in both physical robots and simulation."
                },
                "authors": [
                    {
                        "name": "Md Abir Hossen"
                    },
                    {
                        "name": "Sonam Kharade"
                    },
                    {
                        "name": "Jason M. O'Kane"
                    },
                    {
                        "name": "Bradley Schmerl"
                    },
                    {
                        "name": "David Garlan"
                    },
                    {
                        "name": "Pooyan Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Pooyan Jamshidi"
                },
                "author": "Pooyan Jamshidi",
                "arxiv_comment": "Revised submission in IEEE Transactions on Robotics (T-RO), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03843v1",
                "updated": "2024-09-05T18:08:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    18,
                    8,
                    47,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T18:08:47Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    18,
                    8,
                    47,
                    3,
                    249,
                    0
                ],
                "title": "Persona Setting Pitfall: Persistent Outgroup Biases in Large Language\n  Models Arising from Social Identity Adoption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona Setting Pitfall: Persistent Outgroup Biases in Large Language\n  Models Arising from Social Identity Adoption"
                },
                "summary": "Drawing parallels between human cognition and artificial intelligence, we\nexplored how large language models (LLMs) internalize identities imposed by\ntargeted prompts. Informed by Social Identity Theory, these identity\nassignments lead LLMs to distinguish between \"we\" (the ingroup) and \"they\" (the\noutgroup). This self-categorization generates both ingroup favoritism and\noutgroup bias. Nonetheless, existing literature has predominantly focused on\ningroup favoritism, often overlooking outgroup bias, which is a fundamental\nsource of intergroup prejudice and discrimination. Our experiment addresses\nthis gap by demonstrating that outgroup bias manifests as strongly as ingroup\nfavoritism. Furthermore, we successfully mitigated the inherent pro-liberal,\nanti-conservative bias in LLMs by guiding them to adopt the perspectives of the\ninitially disfavored group. These results were replicated in the context of\ngender bias. Our findings highlight the potential to develop more equitable and\nbalanced language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drawing parallels between human cognition and artificial intelligence, we\nexplored how large language models (LLMs) internalize identities imposed by\ntargeted prompts. Informed by Social Identity Theory, these identity\nassignments lead LLMs to distinguish between \"we\" (the ingroup) and \"they\" (the\noutgroup). This self-categorization generates both ingroup favoritism and\noutgroup bias. Nonetheless, existing literature has predominantly focused on\ningroup favoritism, often overlooking outgroup bias, which is a fundamental\nsource of intergroup prejudice and discrimination. Our experiment addresses\nthis gap by demonstrating that outgroup bias manifests as strongly as ingroup\nfavoritism. Furthermore, we successfully mitigated the inherent pro-liberal,\nanti-conservative bias in LLMs by guiding them to adopt the perspectives of the\ninitially disfavored group. These results were replicated in the context of\ngender bias. Our findings highlight the potential to develop more equitable and\nbalanced language models."
                },
                "authors": [
                    {
                        "name": "Wenchao Dong"
                    },
                    {
                        "name": "Assem Zhunis"
                    },
                    {
                        "name": "Dongyoung Jeong"
                    },
                    {
                        "name": "Hyojin Chin"
                    },
                    {
                        "name": "Jiyoung Han"
                    },
                    {
                        "name": "Meeyoung Cha"
                    }
                ],
                "author_detail": {
                    "name": "Meeyoung Cha"
                },
                "author": "Meeyoung Cha",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03838v1",
                "updated": "2024-09-05T18:02:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    18,
                    2,
                    41,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T18:02:41Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    18,
                    2,
                    41,
                    3,
                    249,
                    0
                ],
                "title": "APITestGenie: Automated API Test Generation through Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APITestGenie: Automated API Test Generation through Generative AI"
                },
                "summary": "Intelligent assistants powered by Large Language Models (LLMs) can generate\nprogram and test code with high accuracy, boosting developers' and testers'\nproductivity. However, there is a lack of studies exploring LLMs for testing\nWeb APIs, which constitute fundamental building blocks of modern software\nsystems and pose significant test challenges. Hence, in this article, we\nintroduce APITestGenie, an approach and tool that leverages LLMs to generate\nexecutable API test scripts from business requirements and API specifications.\nIn experiments with 10 real-world APIs, the tool generated valid test scripts\n57% of the time. With three generation attempts per task, this success rate\nincreased to 80%. Human intervention is recommended to validate or refine\ngenerated scripts before integration into CI/CD pipelines, positioning our tool\nas a productivity assistant rather than a replacement for testers. Feedback\nfrom industry specialists indicated a strong interest in adopting our tool for\nimproving the API test process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent assistants powered by Large Language Models (LLMs) can generate\nprogram and test code with high accuracy, boosting developers' and testers'\nproductivity. However, there is a lack of studies exploring LLMs for testing\nWeb APIs, which constitute fundamental building blocks of modern software\nsystems and pose significant test challenges. Hence, in this article, we\nintroduce APITestGenie, an approach and tool that leverages LLMs to generate\nexecutable API test scripts from business requirements and API specifications.\nIn experiments with 10 real-world APIs, the tool generated valid test scripts\n57% of the time. With three generation attempts per task, this success rate\nincreased to 80%. Human intervention is recommended to validate or refine\ngenerated scripts before integration into CI/CD pipelines, positioning our tool\nas a productivity assistant rather than a replacement for testers. Feedback\nfrom industry specialists indicated a strong interest in adopting our tool for\nimproving the API test process."
                },
                "authors": [
                    {
                        "name": "André Pereira"
                    },
                    {
                        "name": "Bruno Lima"
                    },
                    {
                        "name": "João Pascoal Faria"
                    }
                ],
                "author_detail": {
                    "name": "João Pascoal Faria"
                },
                "author": "João Pascoal Faria",
                "arxiv_comment": "6 pages, 4 figures, submitted to IEEE Software Special Issue on\n  Next-generation Software Testing: AI-powered Test Automation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03752v1",
                "updated": "2024-09-05T17:59:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    12,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:59:12Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    12,
                    3,
                    249,
                    0
                ],
                "title": "Attention Heads of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Heads of Large Language Models: A Survey"
                },
                "summary": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain largely as black-box systems. Consequently, their\ndevelopment relies heavily on data-driven approaches, limiting performance\nenhancement through changes in internal architecture and reasoning pathways. As\na result, many researchers have begun exploring the potential internal\nmechanisms of LLMs, aiming to identify the essence of their reasoning\nbottlenecks, with most studies focusing on attention heads. Our survey aims to\nshed light on the internal reasoning processes of LLMs by concentrating on the\ninterpretability and underlying mechanisms of attention heads. We first distill\nthe human thought process into a four-stage framework: Knowledge Recalling,\nIn-Context Identification, Latent Reasoning, and Expression Preparation. Using\nthis framework, we systematically review existing research to identify and\ncategorize the functions of specific attention heads. Furthermore, we summarize\nthe experimental methodologies used to discover these special heads, dividing\nthem into two categories: Modeling-Free methods and Modeling-Required methods.\nAlso, we outline relevant evaluation methods and benchmarks. Finally, we\ndiscuss the limitations of current research and propose several potential\nfuture directions. Our reference list is open-sourced at\n\\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain largely as black-box systems. Consequently, their\ndevelopment relies heavily on data-driven approaches, limiting performance\nenhancement through changes in internal architecture and reasoning pathways. As\na result, many researchers have begun exploring the potential internal\nmechanisms of LLMs, aiming to identify the essence of their reasoning\nbottlenecks, with most studies focusing on attention heads. Our survey aims to\nshed light on the internal reasoning processes of LLMs by concentrating on the\ninterpretability and underlying mechanisms of attention heads. We first distill\nthe human thought process into a four-stage framework: Knowledge Recalling,\nIn-Context Identification, Latent Reasoning, and Expression Preparation. Using\nthis framework, we systematically review existing research to identify and\ncategorize the functions of specific attention heads. Furthermore, we summarize\nthe experimental methodologies used to discover these special heads, dividing\nthem into two categories: Modeling-Free methods and Modeling-Required methods.\nAlso, we outline relevant evaluation methods and benchmarks. Finally, we\ndiscuss the limitations of current research and propose several potential\nfuture directions. Our reference list is open-sourced at\n\\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}."
                },
                "authors": [
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "20 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03735v1",
                "updated": "2024-09-05T17:50:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    50,
                    31,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:50:31Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    50,
                    31,
                    3,
                    249,
                    0
                ],
                "title": "LLM-CI: Assessing Contextual Integrity Norms in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-CI: Assessing Contextual Integrity Norms in Language Models"
                },
                "summary": "Large language models (LLMs), while memorizing parts of their training data\nscraped from the Internet, may also inadvertently encode societal preferences\nand norms. As these models are integrated into sociotechnical systems, it is\ncrucial that the norms they encode align with societal expectations. These\nnorms could vary across models, hyperparameters, optimization techniques, and\ndatasets. This is especially challenging due to prompt sensitivity$-$small\nvariations in prompts yield different responses, rendering existing assessment\nmethodologies unreliable. There is a need for a comprehensive framework\ncovering various models, optimization, and datasets, along with a reliable\nmethodology to assess encoded norms.\n  We present LLM-CI, the first open-sourced framework to assess privacy norms\nencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette\nmethodology to assess the encoded norms across different contexts and LLMs. We\npropose the multi-prompt assessment methodology to address prompt sensitivity\nby assessing the norms from only the prompts that yield consistent responses\nacross multiple variants. Using LLM-CI and our proposed methodology, we\ncomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior\nwork, examining the impact of model properties (e.g., hyperparameters,\ncapacity) and optimization strategies (e.g., alignment, quantization).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), while memorizing parts of their training data\nscraped from the Internet, may also inadvertently encode societal preferences\nand norms. As these models are integrated into sociotechnical systems, it is\ncrucial that the norms they encode align with societal expectations. These\nnorms could vary across models, hyperparameters, optimization techniques, and\ndatasets. This is especially challenging due to prompt sensitivity$-$small\nvariations in prompts yield different responses, rendering existing assessment\nmethodologies unreliable. There is a need for a comprehensive framework\ncovering various models, optimization, and datasets, along with a reliable\nmethodology to assess encoded norms.\n  We present LLM-CI, the first open-sourced framework to assess privacy norms\nencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette\nmethodology to assess the encoded norms across different contexts and LLMs. We\npropose the multi-prompt assessment methodology to address prompt sensitivity\nby assessing the norms from only the prompts that yield consistent responses\nacross multiple variants. Using LLM-CI and our proposed methodology, we\ncomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior\nwork, examining the impact of model properties (e.g., hyperparameters,\ncapacity) and optimization strategies (e.g., alignment, quantization)."
                },
                "authors": [
                    {
                        "name": "Yan Shvartzshnaider"
                    },
                    {
                        "name": "Vasisht Duddu"
                    },
                    {
                        "name": "John Lacalamita"
                    }
                ],
                "author_detail": {
                    "name": "John Lacalamita"
                },
                "author": "John Lacalamita",
                "arxiv_comment": "20 pages, 8 Figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03810v1",
                "updated": "2024-09-05T17:46:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    46,
                    30,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:46:30Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    46,
                    30,
                    3,
                    249,
                    0
                ],
                "title": "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with\n  High-Quality Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with\n  High-Quality Data"
                },
                "summary": "Recently, there has been a growing interest in studying how to construct\nbetter code instruction tuning data. However, we observe Code models trained\nwith these datasets exhibit high performance on HumanEval but perform worse on\nother benchmarks such as LiveCodeBench. Upon further investigation, we find\nthat many datasets suffer from severe data leakage. After cleaning up most of\nthe leaked data, some well-known high-quality datasets perform poorly. This\ndiscovery reveals a new challenge: identifying which dataset genuinely qualify\nas high-quality code instruction data. To address this, we propose an efficient\ncode data pruning strategy for selecting good samples. Our approach is based on\nthree dimensions: instruction complexity, response quality, and instruction\ndiversity. Based on our selected data, we present XCoder, a family of models\nfinetuned from LLaMA3. Our experiments show XCoder achieves new\nstate-of-the-art performance using fewer training data, which verify the\neffectiveness of our data strategy. Moreover, we perform a comprehensive\nanalysis on the data composition and find existing code datasets have different\ncharacteristics according to their construction methods, which provide new\ninsights for future code LLMs. Our models and dataset are released in\nhttps://github.com/banksy23/XCoder",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a growing interest in studying how to construct\nbetter code instruction tuning data. However, we observe Code models trained\nwith these datasets exhibit high performance on HumanEval but perform worse on\nother benchmarks such as LiveCodeBench. Upon further investigation, we find\nthat many datasets suffer from severe data leakage. After cleaning up most of\nthe leaked data, some well-known high-quality datasets perform poorly. This\ndiscovery reveals a new challenge: identifying which dataset genuinely qualify\nas high-quality code instruction data. To address this, we propose an efficient\ncode data pruning strategy for selecting good samples. Our approach is based on\nthree dimensions: instruction complexity, response quality, and instruction\ndiversity. Based on our selected data, we present XCoder, a family of models\nfinetuned from LLaMA3. Our experiments show XCoder achieves new\nstate-of-the-art performance using fewer training data, which verify the\neffectiveness of our data strategy. Moreover, we perform a comprehensive\nanalysis on the data composition and find existing code datasets have different\ncharacteristics according to their construction methods, which provide new\ninsights for future code LLMs. Our models and dataset are released in\nhttps://github.com/banksy23/XCoder"
                },
                "authors": [
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Keqing He"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Zhuoma Gongque"
                    },
                    {
                        "name": "Heyang Xu"
                    },
                    {
                        "name": "Yanxu Chen"
                    },
                    {
                        "name": "Zhexu Wang"
                    },
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Muxi Diao"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Weiran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Xu"
                },
                "author": "Weiran Xu",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03733v1",
                "updated": "2024-09-05T17:44:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    44,
                    49,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:44:49Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    44,
                    49,
                    3,
                    249,
                    0
                ],
                "title": "Planning In Natural Language Improves LLM Search For Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning In Natural Language Improves LLM Search For Code Generation"
                },
                "summary": "While scaling training compute has led to remarkable improvements in large\nlanguage models (LLMs), scaling inference compute has not yet yielded analogous\ngains. We hypothesize that a core missing component is a lack of diverse LLM\noutputs, leading to inefficient search due to models repeatedly sampling highly\nsimilar, yet incorrect generations. We empirically demonstrate that this lack\nof diversity can be mitigated by searching over candidate plans for solving a\nproblem in natural language. Based on this insight, we propose PLANSEARCH, a\nnovel search algorithm which shows strong results across HumanEval+, MBPP+, and\nLiveCodeBench (a contamination-free benchmark for competitive coding).\nPLANSEARCH generates a diverse set of observations about the problem and then\nuses these observations to construct plans for solving the problem. By\nsearching over plans in natural language rather than directly over code\nsolutions, PLANSEARCH explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PLANSEARCH on top of\nClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on\nLiveCodeBench, outperforming both the best score achieved without search\n(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks\nanalyzed, we can accurately predict performance gains due to search as a direct\nfunction of the diversity over generated ideas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While scaling training compute has led to remarkable improvements in large\nlanguage models (LLMs), scaling inference compute has not yet yielded analogous\ngains. We hypothesize that a core missing component is a lack of diverse LLM\noutputs, leading to inefficient search due to models repeatedly sampling highly\nsimilar, yet incorrect generations. We empirically demonstrate that this lack\nof diversity can be mitigated by searching over candidate plans for solving a\nproblem in natural language. Based on this insight, we propose PLANSEARCH, a\nnovel search algorithm which shows strong results across HumanEval+, MBPP+, and\nLiveCodeBench (a contamination-free benchmark for competitive coding).\nPLANSEARCH generates a diverse set of observations about the problem and then\nuses these observations to construct plans for solving the problem. By\nsearching over plans in natural language rather than directly over code\nsolutions, PLANSEARCH explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PLANSEARCH on top of\nClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on\nLiveCodeBench, outperforming both the best score achieved without search\n(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks\nanalyzed, we can accurately predict performance gains due to search as a direct\nfunction of the diversity over generated ideas."
                },
                "authors": [
                    {
                        "name": "Evan Wang"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Catherine Wu"
                    },
                    {
                        "name": "Yunfeng Bai"
                    },
                    {
                        "name": "Will Song"
                    },
                    {
                        "name": "Vaskar Nath"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Sean Hendryx"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Hugh Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hugh Zhang"
                },
                "author": "Hugh Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05498v2",
                "updated": "2024-09-05T17:33:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    33,
                    33,
                    3,
                    249,
                    0
                ],
                "published": "2024-06-08T15:45:31Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    15,
                    45,
                    31,
                    5,
                    160,
                    0
                ],
                "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner"
                },
                "summary": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance to concurrently protect the target LLM instance in the normal\nstack and collaborate with it for checkpoint-based access control. The\neffectiveness of SelfDefend builds upon our observation that existing LLMs\n(both target and defense LLMs) have the capability to identify harmful prompts\nor intentions in user queries, which we empirically validate using the commonly\nused GPT-3.5/4 models across all major jailbreak attacks. To further improve\nthe defense's robustness and minimize costs, we employ a data distillation\napproach to tune dedicated open-source defense models. These models outperform\nsix state-of-the-art defenses and match the performance of GPT-4-based\nSelfDefend, with significantly lower extra delays. We also empirically show\nthat the tuned models are robust to adaptive jailbreaks and prompt injections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance to concurrently protect the target LLM instance in the normal\nstack and collaborate with it for checkpoint-based access control. The\neffectiveness of SelfDefend builds upon our observation that existing LLMs\n(both target and defense LLMs) have the capability to identify harmful prompts\nor intentions in user queries, which we empirically validate using the commonly\nused GPT-3.5/4 models across all major jailbreak attacks. To further improve\nthe defense's robustness and minimize costs, we employ a data distillation\napproach to tune dedicated open-source defense models. These models outperform\nsix state-of-the-art defenses and match the performance of GPT-4-based\nSelfDefend, with significantly lower extra delays. We also empirically show\nthat the tuned models are robust to adaptive jailbreaks and prompt injections."
                },
                "authors": [
                    {
                        "name": "Xunguang Wang"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Zhenlan Ji"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yingjiu Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Juergen Rahmel"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Rahmel"
                },
                "author": "Juergen Rahmel",
                "arxiv_comment": "This paper completes its earlier vision paper, available at\n  arXiv:2402.15727. Updated to the latest analysis and results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06477v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06477v3",
                "updated": "2024-09-05T17:25:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    25,
                    1,
                    3,
                    249,
                    0
                ],
                "published": "2024-01-12T09:56:57Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    9,
                    56,
                    57,
                    4,
                    12,
                    0
                ],
                "title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation"
                },
                "summary": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun"
                },
                "authors": [
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06477v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06477v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03671v1",
                "updated": "2024-09-05T16:24:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    24,
                    42,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:24:42Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    24,
                    42,
                    3,
                    249,
                    0
                ],
                "title": "TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course\n  Scheduling Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course\n  Scheduling Problems"
                },
                "summary": "We present TRACE-cs, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs) to address contrastive queries in scheduling\nproblems. TRACE-cs leverages SAT solving techniques to encode scheduling\nconstraints and generate explanations for user queries, while utilizing an LLM\nto process the user queries into logical clauses as well as refine the\nexplanations generated by the symbolic solver to natural language sentences. By\nintegrating these components, our approach demonstrates the potential of\ncombining symbolic methods with LLMs to create explainable AI agents with\ncorrectness guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TRACE-cs, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs) to address contrastive queries in scheduling\nproblems. TRACE-cs leverages SAT solving techniques to encode scheduling\nconstraints and generate explanations for user queries, while utilizing an LLM\nto process the user queries into logical clauses as well as refine the\nexplanations generated by the symbolic solver to natural language sentences. By\nintegrating these components, our approach demonstrates the potential of\ncombining symbolic methods with LLMs to create explainable AI agents with\ncorrectness guarantees."
                },
                "authors": [
                    {
                        "name": "Stylianos Loukas Vasileiou"
                    },
                    {
                        "name": "William Yeoh"
                    }
                ],
                "author_detail": {
                    "name": "William Yeoh"
                },
                "author": "William Yeoh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06120v3",
                "updated": "2024-09-05T16:19:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    19,
                    32,
                    3,
                    249,
                    0
                ],
                "published": "2024-02-09T01:10:25Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    1,
                    10,
                    25,
                    4,
                    40,
                    0
                ],
                "title": "Exploring Group and Symmetry Principles in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Group and Symmetry Principles in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released."
                },
                "authors": [
                    {
                        "name": "Shima Imani"
                    },
                    {
                        "name": "Hamid Palangi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Palangi"
                },
                "author": "Hamid Palangi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16639v3",
                "updated": "2024-09-05T16:17:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    17,
                    20,
                    3,
                    249,
                    0
                ],
                "published": "2023-11-28T09:45:02Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    9,
                    45,
                    2,
                    1,
                    332,
                    0
                ],
                "title": "Positioning Political Texts with Large Language Models by Asking and\n  Averaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positioning Political Texts with Large Language Models by Asking and\n  Averaging"
                },
                "summary": "We use instruction-tuned Large Language Models (LLMs) like GPT-4, Llama 3,\nMiXtral, or Aya to position political texts within policy and ideological\nspaces. We ask an LLM where a tweet or a sentence of a political text stands on\nthe focal dimension and take the average of the LLM responses to position\npolitical actors such as US Senators, or longer texts such as UK party\nmanifestos or EU policy speeches given in 10 different languages. The\ncorrelations between the position estimates obtained with the best LLMs and\nbenchmarks based on text coding by experts, crowdworkers, or roll call votes\nexceed .90. This approach is generally more accurate than the positions\nobtained with supervised classifiers trained on large amounts of research data.\nUsing instruction-tuned LLMs to position texts in policy and ideological spaces\nis fast, cost-efficient, reliable, and reproducible (in the case of open LLMs)\neven if the texts are short and written in different languages. We conclude\nwith cautionary notes about the need for empirical validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use instruction-tuned Large Language Models (LLMs) like GPT-4, Llama 3,\nMiXtral, or Aya to position political texts within policy and ideological\nspaces. We ask an LLM where a tweet or a sentence of a political text stands on\nthe focal dimension and take the average of the LLM responses to position\npolitical actors such as US Senators, or longer texts such as UK party\nmanifestos or EU policy speeches given in 10 different languages. The\ncorrelations between the position estimates obtained with the best LLMs and\nbenchmarks based on text coding by experts, crowdworkers, or roll call votes\nexceed .90. This approach is generally more accurate than the positions\nobtained with supervised classifiers trained on large amounts of research data.\nUsing instruction-tuned LLMs to position texts in policy and ideological spaces\nis fast, cost-efficient, reliable, and reproducible (in the case of open LLMs)\neven if the texts are short and written in different languages. We conclude\nwith cautionary notes about the need for empirical validation."
                },
                "authors": [
                    {
                        "name": "Gaël Le Mens"
                    },
                    {
                        "name": "Aina Gallego"
                    }
                ],
                "author_detail": {
                    "name": "Aina Gallego"
                },
                "author": "Aina Gallego",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03662v2",
                "updated": "2024-09-07T12:47:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    12,
                    47,
                    54,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-05T16:15:12Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    15,
                    12,
                    3,
                    249,
                    0
                ],
                "title": "The representation landscape of few-shot learning and fine-tuning in\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The representation landscape of few-shot learning and fine-tuning in\n  large language models"
                },
                "summary": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common\nstrategies for improving the performance of modern large language models (LLMs)\non specific tasks. Despite their different natures, these strategies often lead\nto comparable performance gains. However, little is known about whether they\ninduce similar representations inside LLMs. We approach this problem by\nanalyzing the probability landscape of their hidden representations in the two\ncases. More specifically, we compare how LLMs solve the same question-answering\ntask, finding that ICL and SFT create very different internal structures, in\nboth cases undergoing a sharp transition in the middle of the network. In the\nfirst half of the network, ICL shapes interpretable representations\nhierarchically organized according to their semantic content. In contrast, the\nprobability landscape obtained with SFT is fuzzier and semantically mixed. In\nthe second half of the model, the fine-tuned representations develop\nprobability modes that better encode the identity of answers, while the\nlandscape of ICL representations is characterized by less defined peaks. Our\napproach reveals the diverse computational strategies developed inside LLMs to\nsolve the same task across different conditions, allowing us to make a step\ntowards designing optimal methods to extract information from language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common\nstrategies for improving the performance of modern large language models (LLMs)\non specific tasks. Despite their different natures, these strategies often lead\nto comparable performance gains. However, little is known about whether they\ninduce similar representations inside LLMs. We approach this problem by\nanalyzing the probability landscape of their hidden representations in the two\ncases. More specifically, we compare how LLMs solve the same question-answering\ntask, finding that ICL and SFT create very different internal structures, in\nboth cases undergoing a sharp transition in the middle of the network. In the\nfirst half of the network, ICL shapes interpretable representations\nhierarchically organized according to their semantic content. In contrast, the\nprobability landscape obtained with SFT is fuzzier and semantically mixed. In\nthe second half of the model, the fine-tuned representations develop\nprobability modes that better encode the identity of answers, while the\nlandscape of ICL representations is characterized by less defined peaks. Our\napproach reveals the diverse computational strategies developed inside LLMs to\nsolve the same task across different conditions, allowing us to make a step\ntowards designing optimal methods to extract information from language models."
                },
                "authors": [
                    {
                        "name": "Diego Doimo"
                    },
                    {
                        "name": "Alessandro Serra"
                    },
                    {
                        "name": "Alessio Ansuini"
                    },
                    {
                        "name": "Alberto Cazzaniga"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Cazzaniga"
                },
                "author": "Alberto Cazzaniga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04168v2",
                "updated": "2024-09-05T16:14:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    14,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-08T02:28:43Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    2,
                    28,
                    43,
                    3,
                    221,
                    0
                ],
                "title": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City\n  Navigation without Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City\n  Navigation without Instructions"
                },
                "summary": "This paper considers a scenario in city navigation: an AI agent is provided\nwith language descriptions of the goal location with respect to some well-known\nlandmarks; By only observing the scene around, including recognizing landmarks\nand road network connections, the agent has to make decisions to navigate to\nthe goal location without instructions. This problem is very challenging,\nbecause it requires agent to establish self-position and acquire spatial\nrepresentation of complex urban environment, where landmarks are often\ninvisible. In the absence of navigation instructions, such abilities are vital\nfor the agent to make high-quality decisions in long-range city navigation.\nWith the emergent reasoning ability of large language models (LLMs), a tempting\nbaseline is to prompt LLMs to \"react\" on each observation and make decisions\naccordingly. However, this baseline has very poor performance that the agent\noften repeatedly visits same locations and make short-sighted, inconsistent\ndecisions. To address these issues, this paper introduces a novel agentic\nworkflow featured by its abilities to perceive, reflect and plan. Specifically,\nwe find LLaVA-7B can be fine-tuned to perceive the direction and distance of\nlandmarks with sufficient accuracy for city navigation. Moreover, reflection is\nachieved through a memory mechanism, where past experiences are stored and can\nbe retrieved with current perception for effective decision argumentation.\nPlanning uses reflection results to produce long-term plans, which can avoid\nshort-sighted decisions in long-range navigation. We show the designed workflow\nsignificantly improves navigation ability of the LLM agent compared with the\nstate-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers a scenario in city navigation: an AI agent is provided\nwith language descriptions of the goal location with respect to some well-known\nlandmarks; By only observing the scene around, including recognizing landmarks\nand road network connections, the agent has to make decisions to navigate to\nthe goal location without instructions. This problem is very challenging,\nbecause it requires agent to establish self-position and acquire spatial\nrepresentation of complex urban environment, where landmarks are often\ninvisible. In the absence of navigation instructions, such abilities are vital\nfor the agent to make high-quality decisions in long-range city navigation.\nWith the emergent reasoning ability of large language models (LLMs), a tempting\nbaseline is to prompt LLMs to \"react\" on each observation and make decisions\naccordingly. However, this baseline has very poor performance that the agent\noften repeatedly visits same locations and make short-sighted, inconsistent\ndecisions. To address these issues, this paper introduces a novel agentic\nworkflow featured by its abilities to perceive, reflect and plan. Specifically,\nwe find LLaVA-7B can be fine-tuned to perceive the direction and distance of\nlandmarks with sufficient accuracy for city navigation. Moreover, reflection is\nachieved through a memory mechanism, where past experiences are stored and can\nbe retrieved with current perception for effective decision argumentation.\nPlanning uses reflection results to produce long-term plans, which can avoid\nshort-sighted decisions in long-range navigation. We show the designed workflow\nsignificantly improves navigation ability of the LLM agent compared with the\nstate-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Qingbin Zeng"
                    },
                    {
                        "name": "Qinglong Yang"
                    },
                    {
                        "name": "Shunan Dong"
                    },
                    {
                        "name": "Heming Du"
                    },
                    {
                        "name": "Liang Zheng"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "The experiment and dataset are not enough, and we need more\n  experiments to verify our model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03650v1",
                "updated": "2024-09-05T16:08:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    8,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:08:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    8,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches."
                },
                "authors": [
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "name": "Katherine Metcalf"
                    },
                    {
                        "name": "Barry-John Theobald"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "12 pages, 8 tables, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12547v2",
                "updated": "2024-09-05T16:07:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    7,
                    37,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-22T17:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    1,
                    34,
                    3,
                    235,
                    0
                ],
                "title": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine"
                },
                "summary": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins."
                },
                "authors": [
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Hongfei Gu"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15488v2",
                "updated": "2024-09-05T15:50:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    50,
                    44,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-28T02:27:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    27,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services"
                },
                "summary": "Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks."
                },
                "authors": [
                    {
                        "name": "Jialin Wu"
                    },
                    {
                        "name": "Jiangyi Deng"
                    },
                    {
                        "name": "Shengyuan Pang"
                    },
                    {
                        "name": "Yanjiao Chen"
                    },
                    {
                        "name": "Jiayang Xu"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Wenyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyuan Xu"
                },
                "author": "Wenyuan Xu",
                "arxiv_comment": "Accepted by ACM Conference on Computer and Communications Security\n  (CCS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10468v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10468v4",
                "updated": "2024-09-05T15:47:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    47,
                    45,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-20T00:40:49Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    0,
                    40,
                    49,
                    1,
                    233,
                    0
                ],
                "title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions"
                },
                "summary": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths."
                },
                "authors": [
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zao Yang"
                },
                "author": "Zao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10468v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10468v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03621v1",
                "updated": "2024-09-05T15:33:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T15:33:24Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "title": "Attend First, Consolidate Later: On the Importance of Attention in\n  Different LLM Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attend First, Consolidate Later: On the Importance of Attention in\n  Different LLM Layers"
                },
                "summary": "In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally."
                },
                "authors": [
                    {
                        "name": "Amit Ben Artzy"
                    },
                    {
                        "name": "Roy Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Roy Schwartz"
                },
                "author": "Roy Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16185v2",
                "updated": "2024-09-05T15:03:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    3,
                    51,
                    3,
                    249,
                    0
                ],
                "published": "2024-01-29T14:32:27Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    14,
                    32,
                    27,
                    0,
                    29,
                    0
                ],
                "title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including vulnerability detection. However, current efforts in\nthis area are preliminary, lacking clarity on whether LLMs' vulnerability\nreasoning capabilities stem from the models themselves or external aids such as\nknowledge retrieval and tooling support.\n  This paper aims to isolate LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and structured output generation. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conducted controlled experiments with 97 ground-truth vulnerabilities and\n97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312\nscenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findings\nreveal the varying impacts of knowledge enhancement, context supplementation,\nprompt schemes, and models. Additionally, we identified 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in \\$3,576 in\nbounties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including vulnerability detection. However, current efforts in\nthis area are preliminary, lacking clarity on whether LLMs' vulnerability\nreasoning capabilities stem from the models themselves or external aids such as\nknowledge retrieval and tooling support.\n  This paper aims to isolate LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and structured output generation. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conducted controlled experiments with 97 ground-truth vulnerabilities and\n97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312\nscenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findings\nreveal the varying impacts of knowledge enhancement, context supplementation,\nprompt schemes, and models. Additionally, we identified 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in \\$3,576 in\nbounties."
                },
                "authors": [
                    {
                        "name": "Yuqiang Sun"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Yue Xue"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Lyuye Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yingjiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingjiu Li"
                },
                "author": "Yingjiu Li",
                "arxiv_comment": "This is a technical report by Nanyang Technological University.\n  Updated to support both Solidity and Java",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07666v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07666v4",
                "updated": "2024-09-05T14:37:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    37,
                    59,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-14T16:58:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    58,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities"
                },
                "summary": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."
                },
                "authors": [
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07666v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07666v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03577v1",
                "updated": "2024-09-05T14:31:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    31,
                    5,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:31:05Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    31,
                    5,
                    3,
                    249,
                    0
                ],
                "title": "CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement\n  Learning"
                },
                "summary": "Reinforcement learning agents can achieve superhuman performance in static\ntasks but are costly to train and fragile to task changes. This limits their\ndeployment in real-world scenarios where training experience is expensive or\nthe context changes through factors like sensor degradation, environmental\nprocesses or changing mission priorities. Lifelong reinforcement learning aims\nto improve sample efficiency and adaptability by studying how agents perform in\nevolving problems. The difficulty that these changes pose to an agent is rarely\nmeasured directly, however. Agent performances can be compared across a change,\nbut this is often prohibitively expensive. We propose Change-Induced Regret\nProxy (CHIRP) metrics, a class of metrics for approximating a change's\ndifficulty while avoiding the high costs of using trained agents. A\nrelationship between a CHIRP metric and agent performance is identified in two\nenvironments, a simple grid world and MetaWorld's suite of robotic arm tasks.\nWe demonstrate two uses for these metrics: for learning, an agent that clusters\nMDPs based on a CHIRP metric achieves $17\\%$ higher average returns than three\nexisting agents in a sequence of MetaWorld tasks. We also show how a CHIRP can\nbe calibrated to compare the difficulty of changes across distinctly different\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning agents can achieve superhuman performance in static\ntasks but are costly to train and fragile to task changes. This limits their\ndeployment in real-world scenarios where training experience is expensive or\nthe context changes through factors like sensor degradation, environmental\nprocesses or changing mission priorities. Lifelong reinforcement learning aims\nto improve sample efficiency and adaptability by studying how agents perform in\nevolving problems. The difficulty that these changes pose to an agent is rarely\nmeasured directly, however. Agent performances can be compared across a change,\nbut this is often prohibitively expensive. We propose Change-Induced Regret\nProxy (CHIRP) metrics, a class of metrics for approximating a change's\ndifficulty while avoiding the high costs of using trained agents. A\nrelationship between a CHIRP metric and agent performance is identified in two\nenvironments, a simple grid world and MetaWorld's suite of robotic arm tasks.\nWe demonstrate two uses for these metrics: for learning, an agent that clusters\nMDPs based on a CHIRP metric achieves $17\\%$ higher average returns than three\nexisting agents in a sequence of MetaWorld tasks. We also show how a CHIRP can\nbe calibrated to compare the difficulty of changes across distinctly different\nenvironments."
                },
                "authors": [
                    {
                        "name": "John Birkbeck"
                    },
                    {
                        "name": "Adam Sobey"
                    },
                    {
                        "name": "Federico Cerutti"
                    },
                    {
                        "name": "Katherine Heseltine Hurley Flynn"
                    },
                    {
                        "name": "Timothy J. Norman"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Norman"
                },
                "author": "Timothy J. Norman",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03563v1",
                "updated": "2024-09-05T14:19:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    19,
                    45,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:19:45Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    19,
                    45,
                    3,
                    249,
                    0
                ],
                "title": "100 instances is all you need: predicting the success of a new LLM on\n  unseen data by testing on a few instances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "100 instances is all you need: predicting the success of a new LLM on\n  unseen data by testing on a few instances"
                },
                "summary": "Predicting the performance of LLMs on individual task instances is essential\nto ensure their reliability in high-stakes applications. To do so, a\npossibility is to evaluate the considered LLM on a set of task instances and\ntrain an assessor to predict its performance based on features of the\ninstances. However, this approach requires evaluating each new LLM on a\nsufficiently large set of task instances to train an assessor specific to it.\nIn this work, we leverage the evaluation results of previously tested LLMs to\nreduce the number of evaluations required to predict the performance of a new\nLLM. In practice, we propose to test the new LLM on a small set of reference\ninstances and train a generic assessor which predicts the performance of the\nLLM on an instance based on the performance of the former on the reference set\nand features of the instance of interest. We conduct empirical studies on\nHELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets\nthat we introduce, where we evaluate all instruction-fine-tuned OpenAI models\nuntil the January 2024 version of GPT4. When predicting performance on\ninstances with the same distribution as those used to train the generic\nassessor, we find this achieves performance comparable to the LLM-specific\nassessors trained on the full set of instances. Additionally, we find that\nrandomly selecting the reference instances performs as well as some advanced\nselection methods we tested. For out of distribution, however, no clear winner\nemerges and the overall performance is worse, suggesting that the inherent\npredictability of LLMs is low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the performance of LLMs on individual task instances is essential\nto ensure their reliability in high-stakes applications. To do so, a\npossibility is to evaluate the considered LLM on a set of task instances and\ntrain an assessor to predict its performance based on features of the\ninstances. However, this approach requires evaluating each new LLM on a\nsufficiently large set of task instances to train an assessor specific to it.\nIn this work, we leverage the evaluation results of previously tested LLMs to\nreduce the number of evaluations required to predict the performance of a new\nLLM. In practice, we propose to test the new LLM on a small set of reference\ninstances and train a generic assessor which predicts the performance of the\nLLM on an instance based on the performance of the former on the reference set\nand features of the instance of interest. We conduct empirical studies on\nHELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets\nthat we introduce, where we evaluate all instruction-fine-tuned OpenAI models\nuntil the January 2024 version of GPT4. When predicting performance on\ninstances with the same distribution as those used to train the generic\nassessor, we find this achieves performance comparable to the LLM-specific\nassessors trained on the full set of instances. Additionally, we find that\nrandomly selecting the reference instances performs as well as some advanced\nselection methods we tested. For out of distribution, however, no clear winner\nemerges and the overall performance is worse, suggesting that the inherent\npredictability of LLMs is low."
                },
                "authors": [
                    {
                        "name": "Lorenzo Pacchiardi"
                    },
                    {
                        "name": "Lucy G. Cheke"
                    },
                    {
                        "name": "José Hernández-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "José Hernández-Orallo"
                },
                "author": "José Hernández-Orallo",
                "arxiv_comment": "Presented at the 2024 KDD workshop on Evaluation and Trustworthiness\n  of Generative AI Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03555v1",
                "updated": "2024-09-05T14:15:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    15,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:15:54Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    15,
                    54,
                    3,
                    249,
                    0
                ],
                "title": "Unified Framework for Neural Network Compression via Decomposition and\n  Optimal Rank Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Framework for Neural Network Compression via Decomposition and\n  Optimal Rank Selection"
                },
                "summary": "Despite their high accuracy, complex neural networks demand significant\ncomputational resources, posing challenges for deployment on\nresource-constrained devices such as mobile phones and embedded systems.\nCompression algorithms have been developed to address these challenges by\nreducing model size and computational demands while maintaining accuracy. Among\nthese approaches, factorization methods based on tensor decomposition are\ntheoretically sound and effective. However, they face difficulties in selecting\nthe appropriate rank for decomposition. This paper tackles this issue by\npresenting a unified framework that simultaneously applies decomposition and\noptimal rank selection, employing a composite compression loss within defined\nrank constraints. Our approach includes an automatic rank search in a\ncontinuous space, efficiently identifying optimal rank configurations without\nthe use of training data, making it computationally efficient. Combined with a\nsubsequent fine-tuning step, our approach maintains the performance of highly\ncompressed models on par with their original counterparts. Using various\nbenchmark datasets, we demonstrate the efficacy of our method through a\ncomprehensive analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their high accuracy, complex neural networks demand significant\ncomputational resources, posing challenges for deployment on\nresource-constrained devices such as mobile phones and embedded systems.\nCompression algorithms have been developed to address these challenges by\nreducing model size and computational demands while maintaining accuracy. Among\nthese approaches, factorization methods based on tensor decomposition are\ntheoretically sound and effective. However, they face difficulties in selecting\nthe appropriate rank for decomposition. This paper tackles this issue by\npresenting a unified framework that simultaneously applies decomposition and\noptimal rank selection, employing a composite compression loss within defined\nrank constraints. Our approach includes an automatic rank search in a\ncontinuous space, efficiently identifying optimal rank configurations without\nthe use of training data, making it computationally efficient. Combined with a\nsubsequent fine-tuning step, our approach maintains the performance of highly\ncompressed models on par with their original counterparts. Using various\nbenchmark datasets, we demonstrate the efficacy of our method through a\ncomprehensive analysis."
                },
                "authors": [
                    {
                        "name": "Ali Aghababaei-Harandi"
                    },
                    {
                        "name": "Massih-Reza Amini"
                    }
                ],
                "author_detail": {
                    "name": "Massih-Reza Amini"
                },
                "author": "Massih-Reza Amini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03550v1",
                "updated": "2024-09-05T14:12:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    12,
                    22,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:12:22Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    12,
                    22,
                    3,
                    249,
                    0
                ],
                "title": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any\n  Architecture"
                },
                "summary": "Diffusion models (DMs) have demonstrated exceptional generative capabilities\nacross various areas, while they are hindered by slow inference speeds and high\ncomputational demands during deployment. The most common way to accelerate DMs\ninvolves reducing the number of denoising steps during generation, achieved\nthrough faster sampling solvers or knowledge distillation (KD). In contrast to\nprior approaches, we propose a novel method that transfers the capability of\nlarge pretrained DMs to faster architectures. Specifically, we employ KD in a\ndistinct manner to compress DMs by distilling their generative ability into\nmore rapid variants. Furthermore, considering that the source data is either\nunaccessible or too enormous to store for current generative models, we\nintroduce a new paradigm for their distillation without source data, termed\nData-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our\nestablished DKDM framework comprises two main components: 1) a DKDM objective\nthat uses synthetic denoising data produced by pretrained DMs to optimize\nfaster DMs without source data, and 2) a dynamic iterative distillation method\nthat flexibly organizes the synthesis of denoising data, preventing it from\nslowing down the optimization process as the generation is slow. To our\nknowledge, this is the first attempt at using KD to distill DMs into any\narchitecture in a data-free manner. Importantly, our DKDM is orthogonal to most\nexisting acceleration methods, such as denoising step reduction, quantization\nand pruning. Experiments show that our DKDM is capable of deriving 2x faster\nDMs with performance remaining on par with the baseline. Notably, our DKDM\nenables pretrained DMs to function as \"datasets\" for training new DMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have demonstrated exceptional generative capabilities\nacross various areas, while they are hindered by slow inference speeds and high\ncomputational demands during deployment. The most common way to accelerate DMs\ninvolves reducing the number of denoising steps during generation, achieved\nthrough faster sampling solvers or knowledge distillation (KD). In contrast to\nprior approaches, we propose a novel method that transfers the capability of\nlarge pretrained DMs to faster architectures. Specifically, we employ KD in a\ndistinct manner to compress DMs by distilling their generative ability into\nmore rapid variants. Furthermore, considering that the source data is either\nunaccessible or too enormous to store for current generative models, we\nintroduce a new paradigm for their distillation without source data, termed\nData-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our\nestablished DKDM framework comprises two main components: 1) a DKDM objective\nthat uses synthetic denoising data produced by pretrained DMs to optimize\nfaster DMs without source data, and 2) a dynamic iterative distillation method\nthat flexibly organizes the synthesis of denoising data, preventing it from\nslowing down the optimization process as the generation is slow. To our\nknowledge, this is the first attempt at using KD to distill DMs into any\narchitecture in a data-free manner. Importantly, our DKDM is orthogonal to most\nexisting acceleration methods, such as denoising step reduction, quantization\nand pruning. Experiments show that our DKDM is capable of deriving 2x faster\nDMs with performance remaining on par with the baseline. Notably, our DKDM\nenables pretrained DMs to function as \"datasets\" for training new DMs."
                },
                "authors": [
                    {
                        "name": "Qianlong Xiang"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.09043v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.09043v4",
                "updated": "2024-09-05T14:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    0,
                    11,
                    3,
                    249,
                    0
                ],
                "published": "2023-01-22T02:59:59Z",
                "published_parsed": [
                    2023,
                    1,
                    22,
                    2,
                    59,
                    59,
                    6,
                    22,
                    0
                ],
                "title": "CodeScore: Evaluating Code Generation by Learning Code Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeScore: Evaluating Code Generation by Learning Code Execution"
                },
                "summary": "A proper code evaluation metric (CEM) profoundly impacts the evolution of\ncode generation, which is an important research field in NLP and software\nengineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU)\nsuffer from two significant drawbacks. 1. They primarily measure the surface\ndifferences between codes without considering their functional equivalence.\nHowever, functional equivalence is pivotal in evaluating the effectiveness of\ncode generation, as different codes can perform identical operations. 2. They\nare predominantly designed for the Ref-only input format. However, code\nevaluation necessitates versatility in input formats. Aside from Ref-only,\nthere are NL-only and Ref\\&NL formats, which existing match-based CEMs cannot\neffectively accommodate. In this paper, we propose CodeScore, a large language\nmodel (LLM)-based CEM, which estimates the functional correctness of generated\ncode on three input types. To acquire CodeScore, we present UniCE, a unified\ncode generation learning framework, for LLMs to learn code execution (i.e.,\nlearning PassRatio and Executability of generated code) with unified input.\nExtensive experimental results on multiple code evaluation datasets demonstrate\nthat CodeScore absolutely improves up to 58.87% correlation with functional\ncorrectness compared to other CEMs, achieves state-of-the-art performance, and\neffectively handles three input formats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A proper code evaluation metric (CEM) profoundly impacts the evolution of\ncode generation, which is an important research field in NLP and software\nengineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU)\nsuffer from two significant drawbacks. 1. They primarily measure the surface\ndifferences between codes without considering their functional equivalence.\nHowever, functional equivalence is pivotal in evaluating the effectiveness of\ncode generation, as different codes can perform identical operations. 2. They\nare predominantly designed for the Ref-only input format. However, code\nevaluation necessitates versatility in input formats. Aside from Ref-only,\nthere are NL-only and Ref\\&NL formats, which existing match-based CEMs cannot\neffectively accommodate. In this paper, we propose CodeScore, a large language\nmodel (LLM)-based CEM, which estimates the functional correctness of generated\ncode on three input types. To acquire CodeScore, we present UniCE, a unified\ncode generation learning framework, for LLMs to learn code execution (i.e.,\nlearning PassRatio and Executability of generated code) with unified input.\nExtensive experimental results on multiple code evaluation datasets demonstrate\nthat CodeScore absolutely improves up to 58.87% correlation with functional\ncorrectness compared to other CEMs, achieves state-of-the-art performance, and\neffectively handles three input formats."
                },
                "authors": [
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Jiazheng Ding"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Accepted to TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.09043v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.09043v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12334v2",
                "updated": "2024-09-05T13:47:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    47,
                    26,
                    3,
                    249,
                    0
                ],
                "published": "2024-06-18T06:59:24Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    59,
                    24,
                    1,
                    170,
                    0
                ],
                "title": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering"
                },
                "summary": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance."
                },
                "authors": [
                    {
                        "name": "Federico Errica"
                    },
                    {
                        "name": "Giuseppe Siracusano"
                    },
                    {
                        "name": "Davide Sanvito"
                    },
                    {
                        "name": "Roberto Bifulco"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Bifulco"
                },
                "author": "Roberto Bifulco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03512v1",
                "updated": "2024-09-05T13:22:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    22,
                    51,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T13:22:51Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    22,
                    51,
                    3,
                    249,
                    0
                ],
                "title": "From MOOC to MAIC: Reshaping Online Teaching and Learning through\n  LLM-driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From MOOC to MAIC: Reshaping Online Teaching and Learning through\n  LLM-driven Agents"
                },
                "summary": "Since the first instances of online education, where courses were uploaded to\naccessible and shared online platforms, this form of scaling the dissemination\nof human knowledge to reach a broader audience has sparked extensive discussion\nand widespread adoption. Recognizing that personalized learning still holds\nsignificant potential for improvement, new AI technologies have been\ncontinuously integrated into this learning format, resulting in a variety of\neducational AI applications such as educational recommendation and intelligent\ntutoring. The emergence of intelligence in large language models (LLMs) has\nallowed for these educational enhancements to be built upon a unified\nfoundational model, enabling deeper integration. In this context, we propose\nMAIC (Massive AI-empowered Course), a new form of online education that\nleverages LLM-driven multi-agent systems to construct an AI-augmented\nclassroom, balancing scalability with adaptivity. Beyond exploring the\nconceptual framework and technical innovations, we conduct preliminary\nexperiments at Tsinghua University, one of China's leading universities.\nDrawing from over 100,000 learning records of more than 500 students, we obtain\na series of valuable observations and initial analyses. This project will\ncontinue to evolve, ultimately aiming to establish a comprehensive open\nplatform that supports and unifies research, technology, and applications in\nexploring the possibilities of online education in the era of large model AI.\nWe envision this platform as a collaborative hub, bringing together educators,\nresearchers, and innovators to collectively explore the future of AI-driven\nonline education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the first instances of online education, where courses were uploaded to\naccessible and shared online platforms, this form of scaling the dissemination\nof human knowledge to reach a broader audience has sparked extensive discussion\nand widespread adoption. Recognizing that personalized learning still holds\nsignificant potential for improvement, new AI technologies have been\ncontinuously integrated into this learning format, resulting in a variety of\neducational AI applications such as educational recommendation and intelligent\ntutoring. The emergence of intelligence in large language models (LLMs) has\nallowed for these educational enhancements to be built upon a unified\nfoundational model, enabling deeper integration. In this context, we propose\nMAIC (Massive AI-empowered Course), a new form of online education that\nleverages LLM-driven multi-agent systems to construct an AI-augmented\nclassroom, balancing scalability with adaptivity. Beyond exploring the\nconceptual framework and technical innovations, we conduct preliminary\nexperiments at Tsinghua University, one of China's leading universities.\nDrawing from over 100,000 learning records of more than 500 students, we obtain\na series of valuable observations and initial analyses. This project will\ncontinue to evolve, ultimately aiming to establish a comprehensive open\nplatform that supports and unifies research, technology, and applications in\nexploring the possibilities of online education in the era of large model AI.\nWe envision this platform as a collaborative hub, bringing together educators,\nresearchers, and innovators to collectively explore the future of AI-driven\nonline education."
                },
                "authors": [
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Daniel Zhang-li"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Zhanxin Hao"
                    },
                    {
                        "name": "Rui Miao Li"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Yuanchun Wang"
                    },
                    {
                        "name": "Hanming Li"
                    },
                    {
                        "name": "Linlu Gong"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Jiayin Lin"
                    },
                    {
                        "name": "Jinchang Zhou"
                    },
                    {
                        "name": "Fei Qin"
                    },
                    {
                        "name": "Haohua Wang"
                    },
                    {
                        "name": "Jianxiao Jiang"
                    },
                    {
                        "name": "Lijun Deng"
                    },
                    {
                        "name": "Yisi Zhan"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xusheng Dai"
                    },
                    {
                        "name": "Xuan Yan"
                    },
                    {
                        "name": "Nianyi Lin"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Ruixin Ni"
                    },
                    {
                        "name": "Yang Dang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Manli Li"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Huiqin Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03478v1",
                "updated": "2024-09-05T12:38:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    38,
                    13,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T12:38:13Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    38,
                    13,
                    3,
                    249,
                    0
                ],
                "title": "LLM-based event abstraction and integration for IoT-sourced logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based event abstraction and integration for IoT-sourced logs"
                },
                "summary": "The continuous flow of data collected by Internet of Things (IoT) devices,\nhas revolutionised our ability to understand and interact with the world across\nvarious applications. However, this data must be prepared and transformed into\nevent data before analysis can begin. In this paper, we shed light on the\npotential of leveraging Large Language Models (LLMs) in event abstraction and\nintegration. Our approach aims to create event records from raw sensor readings\nand merge the logs from multiple IoT sources into a single event log suitable\nfor further Process Mining applications. We demonstrate the capabilities of\nLLMs in event abstraction considering a case study for IoT application in\nelderly care and longitudinal health monitoring. The results, showing on\naverage an accuracy of 90% in detecting high-level activities. These results\nhighlight LLMs' promising potential in addressing event abstraction and\nintegration challenges, effectively bridging the existing gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continuous flow of data collected by Internet of Things (IoT) devices,\nhas revolutionised our ability to understand and interact with the world across\nvarious applications. However, this data must be prepared and transformed into\nevent data before analysis can begin. In this paper, we shed light on the\npotential of leveraging Large Language Models (LLMs) in event abstraction and\nintegration. Our approach aims to create event records from raw sensor readings\nand merge the logs from multiple IoT sources into a single event log suitable\nfor further Process Mining applications. We demonstrate the capabilities of\nLLMs in event abstraction considering a case study for IoT application in\nelderly care and longitudinal health monitoring. The results, showing on\naverage an accuracy of 90% in detecting high-level activities. These results\nhighlight LLMs' promising potential in addressing event abstraction and\nintegration challenges, effectively bridging the existing gap."
                },
                "authors": [
                    {
                        "name": "Mohsen Shirali"
                    },
                    {
                        "name": "Mohammadreza Fani Sani"
                    },
                    {
                        "name": "Zahra Ahmadi"
                    },
                    {
                        "name": "Estefania Serral"
                    }
                ],
                "author_detail": {
                    "name": "Estefania Serral"
                },
                "author": "Estefania Serral",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M14",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03454v1",
                "updated": "2024-09-05T12:06:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    6,
                    38,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T12:06:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    6,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes"
                },
                "summary": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains."
                },
                "authors": [
                    {
                        "name": "Inacio Vieira"
                    },
                    {
                        "name": "Will Allred"
                    },
                    {
                        "name": "Seamus Lankford"
                    },
                    {
                        "name": "Sheila Castilho Monteiro De Sousa"
                    },
                    {
                        "name": "Andy Way"
                    }
                ],
                "author_detail": {
                    "name": "Andy Way"
                },
                "author": "Andy Way",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.14735v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.14735v5",
                "updated": "2024-09-05T12:00:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    0,
                    55,
                    3,
                    249,
                    0
                ],
                "published": "2023-10-23T09:15:18Z",
                "published_parsed": [
                    2023,
                    10,
                    23,
                    9,
                    15,
                    18,
                    0,
                    296,
                    0
                ],
                "title": "Unleashing the potential of prompt engineering in Large Language Models:\n  a comprehensive review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the potential of prompt engineering in Large Language Models:\n  a comprehensive review"
                },
                "summary": "This comprehensive review delves into the pivotal role of prompt engineering\nin unleashing the capabilities of Large Language Models (LLMs). The development\nof Artificial Intelligence (AI), from its inception in the 1950s to the\nemergence of advanced neural networks and deep learning architectures, has made\na breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in\nVision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt\nengineering is the process of structuring inputs, which has emerged as a\ncrucial technique to maximize the utility and accuracy of these models. This\npaper explores both foundational and advanced methodologies of prompt\nengineering, including techniques such as self-consistency, chain-of-thought,\nand generated knowledge, which significantly enhance model performance.\nAdditionally, it examines the prompt method of VLMs through innovative\napproaches such as Context Optimization (CoOp), Conditional Context\nOptimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this\ndiscussion is the aspect of AI security, particularly adversarial attacks that\nexploit vulnerabilities in prompt engineering. Strategies to mitigate these\nrisks and enhance model robustness are thoroughly reviewed. The evaluation of\nprompt methods is also addressed, through both subjective and objective\nmetrics, ensuring a robust analysis of their efficacy. This review also\nreflects the essential role of prompt engineering in advancing AI capabilities,\nproviding a structured framework for future research and application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review delves into the pivotal role of prompt engineering\nin unleashing the capabilities of Large Language Models (LLMs). The development\nof Artificial Intelligence (AI), from its inception in the 1950s to the\nemergence of advanced neural networks and deep learning architectures, has made\na breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in\nVision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt\nengineering is the process of structuring inputs, which has emerged as a\ncrucial technique to maximize the utility and accuracy of these models. This\npaper explores both foundational and advanced methodologies of prompt\nengineering, including techniques such as self-consistency, chain-of-thought,\nand generated knowledge, which significantly enhance model performance.\nAdditionally, it examines the prompt method of VLMs through innovative\napproaches such as Context Optimization (CoOp), Conditional Context\nOptimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this\ndiscussion is the aspect of AI security, particularly adversarial attacks that\nexploit vulnerabilities in prompt engineering. Strategies to mitigate these\nrisks and enhance model robustness are thoroughly reviewed. The evaluation of\nprompt methods is also addressed, through both subjective and objective\nmetrics, ensuring a robust analysis of their efficacy. This review also\nreflects the essential role of prompt engineering in advancing AI capabilities,\nproviding a structured framework for future research and application."
                },
                "authors": [
                    {
                        "name": "Banghao Chen"
                    },
                    {
                        "name": "Zhaofeng Zhang"
                    },
                    {
                        "name": "Nicolas Langrené"
                    },
                    {
                        "name": "Shengxin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Shengxin Zhu"
                },
                "author": "Shengxin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.14735v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.14735v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03444v1",
                "updated": "2024-09-05T11:49:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    49,
                    53,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:49:53Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    49,
                    53,
                    3,
                    249,
                    0
                ],
                "title": "Fine-tuning large language models for domain adaptation: Exploration of\n  training strategies, scaling, model merging and synergistic capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models for domain adaptation: Exploration of\n  training strategies, scaling, model merging and synergistic capabilities"
                },
                "summary": "The advancement of Large Language Models (LLMs) for domain applications in\nfields such as materials science and engineering depends on the development of\nfine-tuning strategies that adapt models for specialized, technical\ncapabilities. In this work, we explore the effects of Continued Pretraining\n(CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization\napproaches, including Direct Preference Optimization (DPO) and Odds Ratio\nPreference Optimization (ORPO), on fine-tuned LLM performance. Our analysis\nshows how these strategies influence model outcomes and reveals that the\nmerging of multiple fine-tuned models can lead to the emergence of capabilities\nthat surpass the individual contributions of the parent models. We find that\nmodel merging leads to new functionalities that neither parent model could\nachieve alone, leading to improved performance in domain-specific assessments.\nExperiments with different model architectures are presented, including Llama\n3.1 8B and Mistral 7B models, where similar behaviors are observed. Exploring\nwhether the results hold also for much smaller models, we use a tiny LLM with\n1.7 billion parameters and show that very small LLMs do not necessarily feature\nemergent capabilities under model merging, suggesting that model scaling may be\na key component. In open-ended yet consistent chat conversations between a\nhuman and AI models, our assessment reveals detailed insights into how\ndifferent model variants perform and show that the smallest model achieves a\nhigh intelligence score across key criteria including reasoning depth,\ncreativity, clarity, and quantitative precision. Other experiments include the\ndevelopment of image generation prompts based on disparate biological material\ndesign concepts, to create new microstructures, architectural concepts, and\nurban design based on biological materials-inspired construction principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of Large Language Models (LLMs) for domain applications in\nfields such as materials science and engineering depends on the development of\nfine-tuning strategies that adapt models for specialized, technical\ncapabilities. In this work, we explore the effects of Continued Pretraining\n(CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization\napproaches, including Direct Preference Optimization (DPO) and Odds Ratio\nPreference Optimization (ORPO), on fine-tuned LLM performance. Our analysis\nshows how these strategies influence model outcomes and reveals that the\nmerging of multiple fine-tuned models can lead to the emergence of capabilities\nthat surpass the individual contributions of the parent models. We find that\nmodel merging leads to new functionalities that neither parent model could\nachieve alone, leading to improved performance in domain-specific assessments.\nExperiments with different model architectures are presented, including Llama\n3.1 8B and Mistral 7B models, where similar behaviors are observed. Exploring\nwhether the results hold also for much smaller models, we use a tiny LLM with\n1.7 billion parameters and show that very small LLMs do not necessarily feature\nemergent capabilities under model merging, suggesting that model scaling may be\na key component. In open-ended yet consistent chat conversations between a\nhuman and AI models, our assessment reveals detailed insights into how\ndifferent model variants perform and show that the smallest model achieves a\nhigh intelligence score across key criteria including reasoning depth,\ncreativity, clarity, and quantitative precision. Other experiments include the\ndevelopment of image generation prompts based on disparate biological material\ndesign concepts, to create new microstructures, architectural concepts, and\nurban design based on biological materials-inspired construction principles."
                },
                "authors": [
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Rachel K. Luu"
                    },
                    {
                        "name": "Markus J. Buehler"
                    }
                ],
                "author_detail": {
                    "name": "Markus J. Buehler"
                },
                "author": "Markus J. Buehler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03440v1",
                "updated": "2024-09-05T11:42:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    26,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:42:26Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    26,
                    3,
                    249,
                    0
                ],
                "title": "Rx Strategist: Prescription Verification using LLM Agents System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rx Strategist: Prescription Verification using LLM Agents System"
                },
                "summary": "To protect patient safety, modern pharmaceutical complexity demands strict\nprescription verification. We offer a new approach - Rx Strategist - that makes\nuse of knowledge graphs and different search strategies to enhance the power of\nLarge Language Models (LLMs) inside an agentic framework. This multifaceted\ntechnique allows for a multi-stage LLM pipeline and reliable information\nretrieval from a custom-built active ingredient database. Different facets of\nprescription verification, such as indication, dose, and possible drug\ninteractions, are covered in each stage of the pipeline. We alleviate the\ndrawbacks of monolithic LLM techniques by spreading reasoning over these\nstages, improving correctness and reliability while reducing memory demands.\nOur findings demonstrate that Rx Strategist surpasses many current LLMs,\nachieving performance comparable to that of a highly experienced clinical\npharmacist. In the complicated world of modern medications, this combination of\nLLMs with organized knowledge and sophisticated search methods presents a\nviable avenue for reducing prescription errors and enhancing patient outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect patient safety, modern pharmaceutical complexity demands strict\nprescription verification. We offer a new approach - Rx Strategist - that makes\nuse of knowledge graphs and different search strategies to enhance the power of\nLarge Language Models (LLMs) inside an agentic framework. This multifaceted\ntechnique allows for a multi-stage LLM pipeline and reliable information\nretrieval from a custom-built active ingredient database. Different facets of\nprescription verification, such as indication, dose, and possible drug\ninteractions, are covered in each stage of the pipeline. We alleviate the\ndrawbacks of monolithic LLM techniques by spreading reasoning over these\nstages, improving correctness and reliability while reducing memory demands.\nOur findings demonstrate that Rx Strategist surpasses many current LLMs,\nachieving performance comparable to that of a highly experienced clinical\npharmacist. In the complicated world of modern medications, this combination of\nLLMs with organized knowledge and sophisticated search methods presents a\nviable avenue for reducing prescription errors and enhancing patient outcomes."
                },
                "authors": [
                    {
                        "name": "Phuc Phan Van"
                    },
                    {
                        "name": "Dat Nguyen Minh"
                    },
                    {
                        "name": "An Dinh Ngoc"
                    },
                    {
                        "name": "Huy Phan Thanh"
                    }
                ],
                "author_detail": {
                    "name": "Huy Phan Thanh"
                },
                "author": "Huy Phan Thanh",
                "arxiv_comment": "17 Pages, 6 Figures, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03439v1",
                "updated": "2024-09-05T11:42:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    8,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:42:08Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    8,
                    3,
                    249,
                    0
                ],
                "title": "KiloBot: A Programming Language for Deploying Perception-Guided\n  Industrial Manipulators at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KiloBot: A Programming Language for Deploying Perception-Guided\n  Industrial Manipulators at Scale"
                },
                "summary": "We would like industrial robots to handle unstructured environments with\ncameras and perception pipelines. In contrast to traditional industrial robots\nthat replay offline-crafted trajectories, online behavior planning is required\nfor these perception-guided industrial applications. Aside from perception and\nplanning algorithms, deploying perception-guided manipulators also requires\nsubstantial effort in integration. One approach is writing scripts in a\ntraditional language (such as Python) to construct the planning problem and\nperform integration with other algorithmic modules & external devices. While\nscripting in Python is feasible for a handful of robots and applications,\ndeploying perception-guided manipulation at scale (e.g., more than 10000 robot\nworkstations in over 2000 customer sites) becomes intractable. To resolve this\nchallenge, we propose a Domain-Specific Language (DSL) for perception-guided\nmanipulation applications. To scale up the deployment,our DSL provides: 1) an\neasily accessible interface to construct & solve a sub-class of Task and Motion\nPlanning (TAMP) problems that are important in practical applications; and 2) a\nmechanism to implement flexible control flow to perform integration and address\ncustomized requirements of distinct industrial application. Combined with an\nintuitive graphical programming frontend, our DSL is mainly used by machine\noperators without coding experience in traditional programming languages.\nWithin hours of training, operators are capable of orchestrating interesting\nsophisticated manipulation behaviors with our DSL. Extensive practical\ndeployments demonstrate the efficacy of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We would like industrial robots to handle unstructured environments with\ncameras and perception pipelines. In contrast to traditional industrial robots\nthat replay offline-crafted trajectories, online behavior planning is required\nfor these perception-guided industrial applications. Aside from perception and\nplanning algorithms, deploying perception-guided manipulators also requires\nsubstantial effort in integration. One approach is writing scripts in a\ntraditional language (such as Python) to construct the planning problem and\nperform integration with other algorithmic modules & external devices. While\nscripting in Python is feasible for a handful of robots and applications,\ndeploying perception-guided manipulation at scale (e.g., more than 10000 robot\nworkstations in over 2000 customer sites) becomes intractable. To resolve this\nchallenge, we propose a Domain-Specific Language (DSL) for perception-guided\nmanipulation applications. To scale up the deployment,our DSL provides: 1) an\neasily accessible interface to construct & solve a sub-class of Task and Motion\nPlanning (TAMP) problems that are important in practical applications; and 2) a\nmechanism to implement flexible control flow to perform integration and address\ncustomized requirements of distinct industrial application. Combined with an\nintuitive graphical programming frontend, our DSL is mainly used by machine\noperators without coding experience in traditional programming languages.\nWithin hours of training, operators are capable of orchestrating interesting\nsophisticated manipulation behaviors with our DSL. Extensive practical\ndeployments demonstrate the efficacy of our method."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Jingqiang Wang"
                    },
                    {
                        "name": "Xinv Zhu"
                    },
                    {
                        "name": "Jun Zhong"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Youshuang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Youshuang Ding"
                },
                "author": "Youshuang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]