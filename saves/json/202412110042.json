[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v1",
                "updated": "2024-12-02T11:57:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Kpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v1",
                "updated": "2024-11-29T05:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "Sbastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.06786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06786v1",
                "updated": "2024-12-09T18:59:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    59,
                    46,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:59:46Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    59,
                    46,
                    0,
                    344,
                    0
                ],
                "title": "Retrieving Semantics from the Deep: an RAG Solution for Gesture\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Semantics from the Deep: an RAG Solution for Gesture\n  Synthesis"
                },
                "summary": "Non-verbal communication often comprises of semantically rich gestures that\nhelp convey the meaning of an utterance. Producing such semantic co-speech\ngestures has been a major challenge for the existing neural systems that can\ngenerate rhythmic beat gestures, but struggle to produce semantically\nmeaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based\ngesture generation approach that leverages Retrieval Augmented Generation (RAG)\nto produce natural-looking and semantically rich gestures. Our neuro-explicit\ngesture generation approach is designed to produce semantic gestures grounded\nin interpretable linguistic knowledge. We achieve this by using explicit domain\nknowledge to retrieve exemplar motions from a database of co-speech gestures.\nOnce retrieved, we then inject these semantic exemplar gestures into our\ndiffusion-based gesture generation pipeline using DDIM inversion and retrieval\nguidance at the inference time without any need of training. Further, we\npropose a control paradigm for guidance, that allows the users to modulate the\namount of influence each retrieval insertion has over the generated sequence.\nOur comparative evaluations demonstrate the validity of our approach against\nrecent gesture generation approaches. The reader is urged to explore the\nresults on our project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-verbal communication often comprises of semantically rich gestures that\nhelp convey the meaning of an utterance. Producing such semantic co-speech\ngestures has been a major challenge for the existing neural systems that can\ngenerate rhythmic beat gestures, but struggle to produce semantically\nmeaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based\ngesture generation approach that leverages Retrieval Augmented Generation (RAG)\nto produce natural-looking and semantically rich gestures. Our neuro-explicit\ngesture generation approach is designed to produce semantic gestures grounded\nin interpretable linguistic knowledge. We achieve this by using explicit domain\nknowledge to retrieve exemplar motions from a database of co-speech gestures.\nOnce retrieved, we then inject these semantic exemplar gestures into our\ndiffusion-based gesture generation pipeline using DDIM inversion and retrieval\nguidance at the inference time without any need of training. Further, we\npropose a control paradigm for guidance, that allows the users to modulate the\namount of influence each retrieval insertion has over the generated sequence.\nOur comparative evaluations demonstrate the validity of our approach against\nrecent gesture generation approaches. The reader is urged to explore the\nresults on our project page."
                },
                "authors": [
                    {
                        "name": "M. Hamza Mughal"
                    },
                    {
                        "name": "Rishabh Dabral"
                    },
                    {
                        "name": "Merel C. J. Scholman"
                    },
                    {
                        "name": "Vera Demberg"
                    },
                    {
                        "name": "Christian Theobalt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Theobalt"
                },
                "author": "Christian Theobalt",
                "arxiv_comment": "Preprint. Project page:\n  https://vcai.mpi-inf.mpg.de/projects/RAG-Gesture/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06782v1",
                "updated": "2024-12-09T18:59:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    59,
                    18,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:59:18Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    59,
                    18,
                    0,
                    344,
                    0
                ],
                "title": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive\n  Prediction"
                },
                "summary": "In robotic visuomotor policy learning, diffusion-based models have achieved\nsignificant success in improving the accuracy of action trajectory generation\ncompared to traditional autoregressive models. However, they suffer from\ninefficiency due to multiple denoising steps and limited flexibility from\ncomplex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive\nPolicy (CARP), a novel paradigm for visuomotor policy learning that redefines\nthe autoregressive action generation process as a coarse-to-fine, next-scale\napproach. CARP decouples action generation into two stages: first, an action\nautoencoder learns multi-scale representations of the entire action sequence;\nthen, a GPT-style transformer refines the sequence prediction through a\ncoarse-to-fine autoregressive process. This straightforward and intuitive\napproach produces highly accurate and smooth actions, matching or even\nsurpassing the performance of diffusion-based policies while maintaining\nefficiency on par with autoregressive policies. We conduct extensive\nevaluations across diverse settings, including single-task and multi-task\nscenarios on state-based and image-based simulation benchmarks, as well as\nreal-world tasks. CARP achieves competitive success rates, with up to a 10%\nimprovement, and delivers 10x faster inference compared to state-of-the-art\npolicies, establishing a high-performance, efficient, and flexible paradigm for\naction generation in robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robotic visuomotor policy learning, diffusion-based models have achieved\nsignificant success in improving the accuracy of action trajectory generation\ncompared to traditional autoregressive models. However, they suffer from\ninefficiency due to multiple denoising steps and limited flexibility from\ncomplex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive\nPolicy (CARP), a novel paradigm for visuomotor policy learning that redefines\nthe autoregressive action generation process as a coarse-to-fine, next-scale\napproach. CARP decouples action generation into two stages: first, an action\nautoencoder learns multi-scale representations of the entire action sequence;\nthen, a GPT-style transformer refines the sequence prediction through a\ncoarse-to-fine autoregressive process. This straightforward and intuitive\napproach produces highly accurate and smooth actions, matching or even\nsurpassing the performance of diffusion-based policies while maintaining\nefficiency on par with autoregressive policies. We conduct extensive\nevaluations across diverse settings, including single-task and multi-task\nscenarios on state-based and image-based simulation benchmarks, as well as\nreal-world tasks. CARP achieves competitive success rates, with up to a 10%\nimprovement, and delivers 10x faster inference compared to state-of-the-art\npolicies, establishing a high-performance, efficient, and flexible paradigm for\naction generation in robotic tasks."
                },
                "authors": [
                    {
                        "name": "Zhefei Gong"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Shangke Lyu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Mingyang Sun"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06780v1",
                "updated": "2024-12-09T18:59:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    59,
                    2,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:59:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    59,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "Diverse Score Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Score Distillation"
                },
                "summary": "Score distillation of 2D diffusion models has proven to be a powerful\nmechanism to guide 3D optimization, for example enabling text-based 3D\ngeneration or single-view reconstruction. A common limitation of existing score\ndistillation formulations, however, is that the outputs of the (mode-seeking)\noptimization are limited in diversity despite the underlying diffusion model\nbeing capable of generating diverse samples. In this work, inspired by the\nsampling process in denoising diffusion, we propose a score formulation that\nguides the optimization to follow generation paths defined by random initial\nseeds, thus ensuring diversity. We then present an approximation to adopt this\nformulation for scenarios where the optimization may not precisely follow the\ngeneration paths (e.g. a 3D representation whose renderings evolve in a\nco-dependent manner). We showcase the applications of our `Diverse Score\nDistillation' (DSD) formulation across tasks such as 2D optimization,\ntext-based 3D inference, and single-view reconstruction. We also empirically\nvalidate DSD against prior score distillation formulations and show that it\nsignificantly improves sample diversity while preserving fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score distillation of 2D diffusion models has proven to be a powerful\nmechanism to guide 3D optimization, for example enabling text-based 3D\ngeneration or single-view reconstruction. A common limitation of existing score\ndistillation formulations, however, is that the outputs of the (mode-seeking)\noptimization are limited in diversity despite the underlying diffusion model\nbeing capable of generating diverse samples. In this work, inspired by the\nsampling process in denoising diffusion, we propose a score formulation that\nguides the optimization to follow generation paths defined by random initial\nseeds, thus ensuring diversity. We then present an approximation to adopt this\nformulation for scenarios where the optimization may not precisely follow the\ngeneration paths (e.g. a 3D representation whose renderings evolve in a\nco-dependent manner). We showcase the applications of our `Diverse Score\nDistillation' (DSD) formulation across tasks such as 2D optimization,\ntext-based 3D inference, and single-view reconstruction. We also empirically\nvalidate DSD against prior score distillation formulations and show that it\nsignificantly improves sample diversity while preserving fidelity."
                },
                "authors": [
                    {
                        "name": "Yanbo Xu"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Shubham Tulsiani"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Tulsiani"
                },
                "author": "Shubham Tulsiani",
                "arxiv_comment": "Project Page: https://billyxyb.github.io/Diverse-Score-Distillation/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06777v1",
                "updated": "2024-12-09T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    58,
                    3,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    58,
                    3,
                    0,
                    344,
                    0
                ],
                "title": "Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving"
                },
                "summary": "Realtime 4D reconstruction for dynamic scenes remains a crucial challenge for\nautonomous driving perception. Most existing methods rely on depth estimation\nthrough self-supervision or multi-modality sensor fusion. In this paper, we\npropose Driv3R, a DUSt3R-based framework that directly regresses per-frame\npoint maps from multi-view image sequences. To achieve streaming dense\nreconstruction, we maintain a memory pool to reason both spatial relationships\nacross sensors and dynamic temporal contexts to enhance multi-view 3D\nconsistency and temporal integration. Furthermore, we employ a 4D flow\npredictor to identify moving objects within the scene to direct our network\nfocus more on reconstructing these dynamic regions. Finally, we align all\nper-frame pointmaps consistently to the world coordinate system in an\noptimization-free manner. We conduct extensive experiments on the large-scale\nnuScenes dataset to evaluate the effectiveness of our method. Driv3R\noutperforms previous frameworks in 4D dynamic scene reconstruction, achieving\n15x faster inference speed compared to methods requiring global alignment.\nCode: https://github.com/Barrybarry-Smith/Driv3R.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realtime 4D reconstruction for dynamic scenes remains a crucial challenge for\nautonomous driving perception. Most existing methods rely on depth estimation\nthrough self-supervision or multi-modality sensor fusion. In this paper, we\npropose Driv3R, a DUSt3R-based framework that directly regresses per-frame\npoint maps from multi-view image sequences. To achieve streaming dense\nreconstruction, we maintain a memory pool to reason both spatial relationships\nacross sensors and dynamic temporal contexts to enhance multi-view 3D\nconsistency and temporal integration. Furthermore, we employ a 4D flow\npredictor to identify moving objects within the scene to direct our network\nfocus more on reconstructing these dynamic regions. Finally, we align all\nper-frame pointmaps consistently to the world coordinate system in an\noptimization-free manner. We conduct extensive experiments on the large-scale\nnuScenes dataset to evaluate the effectiveness of our method. Driv3R\noutperforms previous frameworks in 4D dynamic scene reconstruction, achieving\n15x faster inference speed compared to methods requiring global alignment.\nCode: https://github.com/Barrybarry-Smith/Driv3R."
                },
                "authors": [
                    {
                        "name": "Xin Fei"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Yueqi Duan"
                    },
                    {
                        "name": "Wei Zhan"
                    },
                    {
                        "name": "Masayoshi Tomizuka"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/Barrybarry-Smith/Driv3R",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06769v1",
                "updated": "2024-12-09T18:55:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    55,
                    56,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:55:56Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    55,
                    56,
                    0,
                    344,
                    0
                ],
                "title": "Training Large Language Models to Reason in a Continuous Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models to Reason in a Continuous Latent Space"
                },
                "summary": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research."
                },
                "authors": [
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Yuandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuandong Tian"
                },
                "author": "Yuandong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06763v1",
                "updated": "2024-12-09T18:52:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    52,
                    54,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:52:54Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    52,
                    54,
                    0,
                    344,
                    0
                ],
                "title": "New Ionization Models and the Shocking Nitrogen Excess at z > 5",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Ionization Models and the Shocking Nitrogen Excess at z > 5"
                },
                "summary": "The new era of galaxy evolution studies hearkened in by JWST has led to the\ndiscovery of z > 5 galaxies exhibiting excess nitrogen with log(N/O)~1 dex or\nmore than expected from log(N/O) vs 12+log(O/H) trends in the local Universe. A\nvariety of novel enrichment pathways have been presented to explain the\napparent nitrogen excess, invoking a wide range of processes from very massive\nstars to stripped binaries to fine-tuned star-formation histories. However,\nunderstanding the excitation mechanism responsible for the observed nebular\nemission is necessary to accurately infer chemical abundances. As of yet, the\nionization sources of these galaxies have not been thoroughly explored, with\nradiative shocks left out of the picture. We present a suite of homogeneous\nexcitation models for star-forming galaxies, active galactic nuclei, and\nradiative shocks, with which we explore possible explanations for the apparent\nnitrogen excess. We propose new BPT-style diagnostics to classify galaxies at z\n> 5, finding that, when combined with O iii] 1660,66 and He ii 1640, N iii]\n1747-54 / C iii] 1907,09 best selects shock-dominated galaxies while N iv]\n1483,86 / C iii] 1907,09 best distinguishes between active black holes and star\nforming galaxies. From our diagnostics, we find that slow/intermediate\nradiative shocks (v = 75-150 km/s) are most consistent with observed UV\nemission line flux ratios in nitrogen-bright galaxies. Accounting for the\neffects of shocks can bring nitrogen estimates into better agreement with\nabundance patterns observed in the local Universe and may be attributable to\nWolf Rayet populations actively enriching these galaxies with nitrogen and\npossibly driving winds responsible for these shocks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new era of galaxy evolution studies hearkened in by JWST has led to the\ndiscovery of z > 5 galaxies exhibiting excess nitrogen with log(N/O)~1 dex or\nmore than expected from log(N/O) vs 12+log(O/H) trends in the local Universe. A\nvariety of novel enrichment pathways have been presented to explain the\napparent nitrogen excess, invoking a wide range of processes from very massive\nstars to stripped binaries to fine-tuned star-formation histories. However,\nunderstanding the excitation mechanism responsible for the observed nebular\nemission is necessary to accurately infer chemical abundances. As of yet, the\nionization sources of these galaxies have not been thoroughly explored, with\nradiative shocks left out of the picture. We present a suite of homogeneous\nexcitation models for star-forming galaxies, active galactic nuclei, and\nradiative shocks, with which we explore possible explanations for the apparent\nnitrogen excess. We propose new BPT-style diagnostics to classify galaxies at z\n> 5, finding that, when combined with O iii] 1660,66 and He ii 1640, N iii]\n1747-54 / C iii] 1907,09 best selects shock-dominated galaxies while N iv]\n1483,86 / C iii] 1907,09 best distinguishes between active black holes and star\nforming galaxies. From our diagnostics, we find that slow/intermediate\nradiative shocks (v = 75-150 km/s) are most consistent with observed UV\nemission line flux ratios in nitrogen-bright galaxies. Accounting for the\neffects of shocks can bring nitrogen estimates into better agreement with\nabundance patterns observed in the local Universe and may be attributable to\nWolf Rayet populations actively enriching these galaxies with nitrogen and\npossibly driving winds responsible for these shocks."
                },
                "authors": [
                    {
                        "name": "Sophia R. Flury"
                    },
                    {
                        "name": "Karla Z. Arellano-Crdova"
                    },
                    {
                        "name": "Edward C. Moran"
                    },
                    {
                        "name": "Alaina Einsig"
                    }
                ],
                "author_detail": {
                    "name": "Alaina Einsig"
                },
                "author": "Alaina Einsig",
                "arxiv_comment": "submitted to MNRAS, 13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06761v1",
                "updated": "2024-12-09T18:51:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    51,
                    12,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:51:12Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    51,
                    12,
                    0,
                    344,
                    0
                ],
                "title": "Hallmarks of Deception in Asset-Exchange Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallmarks of Deception in Asset-Exchange Models"
                },
                "summary": "We investigate the transient and steady-state dynamics of the\nBennati-Dregulescu-Yakovenko money game in the presence of probabilistic\ncheaters, who can misrepresent their financial status by claiming to have no\nmoney. We derive the steady-state wealth distribution per player analytically\nand show how the presence of hidden cheaters can be inferred from the relative\nvariance of wealth per player. In scenarios with a finite number of cheaters\namidst an infinite pool of honest players, we identify a critical probability\nof cheating at which the total wealth owned by the cheaters experiences a\nsecond-order discontinuity. Below this point, the probability to lose money is\nlarger than the probability to gain; conversely, above this point, the\ndirection is reversed. Additionally, we establish a threshold probability at\nwhich cheaters collectively possess more than half of the total wealth in the\ngame. Lastly, we provide bounds on the rate at which both cheaters and honest\nplayers can gain or lose wealth, contributing to a deeper understanding of\ndeception in asset exchange models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the transient and steady-state dynamics of the\nBennati-Dregulescu-Yakovenko money game in the presence of probabilistic\ncheaters, who can misrepresent their financial status by claiming to have no\nmoney. We derive the steady-state wealth distribution per player analytically\nand show how the presence of hidden cheaters can be inferred from the relative\nvariance of wealth per player. In scenarios with a finite number of cheaters\namidst an infinite pool of honest players, we identify a critical probability\nof cheating at which the total wealth owned by the cheaters experiences a\nsecond-order discontinuity. Below this point, the probability to lose money is\nlarger than the probability to gain; conversely, above this point, the\ndirection is reversed. Additionally, we establish a threshold probability at\nwhich cheaters collectively possess more than half of the total wealth in the\ngame. Lastly, we provide bounds on the rate at which both cheaters and honest\nplayers can gain or lose wealth, contributing to a deeper understanding of\ndeception in asset exchange models."
                },
                "authors": [
                    {
                        "name": "Kristian Blom"
                    },
                    {
                        "name": "Dmitrii E. Makarov"
                    },
                    {
                        "name": "Alja Godec"
                    }
                ],
                "author_detail": {
                    "name": "Alja Godec"
                },
                "author": "Alja Godec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.pop-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06757v1",
                "updated": "2024-12-09T18:47:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    47,
                    31,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:47:31Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    47,
                    31,
                    0,
                    344,
                    0
                ],
                "title": "Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating\n  Usage and Reliance on ChatGPT-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating\n  Usage and Reliance on ChatGPT-Generated Code"
                },
                "summary": "Large language models (LLMs) like ChatGPT have shown the potential to assist\ndevelopers with coding and debugging tasks. However, their role in\ncollaborative issue resolution is underexplored. In this study, we analyzed\n1,152 Developer-ChatGPT conversations across 1,012 issues in GitHub to examine\nthe diverse usage of ChatGPT and reliance on its generated code. Our\ncontributions are fourfold. First, we manually analyzed 289 conversations to\nunderstand ChatGPT's usage in the GitHub Issues. Our analysis revealed that\nChatGPT is primarily utilized for ideation, whereas its usage for validation\n(e.g., code documentation accuracy) is minimal. Second, we applied BERTopic\nmodeling to identify key areas of engagement on the entire dataset. We found\nthat backend issues (e.g., API management) dominate conversations, while\ntesting is surprisingly less covered. Third, we utilized the CPD clone\ndetection tool to check if the code generated by ChatGPT was used to address\nissues. Our findings revealed that ChatGPT-generated code was used as-is to\nresolve only 5.83\\% of the issues. Fourth, we estimated sentiment using a\nRoBERTa-based sentiment analysis model to determine developers' satisfaction\nwith different usages and engagement areas. We found positive sentiment (i.e.,\nhigh satisfaction) about using ChatGPT for refactoring and addressing data\nanalytics (e.g., categorizing table data) issues. On the contrary, we observed\nnegative sentiment when using ChatGPT to debug issues and address automation\ntasks (e.g., GUI interactions). Our findings show the unmet needs and growing\ndissatisfaction among developers. Researchers and ChatGPT developers should\nfocus on developing task-specific solutions that help resolve diverse issues,\nimproving user satisfaction and problem-solving efficiency in software\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like ChatGPT have shown the potential to assist\ndevelopers with coding and debugging tasks. However, their role in\ncollaborative issue resolution is underexplored. In this study, we analyzed\n1,152 Developer-ChatGPT conversations across 1,012 issues in GitHub to examine\nthe diverse usage of ChatGPT and reliance on its generated code. Our\ncontributions are fourfold. First, we manually analyzed 289 conversations to\nunderstand ChatGPT's usage in the GitHub Issues. Our analysis revealed that\nChatGPT is primarily utilized for ideation, whereas its usage for validation\n(e.g., code documentation accuracy) is minimal. Second, we applied BERTopic\nmodeling to identify key areas of engagement on the entire dataset. We found\nthat backend issues (e.g., API management) dominate conversations, while\ntesting is surprisingly less covered. Third, we utilized the CPD clone\ndetection tool to check if the code generated by ChatGPT was used to address\nissues. Our findings revealed that ChatGPT-generated code was used as-is to\nresolve only 5.83\\% of the issues. Fourth, we estimated sentiment using a\nRoBERTa-based sentiment analysis model to determine developers' satisfaction\nwith different usages and engagement areas. We found positive sentiment (i.e.,\nhigh satisfaction) about using ChatGPT for refactoring and addressing data\nanalytics (e.g., categorizing table data) issues. On the contrary, we observed\nnegative sentiment when using ChatGPT to debug issues and address automation\ntasks (e.g., GUI interactions). Our findings show the unmet needs and growing\ndissatisfaction among developers. Researchers and ChatGPT developers should\nfocus on developing task-specific solutions that help resolve diverse issues,\nimproving user satisfaction and problem-solving efficiency in software\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Joy Krishan Das"
                    },
                    {
                        "name": "Saikat Mondal"
                    },
                    {
                        "name": "Chanchal K. Roy"
                    }
                ],
                "author_detail": {
                    "name": "Chanchal K. Roy"
                },
                "author": "Chanchal K. Roy",
                "arxiv_comment": "Accepted in SANER 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06753v1",
                "updated": "2024-12-09T18:43:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    43,
                    46,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    43,
                    46,
                    0,
                    344,
                    0
                ],
                "title": "InstantRestore: Single-Step Personalized Face Restoration with\n  Shared-Image Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstantRestore: Single-Step Personalized Face Restoration with\n  Shared-Image Attention"
                },
                "summary": "Face image restoration aims to enhance degraded facial images while\naddressing challenges such as diverse degradation types, real-time processing\ndemands, and, most crucially, the preservation of identity-specific features.\nExisting methods often struggle with slow processing times and suboptimal\nrestoration, especially under severe degradation, failing to accurately\nreconstruct finer-level identity details. To address these issues, we introduce\nInstantRestore, a novel framework that leverages a single-step image diffusion\nmodel and an attention-sharing mechanism for fast and personalized face\nrestoration. Additionally, InstantRestore incorporates a novel landmark\nattention loss, aligning key facial landmarks to refine the attention maps,\nenhancing identity preservation. At inference time, given a degraded input and\na small (~4) set of reference images, InstantRestore performs a single forward\npass through the network to achieve near real-time performance. Unlike prior\napproaches that rely on full diffusion processes or per-identity model tuning,\nInstantRestore offers a scalable solution suitable for large-scale\napplications. Extensive experiments demonstrate that InstantRestore outperforms\nexisting methods in quality and speed, making it an appealing choice for\nidentity-preserving face restoration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face image restoration aims to enhance degraded facial images while\naddressing challenges such as diverse degradation types, real-time processing\ndemands, and, most crucially, the preservation of identity-specific features.\nExisting methods often struggle with slow processing times and suboptimal\nrestoration, especially under severe degradation, failing to accurately\nreconstruct finer-level identity details. To address these issues, we introduce\nInstantRestore, a novel framework that leverages a single-step image diffusion\nmodel and an attention-sharing mechanism for fast and personalized face\nrestoration. Additionally, InstantRestore incorporates a novel landmark\nattention loss, aligning key facial landmarks to refine the attention maps,\nenhancing identity preservation. At inference time, given a degraded input and\na small (~4) set of reference images, InstantRestore performs a single forward\npass through the network to achieve near real-time performance. Unlike prior\napproaches that rely on full diffusion processes or per-identity model tuning,\nInstantRestore offers a scalable solution suitable for large-scale\napplications. Extensive experiments demonstrate that InstantRestore outperforms\nexisting methods in quality and speed, making it an appealing choice for\nidentity-preserving face restoration."
                },
                "authors": [
                    {
                        "name": "Howard Zhang"
                    },
                    {
                        "name": "Yuval Alaluf"
                    },
                    {
                        "name": "Sizhuo Ma"
                    },
                    {
                        "name": "Achuta Kadambi"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project page: https://snap-research.github.io/InstantRestore/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14279v3",
                "updated": "2024-12-09T18:41:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    41,
                    35,
                    0,
                    344,
                    0
                ],
                "published": "2024-01-25T16:10:33Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    10,
                    33,
                    3,
                    25,
                    0
                ],
                "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using LLMs"
                },
                "summary": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1."
                },
                "authors": [
                    {
                        "name": "Azmain Kabir"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    },
                    {
                        "name": "Muhammad Asaduzzaman"
                    },
                    {
                        "name": "Wenbin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Zhang"
                },
                "author": "Wenbin Zhang",
                "arxiv_doi": "10.1145/3702979",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3702979",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted and published in ACM Transactions on\n  Software Engineering and Methodology (TOSEM), [2024],\n  [https://dl.acm.org/doi/10.1145/3702979]",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06748v1",
                "updated": "2024-12-09T18:40:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    40,
                    44,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:40:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    40,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language\n  Models"
                },
                "summary": "A key component of building safe and reliable language models is enabling the\nmodels to appropriately refuse to follow certain instructions or answer certain\nquestions. We may want models to output refusal messages for various categories\nof user queries, for example, ill-posed questions, instructions for committing\nillegal acts, or queries which require information past the model's knowledge\nhorizon. Engineering models that refuse to answer such questions is complicated\nby the fact that an individual may want their model to exhibit varying levels\nof sensitivity for refusing queries of various categories, and different users\nmay want different refusal rates. The current default approach involves\ntraining multiple models with varying proportions of refusal messages from each\ncategory to achieve the desired refusal rates, which is computationally\nexpensive and may require training a new model to accommodate each user's\ndesired preference over refusal rates. To address these challenges, we propose\nrefusal tokens, one such token for each refusal category or a single refusal\ntoken, which are prepended to the model's responses during training. We then\nshow how to increase or decrease the probability of generating the refusal\ntoken for each category during inference to steer the model's refusal behavior.\nRefusal tokens enable controlling a single model's refusal rates without the\nneed of any further fine-tuning, but only by selectively intervening during\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key component of building safe and reliable language models is enabling the\nmodels to appropriately refuse to follow certain instructions or answer certain\nquestions. We may want models to output refusal messages for various categories\nof user queries, for example, ill-posed questions, instructions for committing\nillegal acts, or queries which require information past the model's knowledge\nhorizon. Engineering models that refuse to answer such questions is complicated\nby the fact that an individual may want their model to exhibit varying levels\nof sensitivity for refusing queries of various categories, and different users\nmay want different refusal rates. The current default approach involves\ntraining multiple models with varying proportions of refusal messages from each\ncategory to achieve the desired refusal rates, which is computationally\nexpensive and may require training a new model to accommodate each user's\ndesired preference over refusal rates. To address these challenges, we propose\nrefusal tokens, one such token for each refusal category or a single refusal\ntoken, which are prepended to the model's responses during training. We then\nshow how to increase or decrease the probability of generating the refusal\ntoken for each category during inference to steer the model's refusal behavior.\nRefusal tokens enable controlling a single model's refusal rates without the\nneed of any further fine-tuning, but only by selectively intervening during\ngeneration."
                },
                "authors": [
                    {
                        "name": "Neel Jain"
                    },
                    {
                        "name": "Aditya Shrivastava"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Daben Liu"
                    },
                    {
                        "name": "Alfy Samuel"
                    },
                    {
                        "name": "Ashwinee Panda"
                    },
                    {
                        "name": "Anoop Kumar"
                    },
                    {
                        "name": "Micah Goldblum"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Tom Goldstein"
                },
                "author": "Tom Goldstein",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06745v1",
                "updated": "2024-12-09T18:37:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    37,
                    14,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:37:14Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    37,
                    14,
                    0,
                    344,
                    0
                ],
                "title": "ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended\n  Capabilities"
                },
                "summary": "Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models."
                },
                "authors": [
                    {
                        "name": "Adhiraj Ghosh"
                    },
                    {
                        "name": "Sebastian Dziadzio"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Vishaal Udandarao"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Matthias Bethge"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Bethge"
                },
                "author": "Matthias Bethge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11796v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11796v4",
                "updated": "2024-12-09T18:31:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    31,
                    1,
                    0,
                    344,
                    0
                ],
                "published": "2024-08-21T17:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pruning and Distillation in Practice: The Minitron Approach"
                },
                "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license."
                },
                "authors": [
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Chenhan Yu"
                    },
                    {
                        "name": "Wei-Chun Chen"
                    },
                    {
                        "name": "Hayley Ross"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "arxiv_comment": "v4: Update author order",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11796v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11796v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06738v1",
                "updated": "2024-12-09T18:27:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    27,
                    32,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:27:32Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    27,
                    32,
                    0,
                    344,
                    0
                ],
                "title": "JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset\n  Generation with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset\n  Generation with LLM"
                },
                "summary": "Recently some studies have highlighted the potential of Large Language Models\n(LLMs) as effective generators of supervised training data, offering advantages\nsuch as enhanced inference efficiency and reduced costs associated with data\ncollection. However, these studies have predominantly focused on English\nlanguage tasks. In this paper, we address the fundamental research question:\nCan LLMs serve as proficient training data generators for other language tasks?\nSpecifically, we leverage LLMs to synthesize supervised training data under\nfew-shot and zero-shot learning scenarios across six diverse Japanese\ndownstream tasks. Subsequently, we utilize this synthesized data to train\ncompact models (e.g., BERT). This novel methodology is termed JAPAGEN. Our\nexperimental findings underscore that JAPAGEN achieves robust performance in\nclassification tasks that necessitate formal text inputs, demonstrating\ncompetitive results compared to conventional LLM prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently some studies have highlighted the potential of Large Language Models\n(LLMs) as effective generators of supervised training data, offering advantages\nsuch as enhanced inference efficiency and reduced costs associated with data\ncollection. However, these studies have predominantly focused on English\nlanguage tasks. In this paper, we address the fundamental research question:\nCan LLMs serve as proficient training data generators for other language tasks?\nSpecifically, we leverage LLMs to synthesize supervised training data under\nfew-shot and zero-shot learning scenarios across six diverse Japanese\ndownstream tasks. Subsequently, we utilize this synthesized data to train\ncompact models (e.g., BERT). This novel methodology is termed JAPAGEN. Our\nexperimental findings underscore that JAPAGEN achieves robust performance in\nclassification tasks that necessitate formal text inputs, demonstrating\ncompetitive results compared to conventional LLM prompting strategies."
                },
                "authors": [
                    {
                        "name": "Takuro Fujii"
                    },
                    {
                        "name": "Satoru Katsumata"
                    }
                ],
                "author_detail": {
                    "name": "Satoru Katsumata"
                },
                "author": "Satoru Katsumata",
                "arxiv_comment": "Accepted by PACLIC38 (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03657v2",
                "updated": "2024-12-09T18:19:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    19,
                    5,
                    0,
                    344,
                    0
                ],
                "published": "2024-04-04T17:59:58Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    17,
                    59,
                    58,
                    3,
                    95,
                    0
                ],
                "title": "OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and\n  Captioning"
                },
                "summary": "We propose the new task 'open-world video instance segmentation and\ncaptioning'. It requires to detect, segment, track and describe with rich\ncaptions never before seen objects. This challenging task can be addressed by\ndeveloping \"abstractors\" which connect a vision model and a language foundation\nmodel. Concretely, we connect a multi-scale visual feature extractor and a\nlarge language model (LLM) by developing an object abstractor and an\nobject-to-text abstractor. The object abstractor, consisting of a prompt\nencoder and transformer blocks, introduces spatially-diverse open-world object\nqueries to discover never before seen objects in videos. An inter-query\ncontrastive loss further encourages the diversity of object queries. The\nobject-to-text abstractor is augmented with masked cross-attention and acts as\na bridge between the object queries and a frozen LLM to generate rich and\ndescriptive object-centric captions for each detected object. Our generalized\napproach surpasses the baseline that jointly addresses the tasks of open-world\nvideo instance segmentation and dense video object captioning by 13% on never\nbefore seen objects, and by 10% on object-centric captions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose the new task 'open-world video instance segmentation and\ncaptioning'. It requires to detect, segment, track and describe with rich\ncaptions never before seen objects. This challenging task can be addressed by\ndeveloping \"abstractors\" which connect a vision model and a language foundation\nmodel. Concretely, we connect a multi-scale visual feature extractor and a\nlarge language model (LLM) by developing an object abstractor and an\nobject-to-text abstractor. The object abstractor, consisting of a prompt\nencoder and transformer blocks, introduces spatially-diverse open-world object\nqueries to discover never before seen objects in videos. An inter-query\ncontrastive loss further encourages the diversity of object queries. The\nobject-to-text abstractor is augmented with masked cross-attention and acts as\na bridge between the object queries and a frozen LLM to generate rich and\ndescriptive object-centric captions for each detected object. Our generalized\napproach surpasses the baseline that jointly addresses the tasks of open-world\nvideo instance segmentation and dense video object captioning by 13% on never\nbefore seen objects, and by 10% on object-centric captions."
                },
                "authors": [
                    {
                        "name": "Anwesa Choudhuri"
                    },
                    {
                        "name": "Girish Chowdhary"
                    },
                    {
                        "name": "Alexander G. Schwing"
                    }
                ],
                "author_detail": {
                    "name": "Alexander G. Schwing"
                },
                "author": "Alexander G. Schwing",
                "arxiv_comment": "Project page: https://anwesachoudhuri.github.io/OpenWorldVISCap/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06724v1",
                "updated": "2024-12-09T18:13:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    13,
                    27,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:13:27Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    13,
                    27,
                    0,
                    344,
                    0
                ],
                "title": "AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and\n  Benchmark"
                },
                "summary": "We investigate the reasoning capabilities of large language models (LLMs) for\nautomatically generating data-cleaning workflows. To evaluate LLMs' ability to\ncomplete data-cleaning tasks, we implemented a pipeline for LLM-based Auto Data\nCleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operations\nto repair three types of data quality issues: duplicates, missing values, and\ninconsistent data formats. Given a dirty table and a purpose (expressed as a\nquery), this pipeline generates a minimal, clean table sufficient to address\nthe purpose and the data cleaning workflow used to produce the table. The\nplanning process involves three main LLM-driven components: (1) Select Target\nColumns: Identifies a set of target columns related to the purpose. (2) Inspect\nColumn Quality: Assesses the data quality for each target column and generates\na Data Quality Report as operation objectives. (3) Generate Operation &\nArguments: Predicts the next operation and arguments based on the data quality\nreport results. Additionally, we propose a data cleaning benchmark to evaluate\nthe capability of LLM agents to automatically generate workflows that address\ndata cleaning purposes of varying difficulty levels. The benchmark comprises\nthe annotated datasets as a collection of purpose, raw table, clean table, data\ncleaning workflow, and answer set. In our experiments, we evaluated three LLMs\nthat auto-generate purpose-driven data cleaning workflows. The results indicate\nthat LLMs perform well in planning and generating data-cleaning workflows\nwithout the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the reasoning capabilities of large language models (LLMs) for\nautomatically generating data-cleaning workflows. To evaluate LLMs' ability to\ncomplete data-cleaning tasks, we implemented a pipeline for LLM-based Auto Data\nCleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operations\nto repair three types of data quality issues: duplicates, missing values, and\ninconsistent data formats. Given a dirty table and a purpose (expressed as a\nquery), this pipeline generates a minimal, clean table sufficient to address\nthe purpose and the data cleaning workflow used to produce the table. The\nplanning process involves three main LLM-driven components: (1) Select Target\nColumns: Identifies a set of target columns related to the purpose. (2) Inspect\nColumn Quality: Assesses the data quality for each target column and generates\na Data Quality Report as operation objectives. (3) Generate Operation &\nArguments: Predicts the next operation and arguments based on the data quality\nreport results. Additionally, we propose a data cleaning benchmark to evaluate\nthe capability of LLM agents to automatically generate workflows that address\ndata cleaning purposes of varying difficulty levels. The benchmark comprises\nthe annotated datasets as a collection of purpose, raw table, clean table, data\ncleaning workflow, and answer set. In our experiments, we evaluated three LLMs\nthat auto-generate purpose-driven data cleaning workflows. The results indicate\nthat LLMs perform well in planning and generating data-cleaning workflows\nwithout the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Lan Li"
                    },
                    {
                        "name": "Liri Fang"
                    },
                    {
                        "name": "Vetle I. Torvik"
                    }
                ],
                "author_detail": {
                    "name": "Vetle I. Torvik"
                },
                "author": "Vetle I. Torvik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05195v2",
                "updated": "2024-12-09T18:09:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    9,
                    25,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-06T17:18:31Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    18,
                    31,
                    4,
                    341,
                    0
                ],
                "title": "Piecewise-linear modeling of multivariate geometric extremes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piecewise-linear modeling of multivariate geometric extremes"
                },
                "summary": "A recent development in extreme value modeling uses the geometry of the\ndataset to perform inference on the multivariate tail. A key quantity in this\ninference is the gauge function, whose values define this geometry. Methodology\nproposed to date for capturing the gauge function either lacks flexibility due\nto parametric specifications, or relies on complex neural network\nspecifications in dimensions greater than three. We propose a semiparametric\ngauge function that is piecewise-linear, making it simple to interpret and\nprovides a good approximation for the true underlying gauge function. This\nlinearity also makes optimization tasks computationally inexpensive. The\npiecewise-linear gauge function can be used to define both a radial and an\nangular model, allowing for the joint fitting of extremal pseudo-polar\ncoordinates, a key aspect of this geometric framework. We further expand the\ntoolkit for geometric extremal modeling through the estimation of high radial\nquantiles at given angular values via kernel density estimation. We apply the\nnew methodology to air pollution data, which exhibits a complex extremal\ndependence structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent development in extreme value modeling uses the geometry of the\ndataset to perform inference on the multivariate tail. A key quantity in this\ninference is the gauge function, whose values define this geometry. Methodology\nproposed to date for capturing the gauge function either lacks flexibility due\nto parametric specifications, or relies on complex neural network\nspecifications in dimensions greater than three. We propose a semiparametric\ngauge function that is piecewise-linear, making it simple to interpret and\nprovides a good approximation for the true underlying gauge function. This\nlinearity also makes optimization tasks computationally inexpensive. The\npiecewise-linear gauge function can be used to define both a radial and an\nangular model, allowing for the joint fitting of extremal pseudo-polar\ncoordinates, a key aspect of this geometric framework. We further expand the\ntoolkit for geometric extremal modeling through the estimation of high radial\nquantiles at given angular values via kernel density estimation. We apply the\nnew methodology to air pollution data, which exhibits a complex extremal\ndependence structure."
                },
                "authors": [
                    {
                        "name": "Ryan Campbell"
                    },
                    {
                        "name": "Jennifer Wadsworth"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Wadsworth"
                },
                "author": "Jennifer Wadsworth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06711v1",
                "updated": "2024-12-09T17:59:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    59,
                    59,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:59:59Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    59,
                    59,
                    0,
                    344,
                    0
                ],
                "title": "MISFEAT: Feature Selection for Subgroups with Systematic Missing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MISFEAT: Feature Selection for Subgroups with Systematic Missing Data"
                },
                "summary": "We investigate the problem of selecting features for datasets that can be\nnaturally partitioned into subgroups (e.g., according to socio-demographic\ngroups and age), each with its own dominant set of features. Within this\nsubgroup-oriented framework, we address the challenge of systematic missing\ndata, a scenario in which some feature values are missing for all tuples of a\nsubgroup, due to flawed data integration, regulatory constraints, or privacy\nconcerns. Feature selection is governed by finding mutual Information, a\npopular quantification of correlation, between features and a target variable.\nOur goal is to identify top-K feature subsets of some fixed size with the\nhighest joint mutual information with a target variable. In the presence of\nsystematic missing data, the closed form of mutual information could not simply\nbe applied. We argue that in such a setting, leveraging relationships between\navailable feature mutual information within a subgroup or across subgroups can\nassist inferring missing mutual information values. We propose a generalizable\nmodel based on heterogeneous graph neural network to identify interdependencies\nbetween feature-subgroup-target variable connections by modeling it as a\nmultiplex graph, and employing information propagation between its nodes. We\naddress two distinct scalability challenges related to training and propose\nprincipled solutions to tackle them. Through an extensive empirical evaluation,\nwe demonstrate the efficacy of the proposed solutions both qualitatively and\nrunning time wise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the problem of selecting features for datasets that can be\nnaturally partitioned into subgroups (e.g., according to socio-demographic\ngroups and age), each with its own dominant set of features. Within this\nsubgroup-oriented framework, we address the challenge of systematic missing\ndata, a scenario in which some feature values are missing for all tuples of a\nsubgroup, due to flawed data integration, regulatory constraints, or privacy\nconcerns. Feature selection is governed by finding mutual Information, a\npopular quantification of correlation, between features and a target variable.\nOur goal is to identify top-K feature subsets of some fixed size with the\nhighest joint mutual information with a target variable. In the presence of\nsystematic missing data, the closed form of mutual information could not simply\nbe applied. We argue that in such a setting, leveraging relationships between\navailable feature mutual information within a subgroup or across subgroups can\nassist inferring missing mutual information values. We propose a generalizable\nmodel based on heterogeneous graph neural network to identify interdependencies\nbetween feature-subgroup-target variable connections by modeling it as a\nmultiplex graph, and employing information propagation between its nodes. We\naddress two distinct scalability challenges related to training and propose\nprincipled solutions to tackle them. Through an extensive empirical evaluation,\nwe demonstrate the efficacy of the proposed solutions both qualitatively and\nrunning time wise."
                },
                "authors": [
                    {
                        "name": "Bar Genossar"
                    },
                    {
                        "name": "Thinh On"
                    },
                    {
                        "name": "Md. Mouinul Islam"
                    },
                    {
                        "name": "Ben Eliav"
                    },
                    {
                        "name": "Senjuti Basu Roy"
                    },
                    {
                        "name": "Avigdor Gal"
                    }
                ],
                "author_detail": {
                    "name": "Avigdor Gal"
                },
                "author": "Avigdor Gal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.04165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.04165v2",
                "updated": "2024-12-09T17:47:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    47,
                    2,
                    0,
                    344,
                    0
                ],
                "published": "2023-10-06T11:27:36Z",
                "published_parsed": [
                    2023,
                    10,
                    6,
                    11,
                    27,
                    36,
                    4,
                    279,
                    0
                ],
                "title": "When Composite Likelihood Meets Stochastic Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Composite Likelihood Meets Stochastic Approximation"
                },
                "summary": "A composite likelihood is an inference function derived by multiplying a set\nof likelihood components. This approach provides a flexible framework for\ndrawing inference when the likelihood function of a statistical model is\ncomputationally intractable. While composite likelihood has computational\nadvantages, it can still be demanding when dealing with numerous likelihood\ncomponents and a large sample size. This paper tackles this challenge by\nemploying an approximation of the conventional composite likelihood estimator,\nwhich is derived from an optimization procedure relying on stochastic\ngradients. This novel estimator is shown to be asymptotically normally\ndistributed around the true parameter. In particular, based on the relative\ndivergent rate of the sample size and the number of iterations of the\noptimization, the variance of the limiting distribution is shown to compound\nfor two sources of uncertainty: the sampling variability of the data and the\noptimization noise, with the latter depending on the sampling distribution used\nto construct the stochastic gradients. The advantages of the proposed framework\nare illustrated through simulation studies on two working examples: an Ising\nmodel for binary data and a gamma frailty model for count data. Finally, a\nreal-data application is presented, showing its effectiveness in a large-scale\nmental health survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A composite likelihood is an inference function derived by multiplying a set\nof likelihood components. This approach provides a flexible framework for\ndrawing inference when the likelihood function of a statistical model is\ncomputationally intractable. While composite likelihood has computational\nadvantages, it can still be demanding when dealing with numerous likelihood\ncomponents and a large sample size. This paper tackles this challenge by\nemploying an approximation of the conventional composite likelihood estimator,\nwhich is derived from an optimization procedure relying on stochastic\ngradients. This novel estimator is shown to be asymptotically normally\ndistributed around the true parameter. In particular, based on the relative\ndivergent rate of the sample size and the number of iterations of the\noptimization, the variance of the limiting distribution is shown to compound\nfor two sources of uncertainty: the sampling variability of the data and the\noptimization noise, with the latter depending on the sampling distribution used\nto construct the stochastic gradients. The advantages of the proposed framework\nare illustrated through simulation studies on two working examples: an Ising\nmodel for binary data and a gamma frailty model for count data. Finally, a\nreal-data application is presented, showing its effectiveness in a large-scale\nmental health survey."
                },
                "authors": [
                    {
                        "name": "Giuseppe Alfonzetti"
                    },
                    {
                        "name": "Ruggero Bellio"
                    },
                    {
                        "name": "Yunxiao Chen"
                    },
                    {
                        "name": "Irini Moustaki"
                    }
                ],
                "author_detail": {
                    "name": "Irini Moustaki"
                },
                "author": "Irini Moustaki",
                "arxiv_doi": "10.1080/01621459.2024.2436219",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/01621459.2024.2436219",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.04165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.04165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06696v1",
                "updated": "2024-12-09T17:42:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    42,
                    44,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:42:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    42,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "Geometric distances between closed universes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric distances between closed universes"
                },
                "summary": "The large-scale structure of the universe is well-approximated by the\nFriedmann equations, parameterised by several energy densities which can be\nobservationally inferred. A natural question to ask is: how different would the\nuniverse be if these densities took on other values? While there are many ways\nthis can be approached depending on interpretation and mathematical rigour, we\nattempt an answer by building a \"history space\" of different cosmologies. A\nmetric is introduced after overcoming technical hurdles related to Lorentzian\nsignature and infinite volume, at least for topologically-closed cases.\nGeodesics connecting two points on the superspace are computed to express how\ndistant -- in a purely geometric sense -- two universes are. Age can be treated\nas a free parameter in such an approach, leading to a more general mathematical\nconstruct relative to geometrodynamical configuration-space. Connections to\nbubble inflation and phase transitions are explored.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale structure of the universe is well-approximated by the\nFriedmann equations, parameterised by several energy densities which can be\nobservationally inferred. A natural question to ask is: how different would the\nuniverse be if these densities took on other values? While there are many ways\nthis can be approached depending on interpretation and mathematical rigour, we\nattempt an answer by building a \"history space\" of different cosmologies. A\nmetric is introduced after overcoming technical hurdles related to Lorentzian\nsignature and infinite volume, at least for topologically-closed cases.\nGeodesics connecting two points on the superspace are computed to express how\ndistant -- in a purely geometric sense -- two universes are. Age can be treated\nas a free parameter in such an approach, leading to a more general mathematical\nconstruct relative to geometrodynamical configuration-space. Connections to\nbubble inflation and phase transitions are explored."
                },
                "authors": [
                    {
                        "name": "Arthur G. Suvorov"
                    }
                ],
                "author_detail": {
                    "name": "Arthur G. Suvorov"
                },
                "author": "Arthur G. Suvorov",
                "arxiv_comment": "9 pages, 6 figures. Accepted for publication in PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06693v1",
                "updated": "2024-12-09T17:39:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    39,
                    43,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:39:43Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    39,
                    43,
                    0,
                    344,
                    0
                ],
                "title": "OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large\n  Language Model and its Omni-Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large\n  Language Model and its Omni-Extensions"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) have significantly\nexpanded their applications, ranging from multilingual support to\ndomain-specific tasks and multimodal integration. In this paper, we present\nOmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and their\nomni-extensions across multilingual, multidomain, and multimodal capabilities.\nUnlike existing benchmarks that often focus on a single aspect, OmniEvalKit\nprovides a modular, lightweight, and automated evaluation system. It is\nstructured with a modular architecture comprising a Static Builder and Dynamic\nData Flow, promoting the seamless integration of new models and datasets.\nOmniEvalKit supports over 100 LLMs and 50 evaluation datasets, covering\ncomprehensive evaluations across thousands of model-dataset combinations.\nOmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployable\nevaluation framework, making downstream applications more convenient and\nversatile for the AI community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) have significantly\nexpanded their applications, ranging from multilingual support to\ndomain-specific tasks and multimodal integration. In this paper, we present\nOmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and their\nomni-extensions across multilingual, multidomain, and multimodal capabilities.\nUnlike existing benchmarks that often focus on a single aspect, OmniEvalKit\nprovides a modular, lightweight, and automated evaluation system. It is\nstructured with a modular architecture comprising a Static Builder and Dynamic\nData Flow, promoting the seamless integration of new models and datasets.\nOmniEvalKit supports over 100 LLMs and 50 evaluation datasets, covering\ncomprehensive evaluations across thousands of model-dataset combinations.\nOmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployable\nevaluation framework, making downstream applications more convenient and\nversatile for the AI community."
                },
                "authors": [
                    {
                        "name": "Yi-Kai Zhang"
                    },
                    {
                        "name": "Xu-Xiang Zhong"
                    },
                    {
                        "name": "Shiyin Lu"
                    },
                    {
                        "name": "Qing-Guo Chen"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00535v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00535v3",
                "updated": "2024-12-09T17:31:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    31,
                    54,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-30T16:58:42Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    58,
                    42,
                    5,
                    335,
                    0
                ],
                "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullStack Bench: Evaluating LLMs as Full Stack Coders"
                },
                "summary": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion."
                },
                "authors": [
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Jerry Liu"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Jinxiang Xia"
                    },
                    {
                        "name": "Z. Y. Peng"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jing Mai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00535v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00535v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06684v1",
                "updated": "2024-12-09T17:27:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    27,
                    4,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:27:04Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    27,
                    4,
                    0,
                    344,
                    0
                ],
                "title": "Exploring Critical Testing Scenarios for Decision-Making Policies: An\n  LLM Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Critical Testing Scenarios for Decision-Making Policies: An\n  LLM Approach"
                },
                "summary": "Recent years have witnessed surprising achievements of decision-making\npolicies across various fields, such as autonomous driving and robotics.\nTesting for decision-making policies is crucial with the existence of critical\nscenarios that may threaten their reliability. Numerous research efforts have\nbeen dedicated to testing these policies. However, there are still significant\nchallenges, such as low testing efficiency and diversity due to the complexity\nof the policies and environments under test. Inspired by the remarkable\ncapabilities of large language models (LLMs), in this paper, we propose an\nLLM-driven online testing framework for efficiently testing decision-making\npolicies. The main idea is to employ an LLM-based test scenario generator to\nintelligently generate challenging test cases through contemplation and\nreasoning. Specifically, we first design a \"generate-test-feedback\" pipeline\nand apply templated prompt engineering to fully leverage the knowledge and\nreasoning abilities of LLMs. Then, we introduce a multi-scale scenario\ngeneration strategy to address the inherent challenges LLMs face in making fine\nadjustments, further enhancing testing efficiency. Finally, we evaluate the\nLLM-driven approach on five widely used benchmarks. The experimental results\ndemonstrate that our method significantly outperforms baseline approaches in\nuncovering both critical and diverse scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed surprising achievements of decision-making\npolicies across various fields, such as autonomous driving and robotics.\nTesting for decision-making policies is crucial with the existence of critical\nscenarios that may threaten their reliability. Numerous research efforts have\nbeen dedicated to testing these policies. However, there are still significant\nchallenges, such as low testing efficiency and diversity due to the complexity\nof the policies and environments under test. Inspired by the remarkable\ncapabilities of large language models (LLMs), in this paper, we propose an\nLLM-driven online testing framework for efficiently testing decision-making\npolicies. The main idea is to employ an LLM-based test scenario generator to\nintelligently generate challenging test cases through contemplation and\nreasoning. Specifically, we first design a \"generate-test-feedback\" pipeline\nand apply templated prompt engineering to fully leverage the knowledge and\nreasoning abilities of LLMs. Then, we introduce a multi-scale scenario\ngeneration strategy to address the inherent challenges LLMs face in making fine\nadjustments, further enhancing testing efficiency. Finally, we evaluate the\nLLM-driven approach on five widely used benchmarks. The experimental results\ndemonstrate that our method significantly outperforms baseline approaches in\nuncovering both critical and diverse scenarios."
                },
                "authors": [
                    {
                        "name": "Weichao Xu"
                    },
                    {
                        "name": "Huaxin Pei"
                    },
                    {
                        "name": "Jingxuan Yang"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhang"
                },
                "author": "Yi Zhang",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06681v1",
                "updated": "2024-12-09T17:24:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    24,
                    41,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:24:41Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    24,
                    41,
                    0,
                    344,
                    0
                ],
                "title": "Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual\n  Framework"
                },
                "summary": "In transportation system demand modeling and simulation, agent-based models\nand microsimulations are current state-of-the-art approaches. However, existing\nagent-based models still have some limitations on behavioral realism and\nresource demand that limit their applicability. In this study, leveraging the\nemerging technology of large language models (LLMs) and LLM-based agents, we\npropose a general LLM-agent-based modeling framework for transportation\nsystems. We argue that LLM agents not only possess the essential capabilities\nto function as agents but also offer promising solutions to overcome some\nlimitations of existing agent-based models. Our conceptual framework design\nclosely replicates the decision-making and interaction processes and traits of\nhuman travelers within transportation networks, and we demonstrate that the\nproposed systems can meet critical behavioral criteria for decision-making and\nlearning behaviors using related studies and a demonstrative example of LLM\nagents' learning and adjustment in the bottleneck setting. Although further\nrefinement of the LLM-agent-based modeling framework is necessary, we believe\nthat this approach has the potential to improve transportation system modeling\nand simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In transportation system demand modeling and simulation, agent-based models\nand microsimulations are current state-of-the-art approaches. However, existing\nagent-based models still have some limitations on behavioral realism and\nresource demand that limit their applicability. In this study, leveraging the\nemerging technology of large language models (LLMs) and LLM-based agents, we\npropose a general LLM-agent-based modeling framework for transportation\nsystems. We argue that LLM agents not only possess the essential capabilities\nto function as agents but also offer promising solutions to overcome some\nlimitations of existing agent-based models. Our conceptual framework design\nclosely replicates the decision-making and interaction processes and traits of\nhuman travelers within transportation networks, and we demonstrate that the\nproposed systems can meet critical behavioral criteria for decision-making and\nlearning behaviors using related studies and a demonstrative example of LLM\nagents' learning and adjustment in the bottleneck setting. Although further\nrefinement of the LLM-agent-based modeling framework is necessary, we believe\nthat this approach has the potential to improve transportation system modeling\nand simulation."
                },
                "authors": [
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Jirong Yang"
                    },
                    {
                        "name": "Yafeng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yafeng Yin"
                },
                "author": "Yafeng Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06673v1",
                "updated": "2024-12-09T17:11:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    11,
                    50,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:11:50Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    11,
                    50,
                    0,
                    344,
                    0
                ],
                "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance"
                },
                "summary": "In this paper, we introduce ILLUME, a unified multimodal large language model\n(MLLM) that seamlessly integrates multimodal understanding and generation\ncapabilities within a single large language model through a unified next-token\nprediction formulation. To address the large dataset size typically required\nfor image-text alignment, we propose to enhance data efficiency through the\ndesign of a vision tokenizer that incorporates semantic information and a\nprogressive multi-stage training procedure. This approach reduces the dataset\nsize to just 15M for pretraining -- over four times fewer than what is\ntypically needed -- while achieving competitive or even superior performance\nwith existing unified MLLMs, such as Janus. Additionally, to promote\nsynergistic enhancement between understanding and generation capabilities,\nwhich is under-explored in previous works, we introduce a novel self-enhancing\nmultimodal alignment scheme. This scheme supervises the MLLM to self-assess the\nconsistency between text descriptions and self-generated images, facilitating\nthe model to interpret images more accurately and avoid unrealistic and\nincorrect predictions caused by misalignment in image generation. Based on\nextensive experiments, our proposed ILLUME stands out and competes with\nstate-of-the-art unified MLLMs and specialized models across various benchmarks\nfor multimodal understanding, generation, and editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce ILLUME, a unified multimodal large language model\n(MLLM) that seamlessly integrates multimodal understanding and generation\ncapabilities within a single large language model through a unified next-token\nprediction formulation. To address the large dataset size typically required\nfor image-text alignment, we propose to enhance data efficiency through the\ndesign of a vision tokenizer that incorporates semantic information and a\nprogressive multi-stage training procedure. This approach reduces the dataset\nsize to just 15M for pretraining -- over four times fewer than what is\ntypically needed -- while achieving competitive or even superior performance\nwith existing unified MLLMs, such as Janus. Additionally, to promote\nsynergistic enhancement between understanding and generation capabilities,\nwhich is under-explored in previous works, we introduce a novel self-enhancing\nmultimodal alignment scheme. This scheme supervises the MLLM to self-assess the\nconsistency between text descriptions and self-generated images, facilitating\nthe model to interpret images more accurately and avoid unrealistic and\nincorrect predictions caused by misalignment in image generation. Based on\nextensive experiments, our proposed ILLUME stands out and competes with\nstate-of-the-art unified MLLMs and specialized models across various benchmarks\nfor multimodal understanding, generation, and editing."
                },
                "authors": [
                    {
                        "name": "Chunwei Wang"
                    },
                    {
                        "name": "Guansong Lu"
                    },
                    {
                        "name": "Junwei Yang"
                    },
                    {
                        "name": "Runhui Huang"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Hang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Xu"
                },
                "author": "Hang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06661v1",
                "updated": "2024-12-09T17:00:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    0,
                    20,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:00:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    0,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion"
                },
                "summary": "Text-to-image generation of Stable Diffusion models has achieved notable\nsuccess due to its remarkable generation ability. However, the repetitive\ndenoising process is computationally intensive during inference, which renders\nDiffusion models less suitable for real-world applications that require low\nlatency and scalability. Recent studies have employed post-training\nquantization (PTQ) and quantization-aware training (QAT) methods to compress\nDiffusion models. Nevertheless, prior research has often neglected to examine\nthe consistency between results generated by quantized models and those from\nfloating-point models. This consistency is crucial in fields such as content\ncreation, design, and edge deployment, as it can significantly enhance both\nefficiency and system stability for practitioners. To ensure that quantized\nmodels generate high-quality and consistent images, we propose an efficient\nquantization framework for Stable Diffusion models. Our approach features a\nSerial-to-Parallel calibration pipeline that addresses the consistency of both\nthe calibration and inference processes, as well as ensuring training\nstability. Based on this pipeline, we further introduce a mix-precision\nquantization strategy, multi-timestep activation quantization, and time\ninformation precalculation techniques to ensure high-fidelity generation in\ncomparison to floating-point models. Through extensive experiments with Stable\nDiffusion v1-4, v2-1, and XL 1.0, we have demonstrated that our method\noutperforms the current state-of-the-art techniques when tested on prompts from\nthe COCO validation dataset and the Stable-Diffusion-Prompts dataset. Under\nW4A8 quantization settings, our approach enhances both distribution similarity\nand visual similarity by 45%-60%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation of Stable Diffusion models has achieved notable\nsuccess due to its remarkable generation ability. However, the repetitive\ndenoising process is computationally intensive during inference, which renders\nDiffusion models less suitable for real-world applications that require low\nlatency and scalability. Recent studies have employed post-training\nquantization (PTQ) and quantization-aware training (QAT) methods to compress\nDiffusion models. Nevertheless, prior research has often neglected to examine\nthe consistency between results generated by quantized models and those from\nfloating-point models. This consistency is crucial in fields such as content\ncreation, design, and edge deployment, as it can significantly enhance both\nefficiency and system stability for practitioners. To ensure that quantized\nmodels generate high-quality and consistent images, we propose an efficient\nquantization framework for Stable Diffusion models. Our approach features a\nSerial-to-Parallel calibration pipeline that addresses the consistency of both\nthe calibration and inference processes, as well as ensuring training\nstability. Based on this pipeline, we further introduce a mix-precision\nquantization strategy, multi-timestep activation quantization, and time\ninformation precalculation techniques to ensure high-fidelity generation in\ncomparison to floating-point models. Through extensive experiments with Stable\nDiffusion v1-4, v2-1, and XL 1.0, we have demonstrated that our method\noutperforms the current state-of-the-art techniques when tested on prompts from\nthe COCO validation dataset and the Stable-Diffusion-Prompts dataset. Under\nW4A8 quantization settings, our approach enhances both distribution similarity\nand visual similarity by 45%-60%."
                },
                "authors": [
                    {
                        "name": "Shuaiting Li"
                    },
                    {
                        "name": "Juncan Deng"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Kedong Xu"
                    },
                    {
                        "name": "Haibin Shen"
                    },
                    {
                        "name": "Kejie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kejie Huang"
                },
                "author": "Kejie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06654v1",
                "updated": "2024-12-09T16:54:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    54,
                    54,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T16:54:54Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    54,
                    54,
                    0,
                    344,
                    0
                ],
                "title": "GEAR: A Simple GENERATE, EMBED, AVERAGE AND RANK Approach for\n  Unsupervised Reverse Dictionary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: A Simple GENERATE, EMBED, AVERAGE AND RANK Approach for\n  Unsupervised Reverse Dictionary"
                },
                "summary": "Reverse Dictionary (RD) is the task of obtaining the most relevant word or\nset of words given a textual description or dictionary definition. Effective RD\nmethods have applications in accessibility, translation or writing support\nsystems. Moreover, in NLP research we find RD to be used to benchmark text\nencoders at various granularities, as it often requires word, definition and\nsentence embeddings. In this paper, we propose a simple approach to RD that\nleverages LLMs in combination with embedding models. Despite its simplicity,\nthis approach outperforms supervised baselines in well studied RD datasets,\nwhile also showing less over-fitting. We also conduct a number of experiments\non different dictionaries and analyze how different styles, registers and\ntarget audiences impact the quality of RD systems. We conclude that, on\naverage, untuned embeddings alone fare way below an LLM-only baseline (although\nthey are competitive in highly technical dictionaries), but are crucial for\nboosting performance in combined methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Dictionary (RD) is the task of obtaining the most relevant word or\nset of words given a textual description or dictionary definition. Effective RD\nmethods have applications in accessibility, translation or writing support\nsystems. Moreover, in NLP research we find RD to be used to benchmark text\nencoders at various granularities, as it often requires word, definition and\nsentence embeddings. In this paper, we propose a simple approach to RD that\nleverages LLMs in combination with embedding models. Despite its simplicity,\nthis approach outperforms supervised baselines in well studied RD datasets,\nwhile also showing less over-fitting. We also conduct a number of experiments\non different dictionaries and analyze how different styles, registers and\ntarget audiences impact the quality of RD systems. We conclude that, on\naverage, untuned embeddings alone fare way below an LLM-only baseline (although\nthey are competitive in highly technical dictionaries), but are crucial for\nboosting performance in combined methods."
                },
                "authors": [
                    {
                        "name": "Fatemah Almeman"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "9 pages, Accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15138v2",
                "updated": "2024-12-09T16:53:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    53,
                    42,
                    0,
                    344,
                    0
                ],
                "published": "2024-08-27T15:23:09Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    15,
                    23,
                    9,
                    1,
                    240,
                    0
                ],
                "title": "How transformers learn structured data: insights from hierarchical\n  filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How transformers learn structured data: insights from hierarchical\n  filtering"
                },
                "summary": "Understanding the learning process and the embedded computation in\ntransformers is becoming a central goal for the development of interpretable\nAI. In the present study, we introduce a hierarchical filtering procedure for\ngenerative models of sequences on trees, allowing us to hand-tune the range of\npositional correlations in the data. Leveraging this controlled setting, we\nprovide evidence that vanilla encoder-only transformers can approximate the\nexact inference algorithm when trained on root classification and masked\nlanguage modeling tasks, and study how this computation is discovered and\nimplemented. We find that correlations at larger distances, corresponding to\nincreasing layers of the hierarchy, are sequentially included by the network\nduring training. Moreover, by comparing attention maps from models trained with\nvarying degrees of filtering and by probing the different encoder levels, we\nfind clear evidence of a reconstruction of correlations on successive length\nscales corresponding to the various levels of the hierarchy, which we relate to\na plausible implementation of the exact inference algorithm within the same\narchitecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the learning process and the embedded computation in\ntransformers is becoming a central goal for the development of interpretable\nAI. In the present study, we introduce a hierarchical filtering procedure for\ngenerative models of sequences on trees, allowing us to hand-tune the range of\npositional correlations in the data. Leveraging this controlled setting, we\nprovide evidence that vanilla encoder-only transformers can approximate the\nexact inference algorithm when trained on root classification and masked\nlanguage modeling tasks, and study how this computation is discovered and\nimplemented. We find that correlations at larger distances, corresponding to\nincreasing layers of the hierarchy, are sequentially included by the network\nduring training. Moreover, by comparing attention maps from models trained with\nvarying degrees of filtering and by probing the different encoder levels, we\nfind clear evidence of a reconstruction of correlations on successive length\nscales corresponding to the various levels of the hierarchy, which we relate to\na plausible implementation of the exact inference algorithm within the same\narchitecture."
                },
                "authors": [
                    {
                        "name": "Jerome Garnier-Brun"
                    },
                    {
                        "name": "Marc Mzard"
                    },
                    {
                        "name": "Emanuele Moscato"
                    },
                    {
                        "name": "Luca Saglietti"
                    }
                ],
                "author_detail": {
                    "name": "Luca Saglietti"
                },
                "author": "Luca Saglietti",
                "arxiv_comment": "21 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06651v1",
                "updated": "2024-12-09T16:50:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T16:50:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben"
                },
                "summary": "This study examines the AI-powered grading tool \"AI Grading Assistant\" by the\nGerman company Fobizz, designed to support teachers in evaluating and providing\nfeedback on student assignments. Against the societal backdrop of an\noverburdened education system and rising expectations for artificial\nintelligence as a solution to these challenges, the investigation evaluates the\ntool's functional suitability through two test series. The results reveal\nsignificant shortcomings: The tool's numerical grades and qualitative feedback\nare often random and do not improve even when its suggestions are incorporated.\nThe highest ratings are achievable only with texts generated by ChatGPT. False\nclaims and nonsensical submissions frequently go undetected, while the\nimplementation of some grading criteria is unreliable and opaque. Since these\ndeficiencies stem from the inherent limitations of large language models\n(LLMs), fundamental improvements to this or similar tools are not immediately\nforeseeable. The study critiques the broader trend of adopting AI as a quick\nfix for systemic problems in education, concluding that Fobizz's marketing of\nthe tool as an objective and time-saving solution is misleading and\nirresponsible. Finally, the study calls for systematic evaluation and\nsubject-specific pedagogical scrutiny of the use of AI tools in educational\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the AI-powered grading tool \"AI Grading Assistant\" by the\nGerman company Fobizz, designed to support teachers in evaluating and providing\nfeedback on student assignments. Against the societal backdrop of an\noverburdened education system and rising expectations for artificial\nintelligence as a solution to these challenges, the investigation evaluates the\ntool's functional suitability through two test series. The results reveal\nsignificant shortcomings: The tool's numerical grades and qualitative feedback\nare often random and do not improve even when its suggestions are incorporated.\nThe highest ratings are achievable only with texts generated by ChatGPT. False\nclaims and nonsensical submissions frequently go undetected, while the\nimplementation of some grading criteria is unreliable and opaque. Since these\ndeficiencies stem from the inherent limitations of large language models\n(LLMs), fundamental improvements to this or similar tools are not immediately\nforeseeable. The study critiques the broader trend of adopting AI as a quick\nfix for systemic problems in education, concluding that Fobizz's marketing of\nthe tool as an objective and time-saving solution is misleading and\nirresponsible. Finally, the study calls for systematic evaluation and\nsubject-specific pedagogical scrutiny of the use of AI tools in educational\ncontexts."
                },
                "authors": [
                    {
                        "name": "Rainer Mhlhoff"
                    },
                    {
                        "name": "Marte Henningsen"
                    }
                ],
                "author_detail": {
                    "name": "Marte Henningsen"
                },
                "author": "Marte Henningsen",
                "arxiv_comment": "32 pages, in German language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "97B10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16851v2",
                "updated": "2024-12-09T16:42:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    42,
                    25,
                    0,
                    344,
                    0
                ],
                "published": "2024-03-25T15:15:09Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    15,
                    15,
                    9,
                    0,
                    85,
                    0
                ],
                "title": "Can tweets predict article retractions? A comparison between human and\n  LLM labelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can tweets predict article retractions? A comparison between human and\n  LLM labelling"
                },
                "summary": "Quickly detecting problematic research articles is crucial to safeguarding\nthe integrity of scientific research. This study explores whether Twitter\nmentions of retracted articles can signal potential problems with the articles\nprior to their retraction, potentially serving as an early warning system for\nscholars. To investigate this, we analysed a dataset of 4,354 Twitter mentions\nassociated with 504 retracted articles. The effectiveness of Twitter mentions\nin predicting article retractions was evaluated by both manual and Large\nLanguage Model (LLM) labelling. Manual labelling results indicated that 25.7%\nof tweets signalled problems before retraction. Using the manual labelling\nresults as the baseline, we found that LLMs (GPT-4o-mini, Gemini 1.5 Flash, and\nClaude-3.5-Haiku) outperformed lexicon-based sentiment analysis tools (e.g.,\nTextBlob) in detecting potential problems, suggesting that automatic detection\nof problematic articles from social media using LLMs is technically feasible.\nNevertheless, since only a small proportion of retracted articles (11.1%) were\ncriticised on Twitter prior to retraction, such automatic systems would detect\nonly a minority of problematic articles. Overall, this study offers insights\ninto how social media data, coupled with emerging generative AI techniques, can\nsupport research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickly detecting problematic research articles is crucial to safeguarding\nthe integrity of scientific research. This study explores whether Twitter\nmentions of retracted articles can signal potential problems with the articles\nprior to their retraction, potentially serving as an early warning system for\nscholars. To investigate this, we analysed a dataset of 4,354 Twitter mentions\nassociated with 504 retracted articles. The effectiveness of Twitter mentions\nin predicting article retractions was evaluated by both manual and Large\nLanguage Model (LLM) labelling. Manual labelling results indicated that 25.7%\nof tweets signalled problems before retraction. Using the manual labelling\nresults as the baseline, we found that LLMs (GPT-4o-mini, Gemini 1.5 Flash, and\nClaude-3.5-Haiku) outperformed lexicon-based sentiment analysis tools (e.g.,\nTextBlob) in detecting potential problems, suggesting that automatic detection\nof problematic articles from social media using LLMs is technically feasible.\nNevertheless, since only a small proportion of retracted articles (11.1%) were\ncriticised on Twitter prior to retraction, such automatic systems would detect\nonly a minority of problematic articles. Overall, this study offers insights\ninto how social media data, coupled with emerging generative AI techniques, can\nsupport research integrity."
                },
                "authors": [
                    {
                        "name": "Er-Te Zheng"
                    },
                    {
                        "name": "Hui-Zhen Fu"
                    },
                    {
                        "name": "Mike Thelwall"
                    },
                    {
                        "name": "Zhichao Fang"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Fang"
                },
                "author": "Zhichao Fang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06644v1",
                "updated": "2024-12-09T16:37:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    37,
                    43,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T16:37:43Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    37,
                    43,
                    0,
                    344,
                    0
                ],
                "title": "\"Bursts, Beats, and Beyond\": Uncovering the landscape from accretion to\n  ignition of 4U 1728--34 using \\textit{AstroSat}",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Bursts, Beats, and Beyond\": Uncovering the landscape from accretion to\n  ignition of 4U 1728--34 using \\textit{AstroSat}"
                },
                "summary": "A comprehensive study on persistent and thermonuclear burst emission of 4U\n1728--34, commonly known as `Slow Burster' is performed using seven archival\nobservations of \\textit{AstroSat} spanning from 2016--2019. The burst-free\npersistent spectra can be well fitted with a blackbody \\texttt{bbody} and a\npowerlaw \\texttt{powerlaw} components, with a powerlaw photon index ($\\Gamma$)\nwas found to be $\\sim$2 indicating the source was in ``high/soft\" bananna state\nor intermediate state. The time averaged power density spectrum reveals the\npresence of twin kilohertz Quasi Periodic Oscillations (kHz QPOs) with centroid\nfrequencies $619\\pm10$~Hz and $965\\pm6$~Hz with a maximum fractional root mean\nsquared amplitude of $6.24\\pm1.31$~\\% at $\\sim$16~keV. From the upper kHz QPO,\nwe infer the magnetospheric disk radius to be $\\sim$17~km, corresponding to a\nmagnetic field strength of 0.35--1.27$~\\times~10^7$~G. The burst spectral\nevolution indicates Photospheric Radius Expansion (PRE) in five bursts,\nyielding a touchdown radius of 3.1--5.47~km. These bursts reached\nnear-Eddington luminosities, through which the distance of the source was\ncalculated to be 5.18--5.21~kpc. Two of the bursts show coherent oscillations\nat 362.81--363.93~Hz. The presence of twin kHz QPOs and coherent Burst\nOscillations allows us to provide two different estimates for the spin\nfrequency of the Neutron Star in the system, for the first time using\n\\textit{AstroSat}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive study on persistent and thermonuclear burst emission of 4U\n1728--34, commonly known as `Slow Burster' is performed using seven archival\nobservations of \\textit{AstroSat} spanning from 2016--2019. The burst-free\npersistent spectra can be well fitted with a blackbody \\texttt{bbody} and a\npowerlaw \\texttt{powerlaw} components, with a powerlaw photon index ($\\Gamma$)\nwas found to be $\\sim$2 indicating the source was in ``high/soft\" bananna state\nor intermediate state. The time averaged power density spectrum reveals the\npresence of twin kilohertz Quasi Periodic Oscillations (kHz QPOs) with centroid\nfrequencies $619\\pm10$~Hz and $965\\pm6$~Hz with a maximum fractional root mean\nsquared amplitude of $6.24\\pm1.31$~\\% at $\\sim$16~keV. From the upper kHz QPO,\nwe infer the magnetospheric disk radius to be $\\sim$17~km, corresponding to a\nmagnetic field strength of 0.35--1.27$~\\times~10^7$~G. The burst spectral\nevolution indicates Photospheric Radius Expansion (PRE) in five bursts,\nyielding a touchdown radius of 3.1--5.47~km. These bursts reached\nnear-Eddington luminosities, through which the distance of the source was\ncalculated to be 5.18--5.21~kpc. Two of the bursts show coherent oscillations\nat 362.81--363.93~Hz. The presence of twin kHz QPOs and coherent Burst\nOscillations allows us to provide two different estimates for the spin\nfrequency of the Neutron Star in the system, for the first time using\n\\textit{AstroSat}."
                },
                "authors": [
                    {
                        "name": "Anirudh Salgundi"
                    },
                    {
                        "name": "Suman Bala"
                    },
                    {
                        "name": "Gayathri Raman"
                    },
                    {
                        "name": "Utkarsh Pathak"
                    },
                    {
                        "name": "Varun Bhalerao"
                    }
                ],
                "author_detail": {
                    "name": "Varun Bhalerao"
                },
                "author": "Varun Bhalerao",
                "arxiv_comment": "18 pages, 13 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04105v3",
                "updated": "2024-12-09T16:36:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    36,
                    34,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-06T18:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    35,
                    32,
                    2,
                    311,
                    0
                ],
                "title": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis"
                },
                "summary": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve. We perform our study on two\nfronts. First, we pursue an understanding of precisely how a three-layer\ntransformer, trained from scratch and attains perfect test accuracy, solves\nthis problem. We are able to identify certain \"planning\" and \"reasoning\"\nmechanisms in the network that necessitate cooperation between the attention\nblocks to implement the desired logic. Second, we study how pretrained LLMs,\nnamely Mistral-7B and Gemma-2-9B, solve this problem. We characterize their\nreasoning circuits through causal intervention experiments, providing necessity\nand sufficiency evidence for the circuits. We find evidence suggesting that the\ntwo models' latent reasoning strategies are surprisingly similar, and\nhuman-like. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve. We perform our study on two\nfronts. First, we pursue an understanding of precisely how a three-layer\ntransformer, trained from scratch and attains perfect test accuracy, solves\nthis problem. We are able to identify certain \"planning\" and \"reasoning\"\nmechanisms in the network that necessitate cooperation between the attention\nblocks to implement the desired logic. Second, we study how pretrained LLMs,\nnamely Mistral-7B and Gemma-2-9B, solve this problem. We characterize their\nreasoning circuits through causal intervention experiments, providing necessity\nand sufficiency evidence for the circuits. We find evidence suggesting that the\ntwo models' latent reasoning strategies are surprisingly similar, and\nhuman-like. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason."
                },
                "authors": [
                    {
                        "name": "Guan Zhe Hong"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Enming Luo"
                    },
                    {
                        "name": "Cyrus Rashtchian"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02830v3",
                "updated": "2024-12-09T16:26:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    26,
                    9,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-03T20:52:35Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    52,
                    35,
                    1,
                    338,
                    0
                ],
                "title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language\n  Models"
                },
                "summary": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical."
                },
                "authors": [
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Junda Wang"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "24 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11255v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11255v5",
                "updated": "2024-12-09T16:15:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    15,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2023-11-19T06:50:52Z",
                "published_parsed": [
                    2023,
                    11,
                    19,
                    6,
                    50,
                    52,
                    6,
                    323,
                    0
                ],
                "title": "M$^{2}$UGen: Multi-modal Music Understanding and Generation with the\n  Power of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M$^{2}$UGen: Multi-modal Music Understanding and Generation with the\n  Power of Large Language Models"
                },
                "summary": "The current landscape of research leveraging large language models (LLMs) is\nexperiencing a surge. Many works harness the powerful reasoning capabilities of\nthese models to comprehend various modalities, such as text, speech, images,\nvideos, etc. They also utilize LLMs to understand human intention and generate\ndesired outputs like images, videos, and music. However, research that combines\nboth understanding and generation using LLMs is still limited and in its\nnascent stage. To address this gap, we introduce a Multi-modal Music\nUnderstanding and Generation (M$^{2}$UGen) framework that integrates LLM's\nabilities to comprehend and generate music for different modalities. The\nM$^{2}$UGen framework is purpose-built to unlock creative potential from\ndiverse sources of inspiration, encompassing music, image, and video through\nthe use of pretrained MERT, ViT, and ViViT models, respectively. To enable\nmusic generation, we explore the use of AudioLDM 2 and MusicGen. Bridging\nmulti-modal understanding and music generation is accomplished through the\nintegration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA\nmodel to generate extensive datasets that support text/image/video-to-music\ngeneration, facilitating the training of our M$^{2}$UGen framework. We conduct\na thorough evaluation of our proposed framework. The experimental results\ndemonstrate that our model achieves or surpasses the performance of the current\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current landscape of research leveraging large language models (LLMs) is\nexperiencing a surge. Many works harness the powerful reasoning capabilities of\nthese models to comprehend various modalities, such as text, speech, images,\nvideos, etc. They also utilize LLMs to understand human intention and generate\ndesired outputs like images, videos, and music. However, research that combines\nboth understanding and generation using LLMs is still limited and in its\nnascent stage. To address this gap, we introduce a Multi-modal Music\nUnderstanding and Generation (M$^{2}$UGen) framework that integrates LLM's\nabilities to comprehend and generate music for different modalities. The\nM$^{2}$UGen framework is purpose-built to unlock creative potential from\ndiverse sources of inspiration, encompassing music, image, and video through\nthe use of pretrained MERT, ViT, and ViViT models, respectively. To enable\nmusic generation, we explore the use of AudioLDM 2 and MusicGen. Bridging\nmulti-modal understanding and music generation is accomplished through the\nintegration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA\nmodel to generate extensive datasets that support text/image/video-to-music\ngeneration, facilitating the training of our M$^{2}$UGen framework. We conduct\na thorough evaluation of our proposed framework. The experimental results\ndemonstrate that our model achieves or surpasses the performance of the current\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Shansong Liu"
                    },
                    {
                        "name": "Atin Sakkeer Hussain"
                    },
                    {
                        "name": "Qilong Wu"
                    },
                    {
                        "name": "Chenshuo Sun"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11255v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11255v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06619v1",
                "updated": "2024-12-09T16:13:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    13,
                    17,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T16:13:17Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    13,
                    17,
                    0,
                    344,
                    0
                ],
                "title": "Copyright-Protected Language Generation via Adaptive Model Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copyright-Protected Language Generation via Adaptive Model Fusion"
                },
                "summary": "The risk of language models reproducing copyrighted material from their\ntraining data has led to the development of various protective measures. Among\nthese, inference-time strategies that impose constraints via post-processing\nhave shown promise in addressing the complexities of copyright regulation.\nHowever, they often incur prohibitive computational costs or suffer from\nperformance trade-offs. To overcome these limitations, we introduce\nCopyright-Protecting Model Fusion (CP-Fuse), a novel approach that combines\nmodels trained on disjoint sets of copyrighted material during inference. In\nparticular, CP-Fuse adaptively aggregates the model outputs to minimize the\nreproduction of copyrighted content, adhering to a crucial balancing property\nthat prevents the regurgitation of memorized data. Through extensive\nexperiments, we show that CP-Fuse significantly reduces the reproduction of\nprotected material without compromising the quality of text and code\ngeneration. Moreover, its post-hoc nature allows seamless integration with\nother protective measures, further enhancing copyright safeguards. Lastly, we\nshow that CP-Fuse is robust against common techniques for extracting training\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The risk of language models reproducing copyrighted material from their\ntraining data has led to the development of various protective measures. Among\nthese, inference-time strategies that impose constraints via post-processing\nhave shown promise in addressing the complexities of copyright regulation.\nHowever, they often incur prohibitive computational costs or suffer from\nperformance trade-offs. To overcome these limitations, we introduce\nCopyright-Protecting Model Fusion (CP-Fuse), a novel approach that combines\nmodels trained on disjoint sets of copyrighted material during inference. In\nparticular, CP-Fuse adaptively aggregates the model outputs to minimize the\nreproduction of copyrighted content, adhering to a crucial balancing property\nthat prevents the regurgitation of memorized data. Through extensive\nexperiments, we show that CP-Fuse significantly reduces the reproduction of\nprotected material without compromising the quality of text and code\ngeneration. Moreover, its post-hoc nature allows seamless integration with\nother protective measures, further enhancing copyright safeguards. Lastly, we\nshow that CP-Fuse is robust against common techniques for extracting training\ndata."
                },
                "authors": [
                    {
                        "name": "Javier Abad"
                    },
                    {
                        "name": "Konstantin Donhauser"
                    },
                    {
                        "name": "Francesco Pinto"
                    },
                    {
                        "name": "Fanny Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Yang"
                },
                "author": "Fanny Yang",
                "arxiv_comment": "47 pages, 21 Figures. arXiv admin note: substantial text overlap with\n  arXiv:2407.20105",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06617v1",
                "updated": "2024-12-09T16:09:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    9,
                    44,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T16:09:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    9,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "AI TrackMate: Finally, Someone Who Will Give Your Music More Than Just\n  \"Sounds Great!\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI TrackMate: Finally, Someone Who Will Give Your Music More Than Just\n  \"Sounds Great!\""
                },
                "summary": "The rise of \"bedroom producers\" has democratized music creation, while\nchallenging producers to objectively evaluate their work. To address this, we\npresent AI TrackMate, an LLM-based music chatbot designed to provide\nconstructive feedback on music productions. By combining LLMs' inherent musical\nknowledge with direct audio track analysis, AI TrackMate offers\nproduction-specific insights, distinguishing it from text-only approaches. Our\nframework integrates a Music Analysis Module, an LLM-Readable Music Report, and\nMusic Production-Oriented Feedback Instruction, creating a plug-and-play,\ntraining-free system compatible with various LLMs and adaptable to future\nadvancements. We demonstrate AI TrackMate's capabilities through an interactive\nweb interface and present findings from a pilot study with a music producer. By\nbridging AI capabilities with the needs of independent producers, AI TrackMate\noffers on-demand analytical feedback, potentially supporting the creative\nprocess and skill development in music production. This system addresses the\ngrowing demand for objective self-assessment tools in the evolving landscape of\nindependent music production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of \"bedroom producers\" has democratized music creation, while\nchallenging producers to objectively evaluate their work. To address this, we\npresent AI TrackMate, an LLM-based music chatbot designed to provide\nconstructive feedback on music productions. By combining LLMs' inherent musical\nknowledge with direct audio track analysis, AI TrackMate offers\nproduction-specific insights, distinguishing it from text-only approaches. Our\nframework integrates a Music Analysis Module, an LLM-Readable Music Report, and\nMusic Production-Oriented Feedback Instruction, creating a plug-and-play,\ntraining-free system compatible with various LLMs and adaptable to future\nadvancements. We demonstrate AI TrackMate's capabilities through an interactive\nweb interface and present findings from a pilot study with a music producer. By\nbridging AI capabilities with the needs of independent producers, AI TrackMate\noffers on-demand analytical feedback, potentially supporting the creative\nprocess and skill development in music production. This system addresses the\ngrowing demand for objective self-assessment tools in the evolving landscape of\nindependent music production."
                },
                "authors": [
                    {
                        "name": "Yi-Lin Jiang"
                    },
                    {
                        "name": "Chia-Ho Hsiung"
                    },
                    {
                        "name": "Yen-Tung Yeh"
                    },
                    {
                        "name": "Lu-Rong Chen"
                    },
                    {
                        "name": "Bo-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Bo-Yu Chen"
                },
                "author": "Bo-Yu Chen",
                "arxiv_comment": "Accepted for the NeurIPS 2024 Creative AI Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10645v2",
                "updated": "2024-12-09T16:01:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    1,
                    4,
                    0,
                    344,
                    0
                ],
                "published": "2024-10-14T15:54:33Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    54,
                    33,
                    0,
                    288,
                    0
                ],
                "title": "Macroscopic Quantum States and Universal Correlations in a\n  Disorder-Order Interface Propagating over a 1D Ground State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Macroscopic Quantum States and Universal Correlations in a\n  Disorder-Order Interface Propagating over a 1D Ground State"
                },
                "summary": "We consider translationally invariant quantum spin-$\\frac{1}{2}$ chains with\nlocal interactions and a discrete symmetry that is spontaneously broken at zero\ntemperature. We envision experimenters switching off the couplings between two\nparts of the system and preparing them in independent equilibrium states. One\nside of the chain settles into a symmetry-breaking ground state. When the\ncouplings are switched back on, time evolution ensues. We argue that in\nintegrable systems the front separating the ordered region recedes at the\nmaximal velocity of quasiparticle excitations over the ground state. We infer\nthat, generically, the order parameters should vary on a subdiffusive scale of\norder $t^{1/3}$, where $t$ is time, and their fluctuations should exhibit the\nsame scaling. Thus, the interfacial region exhibits full range correlations,\nindicating that it cannot be decomposed into nearly uncorrelated subsystems.\nUsing the transverse-field Ising chain as a case study, we demonstrate that all\norder parameters follow the same universal scaling functions. Through an\nanalysis of the skew information, we uncover that the breakdown of cluster\ndecomposition has a quantum contribution: each subsystem within the interfacial\nregion, with extent comparable to the region, exists in a macroscopic quantum\nstate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider translationally invariant quantum spin-$\\frac{1}{2}$ chains with\nlocal interactions and a discrete symmetry that is spontaneously broken at zero\ntemperature. We envision experimenters switching off the couplings between two\nparts of the system and preparing them in independent equilibrium states. One\nside of the chain settles into a symmetry-breaking ground state. When the\ncouplings are switched back on, time evolution ensues. We argue that in\nintegrable systems the front separating the ordered region recedes at the\nmaximal velocity of quasiparticle excitations over the ground state. We infer\nthat, generically, the order parameters should vary on a subdiffusive scale of\norder $t^{1/3}$, where $t$ is time, and their fluctuations should exhibit the\nsame scaling. Thus, the interfacial region exhibits full range correlations,\nindicating that it cannot be decomposed into nearly uncorrelated subsystems.\nUsing the transverse-field Ising chain as a case study, we demonstrate that all\norder parameters follow the same universal scaling functions. Through an\nanalysis of the skew information, we uncover that the breakdown of cluster\ndecomposition has a quantum contribution: each subsystem within the interfacial\nregion, with extent comparable to the region, exists in a macroscopic quantum\nstate."
                },
                "authors": [
                    {
                        "name": "Vanja Mari"
                    },
                    {
                        "name": "Florent Ferro"
                    },
                    {
                        "name": "Maurizio Fagotti"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Fagotti"
                },
                "author": "Maurizio Fagotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05270v2",
                "updated": "2024-12-09T16:01:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    1,
                    0,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-06T18:55:34Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    55,
                    34,
                    4,
                    341,
                    0
                ],
                "title": "APOLLO: SGD-like Memory, AdamW-level Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APOLLO: SGD-like Memory, AdamW-level Performance"
                },
                "summary": "Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization."
                },
                "authors": [
                    {
                        "name": "Hanqing Zhu"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Wenyan Cong"
                    },
                    {
                        "name": "Xi Liu"
                    },
                    {
                        "name": "Sem Park"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Bo Long"
                    },
                    {
                        "name": "David Z. Pan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Jinwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinwon Lee"
                },
                "author": "Jinwon Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06606v1",
                "updated": "2024-12-09T15:55:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    55,
                    20,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:55:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    55,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Vulnerability of Text-Matching in ML/AI Conference Reviewer Assignments\n  to Collusions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability of Text-Matching in ML/AI Conference Reviewer Assignments\n  to Collusions"
                },
                "summary": "In the peer review process of top-tier machine learning (ML) and artificial\nintelligence (AI) conferences, reviewers are assigned to papers through\nautomated methods. These assignment algorithms consider two main factors: (1)\nreviewers' expressed interests indicated by their bids for papers, and (2)\nreviewers' domain expertise inferred from the similarity between the text of\ntheir previously published papers and the submitted manuscripts. A significant\nchallenge these conferences face is the existence of collusion rings, where\ngroups of researchers manipulate the assignment process to review each other's\npapers, providing positive evaluations regardless of their actual quality. Most\nefforts to combat collusion rings have focused on preventing bid manipulation,\nunder the assumption that the text similarity component is secure. In this\npaper, we demonstrate that even in the absence of bidding, colluding reviewers\nand authors can exploit the machine learning based text-matching component of\nreviewer assignment used at top ML/AI venues to get assigned their target\npaper. We also highlight specific vulnerabilities within this system and offer\nsuggestions to enhance its robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the peer review process of top-tier machine learning (ML) and artificial\nintelligence (AI) conferences, reviewers are assigned to papers through\nautomated methods. These assignment algorithms consider two main factors: (1)\nreviewers' expressed interests indicated by their bids for papers, and (2)\nreviewers' domain expertise inferred from the similarity between the text of\ntheir previously published papers and the submitted manuscripts. A significant\nchallenge these conferences face is the existence of collusion rings, where\ngroups of researchers manipulate the assignment process to review each other's\npapers, providing positive evaluations regardless of their actual quality. Most\nefforts to combat collusion rings have focused on preventing bid manipulation,\nunder the assumption that the text similarity component is secure. In this\npaper, we demonstrate that even in the absence of bidding, colluding reviewers\nand authors can exploit the machine learning based text-matching component of\nreviewer assignment used at top ML/AI venues to get assigned their target\npaper. We also highlight specific vulnerabilities within this system and offer\nsuggestions to enhance its robustness."
                },
                "authors": [
                    {
                        "name": "Jhih-Yi"
                    },
                    {
                        "name": "Hsieh"
                    },
                    {
                        "name": "Aditi Raghunathan"
                    },
                    {
                        "name": "Nihar B. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Nihar B. Shah"
                },
                "arxiv_affiliation": "Janet",
                "author": "Nihar B. Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06603v1",
                "updated": "2024-12-09T15:53:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    53,
                    0,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:53:00Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    53,
                    0,
                    0,
                    344,
                    0
                ],
                "title": "Examining the Use and Impact of an AI Code Assistant on Developer\n  Productivity and Experience in the Enterprise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Use and Impact of an AI Code Assistant on Developer\n  Productivity and Experience in the Enterprise"
                },
                "summary": "AI assistants are being created to help software engineers conduct a variety\nof coding-related tasks, such as writing, documenting, and testing code. We\ndescribe the use of the watsonx Code Assistant (WCA), an LLM-powered coding\nassistant deployed internally within IBM. Through surveys of two user cohorts\n(N=669) and unmoderated usability testing (N=15), we examined developers'\nexperiences with WCA and its impact on their productivity. We learned about\ntheir motivations for using (or not using) WCA, we examined their expectations\nof its speed and quality, and we identified new considerations regarding\nownership of and responsibility for generated code. Our case study\ncharacterizes the impact of an LLM-powered assistant on developers' perceptions\nof productivity and it shows that although such tools do often provide net\nproductivity increases, these benefits may not always be experienced by all\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI assistants are being created to help software engineers conduct a variety\nof coding-related tasks, such as writing, documenting, and testing code. We\ndescribe the use of the watsonx Code Assistant (WCA), an LLM-powered coding\nassistant deployed internally within IBM. Through surveys of two user cohorts\n(N=669) and unmoderated usability testing (N=15), we examined developers'\nexperiences with WCA and its impact on their productivity. We learned about\ntheir motivations for using (or not using) WCA, we examined their expectations\nof its speed and quality, and we identified new considerations regarding\nownership of and responsibility for generated code. Our case study\ncharacterizes the impact of an LLM-powered assistant on developers' perceptions\nof productivity and it shows that although such tools do often provide net\nproductivity increases, these benefits may not always be experienced by all\nusers."
                },
                "authors": [
                    {
                        "name": "Justin D. Weisz"
                    },
                    {
                        "name": "Shraddha Kumar"
                    },
                    {
                        "name": "Michael Muller"
                    },
                    {
                        "name": "Karen-Ellen Browne"
                    },
                    {
                        "name": "Arielle Goldberg"
                    },
                    {
                        "name": "Ellice Heintze"
                    },
                    {
                        "name": "Shagun Bajpai"
                    }
                ],
                "author_detail": {
                    "name": "Shagun Bajpai"
                },
                "author": "Shagun Bajpai",
                "arxiv_comment": "21 pages, 3 figures. To be published in CHI EA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06593v1",
                "updated": "2024-12-09T15:45:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    45,
                    3,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:45:03Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    45,
                    3,
                    0,
                    344,
                    0
                ],
                "title": "Anchoring Bias in Large Language Models: An Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchoring Bias in Large Language Models: An Experimental Study"
                },
                "summary": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly\nadvanced artificial intelligence by enabling machines to generate and\ncomprehend human-like text. Despite their impressive capabilities, LLMs are not\nimmune to limitations, including various biases. While much research has\nexplored demographic biases, the cognitive biases in LLMs have not been equally\nscrutinized. This study delves into anchoring bias, a cognitive bias where\ninitial information disproportionately influences judgment. Utilizing an\nexperimental dataset, we examine how anchoring bias manifests in LLMs and\nverify the effectiveness of various mitigation strategies. Our findings\nhighlight the sensitivity of LLM responses to biased hints. At the same time,\nour experiments show that, to mitigate anchoring bias, one needs to collect\nhints from comprehensive angles to prevent the LLMs from being anchored to\nindividual pieces of information, while simple algorithms such as\nChain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection\nare not sufficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly\nadvanced artificial intelligence by enabling machines to generate and\ncomprehend human-like text. Despite their impressive capabilities, LLMs are not\nimmune to limitations, including various biases. While much research has\nexplored demographic biases, the cognitive biases in LLMs have not been equally\nscrutinized. This study delves into anchoring bias, a cognitive bias where\ninitial information disproportionately influences judgment. Utilizing an\nexperimental dataset, we examine how anchoring bias manifests in LLMs and\nverify the effectiveness of various mitigation strategies. Our findings\nhighlight the sensitivity of LLM responses to biased hints. At the same time,\nour experiments show that, to mitigate anchoring bias, one needs to collect\nhints from comprehensive angles to prevent the LLMs from being anchored to\nindividual pieces of information, while simple algorithms such as\nChain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection\nare not sufficient."
                },
                "authors": [
                    {
                        "name": "Jiaxu Lou"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxu Lou"
                },
                "author": "Jiaxu Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05374v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05374v4",
                "updated": "2024-12-09T15:39:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    39,
                    30,
                    0,
                    344,
                    0
                ],
                "published": "2024-02-08T03:12:25Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    3,
                    12,
                    25,
                    3,
                    39,
                    0
                ],
                "title": "CIC: A Framework for Culturally-Aware Image Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIC: A Framework for Culturally-Aware Image Captioning"
                },
                "summary": "Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, Culturally-aware Image Captioning (CIC), that\ngenerates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs.\nResources can be found at https://shane3606.github.io/cic..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, Culturally-aware Image Captioning (CIC), that\ngenerates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs.\nResources can be found at https://shane3606.github.io/cic.."
                },
                "authors": [
                    {
                        "name": "Youngsik Yun"
                    },
                    {
                        "name": "Jihie Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihie Kim"
                },
                "author": "Jihie Kim",
                "arxiv_doi": "10.24963/ijcai.2024/180",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/180",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.05374v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05374v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in IJCAI 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06575v1",
                "updated": "2024-12-09T15:28:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    28,
                    39,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:28:39Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    28,
                    39,
                    0,
                    344,
                    0
                ],
                "title": "Data Quality Enhancement on the Basis of Diversity with Large Language\n  Models for Text Classification: Uncovered, Difficult, and Noisy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Quality Enhancement on the Basis of Diversity with Large Language\n  Models for Text Classification: Uncovered, Difficult, and Noisy"
                },
                "summary": "In recent years, the use of large language models (LLMs) for text\nclassification has attracted widespread attention. Despite this, the\nclassification accuracy of LLMs has not yet universally surpassed that of\nsmaller models. LLMs can enhance their performance in text classification\nthrough fine-tuning. However, existing data quality research based on LLMs is\nchallenging to apply directly to solve text classification problems. To further\nimprove the performance of LLMs in classification tasks, this paper proposes a\ndata quality enhancement (DQE) method for text classification based on LLMs.\nThis method starts by using a greedy algorithm to select data, dividing the\ndataset into sampled and unsampled subsets, and then performing fine-tuning of\nthe LLMs using the sampled data. Subsequently, this model is used to predict\nthe outcomes for the unsampled data, categorizing incorrectly predicted data\ninto uncovered, difficult, and noisy data. Experimental results demonstrate\nthat our method effectively enhances the performance of LLMs in text\nclassification tasks and significantly improves training efficiency, saving\nnearly half of the training time. Our method has achieved state-of-the-art\nperformance in several open-source classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the use of large language models (LLMs) for text\nclassification has attracted widespread attention. Despite this, the\nclassification accuracy of LLMs has not yet universally surpassed that of\nsmaller models. LLMs can enhance their performance in text classification\nthrough fine-tuning. However, existing data quality research based on LLMs is\nchallenging to apply directly to solve text classification problems. To further\nimprove the performance of LLMs in classification tasks, this paper proposes a\ndata quality enhancement (DQE) method for text classification based on LLMs.\nThis method starts by using a greedy algorithm to select data, dividing the\ndataset into sampled and unsampled subsets, and then performing fine-tuning of\nthe LLMs using the sampled data. Subsequently, this model is used to predict\nthe outcomes for the unsampled data, categorizing incorrectly predicted data\ninto uncovered, difficult, and noisy data. Experimental results demonstrate\nthat our method effectively enhances the performance of LLMs in text\nclassification tasks and significantly improves training efficiency, saving\nnearly half of the training time. Our method has achieved state-of-the-art\nperformance in several open-source classification tasks."
                },
                "authors": [
                    {
                        "name": "Min Zeng"
                    },
                    {
                        "name": "Caiquan Liu"
                    },
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Li Xie"
                    },
                    {
                        "name": "Chen Sang"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "Accepted by COLING 2025(main, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06566v1",
                "updated": "2024-12-09T15:18:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    18,
                    4,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:18:04Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    18,
                    4,
                    0,
                    344,
                    0
                ],
                "title": "DEX: Data Channel Extension for Efficient CNN Inference on Tiny AI\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEX: Data Channel Extension for Efficient CNN Inference on Tiny AI\n  Accelerators"
                },
                "summary": "Tiny machine learning (TinyML) aims to run ML models on small devices and is\nincreasingly favored for its enhanced privacy, reduced latency, and low cost.\nRecently, the advent of tiny AI accelerators has revolutionized the TinyML\nfield by significantly enhancing hardware processing power. These accelerators,\nequipped with multiple parallel processors and dedicated per-processor memory\ninstances, offer substantial performance improvements over traditional\nmicrocontroller units (MCUs). However, their limited data memory often\nnecessitates downsampling input images, resulting in accuracy degradation. To\naddress this challenge, we propose Data channel EXtension (DEX), a novel\napproach for efficient CNN execution on tiny AI accelerators. DEX incorporates\nadditional spatial information from original images into input images through\npatch-wise even sampling and channel-wise stacking, effectively extending data\nacross input channels. By leveraging underutilized processors and data memory\nfor channel extension, DEX facilitates parallel execution without increasing\ninference latency. Our evaluation with four models and four datasets on tiny AI\naccelerators demonstrates that this simple idea improves accuracy on average by\n3.5%p while keeping the inference latency the same on the AI accelerator. The\nsource code is available at\nhttps://github.com/Nokia-Bell-Labs/data-channel-extension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny machine learning (TinyML) aims to run ML models on small devices and is\nincreasingly favored for its enhanced privacy, reduced latency, and low cost.\nRecently, the advent of tiny AI accelerators has revolutionized the TinyML\nfield by significantly enhancing hardware processing power. These accelerators,\nequipped with multiple parallel processors and dedicated per-processor memory\ninstances, offer substantial performance improvements over traditional\nmicrocontroller units (MCUs). However, their limited data memory often\nnecessitates downsampling input images, resulting in accuracy degradation. To\naddress this challenge, we propose Data channel EXtension (DEX), a novel\napproach for efficient CNN execution on tiny AI accelerators. DEX incorporates\nadditional spatial information from original images into input images through\npatch-wise even sampling and channel-wise stacking, effectively extending data\nacross input channels. By leveraging underutilized processors and data memory\nfor channel extension, DEX facilitates parallel execution without increasing\ninference latency. Our evaluation with four models and four datasets on tiny AI\naccelerators demonstrates that this simple idea improves accuracy on average by\n3.5%p while keeping the inference latency the same on the AI accelerator. The\nsource code is available at\nhttps://github.com/Nokia-Bell-Labs/data-channel-extension."
                },
                "authors": [
                    {
                        "name": "Taesik Gong"
                    },
                    {
                        "name": "Fahim Kawsar"
                    },
                    {
                        "name": "Chulhong Min"
                    }
                ],
                "author_detail": {
                    "name": "Chulhong Min"
                },
                "author": "Chulhong Min",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06564v1",
                "updated": "2024-12-09T15:17:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    17,
                    36,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:17:36Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    17,
                    36,
                    0,
                    344,
                    0
                ],
                "title": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering"
                },
                "summary": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research."
                },
                "authors": [
                    {
                        "name": "Matheus de Morais Lea"
                    },
                    {
                        "name": "Lucas Valena"
                    },
                    {
                        "name": "Reydne Santos"
                    },
                    {
                        "name": "Ronnie de Souza Santos"
                    }
                ],
                "author_detail": {
                    "name": "Ronnie de Souza Santos"
                },
                "author": "Ronnie de Souza Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06549v1",
                "updated": "2024-12-09T14:59:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    59,
                    27,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T14:59:27Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    59,
                    27,
                    0,
                    344,
                    0
                ],
                "title": "Prediction of Occluded Pedestrians in Road Scenes using Human-like\n  Reasoning: Insights from the OccluRoads Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction of Occluded Pedestrians in Road Scenes using Human-like\n  Reasoning: Insights from the OccluRoads Dataset"
                },
                "summary": "Pedestrian detection is a critical task in autonomous driving, aimed at\nenhancing safety and reducing risks on the road. Over recent years, significant\nadvancements have been made in improving detection performance. However, these\nachievements still fall short of human perception, particularly in cases\ninvolving occluded pedestrians, especially entirely invisible ones. In this\nwork, we present the Occlusion-Rich Road Scenes with Pedestrians (OccluRoads)\ndataset, which features a diverse collection of road scenes with partially and\nfully occluded pedestrians in both real and virtual environments. All scenes\nare meticulously labeled and enriched with contextual information that\nencapsulates human perception in such scenarios. Using this dataset, we\ndeveloped a pipeline to predict the presence of occluded pedestrians,\nleveraging Knowledge Graph (KG), Knowledge Graph Embedding (KGE), and a\nBayesian inference process. Our approach achieves a F1 score of 0.91,\nrepresenting an improvement of up to 42% compared to traditional machine\nlearning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedestrian detection is a critical task in autonomous driving, aimed at\nenhancing safety and reducing risks on the road. Over recent years, significant\nadvancements have been made in improving detection performance. However, these\nachievements still fall short of human perception, particularly in cases\ninvolving occluded pedestrians, especially entirely invisible ones. In this\nwork, we present the Occlusion-Rich Road Scenes with Pedestrians (OccluRoads)\ndataset, which features a diverse collection of road scenes with partially and\nfully occluded pedestrians in both real and virtual environments. All scenes\nare meticulously labeled and enriched with contextual information that\nencapsulates human perception in such scenarios. Using this dataset, we\ndeveloped a pipeline to predict the presence of occluded pedestrians,\nleveraging Knowledge Graph (KG), Knowledge Graph Embedding (KGE), and a\nBayesian inference process. Our approach achieves a F1 score of 0.91,\nrepresenting an improvement of up to 42% compared to traditional machine\nlearning models."
                },
                "authors": [
                    {
                        "name": "Melo Castillo Angie Nataly"
                    },
                    {
                        "name": "Martin Serrano Sergio"
                    },
                    {
                        "name": "Salinas Carlota"
                    },
                    {
                        "name": "Sotelo Miguel Angel"
                    }
                ],
                "author_detail": {
                    "name": "Sotelo Miguel Angel"
                },
                "author": "Sotelo Miguel Angel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06543v1",
                "updated": "2024-12-09T14:54:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    54,
                    44,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T14:54:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    54,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "Challenges and Opportunities for Visual Analytics in Jurisprudence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges and Opportunities for Visual Analytics in Jurisprudence"
                },
                "summary": "Exploring, analyzing, and interpreting law can be tedious and challenging,\neven for legal scholars, since legal texts contain domain-specific language,\nrequire knowledge of tacit legal concepts, and are sometimes intentionally\nambiguous. In related, text-based domains, Visual Analytics (VA) and large\nlanguage models (LLMs) have become essential for working with documents as they\nsupport data navigation, knowledge representation, and analytical reasoning.\nHowever, legal scholars must simultaneously manage hierarchical information\nsources, leverage implicit domain knowledge, and document complex reasoning\nprocesses, which are neither adequately accessible through existing VA designs\nnor sufficiently supported by current LLMs. To address the needs of legal\nscholars, we identify previously unexamined challenges and opportunities when\napplying VA to jurisprudence. We conducted semi-structured interviews with nine\nexperts from the legal domain and found that they lacked the ability to\narticulate their tacit domain knowledge as explicit, machine-interpretable\nknowledge. Hence, we propose leveraging interactive visualization for this\narticulation, teaching the machine relevant semantic relationships between\nlegal documents. These relationships inform the predictions of VA and LLMs,\nfacilitating the navigation between the hierarchies of legal document\ncollections. The enhanced navigation can uncover additional relevant legal\ndocuments, reinforcing the legal reasoning process by generating legal insights\nthat reflect internalized, tacit domain knowledge. In summary, we provide a\nhuman-is-the-loop VA workflow for jurisprudence that recognizes tacit domain\nknowledge as essential for deriving legal insights. More broadly, we compare\nthis workflow with related text-based research practices, revealing research\ngaps and guiding visualization researchers in knowledge-assisted VA for law and\nbeyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring, analyzing, and interpreting law can be tedious and challenging,\neven for legal scholars, since legal texts contain domain-specific language,\nrequire knowledge of tacit legal concepts, and are sometimes intentionally\nambiguous. In related, text-based domains, Visual Analytics (VA) and large\nlanguage models (LLMs) have become essential for working with documents as they\nsupport data navigation, knowledge representation, and analytical reasoning.\nHowever, legal scholars must simultaneously manage hierarchical information\nsources, leverage implicit domain knowledge, and document complex reasoning\nprocesses, which are neither adequately accessible through existing VA designs\nnor sufficiently supported by current LLMs. To address the needs of legal\nscholars, we identify previously unexamined challenges and opportunities when\napplying VA to jurisprudence. We conducted semi-structured interviews with nine\nexperts from the legal domain and found that they lacked the ability to\narticulate their tacit domain knowledge as explicit, machine-interpretable\nknowledge. Hence, we propose leveraging interactive visualization for this\narticulation, teaching the machine relevant semantic relationships between\nlegal documents. These relationships inform the predictions of VA and LLMs,\nfacilitating the navigation between the hierarchies of legal document\ncollections. The enhanced navigation can uncover additional relevant legal\ndocuments, reinforcing the legal reasoning process by generating legal insights\nthat reflect internalized, tacit domain knowledge. In summary, we provide a\nhuman-is-the-loop VA workflow for jurisprudence that recognizes tacit domain\nknowledge as essential for deriving legal insights. More broadly, we compare\nthis workflow with related text-based research practices, revealing research\ngaps and guiding visualization researchers in knowledge-assisted VA for law and\nbeyond."
                },
                "authors": [
                    {
                        "name": "Daniel Frst"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "name": "Daniel A. Keim"
                    },
                    {
                        "name": "Maximilian T. Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian T. Fischer"
                },
                "author": "Maximilian T. Fischer",
                "arxiv_comment": "13 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06540v1",
                "updated": "2024-12-09T14:51:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    51,
                    26,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T14:51:26Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    51,
                    26,
                    0,
                    344,
                    0
                ],
                "title": "Sloth: scaling laws for LLM skills to predict multi-benchmark\n  performance across families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sloth: scaling laws for LLM skills to predict multi-benchmark\n  performance across families"
                },
                "summary": "Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training\nconfigurations and data processing across model families lead to significant\nvariations in benchmark performance, making it difficult for a single scaling\nlaw to generalize across all LLMs. On the other hand, training family-specific\nscaling laws requires training models of varying sizes for every family. In\nthis work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a\nnovel scaling law that leverages publicly available benchmark data and assumes\nLLM performance is driven by low-dimensional latent skills, such as reasoning\nand instruction following. These latent skills are influenced by computational\nresources like model size and training tokens but with varying efficiencies\nacross model families. Sloth exploits correlations across benchmarks to provide\nmore accurate and interpretable predictions while alleviating the need to train\nmultiple LLMs per family. We present both theoretical results on parameter\nidentification and empirical evaluations on 12 prominent benchmarks, from Open\nLLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance\nefficiently and offers insights into scaling behaviors for downstream tasks\nsuch as coding and emotional intelligence applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training\nconfigurations and data processing across model families lead to significant\nvariations in benchmark performance, making it difficult for a single scaling\nlaw to generalize across all LLMs. On the other hand, training family-specific\nscaling laws requires training models of varying sizes for every family. In\nthis work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a\nnovel scaling law that leverages publicly available benchmark data and assumes\nLLM performance is driven by low-dimensional latent skills, such as reasoning\nand instruction following. These latent skills are influenced by computational\nresources like model size and training tokens but with varying efficiencies\nacross model families. Sloth exploits correlations across benchmarks to provide\nmore accurate and interpretable predictions while alleviating the need to train\nmultiple LLMs per family. We present both theoretical results on parameter\nidentification and empirical evaluations on 12 prominent benchmarks, from Open\nLLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance\nefficiently and offers insights into scaling behaviors for downstream tasks\nsuch as coding and emotional intelligence applications."
                },
                "authors": [
                    {
                        "name": "Felipe Maia Polo"
                    },
                    {
                        "name": "Seamus Somerstep"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Yuekai Sun"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04413v2",
                "updated": "2024-12-09T14:46:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    46,
                    9,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-05T18:33:59Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    33,
                    59,
                    3,
                    340,
                    0
                ],
                "title": "Efficient Task Grouping Through Samplewise Optimisation Landscape\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Task Grouping Through Samplewise Optimisation Landscape\n  Analysis"
                },
                "summary": "Shared training approaches, such as multi-task learning (MTL) and\ngradient-based meta-learning, are widely used in various machine learning\napplications, but they often suffer from negative transfer, leading to\nperformance degradation in specific tasks. While several optimisation\ntechniques have been developed to mitigate this issue for pre-selected task\ncohorts, identifying optimal task combinations for joint learning - known as\ntask grouping - remains underexplored and computationally challenging due to\nthe exponential growth in task combinations and the need for extensive training\nand evaluation cycles. This paper introduces an efficient task grouping\nframework designed to reduce these overwhelming computational demands of the\nexisting methods. The proposed framework infers pairwise task similarities\nthrough a sample-wise optimisation landscape analysis, eliminating the need for\nthe shared model training required to infer task similarities in existing\nmethods. With task similarities acquired, a graph-based clustering algorithm is\nemployed to pinpoint near-optimal task groups, providing an approximate yet\nefficient and effective solution to the originally NP-hard problem. Empirical\nassessments conducted on 8 different datasets highlight the effectiveness of\nthe proposed framework, revealing a five-fold speed enhancement compared to\nprevious state-of-the-art methods. Moreover, the framework consistently\ndemonstrates comparable performance, confirming its remarkable efficiency and\neffectiveness in task grouping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared training approaches, such as multi-task learning (MTL) and\ngradient-based meta-learning, are widely used in various machine learning\napplications, but they often suffer from negative transfer, leading to\nperformance degradation in specific tasks. While several optimisation\ntechniques have been developed to mitigate this issue for pre-selected task\ncohorts, identifying optimal task combinations for joint learning - known as\ntask grouping - remains underexplored and computationally challenging due to\nthe exponential growth in task combinations and the need for extensive training\nand evaluation cycles. This paper introduces an efficient task grouping\nframework designed to reduce these overwhelming computational demands of the\nexisting methods. The proposed framework infers pairwise task similarities\nthrough a sample-wise optimisation landscape analysis, eliminating the need for\nthe shared model training required to infer task similarities in existing\nmethods. With task similarities acquired, a graph-based clustering algorithm is\nemployed to pinpoint near-optimal task groups, providing an approximate yet\nefficient and effective solution to the originally NP-hard problem. Empirical\nassessments conducted on 8 different datasets highlight the effectiveness of\nthe proposed framework, revealing a five-fold speed enhancement compared to\nprevious state-of-the-art methods. Moreover, the framework consistently\ndemonstrates comparable performance, confirming its remarkable efficiency and\neffectiveness in task grouping."
                },
                "authors": [
                    {
                        "name": "Anshul Thakur"
                    },
                    {
                        "name": "Yichen Huang"
                    },
                    {
                        "name": "Soheila Molaei"
                    },
                    {
                        "name": "Yujiang Wang"
                    },
                    {
                        "name": "David A. Clifton"
                    }
                ],
                "author_detail": {
                    "name": "David A. Clifton"
                },
                "author": "David A. Clifton",
                "arxiv_comment": "Under review at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05691v2",
                "updated": "2024-12-09T14:43:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    43,
                    56,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-09T11:41:27Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    11,
                    41,
                    27,
                    3,
                    130,
                    0
                ],
                "title": "StableMoFusion: Towards Robust and Efficient Diffusion-based Motion\n  Generation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableMoFusion: Towards Robust and Efficient Diffusion-based Motion\n  Generation Framework"
                },
                "summary": "Thanks to the powerful generative capacity of diffusion models, recent years\nhave witnessed rapid progress in human motion generation. Existing\ndiffusion-based methods employ disparate network architectures and training\nstrategies. The effect of the design of each component is still unclear. In\naddition, the iterative denoising process consumes considerable computational\noverhead, which is prohibitive for real-time scenarios such as virtual\ncharacters and humanoid robots. For this reason, we first conduct a\ncomprehensive investigation into network architectures, training strategies,\nand inference processs. Based on the profound analysis, we tailor each\ncomponent for efficient high-quality human motion generation. Despite the\npromising performance, the tailored model still suffers from foot skating which\nis an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we\nidentify foot-ground contact and correct foot motions along the denoising\nprocess. By organically combining these well-designed components together, we\npresent StableMoFusion, a robust and efficient framework for human motion\ngeneration. Extensive experimental results show that our StableMoFusion\nperforms favorably against current state-of-the-art methods. Project page:\nhttps://h-y1heng.github.io/StableMoFusion-page/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to the powerful generative capacity of diffusion models, recent years\nhave witnessed rapid progress in human motion generation. Existing\ndiffusion-based methods employ disparate network architectures and training\nstrategies. The effect of the design of each component is still unclear. In\naddition, the iterative denoising process consumes considerable computational\noverhead, which is prohibitive for real-time scenarios such as virtual\ncharacters and humanoid robots. For this reason, we first conduct a\ncomprehensive investigation into network architectures, training strategies,\nand inference processs. Based on the profound analysis, we tailor each\ncomponent for efficient high-quality human motion generation. Despite the\npromising performance, the tailored model still suffers from foot skating which\nis an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we\nidentify foot-ground contact and correct foot motions along the denoising\nprocess. By organically combining these well-designed components together, we\npresent StableMoFusion, a robust and efficient framework for human motion\ngeneration. Extensive experimental results show that our StableMoFusion\nperforms favorably against current state-of-the-art methods. Project page:\nhttps://h-y1heng.github.io/StableMoFusion-page/"
                },
                "authors": [
                    {
                        "name": "Yiheng Huang"
                    },
                    {
                        "name": "Hui Yang"
                    },
                    {
                        "name": "Chuanchen Luo"
                    },
                    {
                        "name": "Yuxi Wang"
                    },
                    {
                        "name": "Shibiao Xu"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Man Zhang"
                    },
                    {
                        "name": "Junran Peng"
                    }
                ],
                "author_detail": {
                    "name": "Junran Peng"
                },
                "author": "Junran Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18659v5",
                "updated": "2024-12-09T14:41:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    41,
                    4,
                    0,
                    344,
                    0
                ],
                "published": "2024-02-28T19:09:08Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    9,
                    8,
                    2,
                    59,
                    0
                ],
                "title": "Large Language Models and Games: A Survey and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Games: A Survey and Roadmap"
                },
                "summary": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field."
                },
                "authors": [
                    {
                        "name": "Roberto Gallotta"
                    },
                    {
                        "name": "Graham Todd"
                    },
                    {
                        "name": "Marvin Zammit"
                    },
                    {
                        "name": "Sam Earle"
                    },
                    {
                        "name": "Antonios Liapis"
                    },
                    {
                        "name": "Julian Togelius"
                    },
                    {
                        "name": "Georgios N. Yannakakis"
                    }
                ],
                "author_detail": {
                    "name": "Georgios N. Yannakakis"
                },
                "author": "Georgios N. Yannakakis",
                "arxiv_doi": "10.1109/TG.2024.3461510",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TG.2024.3461510",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.18659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the IEEE Transactions on Games (19 pages,\n  6 figures)",
                "arxiv_journal_ref": "IEEE Transactions on Games, 2024 (early access)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14229v2",
                "updated": "2024-12-09T14:40:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    40,
                    51,
                    0,
                    344,
                    0
                ],
                "published": "2024-07-19T11:57:34Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    11,
                    57,
                    34,
                    4,
                    201,
                    0
                ],
                "title": "Words2Contact: Identifying Support Contacts from Verbal Instructions\n  Using Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words2Contact: Identifying Support Contacts from Verbal Instructions\n  Using Foundation Models"
                },
                "summary": "This paper presents Words2Contact, a language-guided multi-contact placement\npipeline leveraging large language models and vision language models. Our\nmethod is a key component for language-assisted teleoperation and human-robot\ncooperation, where human operators can instruct the robots where to place their\nsupport contacts before whole-body reaching or manipulation using natural\nlanguage. Words2Contact transforms the verbal instructions of a human operator\ninto contact placement predictions; it also deals with iterative corrections,\nuntil the human is satisfied with the contact location identified in the\nrobot's field of view. We benchmark state-of-the-art LLMs and VLMs for size and\nperformance in contact prediction. We demonstrate the effectiveness of the\niterative correction process, showing that users, even naive, quickly learn how\nto instruct the system to obtain accurate locations. Finally, we validate\nWords2Contact in real-world experiments with the Talos humanoid robot,\ninstructed by human operators to place support contacts on different locations\nand surfaces to avoid falling when reaching for distant objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Words2Contact, a language-guided multi-contact placement\npipeline leveraging large language models and vision language models. Our\nmethod is a key component for language-assisted teleoperation and human-robot\ncooperation, where human operators can instruct the robots where to place their\nsupport contacts before whole-body reaching or manipulation using natural\nlanguage. Words2Contact transforms the verbal instructions of a human operator\ninto contact placement predictions; it also deals with iterative corrections,\nuntil the human is satisfied with the contact location identified in the\nrobot's field of view. We benchmark state-of-the-art LLMs and VLMs for size and\nperformance in contact prediction. We demonstrate the effectiveness of the\niterative correction process, showing that users, even naive, quickly learn how\nto instruct the system to obtain accurate locations. Finally, we validate\nWords2Contact in real-world experiments with the Talos humanoid robot,\ninstructed by human operators to place support contacts on different locations\nand surfaces to avoid falling when reaching for distant objects."
                },
                "authors": [
                    {
                        "name": "Dionis Totsila"
                    },
                    {
                        "name": "Quentin Rouxel"
                    },
                    {
                        "name": "Jean-Baptiste Mouret"
                    },
                    {
                        "name": "Serena Ivaldi"
                    }
                ],
                "author_detail": {
                    "name": "Serena Ivaldi"
                },
                "author": "Serena Ivaldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00929v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00929v3",
                "updated": "2024-12-09T14:30:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    30,
                    11,
                    0,
                    344,
                    0
                ],
                "published": "2024-04-01T05:13:56Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    5,
                    13,
                    56,
                    0,
                    92,
                    0
                ],
                "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and\n  Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and\n  Bias"
                },
                "summary": "Based on the foundation of Large Language Models (LLMs), Multilingual LLMs\n(MLLMs) have been developed to address the challenges faced in multilingual\nnatural language processing, hoping to achieve knowledge transfer from\nhigh-resource languages to low-resource languages. However, significant\nlimitations and challenges still exist, such as language imbalance,\nmultilingual alignment, and inherent bias. In this paper, we aim to provide a\ncomprehensive analysis of MLLMs, delving deeply into discussions surrounding\nthese critical issues. First of all, we start by presenting an overview of\nMLLMs, covering their evolutions, key techniques, and multilingual capacities.\nSecondly, we explore the multilingual training corpora of MLLMs and the\nmultilingual datasets oriented for downstream tasks that are crucial to enhance\nthe cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art\nstudies of multilingual representations and investigate whether the current\nMLLMs can learn a universal language representation. Fourthly, we discuss bias\non MLLMs, including its categories, evaluation metrics, and debiasing\ntechniques. Finally, we discuss existing challenges and point out promising\nresearch directions of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the foundation of Large Language Models (LLMs), Multilingual LLMs\n(MLLMs) have been developed to address the challenges faced in multilingual\nnatural language processing, hoping to achieve knowledge transfer from\nhigh-resource languages to low-resource languages. However, significant\nlimitations and challenges still exist, such as language imbalance,\nmultilingual alignment, and inherent bias. In this paper, we aim to provide a\ncomprehensive analysis of MLLMs, delving deeply into discussions surrounding\nthese critical issues. First of all, we start by presenting an overview of\nMLLMs, covering their evolutions, key techniques, and multilingual capacities.\nSecondly, we explore the multilingual training corpora of MLLMs and the\nmultilingual datasets oriented for downstream tasks that are crucial to enhance\nthe cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art\nstudies of multilingual representations and investigate whether the current\nMLLMs can learn a universal language representation. Fourthly, we discuss bias\non MLLMs, including its categories, evaluation metrics, and debiasing\ntechniques. Finally, we discuss existing challenges and point out promising\nresearch directions of MLLMs."
                },
                "authors": [
                    {
                        "name": "Yuemei Xu"
                    },
                    {
                        "name": "Ling Hu"
                    },
                    {
                        "name": "Jiayi Zhao"
                    },
                    {
                        "name": "Zihan Qiu"
                    },
                    {
                        "name": "Kexin XU"
                    },
                    {
                        "name": "Yuqi Ye"
                    },
                    {
                        "name": "Hanwen Gu"
                    }
                ],
                "author_detail": {
                    "name": "Hanwen Gu"
                },
                "author": "Hanwen Gu",
                "arxiv_doi": "10.1007/s11704-024-40579-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11704-024-40579-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.00929v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00929v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40579-4}",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04211v2",
                "updated": "2024-12-09T14:28:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    28,
                    38,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-05T14:46:40Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    46,
                    40,
                    3,
                    340,
                    0
                ],
                "title": "BEACON: JWST NIRCam Pure-parallel Imaging Survey. I. Survey Design and\n  Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEACON: JWST NIRCam Pure-parallel Imaging Survey. I. Survey Design and\n  Initial Results"
                },
                "summary": "We introduce the Bias-free Extragalactic Analysis for Cosmic Origins with\nNIRCam (BEACON) survey, a JWST Cycle2 program allocated up to 600 pure-parallel\nhours of observations. BEACON explores high-latitude areas of the sky with\nJWST/NIRCam over $\\sim100$ independent sightlines, totaling $\\sim0.3$deg$^2$,\nreaching a median F444W depth of $\\approx28.2$AB mag (5$\\sigma$). Based on\nexisting JWST observations in legacy fields, we estimate that BEACON will\nphotometrically identify 25--150 galaxies at $z>10$ and 500--1000 at\n$z\\sim7$--10 uniquely enabled by an efficient multiple filter configuration\nspanning $0.9$--5.0$\\mu$m. The expected sample size of $z>10$ galaxies will\nallow us to obtain robust number density estimates and to discriminate between\ndifferent models of early star formation. In this paper, we present an overview\nof the survey design and initial results using the first 19 fields. We present\n129 galaxy candidates at $z>7$ identified in those fields, including 11\ngalaxies at $z>10$ and several UV-luminous ($M_{\\rm UV}<-21$mag) galaxies at\n$z\\sim8$. The number densities of $z<13$ galaxies inferred from the initial\nfields are overall consistent with those in the literature. Despite reaching a\nconsiderably large volume ($\\sim10^5$Mpc$^3$), however, we find no galaxy\ncandidates at $z>13$, providing us with a complimentary insight into early\ngalaxy evolution with minimal cosmic variance. We publish imaging and catalog\ndata products for these initial fields. Upon survey completion, all BEACON data\nwill be coherently processed and distributed to the community along with\ncatalogs for redshift and other physical quantities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Bias-free Extragalactic Analysis for Cosmic Origins with\nNIRCam (BEACON) survey, a JWST Cycle2 program allocated up to 600 pure-parallel\nhours of observations. BEACON explores high-latitude areas of the sky with\nJWST/NIRCam over $\\sim100$ independent sightlines, totaling $\\sim0.3$deg$^2$,\nreaching a median F444W depth of $\\approx28.2$AB mag (5$\\sigma$). Based on\nexisting JWST observations in legacy fields, we estimate that BEACON will\nphotometrically identify 25--150 galaxies at $z>10$ and 500--1000 at\n$z\\sim7$--10 uniquely enabled by an efficient multiple filter configuration\nspanning $0.9$--5.0$\\mu$m. The expected sample size of $z>10$ galaxies will\nallow us to obtain robust number density estimates and to discriminate between\ndifferent models of early star formation. In this paper, we present an overview\nof the survey design and initial results using the first 19 fields. We present\n129 galaxy candidates at $z>7$ identified in those fields, including 11\ngalaxies at $z>10$ and several UV-luminous ($M_{\\rm UV}<-21$mag) galaxies at\n$z\\sim8$. The number densities of $z<13$ galaxies inferred from the initial\nfields are overall consistent with those in the literature. Despite reaching a\nconsiderably large volume ($\\sim10^5$Mpc$^3$), however, we find no galaxy\ncandidates at $z>13$, providing us with a complimentary insight into early\ngalaxy evolution with minimal cosmic variance. We publish imaging and catalog\ndata products for these initial fields. Upon survey completion, all BEACON data\nwill be coherently processed and distributed to the community along with\ncatalogs for redshift and other physical quantities."
                },
                "authors": [
                    {
                        "name": "Takahiro Morishita"
                    },
                    {
                        "name": "Charlotte A. Mason"
                    },
                    {
                        "name": "Kimi C. Kreilgaard"
                    },
                    {
                        "name": "Michele Trenti"
                    },
                    {
                        "name": "Tommaso Treu"
                    },
                    {
                        "name": "Benedetta Vulcani"
                    },
                    {
                        "name": "Yechi Zhang"
                    },
                    {
                        "name": "Abdurro'uf"
                    },
                    {
                        "name": "Anahita Alavi"
                    },
                    {
                        "name": "Hakim Atek"
                    },
                    {
                        "name": "Yannick Bahe"
                    },
                    {
                        "name": "Marusa Bradac"
                    },
                    {
                        "name": "Larry D. Bradley"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Dan Coe"
                    },
                    {
                        "name": "James Colbert"
                    },
                    {
                        "name": "Viola Gelli"
                    },
                    {
                        "name": "Matthew J. Hayes"
                    },
                    {
                        "name": "Tucker Jones"
                    },
                    {
                        "name": "Tadayuki Kodama"
                    },
                    {
                        "name": "Nicha Leethochawalit"
                    },
                    {
                        "name": "Zhaoran Liu"
                    },
                    {
                        "name": "Matthew A. Malkan"
                    },
                    {
                        "name": "Vihang Mehta"
                    },
                    {
                        "name": "Benjamin Metha"
                    },
                    {
                        "name": "Andrew B. Newman"
                    },
                    {
                        "name": "Marc Rafelski"
                    },
                    {
                        "name": "Guido Roberts-Borsani"
                    },
                    {
                        "name": "Michael J. Rutkowski"
                    },
                    {
                        "name": "Claudia Scarlata"
                    },
                    {
                        "name": "Massimo Stiavelli"
                    },
                    {
                        "name": "Ryo A. Sutanto"
                    },
                    {
                        "name": "Kosuke Takahashi"
                    },
                    {
                        "name": "Harry I. Teplitz"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "Submitted to ApJ; DR1 data release will be made on the team website\n  (https://beacon-jwst.github.io); Fig. 8 has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01369v2",
                "updated": "2024-12-09T14:26:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    26,
                    42,
                    0,
                    344,
                    0
                ],
                "published": "2024-09-02T16:48:57Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    16,
                    48,
                    57,
                    0,
                    246,
                    0
                ],
                "title": "Imitating Language via Scalable Inverse Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitating Language via Scalable Inverse Reinforcement Learning"
                },
                "summary": "The majority of language model training builds on imitation learning. It\ncovers pretraining, supervised fine-tuning, and affects the starting conditions\nfor reinforcement learning from human feedback (RLHF). The simplicity and\nscalability of maximum likelihood estimation (MLE) for next token prediction\nled to its role as predominant paradigm. However, the broader field of\nimitation learning can more effectively utilize the sequential structure\nunderlying autoregressive generation. We focus on investigating the inverse\nreinforcement learning (IRL) perspective to imitation, extracting rewards and\ndirectly optimizing sequences instead of individual token likelihoods and\nevaluate its benefits for fine-tuning large language models. We provide a new\nangle, reformulating inverse soft-Q-learning as a temporal difference\nregularized extension of MLE. This creates a principled connection between MLE\nand IRL and allows trading off added complexity with increased performance and\ndiversity of generations in the supervised fine-tuning (SFT) setting. We find\nclear advantages for IRL-based imitation, in particular for retaining diversity\nwhile maximizing task performance, rendering IRL a strong alternative on fixed\nSFT datasets even without online data generation. Our analysis of IRL-extracted\nreward functions further indicates benefits for more robust reward functions\nvia tighter integration of supervised and preference-based LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of language model training builds on imitation learning. It\ncovers pretraining, supervised fine-tuning, and affects the starting conditions\nfor reinforcement learning from human feedback (RLHF). The simplicity and\nscalability of maximum likelihood estimation (MLE) for next token prediction\nled to its role as predominant paradigm. However, the broader field of\nimitation learning can more effectively utilize the sequential structure\nunderlying autoregressive generation. We focus on investigating the inverse\nreinforcement learning (IRL) perspective to imitation, extracting rewards and\ndirectly optimizing sequences instead of individual token likelihoods and\nevaluate its benefits for fine-tuning large language models. We provide a new\nangle, reformulating inverse soft-Q-learning as a temporal difference\nregularized extension of MLE. This creates a principled connection between MLE\nand IRL and allows trading off added complexity with increased performance and\ndiversity of generations in the supervised fine-tuning (SFT) setting. We find\nclear advantages for IRL-based imitation, in particular for retaining diversity\nwhile maximizing task performance, rendering IRL a strong alternative on fixed\nSFT datasets even without online data generation. Our analysis of IRL-extracted\nreward functions further indicates benefits for more robust reward functions\nvia tighter integration of supervised and preference-based LLM post-training."
                },
                "authors": [
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Michael Bloesch"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Arun Ahuja"
                    },
                    {
                        "name": "Jorg Bornschein"
                    },
                    {
                        "name": "Sandy Huang"
                    },
                    {
                        "name": "Artem Sokolov"
                    },
                    {
                        "name": "Matt Barnes"
                    },
                    {
                        "name": "Guillaume Desjardins"
                    },
                    {
                        "name": "Alex Bewley"
                    },
                    {
                        "name": "Sarah Maria Elisabeth Bechtle"
                    },
                    {
                        "name": "Jost Tobias Springenberg"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Matthieu Geist"
                    },
                    {
                        "name": "Martin Riedmiller"
                    }
                ],
                "author_detail": {
                    "name": "Martin Riedmiller"
                },
                "author": "Martin Riedmiller",
                "arxiv_comment": "Published at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06512v1",
                "updated": "2024-12-09T14:14:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    14,
                    21,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T14:14:21Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    14,
                    21,
                    0,
                    344,
                    0
                ],
                "title": "The Fusion of Large Language Models and Formal Methods for Trustworthy\n  AI Agents: A Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fusion of Large Language Models and Formal Methods for Trustworthy\n  AI Agents: A Roadmap"
                },
                "summary": "Large Language Models (LLMs) have emerged as a transformative AI paradigm,\nprofoundly influencing daily life through their exceptional language\nunderstanding and contextual generation capabilities. Despite their remarkable\nperformance, LLMs face a critical challenge: the propensity to produce\nunreliable outputs due to the inherent limitations of their learning-based\nnature. Formal methods (FMs), on the other hand, are a well-established\ncomputation paradigm that provides mathematically rigorous techniques for\nmodeling, specifying, and verifying the correctness of systems. FMs have been\nextensively applied in mission-critical software engineering, embedded systems,\nand cybersecurity. However, the primary challenge impeding the deployment of\nFMs in real-world settings lies in their steep learning curves, the absence of\nuser-friendly interfaces, and issues with efficiency and adaptability.\n  This position paper outlines a roadmap for advancing the next generation of\ntrustworthy AI systems by leveraging the mutual enhancement of LLMs and FMs.\nFirst, we illustrate how FMs, including reasoning and certification techniques,\ncan help LLMs generate more reliable and formally certified outputs.\nSubsequently, we highlight how the advanced learning capabilities and\nadaptability of LLMs can significantly enhance the usability, efficiency, and\nscalability of existing FM tools. Finally, we show that unifying these two\ncomputation paradigms -- integrating the flexibility and intelligence of LLMs\nwith the rigorous reasoning abilities of FMs -- has transformative potential\nfor the development of trustworthy AI software systems. We acknowledge that\nthis integration has the potential to enhance both the trustworthiness and\nefficiency of software engineering practices while fostering the development of\nintelligent FM tools capable of addressing complex yet real-world challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a transformative AI paradigm,\nprofoundly influencing daily life through their exceptional language\nunderstanding and contextual generation capabilities. Despite their remarkable\nperformance, LLMs face a critical challenge: the propensity to produce\nunreliable outputs due to the inherent limitations of their learning-based\nnature. Formal methods (FMs), on the other hand, are a well-established\ncomputation paradigm that provides mathematically rigorous techniques for\nmodeling, specifying, and verifying the correctness of systems. FMs have been\nextensively applied in mission-critical software engineering, embedded systems,\nand cybersecurity. However, the primary challenge impeding the deployment of\nFMs in real-world settings lies in their steep learning curves, the absence of\nuser-friendly interfaces, and issues with efficiency and adaptability.\n  This position paper outlines a roadmap for advancing the next generation of\ntrustworthy AI systems by leveraging the mutual enhancement of LLMs and FMs.\nFirst, we illustrate how FMs, including reasoning and certification techniques,\ncan help LLMs generate more reliable and formally certified outputs.\nSubsequently, we highlight how the advanced learning capabilities and\nadaptability of LLMs can significantly enhance the usability, efficiency, and\nscalability of existing FM tools. Finally, we show that unifying these two\ncomputation paradigms -- integrating the flexibility and intelligence of LLMs\nwith the rigorous reasoning abilities of FMs -- has transformative potential\nfor the development of trustworthy AI software systems. We acknowledge that\nthis integration has the potential to enhance both the trustworthiness and\nefficiency of software engineering practices while fostering the development of\nintelligent FM tools capable of addressing complex yet real-world challenges."
                },
                "authors": [
                    {
                        "name": "Yedi Zhang"
                    },
                    {
                        "name": "Yufan Cai"
                    },
                    {
                        "name": "Xinyue Zuo"
                    },
                    {
                        "name": "Xiaokun Luan"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Zhe Hou"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Wei"
                    },
                    {
                        "name": "Meng Sun"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Jin Song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin Song Dong"
                },
                "author": "Jin Song Dong",
                "arxiv_comment": "24 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19213v2",
                "updated": "2024-12-09T13:47:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    47,
                    36,
                    0,
                    344,
                    0
                ],
                "published": "2024-10-24T23:51:40Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    23,
                    51,
                    40,
                    3,
                    298,
                    0
                ],
                "title": "Prototypical Hash Encoding for On-the-Fly Fine-Grained Category\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prototypical Hash Encoding for On-the-Fly Fine-Grained Category\n  Discovery"
                },
                "summary": "In this paper, we study a practical yet challenging task, On-the-fly Category\nDiscovery (OCD), aiming to online discover the newly-coming stream data that\nbelong to both known and unknown classes, by leveraging only known category\nknowledge contained in labeled data. Previous OCD methods employ the hash-based\ntechnique to represent old/new categories by hash codes for instance-wise\ninference. However, directly mapping features into low-dimensional hash space\nnot only inevitably damages the ability to distinguish classes and but also\ncauses \"high sensitivity\" issue, especially for fine-grained classes, leading\nto inferior performance. To address these issues, we propose a novel\nPrototypical Hash Encoding (PHE) framework consisting of Category-aware\nPrototype Generation (CPG) and Discriminative Category Encoding (DCE) to\nmitigate the sensitivity of hash code while preserving rich discriminative\ninformation contained in high-dimension feature space, in a two-stage\nprojection fashion. CPG enables the model to fully capture the intra-category\ndiversity by representing each category with multiple prototypes. DCE boosts\nthe discrimination ability of hash code with the guidance of the generated\ncategory prototypes and the constraint of minimum separation distance. By\njointly optimizing CPG and DCE, we demonstrate that these two components are\nmutually beneficial towards an effective OCD. Extensive experiments show the\nsignificant superiority of our PHE over previous methods, e.g., obtaining an\nimprovement of +5.3% in ALL ACC averaged on all datasets. Moreover, due to the\nnature of the interpretable prototypes, we visually analyze the underlying\nmechanism of how PHE helps group certain samples into either known or unknown\ncategories. Code is available at https://github.com/HaiyangZheng/PHE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study a practical yet challenging task, On-the-fly Category\nDiscovery (OCD), aiming to online discover the newly-coming stream data that\nbelong to both known and unknown classes, by leveraging only known category\nknowledge contained in labeled data. Previous OCD methods employ the hash-based\ntechnique to represent old/new categories by hash codes for instance-wise\ninference. However, directly mapping features into low-dimensional hash space\nnot only inevitably damages the ability to distinguish classes and but also\ncauses \"high sensitivity\" issue, especially for fine-grained classes, leading\nto inferior performance. To address these issues, we propose a novel\nPrototypical Hash Encoding (PHE) framework consisting of Category-aware\nPrototype Generation (CPG) and Discriminative Category Encoding (DCE) to\nmitigate the sensitivity of hash code while preserving rich discriminative\ninformation contained in high-dimension feature space, in a two-stage\nprojection fashion. CPG enables the model to fully capture the intra-category\ndiversity by representing each category with multiple prototypes. DCE boosts\nthe discrimination ability of hash code with the guidance of the generated\ncategory prototypes and the constraint of minimum separation distance. By\njointly optimizing CPG and DCE, we demonstrate that these two components are\nmutually beneficial towards an effective OCD. Extensive experiments show the\nsignificant superiority of our PHE over previous methods, e.g., obtaining an\nimprovement of +5.3% in ALL ACC averaged on all datasets. Moreover, due to the\nnature of the interpretable prototypes, we visually analyze the underlying\nmechanism of how PHE helps group certain samples into either known or unknown\ncategories. Code is available at https://github.com/HaiyangZheng/PHE."
                },
                "authors": [
                    {
                        "name": "Haiyang Zheng"
                    },
                    {
                        "name": "Nan Pu"
                    },
                    {
                        "name": "Wenjing Li"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Zhun Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhun Zhong"
                },
                "author": "Zhun Zhong",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01159v2",
                "updated": "2024-12-09T13:41:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    41,
                    31,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-02T10:28:52Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    10,
                    28,
                    52,
                    3,
                    123,
                    0
                ],
                "title": "TartuNLP at EvaLatin 2024: Emotion Polarity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TartuNLP at EvaLatin 2024: Emotion Polarity Detection"
                },
                "summary": "This paper presents the TartuNLP team submission to EvaLatin 2024 shared task\nof the emotion polarity detection for historical Latin texts. Our system relies\non two distinct approaches to annotating training data for supervised learning:\n1) creating heuristics-based labels by adopting the polarity lexicon provided\nby the organizers and 2) generating labels with GPT4. We employed parameter\nefficient fine-tuning using the adapters framework and experimented with both\nmonolingual and cross-lingual knowledge transfer for training language and task\nadapters. Our submission with the LLM-generated labels achieved the overall\nfirst place in the emotion polarity detection task. Our results show that\nLLM-based annotations show promising results on texts in Latin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the TartuNLP team submission to EvaLatin 2024 shared task\nof the emotion polarity detection for historical Latin texts. Our system relies\non two distinct approaches to annotating training data for supervised learning:\n1) creating heuristics-based labels by adopting the polarity lexicon provided\nby the organizers and 2) generating labels with GPT4. We employed parameter\nefficient fine-tuning using the adapters framework and experimented with both\nmonolingual and cross-lingual knowledge transfer for training language and task\nadapters. Our submission with the LLM-generated labels achieved the overall\nfirst place in the emotion polarity detection task. Our results show that\nLLM-based annotations show promising results on texts in Latin."
                },
                "authors": [
                    {
                        "name": "Aleksei Dorkin"
                    },
                    {
                        "name": "Kairit Sirts"
                    }
                ],
                "author_detail": {
                    "name": "Kairit Sirts"
                },
                "author": "Kairit Sirts",
                "arxiv_comment": "Added Acknowledgments section",
                "arxiv_journal_ref": "Proceedings of the Third Workshop on Language Technologies for\n  Historical and Ancient Languages (LT4HALA) @ LREC-COLING-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06485v1",
                "updated": "2024-12-09T13:35:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    35,
                    28,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T13:35:28Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    35,
                    28,
                    0,
                    344,
                    0
                ],
                "title": "Fourier-enhanced reduced-order surrogate modeling for uncertainty\n  quantification in electric machine design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fourier-enhanced reduced-order surrogate modeling for uncertainty\n  quantification in electric machine design"
                },
                "summary": "This work proposes a data-driven surrogate modeling framework for\ncost-effectively inferring the torque of a permanent magnet synchronous machine\nunder geometric design variations. The framework is separated into a\nreduced-order modeling and an inference part. Given a dataset of torque\nsignals, each corresponding to a different set of design parameters, torque\ndimension is first reduced by post-processing a discrete Fourier transform and\nkeeping a reduced number of frequency components. This allows to take advantage\nof torque periodicity and preserve physical information contained in the\nfrequency components. Next, a response surface model is computed by means of\nmachine learning regression, which maps the design parameters to the reduced\nfrequency components. The response surface models of choice are polynomial\nchaos expansions, feedforward neural networks, and Gaussian processes. Torque\ninference is performed by evaluating the response surface model for new design\nparameters and then inverting the dimension reduction. Numerical results show\nthat the resulting surrogate models lead to sufficiently accurate torque\npredictions for previously unseen design configurations. The framework is found\nto be significantly advantageous compared to approximating the original (not\nreduced) torque signal directly, as well as slightly advantageous compared to\nusing principal component analysis for dimension reduction. The combination of\ndiscrete Fourier transform-based dimension reduction with Gaussian\nprocess-based response surfaces yields the best-in-class surrogate model for\nthis use case. The surrogate models replace the original, high-fidelity model\nin Monte Carlo-based uncertainty quantification studies, where they provide\naccurate torque statistics estimates at significantly reduced computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a data-driven surrogate modeling framework for\ncost-effectively inferring the torque of a permanent magnet synchronous machine\nunder geometric design variations. The framework is separated into a\nreduced-order modeling and an inference part. Given a dataset of torque\nsignals, each corresponding to a different set of design parameters, torque\ndimension is first reduced by post-processing a discrete Fourier transform and\nkeeping a reduced number of frequency components. This allows to take advantage\nof torque periodicity and preserve physical information contained in the\nfrequency components. Next, a response surface model is computed by means of\nmachine learning regression, which maps the design parameters to the reduced\nfrequency components. The response surface models of choice are polynomial\nchaos expansions, feedforward neural networks, and Gaussian processes. Torque\ninference is performed by evaluating the response surface model for new design\nparameters and then inverting the dimension reduction. Numerical results show\nthat the resulting surrogate models lead to sufficiently accurate torque\npredictions for previously unseen design configurations. The framework is found\nto be significantly advantageous compared to approximating the original (not\nreduced) torque signal directly, as well as slightly advantageous compared to\nusing principal component analysis for dimension reduction. The combination of\ndiscrete Fourier transform-based dimension reduction with Gaussian\nprocess-based response surfaces yields the best-in-class surrogate model for\nthis use case. The surrogate models replace the original, high-fidelity model\nin Monte Carlo-based uncertainty quantification studies, where they provide\naccurate torque statistics estimates at significantly reduced computational\ncost."
                },
                "authors": [
                    {
                        "name": "Aylar Partovizadeh"
                    },
                    {
                        "name": "Sebastian Schps"
                    },
                    {
                        "name": "Dimitrios Loukrezis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Loukrezis"
                },
                "author": "Dimitrios Loukrezis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06483v1",
                "updated": "2024-12-09T13:31:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    31,
                    46,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T13:31:46Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    31,
                    46,
                    0,
                    344,
                    0
                ],
                "title": "SafeWorld: Geo-Diverse Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeWorld: Geo-Diverse Safety Alignment"
                },
                "summary": "In the rapidly evolving field of Large Language Models (LLMs), ensuring\nsafety is a crucial and widely discussed topic. However, existing works often\noverlook the geo-diversity of cultural and legal standards across the world. To\ndemonstrate the challenges posed by geo-diverse safety standards, we introduce\nSafeWorld, a novel benchmark specifically designed to evaluate LLMs' ability to\ngenerate responses that are not only helpful but also culturally sensitive and\nlegally compliant across diverse global contexts. SafeWorld encompasses 2,342\ntest user queries, each grounded in high-quality, human-verified cultural norms\nand legal policies from 50 countries and 493 regions/races. On top of it, we\npropose a multi-dimensional automatic safety evaluation framework that assesses\nthe contextual appropriateness, accuracy, and comprehensiveness of responses.\nOur evaluations reveal that current LLMs struggle to meet these criteria. To\nenhance LLMs' alignment with geo-diverse safety standards, we synthesize\nhelpful preference pairs for Direct Preference Optimization (DPO) alignment\ntraining. The preference pair construction aims to encourage LLMs to behave\nappropriately and provide precise references to relevant cultural norms and\npolicies when necessary. Our trained SafeWorldLM outperforms all competing\nmodels, including GPT-4o on all three evaluation dimensions by a large margin.\nGlobal human evaluators also note a nearly 20% higher winning rate in\nhelpfulness and harmfulness evaluation. Our code and data can be found here:\nhttps://github.com/PlusLabNLP/SafeWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving field of Large Language Models (LLMs), ensuring\nsafety is a crucial and widely discussed topic. However, existing works often\noverlook the geo-diversity of cultural and legal standards across the world. To\ndemonstrate the challenges posed by geo-diverse safety standards, we introduce\nSafeWorld, a novel benchmark specifically designed to evaluate LLMs' ability to\ngenerate responses that are not only helpful but also culturally sensitive and\nlegally compliant across diverse global contexts. SafeWorld encompasses 2,342\ntest user queries, each grounded in high-quality, human-verified cultural norms\nand legal policies from 50 countries and 493 regions/races. On top of it, we\npropose a multi-dimensional automatic safety evaluation framework that assesses\nthe contextual appropriateness, accuracy, and comprehensiveness of responses.\nOur evaluations reveal that current LLMs struggle to meet these criteria. To\nenhance LLMs' alignment with geo-diverse safety standards, we synthesize\nhelpful preference pairs for Direct Preference Optimization (DPO) alignment\ntraining. The preference pair construction aims to encourage LLMs to behave\nappropriately and provide precise references to relevant cultural norms and\npolicies when necessary. Our trained SafeWorldLM outperforms all competing\nmodels, including GPT-4o on all three evaluation dimensions by a large margin.\nGlobal human evaluators also note a nearly 20% higher winning rate in\nhelpfulness and harmfulness evaluation. Our code and data can be found here:\nhttps://github.com/PlusLabNLP/SafeWorld."
                },
                "authors": [
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08582v2",
                "updated": "2024-12-09T13:26:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    26,
                    37,
                    0,
                    344,
                    0
                ],
                "published": "2024-07-11T15:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    15,
                    7,
                    26,
                    3,
                    193,
                    0
                ],
                "title": "On the Universal Truthfulness Hyperplane Inside LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Universal Truthfulness Hyperplane Inside LLMs"
                },
                "summary": "While large language models (LLMs) have demonstrated remarkable abilities\nacross various fields, hallucination remains a significant challenge. Recent\nstudies have explored hallucinations through the lens of internal\nrepresentations, proposing mechanisms to decipher LLMs' adherence to facts.\nHowever, these approaches often fail to generalize to out-of-distribution data,\nleading to concerns about whether internal representation patterns reflect\nfundamental factual awareness, or only overfit spurious correlations on the\nspecific datasets. In this work, we investigate whether a universal\ntruthfulness hyperplane that distinguishes the model's factually correct and\nincorrect outputs exists within the model. To this end, we scale up the number\nof training datasets and conduct an extensive evaluation -- we train the\ntruthfulness hyperplane on a diverse collection of over 40 datasets and examine\nits cross-task, cross-domain, and in-domain generalization. Our results\nindicate that increasing the diversity of the training datasets significantly\nenhances the performance in all scenarios, while the volume of data samples\nplays a less critical role. This finding supports the optimistic hypothesis\nthat a universal truthfulness hyperplane may indeed exist within the model,\noffering promising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated remarkable abilities\nacross various fields, hallucination remains a significant challenge. Recent\nstudies have explored hallucinations through the lens of internal\nrepresentations, proposing mechanisms to decipher LLMs' adherence to facts.\nHowever, these approaches often fail to generalize to out-of-distribution data,\nleading to concerns about whether internal representation patterns reflect\nfundamental factual awareness, or only overfit spurious correlations on the\nspecific datasets. In this work, we investigate whether a universal\ntruthfulness hyperplane that distinguishes the model's factually correct and\nincorrect outputs exists within the model. To this end, we scale up the number\nof training datasets and conduct an extensive evaluation -- we train the\ntruthfulness hyperplane on a diverse collection of over 40 datasets and examine\nits cross-task, cross-domain, and in-domain generalization. Our results\nindicate that increasing the diversity of the training datasets significantly\nenhances the performance in all scenarios, while the volume of data samples\nplays a less critical role. This finding supports the optimistic hypothesis\nthat a universal truthfulness hyperplane may indeed exist within the model,\noffering promising directions for future research."
                },
                "authors": [
                    {
                        "name": "Junteng Liu"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08484v2",
                "updated": "2024-12-09T13:23:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    23,
                    18,
                    0,
                    344,
                    0
                ],
                "published": "2024-06-12T17:59:34Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    17,
                    59,
                    34,
                    2,
                    164,
                    0
                ],
                "title": "Exploiting the diversity of modeling methods to probe systematic biases\n  in strong lensing analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting the diversity of modeling methods to probe systematic biases\n  in strong lensing analyses"
                },
                "summary": "Challenges inherent to high-resolution and high signal-to-noise data as well\nas model degeneracies can cause systematic biases in analyses of strong lens\nsystems. In the past decade, the number of lens modeling methods has\nsignificantly increased, from purely analytical methods, to pixelated and\nnon-parametric ones, or ones based on deep learning. We embraced this diversity\nby selecting different software packages and use them to blindly model\nindependently simulated Hubble Space Telescope (HST) imaging data. To overcome\nthe difficulties arising from using different codes and conventions, we used\nthe COde-independent Organized LEns STandard (COOLEST) to store, compare, and\nrelease all models in a self-consistent and human-readable manner. From an\nensemble of six modeling methods, we studied the recovery of the lens potential\nparameters and properties of the reconstructed source. We find that, overall,\nboth lens and source properties are recovered reasonably well, but systematic\nbiases arise in all methods. Interestingly, we do not observe that a single\nmethod is significantly more accurate than others, and the amount of bias\nlargely depends on the specific lens or source property of interest. By\ncombining posterior distributions from individual methods using equal weights,\nthe maximal systematic biases on lens model parameters inferred from individual\nmodels are reduced by a factor of 5.4 on average. We investigated a selection\nof modeling effects that partly explain the observed biases, such as the cuspy\nnature of the background source and the accuracy of the point spread function.\nThis work introduces, for the first time, a generic framework to compare and\nease the combination of models obtained from different codes and methods, which\nwill be key to retain accuracy in future strong lensing analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges inherent to high-resolution and high signal-to-noise data as well\nas model degeneracies can cause systematic biases in analyses of strong lens\nsystems. In the past decade, the number of lens modeling methods has\nsignificantly increased, from purely analytical methods, to pixelated and\nnon-parametric ones, or ones based on deep learning. We embraced this diversity\nby selecting different software packages and use them to blindly model\nindependently simulated Hubble Space Telescope (HST) imaging data. To overcome\nthe difficulties arising from using different codes and conventions, we used\nthe COde-independent Organized LEns STandard (COOLEST) to store, compare, and\nrelease all models in a self-consistent and human-readable manner. From an\nensemble of six modeling methods, we studied the recovery of the lens potential\nparameters and properties of the reconstructed source. We find that, overall,\nboth lens and source properties are recovered reasonably well, but systematic\nbiases arise in all methods. Interestingly, we do not observe that a single\nmethod is significantly more accurate than others, and the amount of bias\nlargely depends on the specific lens or source property of interest. By\ncombining posterior distributions from individual methods using equal weights,\nthe maximal systematic biases on lens model parameters inferred from individual\nmodels are reduced by a factor of 5.4 on average. We investigated a selection\nof modeling effects that partly explain the observed biases, such as the cuspy\nnature of the background source and the accuracy of the point spread function.\nThis work introduces, for the first time, a generic framework to compare and\nease the combination of models obtained from different codes and methods, which\nwill be key to retain accuracy in future strong lensing analyses."
                },
                "authors": [
                    {
                        "name": "A. Galan"
                    },
                    {
                        "name": "G. Vernardos"
                    },
                    {
                        "name": "Q. Minor"
                    },
                    {
                        "name": "D. Sluse"
                    },
                    {
                        "name": "L. Van de Vyvere"
                    },
                    {
                        "name": "M. Gomer"
                    }
                ],
                "author_detail": {
                    "name": "M. Gomer"
                },
                "author": "M. Gomer",
                "arxiv_doi": "10.1051/0004-6361/202451095",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202451095",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.08484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 12 figures (excluding appendix). Published in A&A",
                "arxiv_journal_ref": "A&A 692, A87 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06474v1",
                "updated": "2024-12-09T13:21:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    21,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T13:21:07Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    21,
                    7,
                    0,
                    344,
                    0
                ],
                "title": "From Uncertainty to Trust: Enhancing Reliability in Vision-Language\n  Models with Uncertainty-Guided Dropout Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Uncertainty to Trust: Enhancing Reliability in Vision-Language\n  Models with Uncertainty-Guided Dropout Decoding"
                },
                "summary": "Large vision-language models (LVLMs) demonstrate remarkable capabilities in\nmultimodal tasks but are prone to misinterpreting visual inputs, often\nresulting in hallucinations and unreliable outputs. To address these\nchallenges, we propose Dropout Decoding, a novel inference-time approach that\nquantifies the uncertainty of visual tokens and selectively masks uncertain\ntokens to improve decoding. Our method measures the uncertainty of each visual\ntoken by projecting it onto the text space and decomposing it into aleatoric\nand epistemic components. Specifically, we focus on epistemic uncertainty,\nwhich captures perception-related errors more effectively. Inspired by dropout\nregularization, we introduce uncertainty-guided token dropout, which applies\nthe dropout principle to input visual tokens instead of model parameters, and\nduring inference rather than training. By aggregating predictions from an\nensemble of masked decoding contexts, Dropout Decoding robustly mitigates\nerrors arising from visual token misinterpretations. Evaluations on benchmarks\nincluding CHAIR, THRONE, and MMBench demonstrate that Dropout Decoding\nsignificantly reduces object hallucinations (OH) and enhances both reliability\nand quality of LVLM outputs across diverse visual contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) demonstrate remarkable capabilities in\nmultimodal tasks but are prone to misinterpreting visual inputs, often\nresulting in hallucinations and unreliable outputs. To address these\nchallenges, we propose Dropout Decoding, a novel inference-time approach that\nquantifies the uncertainty of visual tokens and selectively masks uncertain\ntokens to improve decoding. Our method measures the uncertainty of each visual\ntoken by projecting it onto the text space and decomposing it into aleatoric\nand epistemic components. Specifically, we focus on epistemic uncertainty,\nwhich captures perception-related errors more effectively. Inspired by dropout\nregularization, we introduce uncertainty-guided token dropout, which applies\nthe dropout principle to input visual tokens instead of model parameters, and\nduring inference rather than training. By aggregating predictions from an\nensemble of masked decoding contexts, Dropout Decoding robustly mitigates\nerrors arising from visual token misinterpretations. Evaluations on benchmarks\nincluding CHAIR, THRONE, and MMBench demonstrate that Dropout Decoding\nsignificantly reduces object hallucinations (OH) and enhances both reliability\nand quality of LVLM outputs across diverse visual contexts."
                },
                "authors": [
                    {
                        "name": "Yixiong Fang"
                    },
                    {
                        "name": "Ziran Yang"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Jiawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhou"
                },
                "author": "Jiawei Zhou",
                "arxiv_comment": "Code is released at https://github.com/kigb/DropoutDecoding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06458v1",
                "updated": "2024-12-09T13:02:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    2,
                    35,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T13:02:35Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    2,
                    35,
                    0,
                    344,
                    0
                ],
                "title": "Pruning All-Rounder: Rethinking and Improving Inference Efficiency for\n  Large Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning All-Rounder: Rethinking and Improving Inference Efficiency for\n  Large Vision Language Models"
                },
                "summary": "Although Large Vision-Language Models (LVLMs) have achieved impressive\nresults, their high computational cost poses a significant barrier to wider\napplication. To enhance inference efficiency, most existing approaches depend\non parameter-dependent or token-dependent strategies to reduce computational\ndemands. However, these methods typically require complex training processes\nand struggle to consistently select the most relevant tokens. In this paper, we\nsystematically analyze the above challenges and provide a series of valuable\ninsights for inference acceleration. Based on these findings, we propose a\nnovel framework, the Pruning All-Rounder (PAR). Different from previous works,\nPAR develops a meta-router to adaptively organize pruning flows across both\ntokens and layers. With a self-supervised learning manner, our method achieves\na superior balance between performance and efficiency. Notably, PAR is highly\nflexible, offering multiple pruning versions to address a range of pruning\nscenarios. The code for this work will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Vision-Language Models (LVLMs) have achieved impressive\nresults, their high computational cost poses a significant barrier to wider\napplication. To enhance inference efficiency, most existing approaches depend\non parameter-dependent or token-dependent strategies to reduce computational\ndemands. However, these methods typically require complex training processes\nand struggle to consistently select the most relevant tokens. In this paper, we\nsystematically analyze the above challenges and provide a series of valuable\ninsights for inference acceleration. Based on these findings, we propose a\nnovel framework, the Pruning All-Rounder (PAR). Different from previous works,\nPAR develops a meta-router to adaptively organize pruning flows across both\ntokens and layers. With a self-supervised learning manner, our method achieves\na superior balance between performance and efficiency. Notably, PAR is highly\nflexible, offering multiple pruning versions to address a range of pruning\nscenarios. The code for this work will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Wei Suo"
                    },
                    {
                        "name": "Ji Ma"
                    },
                    {
                        "name": "Mengyang Sun"
                    },
                    {
                        "name": "Lin Yuanbo Wu"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Yanning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanning Zhang"
                },
                "author": "Yanning Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10622v2",
                "updated": "2024-12-09T12:31:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    31,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2024-04-16T14:45:27Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    14,
                    45,
                    27,
                    1,
                    107,
                    0
                ],
                "title": "Learning Deep Dynamical Systems using Stable Neural ODEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Deep Dynamical Systems using Stable Neural ODEs"
                },
                "summary": "Learning complex trajectories from demonstrations in robotic tasks has been\neffectively addressed through the utilization of Dynamical Systems (DS).\nState-of-the-art DS learning methods ensure stability of the generated\ntrajectories; however, they have three shortcomings: a) the DS is assumed to\nhave a single attractor, which limits the diversity of tasks it can achieve, b)\nstate derivative information is assumed to be available in the learning process\nand c) the state of the DS is assumed to be measurable at inference time. We\npropose a class of provably stable latent DS with possibly multiple attractors,\nthat inherit the training methods of Neural Ordinary Differential Equations,\nthus, dropping the dependency on state derivative information. A diffeomorphic\nmapping for the output and a loss that captures time-invariant trajectory\nsimilarity are proposed. We validate the efficacy of our approach through\nexperiments conducted on a public dataset of handwritten shapes and within a\nsimulated object manipulation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning complex trajectories from demonstrations in robotic tasks has been\neffectively addressed through the utilization of Dynamical Systems (DS).\nState-of-the-art DS learning methods ensure stability of the generated\ntrajectories; however, they have three shortcomings: a) the DS is assumed to\nhave a single attractor, which limits the diversity of tasks it can achieve, b)\nstate derivative information is assumed to be available in the learning process\nand c) the state of the DS is assumed to be measurable at inference time. We\npropose a class of provably stable latent DS with possibly multiple attractors,\nthat inherit the training methods of Neural Ordinary Differential Equations,\nthus, dropping the dependency on state derivative information. A diffeomorphic\nmapping for the output and a loss that captures time-invariant trajectory\nsimilarity are proposed. We validate the efficacy of our approach through\nexperiments conducted on a public dataset of handwritten shapes and within a\nsimulated object manipulation task."
                },
                "authors": [
                    {
                        "name": "Andreas Sochopoulos"
                    },
                    {
                        "name": "Michael Gienger"
                    },
                    {
                        "name": "Sethu Vijayakumar"
                    }
                ],
                "author_detail": {
                    "name": "Sethu Vijayakumar"
                },
                "author": "Sethu Vijayakumar",
                "arxiv_comment": "9 pages, 8 figures, accepted in IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06438v1",
                "updated": "2024-12-09T12:27:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    27,
                    21,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T12:27:21Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    27,
                    21,
                    0,
                    344,
                    0
                ],
                "title": "Can foundation models actively gather information in interactive\n  environments to test hypotheses?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can foundation models actively gather information in interactive\n  environments to test hypotheses?"
                },
                "summary": "While problem solving is a standard evaluation task for foundation models, a\ncrucial component of problem solving -- actively and strategically gathering\ninformation to test hypotheses -- has not been closely investigated. To assess\nthe information gathering abilities of foundation models in interactive\nenvironments, we introduce a framework in which a model must determine the\nfactors influencing a hidden reward function by iteratively reasoning about its\npreviously gathered information and proposing its next exploratory action to\nmaximize information gain at each step. We implement this framework in both a\ntext-based environment, which offers a tightly controlled setting and enables\nhigh-throughput parameter sweeps, and in an embodied 3D environment, which\nrequires addressing complexities of multi-modal interaction more relevant to\nreal-world applications. We further investigate whether approaches such as\nself-correction and increased inference time improve information gathering\nefficiency. In a relatively simple task that requires identifying a single\nrewarding feature, we find that LLM's information gathering capability is close\nto optimal. However, when the model must identify a conjunction of rewarding\nfeatures, performance is suboptimal. The hit in performance is due partly to\nthe model translating task description to a policy and partly to the model's\neffectiveness in using its in-context memory. Performance is comparable in both\ntext and 3D embodied environments, although imperfect visual object recognition\nreduces its accuracy in drawing conclusions from gathered information in the 3D\nembodied case. For single-feature-based rewards, we find that smaller models\ncuriously perform better; for conjunction-based rewards, incorporating self\ncorrection into the model improves performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While problem solving is a standard evaluation task for foundation models, a\ncrucial component of problem solving -- actively and strategically gathering\ninformation to test hypotheses -- has not been closely investigated. To assess\nthe information gathering abilities of foundation models in interactive\nenvironments, we introduce a framework in which a model must determine the\nfactors influencing a hidden reward function by iteratively reasoning about its\npreviously gathered information and proposing its next exploratory action to\nmaximize information gain at each step. We implement this framework in both a\ntext-based environment, which offers a tightly controlled setting and enables\nhigh-throughput parameter sweeps, and in an embodied 3D environment, which\nrequires addressing complexities of multi-modal interaction more relevant to\nreal-world applications. We further investigate whether approaches such as\nself-correction and increased inference time improve information gathering\nefficiency. In a relatively simple task that requires identifying a single\nrewarding feature, we find that LLM's information gathering capability is close\nto optimal. However, when the model must identify a conjunction of rewarding\nfeatures, performance is suboptimal. The hit in performance is due partly to\nthe model translating task description to a policy and partly to the model's\neffectiveness in using its in-context memory. Performance is comparable in both\ntext and 3D embodied environments, although imperfect visual object recognition\nreduces its accuracy in drawing conclusions from gathered information in the 3D\nembodied case. For single-feature-based rewards, we find that smaller models\ncuriously perform better; for conjunction-based rewards, incorporating self\ncorrection into the model improves performance."
                },
                "authors": [
                    {
                        "name": "Nan Rosemary Ke"
                    },
                    {
                        "name": "Danny P. Sawyer"
                    },
                    {
                        "name": "Hubert Soyer"
                    },
                    {
                        "name": "Martin Engelcke"
                    },
                    {
                        "name": "David P Reichert"
                    },
                    {
                        "name": "Drew A. Hudson"
                    },
                    {
                        "name": "John Reid"
                    },
                    {
                        "name": "Alexander Lerchner"
                    },
                    {
                        "name": "Danilo Jimenez Rezende"
                    },
                    {
                        "name": "Timothy P Lillicrap"
                    },
                    {
                        "name": "Michael Mozer"
                    },
                    {
                        "name": "Jane X Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jane X Wang"
                },
                "author": "Jane X Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06435v1",
                "updated": "2024-12-09T12:21:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    21,
                    20,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T12:21:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    21,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Simulating Human-like Daily Activities with Desire-driven Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Human-like Daily Activities with Desire-driven Autonomy"
                },
                "summary": "Existing task-oriented AI agents often depend on explicit instructions or\nexternal rewards, limiting their ability to be driven by intrinsic motivations\nlike humans. In this paper, we present a desire-driven autonomy framework to\nguide a Large Language Model-based (LLM-based) agent to simulate human-like\ndaily activities. In contrast to previous agents, our Desire-driven Autonomous\nAgent (D2A) operates on the principle of intrinsic desire, allowing it to\npropose and select tasks that fulfill its motivational framework autonomously.\nInspired by the Theory of Needs, the motivational framework incorporates an\nunderstanding of human-like desires, such as the need for social interaction,\npersonal fulfillment, and self-care. Utilizing a desire-driven task generation\nmechanism, the agent evaluates its current state and takes a sequence of\nactivities aligned with its intrinsic motivations. Through simulations, we\ndemonstrate that our Desire-driven Autonomous Agent (D2A) generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based frameworks demonstrates that our approach significantly enhances the\nrationality of the simulated activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing task-oriented AI agents often depend on explicit instructions or\nexternal rewards, limiting their ability to be driven by intrinsic motivations\nlike humans. In this paper, we present a desire-driven autonomy framework to\nguide a Large Language Model-based (LLM-based) agent to simulate human-like\ndaily activities. In contrast to previous agents, our Desire-driven Autonomous\nAgent (D2A) operates on the principle of intrinsic desire, allowing it to\npropose and select tasks that fulfill its motivational framework autonomously.\nInspired by the Theory of Needs, the motivational framework incorporates an\nunderstanding of human-like desires, such as the need for social interaction,\npersonal fulfillment, and self-care. Utilizing a desire-driven task generation\nmechanism, the agent evaluates its current state and takes a sequence of\nactivities aligned with its intrinsic motivations. Through simulations, we\ndemonstrate that our Desire-driven Autonomous Agent (D2A) generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based frameworks demonstrates that our approach significantly enhances the\nrationality of the simulated activities."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06432v1",
                "updated": "2024-12-09T12:20:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    20,
                    33,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T12:20:33Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    20,
                    33,
                    0,
                    344,
                    0
                ],
                "title": "Integrating Expert Labels into LLM-based Emission Goal Detection:\n  Example Selection vs Automatic Prompt Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Expert Labels into LLM-based Emission Goal Detection:\n  Example Selection vs Automatic Prompt Design"
                },
                "summary": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task."
                },
                "authors": [
                    {
                        "name": "Marco Wrzalik"
                    },
                    {
                        "name": "Adrian Ulges"
                    },
                    {
                        "name": "Anne Uersfeld"
                    },
                    {
                        "name": "Florian Faust"
                    }
                ],
                "author_detail": {
                    "name": "Florian Faust"
                },
                "author": "Florian Faust",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02972v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02972v3",
                "updated": "2024-12-09T12:11:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    11,
                    12,
                    0,
                    344,
                    0
                ],
                "published": "2023-12-05T18:58:52Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    18,
                    58,
                    52,
                    1,
                    339,
                    0
                ],
                "title": "PROSPECT: A profile likelihood code for frequentist cosmological\n  parameter inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PROSPECT: A profile likelihood code for frequentist cosmological\n  parameter inference"
                },
                "summary": "Cosmological parameter inference has been dominated by the Bayesian approach\nfor the past two decades, primarily due to its computational efficiency.\nHowever, the Bayesian approach involves integration of the posterior\nprobability and therefore depends on both the choice of model parametrisation\nand the choice of prior on the model parameter space. In some cases, this can\nlead to conclusions which are driven by choice of parametrisation and priors\nrather than by data. The profile likelihood method provides a complementary\nfrequentist tool which can be used to investigate this effect.\n  In this paper, we present the code PROSPECT for computing profile likelihoods\nin cosmology. We showcase the code using a phenomenological model for\nconverting dark matter into dark radiation that suffers from large volume\neffects and prior dependence. PROSPECT is compatible with both cobaya and\nMontePython, and is publicly available at\nhttps://github.com/AarhusCosmology/prospect_public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological parameter inference has been dominated by the Bayesian approach\nfor the past two decades, primarily due to its computational efficiency.\nHowever, the Bayesian approach involves integration of the posterior\nprobability and therefore depends on both the choice of model parametrisation\nand the choice of prior on the model parameter space. In some cases, this can\nlead to conclusions which are driven by choice of parametrisation and priors\nrather than by data. The profile likelihood method provides a complementary\nfrequentist tool which can be used to investigate this effect.\n  In this paper, we present the code PROSPECT for computing profile likelihoods\nin cosmology. We showcase the code using a phenomenological model for\nconverting dark matter into dark radiation that suffers from large volume\neffects and prior dependence. PROSPECT is compatible with both cobaya and\nMontePython, and is publicly available at\nhttps://github.com/AarhusCosmology/prospect_public."
                },
                "authors": [
                    {
                        "name": "Emil Brinch Holm"
                    },
                    {
                        "name": "Andreas Nygaard"
                    },
                    {
                        "name": "Jeppe Dakin"
                    },
                    {
                        "name": "Steen Hannestad"
                    },
                    {
                        "name": "Thomas Tram"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Tram"
                },
                "author": "Thomas Tram",
                "arxiv_comment": "27 pages, 9 figures; v3: matches version accepted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02972v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06419v1",
                "updated": "2024-12-09T11:57:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    57,
                    16,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T11:57:16Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    57,
                    16,
                    0,
                    344,
                    0
                ],
                "title": "LLM-BIP: Structured Pruning for Large Language Models with Block-Wise\n  Forward Importance Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-BIP: Structured Pruning for Large Language Models with Block-Wise\n  Forward Importance Propagation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious language tasks, but their widespread deployment is impeded by their\nlarge size and high computational costs. Structural pruning is a prevailing\ntechnique used to introduce sparsity into pre-trained models and facilitate\ndirect hardware acceleration during inference by removing redundant connections\n(structurally-grouped parameters), such as channels and attention heads.\nExisting structural pruning approaches often employ either global or layer-wise\npruning criteria; however, they are hindered by ineffectiveness stemming from\ninaccurate evaluation of connection importance. Global pruning methods\ntypically assess component importance using near-zero and unreliable gradients,\nwhile layer-wise pruning approaches encounter significant pruning error\naccumulation issues. To this end, we propose a more accurate pruning metric\nbased on the block-wise importance score propagation, termed LLM-BIP.\nSpecifically, LLM-BIP precisely evaluates connection importance by gauging its\ninfluence on the respective transformer block output, which can be efficiently\napproximated in a single forward pass through an upper bound derived from the\nassumption of Lipschitz continuity. We evaluate the proposed method using\nLLaMA-7B, Vicuna-7B, and LLaMA-13B across common zero-shot tasks. The results\ndemonstrate that our approach achieves an average of 3.26% increase in accuracy\nfor common reasoning tasks compared to previous best baselines. It also reduces\nperplexity by 14.09 and 68.76 on average for the WikiText2 dataset and PTB\ndataset, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious language tasks, but their widespread deployment is impeded by their\nlarge size and high computational costs. Structural pruning is a prevailing\ntechnique used to introduce sparsity into pre-trained models and facilitate\ndirect hardware acceleration during inference by removing redundant connections\n(structurally-grouped parameters), such as channels and attention heads.\nExisting structural pruning approaches often employ either global or layer-wise\npruning criteria; however, they are hindered by ineffectiveness stemming from\ninaccurate evaluation of connection importance. Global pruning methods\ntypically assess component importance using near-zero and unreliable gradients,\nwhile layer-wise pruning approaches encounter significant pruning error\naccumulation issues. To this end, we propose a more accurate pruning metric\nbased on the block-wise importance score propagation, termed LLM-BIP.\nSpecifically, LLM-BIP precisely evaluates connection importance by gauging its\ninfluence on the respective transformer block output, which can be efficiently\napproximated in a single forward pass through an upper bound derived from the\nassumption of Lipschitz continuity. We evaluate the proposed method using\nLLaMA-7B, Vicuna-7B, and LLaMA-13B across common zero-shot tasks. The results\ndemonstrate that our approach achieves an average of 3.26% increase in accuracy\nfor common reasoning tasks compared to previous best baselines. It also reduces\nperplexity by 14.09 and 68.76 on average for the WikiText2 dataset and PTB\ndataset, respectively."
                },
                "authors": [
                    {
                        "name": "Haihang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Haihang Wu"
                },
                "author": "Haihang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06412v1",
                "updated": "2024-12-09T11:40:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    40,
                    6,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T11:40:06Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    40,
                    6,
                    0,
                    344,
                    0
                ],
                "title": "StarWhisper Telescope: Agent-Based Observation Assistant System to\n  Approach AI Astrophysicist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarWhisper Telescope: Agent-Based Observation Assistant System to\n  Approach AI Astrophysicist"
                },
                "summary": "With the rapid advancements in Large Language Models (LLMs), LLM-based agents\nhave introduced convenient and user-friendly methods for leveraging tools\nacross various domains. In the field of astronomical observation, the\nconstruction of new telescopes has significantly increased astronomers'\nworkload. Deploying LLM-powered agents can effectively alleviate this burden\nand reduce the costs associated with training personnel. Within the Nearby\nGalaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes\nacross three observation sites, aiming to find the transients from the galaxies\nin 50 mpc, we have developed the \\textbf{StarWhisper Telescope System} to\nmanage the entire observation process. This system automates tasks such as\ngenerating observation lists, conducting observations, analyzing data, and\nproviding feedback to the observer. Observation lists are customized for\ndifferent sites and strategies to ensure comprehensive coverage of celestial\nobjects. After manual verification, these lists are uploaded to the telescopes\nvia the agents in the system, which initiates observations upon neutral\nlanguage. The observed images are analyzed in real-time, and the transients are\npromptly communicated to the observer. The agent modifies them into a real-time\nfollow-up observation proposal and send to the Xinglong observatory group chat,\nthen add them to the next-day observation lists. Additionally, the integration\nof AI agents within the system provides online accessibility, saving\nastronomers' time and encouraging greater participation from amateur\nastronomers in the NGSS project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancements in Large Language Models (LLMs), LLM-based agents\nhave introduced convenient and user-friendly methods for leveraging tools\nacross various domains. In the field of astronomical observation, the\nconstruction of new telescopes has significantly increased astronomers'\nworkload. Deploying LLM-powered agents can effectively alleviate this burden\nand reduce the costs associated with training personnel. Within the Nearby\nGalaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes\nacross three observation sites, aiming to find the transients from the galaxies\nin 50 mpc, we have developed the \\textbf{StarWhisper Telescope System} to\nmanage the entire observation process. This system automates tasks such as\ngenerating observation lists, conducting observations, analyzing data, and\nproviding feedback to the observer. Observation lists are customized for\ndifferent sites and strategies to ensure comprehensive coverage of celestial\nobjects. After manual verification, these lists are uploaded to the telescopes\nvia the agents in the system, which initiates observations upon neutral\nlanguage. The observed images are analyzed in real-time, and the transients are\npromptly communicated to the observer. The agent modifies them into a real-time\nfollow-up observation proposal and send to the Xinglong observatory group chat,\nthen add them to the next-day observation lists. Additionally, the integration\nof AI agents within the system provides online accessibility, saving\nastronomers' time and encouraging greater participation from amateur\nastronomers in the NGSS project."
                },
                "authors": [
                    {
                        "name": "Cunshi Wang"
                    },
                    {
                        "name": "Xinjie Hu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xunhao Chen"
                    },
                    {
                        "name": "Pengliang Du"
                    },
                    {
                        "name": "Yiming Mao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Ying Wu"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Yansong Li"
                    },
                    {
                        "name": "Beichuan Wang"
                    },
                    {
                        "name": "Haiyang Mu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Jianfeng Tian"
                    },
                    {
                        "name": "Liang Ge"
                    },
                    {
                        "name": "Yongna Mao"
                    },
                    {
                        "name": "Shengming Li"
                    },
                    {
                        "name": "Xiaomeng Lu"
                    },
                    {
                        "name": "Jinhang Zou"
                    },
                    {
                        "name": "Yang Huang"
                    },
                    {
                        "name": "Ningchen Sun"
                    },
                    {
                        "name": "Jie Zheng"
                    },
                    {
                        "name": "Min He"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Junjie Jin"
                    },
                    {
                        "name": "Hong Wu"
                    },
                    {
                        "name": "Chaohui Shang"
                    },
                    {
                        "name": "Jifeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Liu"
                },
                "author": "Jifeng Liu",
                "arxiv_comment": "21 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08467v2",
                "updated": "2024-12-09T11:34:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    34,
                    28,
                    0,
                    344,
                    0
                ],
                "published": "2024-02-13T13:50:08Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    13,
                    50,
                    8,
                    1,
                    44,
                    0
                ],
                "title": "Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect\n  Disinformation Claims",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect\n  Disinformation Claims"
                },
                "summary": "As Large Language Models become more proficient, their misuse in coordinated\ndisinformation campaigns is a growing concern. This study explores the\ncapability of ChatGPT with GPT-3.5 to generate short-form disinformation claims\nabout the war in Ukraine, both in general and on a specific event, which is\nbeyond the GPT-3.5 knowledge cutoff. Unlike prior work, we do not provide the\nmodel with human-written disinformation narratives by including them in the\nprompt. Thus the generated short claims are hallucinations based on prior world\nknowledge and inference from the minimal prompt. With a straightforward\nprompting technique, we are able to bypass model safeguards and generate\nnumerous short claims. We compare those against human-authored false claims on\nthe war in Ukraine from ClaimReview, specifically with respect to differences\nin their linguistic properties. We also evaluate whether AI authorship can be\ndifferentiated by human readers or state-of-the-art authorship detection tools.\nThus, we demonstrate that ChatGPT can produce realistic, target-specific\ndisinformation claims, even on a specific post-cutoff event, and that they\ncannot be reliably distinguished by humans or existing automated tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models become more proficient, their misuse in coordinated\ndisinformation campaigns is a growing concern. This study explores the\ncapability of ChatGPT with GPT-3.5 to generate short-form disinformation claims\nabout the war in Ukraine, both in general and on a specific event, which is\nbeyond the GPT-3.5 knowledge cutoff. Unlike prior work, we do not provide the\nmodel with human-written disinformation narratives by including them in the\nprompt. Thus the generated short claims are hallucinations based on prior world\nknowledge and inference from the minimal prompt. With a straightforward\nprompting technique, we are able to bypass model safeguards and generate\nnumerous short claims. We compare those against human-authored false claims on\nthe war in Ukraine from ClaimReview, specifically with respect to differences\nin their linguistic properties. We also evaluate whether AI authorship can be\ndifferentiated by human readers or state-of-the-art authorship detection tools.\nThus, we demonstrate that ChatGPT can produce realistic, target-specific\ndisinformation claims, even on a specific post-cutoff event, and that they\ncannot be reliably distinguished by humans or existing automated tools."
                },
                "authors": [
                    {
                        "name": "Freddy Heppell"
                    },
                    {
                        "name": "Mehmet E. Bakir"
                    },
                    {
                        "name": "Kalina Bontcheva"
                    }
                ],
                "author_detail": {
                    "name": "Kalina Bontcheva"
                },
                "author": "Kalina Bontcheva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06394v1",
                "updated": "2024-12-09T11:22:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    22,
                    59,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T11:22:59Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    22,
                    59,
                    0,
                    344,
                    0
                ],
                "title": "GameArena: Evaluating LLM Reasoning through Live Computer Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GameArena: Evaluating LLM Reasoning through Live Computer Games"
                },
                "summary": "Evaluating the reasoning abilities of large language models (LLMs) is\nchallenging. Existing benchmarks often depend on static datasets, which are\nvulnerable to data contamination and may get saturated over time, or on binary\nlive human feedback that conflates reasoning with other abilities. As the most\nprominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in\nreal-world settings, but lacks the granularity in assessing specific reasoning\ncapabilities. We introduce GameArena, a dynamic benchmark designed to evaluate\nLLM reasoning capabilities through interactive gameplay with humans. GameArena\nconsists of three games designed to test specific reasoning capabilities (e.g.,\ndeductive and inductive reasoning), while keeping participants entertained and\nengaged. We analyze the gaming data retrospectively to uncover the underlying\nreasoning processes of LLMs and measure their fine-grained reasoning\ncapabilities. We collect over 2000 game sessions and provide detailed\nassessments of various reasoning capabilities for five state-of-the-art LLMs.\nOur user study with 100 participants suggests that GameArena improves user\nengagement compared to Chatbot Arena. For the first time, GameArena enables the\ncollection of step-by-step LLM reasoning data in the wild.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the reasoning abilities of large language models (LLMs) is\nchallenging. Existing benchmarks often depend on static datasets, which are\nvulnerable to data contamination and may get saturated over time, or on binary\nlive human feedback that conflates reasoning with other abilities. As the most\nprominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in\nreal-world settings, but lacks the granularity in assessing specific reasoning\ncapabilities. We introduce GameArena, a dynamic benchmark designed to evaluate\nLLM reasoning capabilities through interactive gameplay with humans. GameArena\nconsists of three games designed to test specific reasoning capabilities (e.g.,\ndeductive and inductive reasoning), while keeping participants entertained and\nengaged. We analyze the gaming data retrospectively to uncover the underlying\nreasoning processes of LLMs and measure their fine-grained reasoning\ncapabilities. We collect over 2000 game sessions and provide detailed\nassessments of various reasoning capabilities for five state-of-the-art LLMs.\nOur user study with 100 participants suggests that GameArena improves user\nengagement compared to Chatbot Arena. For the first time, GameArena enables the\ncollection of step-by-step LLM reasoning data in the wild."
                },
                "authors": [
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Qiyu Li"
                    },
                    {
                        "name": "Anze Xie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Haojian Jin"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.07951v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.07951v4",
                "updated": "2024-12-09T10:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    10,
                    47,
                    6,
                    0,
                    344,
                    0
                ],
                "published": "2023-06-13T17:48:27Z",
                "published_parsed": [
                    2023,
                    6,
                    13,
                    17,
                    48,
                    27,
                    1,
                    164,
                    0
                ],
                "title": "Questioning the Survey Responses of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Questioning the Survey Responses of Large Language Models"
                },
                "summary": "Surveys have recently gained popularity as a tool to study large language\nmodels. By comparing survey responses of models to those of human reference\npopulations, researchers aim to infer the demographics, political opinions, or\nvalues best represented by current language models. In this work, we critically\nexamine this methodology on the basis of the well-established American\nCommunity Survey by the U.S. Census Bureau. Evaluating 43 different language\nmodels using de-facto standard prompting methodologies, we establish two\ndominant patterns. First, models' responses are governed by ordering and\nlabeling biases, for example, towards survey responses labeled with the letter\n\"A\". Second, when adjusting for these systematic biases through randomized\nanswer ordering, models across the board trend towards uniformly random survey\nresponses, irrespective of model size or pre-training data. As a result, in\ncontrast to conjectures from prior work, survey-derived alignment measures\noften permit a simple explanation: models consistently appear to better\nrepresent subgroups whose aggregate statistics are closest to uniform for any\nsurvey under consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveys have recently gained popularity as a tool to study large language\nmodels. By comparing survey responses of models to those of human reference\npopulations, researchers aim to infer the demographics, political opinions, or\nvalues best represented by current language models. In this work, we critically\nexamine this methodology on the basis of the well-established American\nCommunity Survey by the U.S. Census Bureau. Evaluating 43 different language\nmodels using de-facto standard prompting methodologies, we establish two\ndominant patterns. First, models' responses are governed by ordering and\nlabeling biases, for example, towards survey responses labeled with the letter\n\"A\". Second, when adjusting for these systematic biases through randomized\nanswer ordering, models across the board trend towards uniformly random survey\nresponses, irrespective of model size or pre-training data. As a result, in\ncontrast to conjectures from prior work, survey-derived alignment measures\noften permit a simple explanation: models consistently appear to better\nrepresent subgroups whose aggregate statistics are closest to uniform for any\nsurvey under consideration."
                },
                "authors": [
                    {
                        "name": "Ricardo Dominguez-Olmedo"
                    },
                    {
                        "name": "Moritz Hardt"
                    },
                    {
                        "name": "Celestine Mendler-Dnner"
                    }
                ],
                "author_detail": {
                    "name": "Celestine Mendler-Dnner"
                },
                "author": "Celestine Mendler-Dnner",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.07951v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.07951v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13316v3",
                "updated": "2024-12-09T10:46:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    10,
                    46,
                    23,
                    0,
                    344,
                    0
                ],
                "published": "2024-07-18T09:17:47Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    17,
                    47,
                    3,
                    200,
                    0
                ],
                "title": "Deterministic Trajectory Optimization through Probabilistic Optimal\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deterministic Trajectory Optimization through Probabilistic Optimal\n  Control"
                },
                "summary": "In this article, we discuss two algorithms tailored to discrete-time\ndeterministic finite-horizon nonlinear optimal control problems or so-called\ndeterministic trajectory optimization problems. Both algorithms can be derived\nfrom an emerging theoretical paradigm that we refer to as probabilistic optimal\ncontrol. The paradigm reformulates stochastic optimal control as an equivalent\nprobabilistic inference problem and can be viewed as a generalisation of the\nformer. The merit of this perspective is that it allows to address the problem\nusing the Expectation-Maximization algorithm. It is shown that the application\nof this algorithm results in a fixed point iteration of probabilistic policies\nthat converge to the deterministic optimal policy. Two strategies for policy\nevaluation are discussed, using state-of-the-art uncertainty quantification\nmethods resulting into two distinct algorithms. The algorithms are structurally\nclosest related to the differential dynamic programming algorithm and related\nmethods that use sigma-point methods to avoid direct gradient evaluations. The\nmain advantage of the algorithms is an improved balance between exploration and\nexploitation over the iterations, leading to improved numerical stability and\naccelerated convergence. These properties are demonstrated on different\nnonlinear systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we discuss two algorithms tailored to discrete-time\ndeterministic finite-horizon nonlinear optimal control problems or so-called\ndeterministic trajectory optimization problems. Both algorithms can be derived\nfrom an emerging theoretical paradigm that we refer to as probabilistic optimal\ncontrol. The paradigm reformulates stochastic optimal control as an equivalent\nprobabilistic inference problem and can be viewed as a generalisation of the\nformer. The merit of this perspective is that it allows to address the problem\nusing the Expectation-Maximization algorithm. It is shown that the application\nof this algorithm results in a fixed point iteration of probabilistic policies\nthat converge to the deterministic optimal policy. Two strategies for policy\nevaluation are discussed, using state-of-the-art uncertainty quantification\nmethods resulting into two distinct algorithms. The algorithms are structurally\nclosest related to the differential dynamic programming algorithm and related\nmethods that use sigma-point methods to avoid direct gradient evaluations. The\nmain advantage of the algorithms is an improved balance between exploration and\nexploitation over the iterations, leading to improved numerical stability and\naccelerated convergence. These properties are demonstrated on different\nnonlinear systems."
                },
                "authors": [
                    {
                        "name": "Mohammad Mahmoudi Filabadi"
                    },
                    {
                        "name": "Tom Lefebvre"
                    },
                    {
                        "name": "Guillaume Crevecoeur"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Crevecoeur"
                },
                "author": "Guillaume Crevecoeur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06370v1",
                "updated": "2024-12-09T10:44:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    10,
                    44,
                    47,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T10:44:47Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    10,
                    44,
                    47,
                    0,
                    344,
                    0
                ],
                "title": "Exploring Memorization and Copyright Violation in Frontier LLMs: A Study\n  of the New York Times v. OpenAI 2023 Lawsuit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Memorization and Copyright Violation in Frontier LLMs: A Study\n  of the New York Times v. OpenAI 2023 Lawsuit"
                },
                "summary": "Copyright infringement in frontier LLMs has received much attention recently\ndue to the New York Times v. OpenAI lawsuit, filed in December 2023. The New\nYork Times claims that GPT-4 has infringed its copyrights by reproducing\narticles for use in LLM training and by memorizing the inputs, thereby publicly\ndisplaying them in LLM outputs. Our work aims to measure the propensity of\nOpenAI's LLMs to exhibit verbatim memorization in its outputs relative to other\nLLMs, specifically focusing on news articles. We discover that both GPT and\nClaude models use refusal training and output filters to prevent verbatim\noutput of the memorized articles. We apply a basic prompt template to bypass\nthe refusal training and show that OpenAI models are currently less prone to\nmemorization elicitation than models from Meta, Mistral, and Anthropic. We find\nthat as models increase in size, especially beyond 100 billion parameters, they\ndemonstrate significantly greater capacity for memorization. Our findings have\npractical implications for training: more attention must be placed on\npreventing verbatim memorization in very large models. Our findings also have\nlegal significance: in assessing the relative memorization capacity of OpenAI's\nLLMs, we probe the strength of The New York Times's copyright infringement\nclaims and OpenAI's legal defenses, while underscoring issues at the\nintersection of generative AI, law, and policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copyright infringement in frontier LLMs has received much attention recently\ndue to the New York Times v. OpenAI lawsuit, filed in December 2023. The New\nYork Times claims that GPT-4 has infringed its copyrights by reproducing\narticles for use in LLM training and by memorizing the inputs, thereby publicly\ndisplaying them in LLM outputs. Our work aims to measure the propensity of\nOpenAI's LLMs to exhibit verbatim memorization in its outputs relative to other\nLLMs, specifically focusing on news articles. We discover that both GPT and\nClaude models use refusal training and output filters to prevent verbatim\noutput of the memorized articles. We apply a basic prompt template to bypass\nthe refusal training and show that OpenAI models are currently less prone to\nmemorization elicitation than models from Meta, Mistral, and Anthropic. We find\nthat as models increase in size, especially beyond 100 billion parameters, they\ndemonstrate significantly greater capacity for memorization. Our findings have\npractical implications for training: more attention must be placed on\npreventing verbatim memorization in very large models. Our findings also have\nlegal significance: in assessing the relative memorization capacity of OpenAI's\nLLMs, we probe the strength of The New York Times's copyright infringement\nclaims and OpenAI's legal defenses, while underscoring issues at the\nintersection of generative AI, law, and policy."
                },
                "authors": [
                    {
                        "name": "Joshua Freeman"
                    },
                    {
                        "name": "Chloe Rippe"
                    },
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ],
                "author_detail": {
                    "name": "Maksym Andriushchenko"
                },
                "author": "Maksym Andriushchenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21348v2",
                "updated": "2024-12-09T10:11:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    10,
                    11,
                    22,
                    0,
                    344,
                    0
                ],
                "published": "2024-10-28T11:07:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    7,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Large Language Model Benchmarks in Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Benchmarks in Medical Tasks"
                },
                "summary": "With the increasing application of large language models (LLMs) in the\nmedical domain, evaluating these models' performance using benchmark datasets\nhas become crucial. This paper presents a comprehensive survey of various\nbenchmark datasets employed in medical LLM tasks. These datasets span multiple\nmodalities including text, image, and multimodal benchmarks, focusing on\ndifferent aspects of medical knowledge such as electronic health records\n(EHRs), doctor-patient dialogues, medical question-answering, and medical image\ncaptioning. The survey categorizes the datasets by modality, discussing their\nsignificance, data structure, and impact on the development of LLMs for\nclinical tasks such as diagnosis, report generation, and predictive decision\nsupport. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and\nCheXpert, which have facilitated advancements in tasks like medical report\ngeneration, clinical summarization, and synthetic data generation. The paper\nsummarizes the challenges and opportunities in leveraging these benchmarks for\nadvancing multimodal medical intelligence, emphasizing the need for datasets\nwith a greater degree of language diversity, structured omics data, and\ninnovative approaches to synthesis. This work also provides a foundation for\nfuture research in the application of LLMs in medicine, contributing to the\nevolving field of medical artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing application of large language models (LLMs) in the\nmedical domain, evaluating these models' performance using benchmark datasets\nhas become crucial. This paper presents a comprehensive survey of various\nbenchmark datasets employed in medical LLM tasks. These datasets span multiple\nmodalities including text, image, and multimodal benchmarks, focusing on\ndifferent aspects of medical knowledge such as electronic health records\n(EHRs), doctor-patient dialogues, medical question-answering, and medical image\ncaptioning. The survey categorizes the datasets by modality, discussing their\nsignificance, data structure, and impact on the development of LLMs for\nclinical tasks such as diagnosis, report generation, and predictive decision\nsupport. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and\nCheXpert, which have facilitated advancements in tasks like medical report\ngeneration, clinical summarization, and synthetic data generation. The paper\nsummarizes the challenges and opportunities in leveraging these benchmarks for\nadvancing multimodal medical intelligence, emphasizing the need for datasets\nwith a greater degree of language diversity, structured omics data, and\ninnovative approaches to synthesis. This work also provides a foundation for\nfuture research in the application of LLMs in medicine, contributing to the\nevolving field of medical artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Lawrence K. Q. Yan"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Cheng Fei"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Yunze Wang"
                    },
                    {
                        "name": "Silin Chen"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Junyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Junyu Liu"
                },
                "author": "Junyu Liu",
                "arxiv_comment": "25 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10957v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10957v4",
                "updated": "2024-12-09T09:57:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    57,
                    5,
                    0,
                    344,
                    0
                ],
                "published": "2024-06-16T14:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    14,
                    24,
                    30,
                    6,
                    168,
                    0
                ],
                "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "EMNLP 2024 Main, Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10957v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10957v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.08339v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.08339v5",
                "updated": "2024-12-09T09:56:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    56,
                    59,
                    0,
                    344,
                    0
                ],
                "published": "2023-05-15T04:10:13Z",
                "published_parsed": [
                    2023,
                    5,
                    15,
                    4,
                    10,
                    13,
                    0,
                    135,
                    0
                ],
                "title": "Assessing the potential of LLM-assisted annotation for corpus-based\n  pragmatics and discourse analysis: The case of apology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the potential of LLM-assisted annotation for corpus-based\n  pragmatics and discourse analysis: The case of apology"
                },
                "summary": "Certain forms of linguistic annotation, like part of speech and semantic\ntagging, can be automated with high accuracy. However, manual annotation is\nstill necessary for complex pragmatic and discursive features that lack a\ndirect mapping to lexical forms. This manual process is time-consuming and\nerror-prone, limiting the scalability of function-to-form approaches in corpus\nlinguistics. To address this, our study explores the possibility of using large\nlanguage models (LLMs) to automate pragma-discursive corpus annotation. We\ncompare GPT-3.5 (the model behind the free-to-use version of ChatGPT), GPT-4\n(the model underpinning the precise mode of Bing chatbot), and a human coder in\nannotating apology components in English based on the local grammar framework.\nWe find that GPT-4 outperformed GPT-3.5, with accuracy approaching that of a\nhuman coder. These results suggest that LLMs can be successfully deployed to\naid pragma-discursive corpus annotation, making the process more efficient,\nscalable and accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certain forms of linguistic annotation, like part of speech and semantic\ntagging, can be automated with high accuracy. However, manual annotation is\nstill necessary for complex pragmatic and discursive features that lack a\ndirect mapping to lexical forms. This manual process is time-consuming and\nerror-prone, limiting the scalability of function-to-form approaches in corpus\nlinguistics. To address this, our study explores the possibility of using large\nlanguage models (LLMs) to automate pragma-discursive corpus annotation. We\ncompare GPT-3.5 (the model behind the free-to-use version of ChatGPT), GPT-4\n(the model underpinning the precise mode of Bing chatbot), and a human coder in\nannotating apology components in English based on the local grammar framework.\nWe find that GPT-4 outperformed GPT-3.5, with accuracy approaching that of a\nhuman coder. These results suggest that LLMs can be successfully deployed to\naid pragma-discursive corpus annotation, making the process more efficient,\nscalable and accessible."
                },
                "authors": [
                    {
                        "name": "Danni Yu"
                    },
                    {
                        "name": "Luyang Li"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Matteo Fuoli"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Fuoli"
                },
                "author": "Matteo Fuoli",
                "arxiv_doi": "10.1075/ijcl.23087.yu",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1075/ijcl.23087.yu",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.08339v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.08339v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 2 figures, 3 tablels",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11811v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11811v4",
                "updated": "2024-12-09T09:53:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    53,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2024-02-19T03:56:44Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    3,
                    56,
                    44,
                    0,
                    50,
                    0
                ],
                "title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema"
                },
                "summary": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and six testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and six testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "COLING 2025, Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11811v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11811v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06342v1",
                "updated": "2024-12-09T09:49:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    49,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T09:49:15Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    49,
                    15,
                    0,
                    344,
                    0
                ],
                "title": "Tracking control of latent dynamic systems with application to\n  spacecraft attitude control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking control of latent dynamic systems with application to\n  spacecraft attitude control"
                },
                "summary": "When intelligent spacecraft or space robots perform tasks in a complex\nenvironment, the controllable variables are usually not directly available and\nhave to be inferred from high-dimensional observable variables, such as outputs\nof neural networks or images. While the dynamics of these observations are\nhighly complex, the mechanisms behind them may be simple, which makes it\npossible to regard them as latent dynamic systems. For control of latent\ndynamic systems, methods based on reinforcement learning suffer from sample\ninefficiency and generalization problems. In this work, we propose an\nasymptotic tracking controller for latent dynamic systems. The latent variables\nare related to the high-dimensional observations through an unknown nonlinear\nfunction. The dynamics are unknown but assumed to be affine nonlinear. To\nrealize asymptotic tracking, an identifiable latent dynamic model is learned to\nrecover the latents and estimate the dynamics. This training process does not\ndepend on the goals or reference trajectories. Based on the learned model, we\nuse a manually designed feedback linearization controller to ensure the\nasymptotic tracking property of the closed-loop system. After considering fully\ncontrollable systems, the results are extended to the case that uncontrollable\nenvironmental latents exist. As an application, simulation experiments on a\nlatent spacecraft attitude dynamic model are conducted to verify the proposed\nmethods, and the observation noise and control deviation are taken into\nconsideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When intelligent spacecraft or space robots perform tasks in a complex\nenvironment, the controllable variables are usually not directly available and\nhave to be inferred from high-dimensional observable variables, such as outputs\nof neural networks or images. While the dynamics of these observations are\nhighly complex, the mechanisms behind them may be simple, which makes it\npossible to regard them as latent dynamic systems. For control of latent\ndynamic systems, methods based on reinforcement learning suffer from sample\ninefficiency and generalization problems. In this work, we propose an\nasymptotic tracking controller for latent dynamic systems. The latent variables\nare related to the high-dimensional observations through an unknown nonlinear\nfunction. The dynamics are unknown but assumed to be affine nonlinear. To\nrealize asymptotic tracking, an identifiable latent dynamic model is learned to\nrecover the latents and estimate the dynamics. This training process does not\ndepend on the goals or reference trajectories. Based on the learned model, we\nuse a manually designed feedback linearization controller to ensure the\nasymptotic tracking property of the closed-loop system. After considering fully\ncontrollable systems, the results are extended to the case that uncontrollable\nenvironmental latents exist. As an application, simulation experiments on a\nlatent spacecraft attitude dynamic model are conducted to verify the proposed\nmethods, and the observation noise and control deviation are taken into\nconsideration."
                },
                "authors": [
                    {
                        "name": "Congxi Zhang"
                    },
                    {
                        "name": "Yongchun Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yongchun Xie"
                },
                "author": "Yongchun Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01432v3",
                "updated": "2024-12-09T09:39:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    39,
                    12,
                    0,
                    344,
                    0
                ],
                "published": "2023-09-29T14:38:58Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    14,
                    38,
                    58,
                    4,
                    272,
                    0
                ],
                "title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split and Merge: Aligning Position Biases in LLM-based Evaluators"
                },
                "summary": "Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications."
                },
                "authors": [
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Accepted by EMNLP 2024. Please cite the conference version of this\n  paper, e.g., \"Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai\n  Wang, Cuiyun Gao, and Yang Liu. 2024. Split and Merge: Aligning Position\n  Biases in LLM-based Evaluators. (EMNLP 2024)\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09335v2",
                "updated": "2024-12-09T09:31:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    31,
                    39,
                    0,
                    344,
                    0
                ],
                "published": "2024-10-12T02:48:34Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    48,
                    34,
                    5,
                    286,
                    0
                ],
                "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You\n  Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Data Selection at Scale: Random Selection is Almost All You\n  Need"
                },
                "summary": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models\n(LLMs) with human instructions. The primary goal during SFT is to select a\nsmall yet representative subset of training data from the larger pool, such\nthat fine-tuning with this subset achieves results comparable to or even\nexceeding those obtained using the entire dataset. However, most existing data\nselection techniques are designed for small-scale data pools, which fail to\nmeet the demands of real-world SFT scenarios. In this paper, we replicated\nseveral self-scoring methods those that do not rely on external model\nassistance on two million scale datasets, and found that nearly all methods\nstruggled to significantly outperform random selection when dealing with such\nlarge-scale data pools. Moreover, our comparisons suggest that, during SFT,\ndiversity in data selection is more critical than simply focusing on high\nquality data. We also analyzed the limitations of several current approaches,\nexplaining why they perform poorly on large-scale datasets and why they are\nunsuitable for such contexts. Finally, we found that filtering data by token\nlength offers a stable and efficient method for improving results. This\napproach, particularly when training on long text data, proves highly\nbeneficial for relatively weaker base models, such as Llama3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models\n(LLMs) with human instructions. The primary goal during SFT is to select a\nsmall yet representative subset of training data from the larger pool, such\nthat fine-tuning with this subset achieves results comparable to or even\nexceeding those obtained using the entire dataset. However, most existing data\nselection techniques are designed for small-scale data pools, which fail to\nmeet the demands of real-world SFT scenarios. In this paper, we replicated\nseveral self-scoring methods those that do not rely on external model\nassistance on two million scale datasets, and found that nearly all methods\nstruggled to significantly outperform random selection when dealing with such\nlarge-scale data pools. Moreover, our comparisons suggest that, during SFT,\ndiversity in data selection is more critical than simply focusing on high\nquality data. We also analyzed the limitations of several current approaches,\nexplaining why they perform poorly on large-scale datasets and why they are\nunsuitable for such contexts. Finally, we found that filtering data by token\nlength offers a stable and efficient method for improving results. This\napproach, particularly when training on long text data, proves highly\nbeneficial for relatively weaker base models, such as Llama3."
                },
                "authors": [
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Yuan Wu"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01004v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01004v3",
                "updated": "2024-12-09T09:25:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    25,
                    36,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-01T23:41:42Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    23,
                    41,
                    42,
                    6,
                    336,
                    0
                ],
                "title": "Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual\n  Learning Vision-Language Models with Dynamic Rank-Selective LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual\n  Learning Vision-Language Models with Dynamic Rank-Selective LoRA"
                },
                "summary": "We investigate whether the pre-trained knowledge of vision-language models\n(VLMs), such as CLIP, can be retained or even enhanced during continual\nlearning (CL) while absorbing knowledge from a data stream. Existing methods\noften rely on additional reference data, isolated components for distribution\nor domain predictions, leading to high training costs, increased inference\ncomplexity, and limited improvement potential for pre-trained models. To\naddress these challenges, we first comprehensively analyze the effects of\nparameter update locations and ranks on downstream adaptation and knowledge\nretention. Based on these insights, we propose Dynamic Rank-Selective Low Rank\nAdaptation (LoRA), a universal and efficient CL approach that adaptively\nassigns ranks to LoRA modules based on their relevance to the current data.\nUnlike prior methods, our approach continually enhances the pre-trained VLM by\nretaining both the pre-trained knowledge and the knowledge acquired during CL.\nOur approach eliminates the need for explicit domain or distribution prediction\nand additional reference data, enabling seamless integration of new tasks while\npreserving pre-trained capabilities. It also maintains the original\narchitecture and deployment pipeline of the pre-trained model without incurring\nany additional inference overhead. Extensive experiments and analyses\ndemonstrate that our method outperforms state-of-the-art approaches in\ncontinually absorbing knowledge of downstream tasks while retaining pre-trained\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the pre-trained knowledge of vision-language models\n(VLMs), such as CLIP, can be retained or even enhanced during continual\nlearning (CL) while absorbing knowledge from a data stream. Existing methods\noften rely on additional reference data, isolated components for distribution\nor domain predictions, leading to high training costs, increased inference\ncomplexity, and limited improvement potential for pre-trained models. To\naddress these challenges, we first comprehensively analyze the effects of\nparameter update locations and ranks on downstream adaptation and knowledge\nretention. Based on these insights, we propose Dynamic Rank-Selective Low Rank\nAdaptation (LoRA), a universal and efficient CL approach that adaptively\nassigns ranks to LoRA modules based on their relevance to the current data.\nUnlike prior methods, our approach continually enhances the pre-trained VLM by\nretaining both the pre-trained knowledge and the knowledge acquired during CL.\nOur approach eliminates the need for explicit domain or distribution prediction\nand additional reference data, enabling seamless integration of new tasks while\npreserving pre-trained capabilities. It also maintains the original\narchitecture and deployment pipeline of the pre-trained model without incurring\nany additional inference overhead. Extensive experiments and analyses\ndemonstrate that our method outperforms state-of-the-art approaches in\ncontinually absorbing knowledge of downstream tasks while retaining pre-trained\nknowledge."
                },
                "authors": [
                    {
                        "name": "Haodong Lu"
                    },
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Jason Xue"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01004v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01004v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05248v2",
                "updated": "2024-12-09T09:21:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    21,
                    49,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-06T18:27:15Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    27,
                    15,
                    4,
                    341,
                    0
                ],
                "title": "Enhancing FKG.in: automating Indian food composition analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing FKG.in: automating Indian food composition analysis"
                },
                "summary": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG.in and iteratively supplement\nfood composition data from verified knowledge bases. Additionally, this paper\nhighlights the challenges of representing Indian food and accessing food\ncomposition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG.in and iteratively supplement\nfood composition data from verified knowledge bases. Additionally, this paper\nhighlights the challenges of representing Indian food and accessing food\ncomposition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain."
                },
                "authors": [
                    {
                        "name": "Saransh Kumar Gupta"
                    },
                    {
                        "name": "Lipika Dey"
                    },
                    {
                        "name": "Partha Pratim Das"
                    },
                    {
                        "name": "Geeta Trilok-Kumar"
                    },
                    {
                        "name": "Ramesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Jain"
                },
                "author": "Ramesh Jain",
                "arxiv_comment": "15 pages, 5 figures, 30 references, International Conference on\n  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03817v3",
                "updated": "2024-12-09T09:20:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    20,
                    11,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-06T10:35:11Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    35,
                    11,
                    2,
                    311,
                    0
                ],
                "title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning"
                },
                "summary": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Zhirui Deng"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06321v1",
                "updated": "2024-12-09T09:15:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    15,
                    35,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T09:15:35Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    15,
                    35,
                    0,
                    344,
                    0
                ],
                "title": "A Flexible Template for Edge Generative AI with High-Accuracy\n  Accelerated Softmax & GELU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Flexible Template for Edge Generative AI with High-Accuracy\n  Accelerated Softmax & GELU"
                },
                "summary": "Transformer-based generative Artificial Intelligence (GenAI) models achieve\nremarkable results in a wide range of fields, including natural language\nprocessing, computer vision, and audio processing. However, this comes at the\ncost of increased complexity and the need of sophisticated non-linearities such\nas softmax and GELU. Even if Transformers are computationally dominated by\nmatrix multiplications (MatMul), these non-linearities can become a performance\nbottleneck, especially if dedicated hardware is used to accelerate MatMul\noperators. In this work, we introduce a GenAI BFloat16 Transformer acceleration\ntemplate based on a heterogeneous tightly-coupled cluster containing 256KiB of\nshared SRAM, 8 general-purpose RISC-V cores, a 24x8 systolic array MatMul\naccelerator, and a novel accelerator for Transformer softmax and GELU\nnon-linearities: SoftEx. SoftEx introduces an approximate exponentiation\nalgorithm balancing efficiency (121x speedup over glibc's implementation) with\naccuracy (mean relative error of 0.14%). In 12nm technology, SoftEx occupies\n0.039 mm$^2$, only 3.22% of the cluster, which achieves an operating frequency\nof 1.12 GHz. Compared to optimized software running on the RISC-V cores, SoftEx\nachieves significant improvements, accelerating softmax and GELU computations\nby up to 10.8x and 5.11x, respectively, while reducing their energy consumption\nby up to 10.8x and 5.29x. These enhancements translate into a 1.58x increase in\nthroughput (310 GOPS at 0.8V) and a 1.42x improvement in energy efficiency\n(1.34 TOPS/W at 0.55V) on end-to-end ViT inference workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative Artificial Intelligence (GenAI) models achieve\nremarkable results in a wide range of fields, including natural language\nprocessing, computer vision, and audio processing. However, this comes at the\ncost of increased complexity and the need of sophisticated non-linearities such\nas softmax and GELU. Even if Transformers are computationally dominated by\nmatrix multiplications (MatMul), these non-linearities can become a performance\nbottleneck, especially if dedicated hardware is used to accelerate MatMul\noperators. In this work, we introduce a GenAI BFloat16 Transformer acceleration\ntemplate based on a heterogeneous tightly-coupled cluster containing 256KiB of\nshared SRAM, 8 general-purpose RISC-V cores, a 24x8 systolic array MatMul\naccelerator, and a novel accelerator for Transformer softmax and GELU\nnon-linearities: SoftEx. SoftEx introduces an approximate exponentiation\nalgorithm balancing efficiency (121x speedup over glibc's implementation) with\naccuracy (mean relative error of 0.14%). In 12nm technology, SoftEx occupies\n0.039 mm$^2$, only 3.22% of the cluster, which achieves an operating frequency\nof 1.12 GHz. Compared to optimized software running on the RISC-V cores, SoftEx\nachieves significant improvements, accelerating softmax and GELU computations\nby up to 10.8x and 5.11x, respectively, while reducing their energy consumption\nby up to 10.8x and 5.29x. These enhancements translate into a 1.58x increase in\nthroughput (310 GOPS at 0.8V) and a 1.42x improvement in energy efficiency\n(1.34 TOPS/W at 0.55V) on end-to-end ViT inference workloads."
                },
                "authors": [
                    {
                        "name": "Andrea Belano"
                    },
                    {
                        "name": "Yvan Tortorella"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    },
                    {
                        "name": "Francesco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Conti"
                },
                "author": "Francesco Conti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06308v1",
                "updated": "2024-12-09T08:55:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    55,
                    48,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:55:48Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    55,
                    48,
                    0,
                    344,
                    0
                ],
                "title": "PRECISE: Pre-training Sequential Recommenders with Collaborative and\n  Semantic Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRECISE: Pre-training Sequential Recommenders with Collaborative and\n  Semantic Information"
                },
                "summary": "Real-world recommendation systems commonly offer diverse content scenarios\nfor users to interact with. Considering the enormous number of users in\nindustrial platforms, it is infeasible to utilize a single unified\nrecommendation model to meet the requirements of all scenarios. Usually,\nseparate recommendation pipelines are established for each distinct scenario.\nThis practice leads to challenges in comprehensively grasping users' interests.\nRecent research endeavors have been made to tackle this problem by pre-training\nmodels to encapsulate the overall interests of users. Traditional pre-trained\nrecommendation models mainly capture user interests by leveraging collaborative\nsignals. Nevertheless, a prevalent drawback of these systems is their\nincapacity to handle long-tail items and cold-start scenarios. With the recent\nadvent of large language models, there has been a significant increase in\nresearch efforts focused on exploiting LLMs to extract semantic information for\nusers and items. However, text-based recommendations highly rely on elaborate\nfeature engineering and frequently fail to capture collaborative similarities.\nTo overcome these limitations, we propose a novel pre-training framework for\nsequential recommendation, termed PRECISE. This framework combines\ncollaborative signals with semantic information. Moreover, PRECISE employs a\nlearning framework that initially models users' comprehensive interests across\nall recommendation scenarios and subsequently concentrates on the specific\ninterests of target-scene behaviors. We demonstrate that PRECISE precisely\ncaptures the entire range of user interests and effectively transfers them to\nthe target interests. Empirical findings reveal that the PRECISE framework\nattains outstanding performance on both public and industrial datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world recommendation systems commonly offer diverse content scenarios\nfor users to interact with. Considering the enormous number of users in\nindustrial platforms, it is infeasible to utilize a single unified\nrecommendation model to meet the requirements of all scenarios. Usually,\nseparate recommendation pipelines are established for each distinct scenario.\nThis practice leads to challenges in comprehensively grasping users' interests.\nRecent research endeavors have been made to tackle this problem by pre-training\nmodels to encapsulate the overall interests of users. Traditional pre-trained\nrecommendation models mainly capture user interests by leveraging collaborative\nsignals. Nevertheless, a prevalent drawback of these systems is their\nincapacity to handle long-tail items and cold-start scenarios. With the recent\nadvent of large language models, there has been a significant increase in\nresearch efforts focused on exploiting LLMs to extract semantic information for\nusers and items. However, text-based recommendations highly rely on elaborate\nfeature engineering and frequently fail to capture collaborative similarities.\nTo overcome these limitations, we propose a novel pre-training framework for\nsequential recommendation, termed PRECISE. This framework combines\ncollaborative signals with semantic information. Moreover, PRECISE employs a\nlearning framework that initially models users' comprehensive interests across\nall recommendation scenarios and subsequently concentrates on the specific\ninterests of target-scene behaviors. We demonstrate that PRECISE precisely\ncaptures the entire range of user interests and effectively transfers them to\nthe target interests. Empirical findings reveal that the PRECISE framework\nattains outstanding performance on both public and industrial datasets."
                },
                "authors": [
                    {
                        "name": "Chonggang Song"
                    },
                    {
                        "name": "Chunxu Shen"
                    },
                    {
                        "name": "Hao Gu"
                    },
                    {
                        "name": "Yaoming Wu"
                    },
                    {
                        "name": "Lingling Yi"
                    },
                    {
                        "name": "Jie Wen"
                    },
                    {
                        "name": "Chuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Chen"
                },
                "author": "Chuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06303v1",
                "updated": "2024-12-09T08:47:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    47,
                    5,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:47:05Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    47,
                    5,
                    0,
                    344,
                    0
                ],
                "title": "DSAI: Unbiased and Interpretable Latent Feature Extraction for\n  Data-Centric AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSAI: Unbiased and Interpretable Latent Feature Extraction for\n  Data-Centric AI"
                },
                "summary": "Large language models (LLMs) often struggle to objectively identify latent\ncharacteristics in large datasets due to their reliance on pre-trained\nknowledge rather than actual data patterns. To address this data grounding\nissue, we propose Data Scientist AI (DSAI), a framework that enables unbiased\nand interpretable feature extraction through a multi-stage pipeline with\nquantifiable prominence metrics for evaluating extracted features. On synthetic\ndatasets with known ground-truth features, DSAI demonstrates high recall in\nidentifying expert-defined features while faithfully reflecting the underlying\ndata. Applications on real-world datasets illustrate the framework's practical\nutility in uncovering meaningful patterns with minimal expert oversight,\nsupporting use cases such as interpretable classification.\n  The title of our paper is chosen from multiple candidates based on\nDSAI-generated criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often struggle to objectively identify latent\ncharacteristics in large datasets due to their reliance on pre-trained\nknowledge rather than actual data patterns. To address this data grounding\nissue, we propose Data Scientist AI (DSAI), a framework that enables unbiased\nand interpretable feature extraction through a multi-stage pipeline with\nquantifiable prominence metrics for evaluating extracted features. On synthetic\ndatasets with known ground-truth features, DSAI demonstrates high recall in\nidentifying expert-defined features while faithfully reflecting the underlying\ndata. Applications on real-world datasets illustrate the framework's practical\nutility in uncovering meaningful patterns with minimal expert oversight,\nsupporting use cases such as interpretable classification.\n  The title of our paper is chosen from multiple candidates based on\nDSAI-generated criteria."
                },
                "authors": [
                    {
                        "name": "Hyowon Cho"
                    },
                    {
                        "name": "Soonwon Ka"
                    },
                    {
                        "name": "Daechul Park"
                    },
                    {
                        "name": "Jaewook Kang"
                    },
                    {
                        "name": "Minjoon Seo"
                    },
                    {
                        "name": "Bokyung Son"
                    }
                ],
                "author_detail": {
                    "name": "Bokyung Son"
                },
                "author": "Bokyung Son",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06294v1",
                "updated": "2024-12-09T08:37:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    37,
                    6,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:37:06Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    37,
                    6,
                    0,
                    344,
                    0
                ],
                "title": "Beyond pip install: Evaluating LLM Agents for the Automated Installation\n  of Python Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond pip install: Evaluating LLM Agents for the Automated Installation\n  of Python Projects"
                },
                "summary": "Many works have recently proposed the use of Large Language Model (LLM) based\nagents for performing `repository level' tasks, loosely defined as a set of\ntasks whose scopes are greater than a single file. This has led to speculation\nthat the orchestration of these repository-level tasks could lead to software\nengineering agents capable of performing almost independently of human\nintervention. However, of the suite of tasks that would need to be performed by\nthis autonomous software engineering agent, we argue that one important task is\nmissing, which is to fulfil project level dependency by installing other\nrepositories. To investigate the feasibility of this repository level\ninstallation task, we introduce a benchmark of of repository installation tasks\ncurated from 40 open source Python projects, which includes a ground truth\ninstallation process for each target repository. Further, we propose\nInstallamatic, an agent which aims to perform and verify the installation of a\ngiven repository by searching for relevant instructions from documentation in\nthe repository. Empirical experiments reveal that that 55% of the studied\nrepositories can be automatically installed by our agent at least one out of\nten times. Through further analysis, we identify the common causes for our\nagent's inability to install a repository, discuss the challenges faced in the\ndesign and implementation of such an agent and consider the implications that\nsuch an agent could have for developers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many works have recently proposed the use of Large Language Model (LLM) based\nagents for performing `repository level' tasks, loosely defined as a set of\ntasks whose scopes are greater than a single file. This has led to speculation\nthat the orchestration of these repository-level tasks could lead to software\nengineering agents capable of performing almost independently of human\nintervention. However, of the suite of tasks that would need to be performed by\nthis autonomous software engineering agent, we argue that one important task is\nmissing, which is to fulfil project level dependency by installing other\nrepositories. To investigate the feasibility of this repository level\ninstallation task, we introduce a benchmark of of repository installation tasks\ncurated from 40 open source Python projects, which includes a ground truth\ninstallation process for each target repository. Further, we propose\nInstallamatic, an agent which aims to perform and verify the installation of a\ngiven repository by searching for relevant instructions from documentation in\nthe repository. Empirical experiments reveal that that 55% of the studied\nrepositories can be automatically installed by our agent at least one out of\nten times. Through further analysis, we identify the common causes for our\nagent's inability to install a repository, discuss the challenges faced in the\ndesign and implementation of such an agent and consider the implications that\nsuch an agent could have for developers."
                },
                "authors": [
                    {
                        "name": "Louis Milliken"
                    },
                    {
                        "name": "Sungmin Kang"
                    },
                    {
                        "name": "Shin Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shin Yoo"
                },
                "author": "Shin Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00627v2",
                "updated": "2024-12-09T08:27:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    27,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-01T00:52:51Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    0,
                    52,
                    51,
                    6,
                    336,
                    0
                ],
                "title": "ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by\n  Multimodal Gemini LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by\n  Multimodal Gemini LLM"
                },
                "summary": "Cooking meals can be difficult, causing many to resort to cookbooks and\nonline recipes. However, relying on these traditional methods of cooking often\nresults in missing ingredients, nutritional hazards, and unsatisfactory meals.\nUsing Augmented Reality (AR) can address these issues; however, current AR\ncooking applications have poor user interfaces and limited accessibility. This\npaper proposes a prototype of an iOS application that integrates AR and\nComputer Vision (CV) into the cooking process. We leverage Google's Gemini\nLarge Language Model (LLM) to identify ingredients in the camera's field of\nvision and generate recipe choices with detailed nutritional information.\nAdditionally, this application uses Apple's ARKit to create an AR user\ninterface compatible with iOS devices. Users can personalize their meal\nsuggestions by inputting their dietary preferences and rating each meal. The\napplication's effectiveness is evaluated through three rounds of user\nexperience surveys. This application advances the field of accessible cooking\nassistance technologies, aiming to reduce food wastage and improve the meal\nplanning experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooking meals can be difficult, causing many to resort to cookbooks and\nonline recipes. However, relying on these traditional methods of cooking often\nresults in missing ingredients, nutritional hazards, and unsatisfactory meals.\nUsing Augmented Reality (AR) can address these issues; however, current AR\ncooking applications have poor user interfaces and limited accessibility. This\npaper proposes a prototype of an iOS application that integrates AR and\nComputer Vision (CV) into the cooking process. We leverage Google's Gemini\nLarge Language Model (LLM) to identify ingredients in the camera's field of\nvision and generate recipe choices with detailed nutritional information.\nAdditionally, this application uses Apple's ARKit to create an AR user\ninterface compatible with iOS devices. Users can personalize their meal\nsuggestions by inputting their dietary preferences and rating each meal. The\napplication's effectiveness is evaluated through three rounds of user\nexperience surveys. This application advances the field of accessible cooking\nassistance technologies, aiming to reduce food wastage and improve the meal\nplanning experience."
                },
                "authors": [
                    {
                        "name": "Rithik Vir"
                    },
                    {
                        "name": "Parsa Madinei"
                    }
                ],
                "author_detail": {
                    "name": "Parsa Madinei"
                },
                "author": "Parsa Madinei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06289v1",
                "updated": "2024-12-09T08:24:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    24,
                    11,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:24:11Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    24,
                    11,
                    0,
                    344,
                    0
                ],
                "title": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity"
                },
                "summary": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents overfitting and\nforgetting, delivers SOTA performance on both commonsense and arithmetic\nreasoning with 4.6% and 1.3% average improvements compared to LoRA, and\nsurpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents overfitting and\nforgetting, delivers SOTA performance on both commonsense and arithmetic\nreasoning with 4.6% and 1.3% average improvements compared to LoRA, and\nsurpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Jixuan Leng"
                    },
                    {
                        "name": "Geyang Guo"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Ryumei Nakada"
                    },
                    {
                        "name": "Linjun Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06287v1",
                "updated": "2024-12-09T08:19:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    19,
                    28,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:19:28Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    19,
                    28,
                    0,
                    344,
                    0
                ],
                "title": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking\n  Large Language Models"
                },
                "summary": "The emergence of Large Language Models (LLMs) in the medical domain has\nstressed a compelling need for standard datasets to evaluate their\nquestion-answering (QA) performance. Although there have been several benchmark\ndatasets for medical QA, they either cover common knowledge across different\ndepartments or are specific to another department rather than pediatrics.\nMoreover, some of them are limited to objective questions and do not measure\nthe generation capacity of LLMs. Therefore, they cannot comprehensively assess\nthe QA ability of LLMs in pediatrics. To fill this gap, we construct\nPediaBench, the first Chinese pediatric dataset for LLM evaluation.\nSpecifically, it contains 4,565 objective questions and 1,632 subjective\nquestions spanning 12 pediatric disease groups. It adopts an integrated scoring\ncriterion based on different difficulty levels to thoroughly assess the\nproficiency of an LLM in instruction following, knowledge understanding,\nclinical case analysis, etc. Finally, we validate the effectiveness of\nPediaBench with extensive experiments on 20 open-source and commercial LLMs.\nThrough an in-depth analysis of experimental results, we offer insights into\nthe ability of LLMs to answer pediatric questions in the Chinese context,\nhighlighting their limitations for further improvements. Our code and data are\npublished at https://github.com/ACMISLab/PediaBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) in the medical domain has\nstressed a compelling need for standard datasets to evaluate their\nquestion-answering (QA) performance. Although there have been several benchmark\ndatasets for medical QA, they either cover common knowledge across different\ndepartments or are specific to another department rather than pediatrics.\nMoreover, some of them are limited to objective questions and do not measure\nthe generation capacity of LLMs. Therefore, they cannot comprehensively assess\nthe QA ability of LLMs in pediatrics. To fill this gap, we construct\nPediaBench, the first Chinese pediatric dataset for LLM evaluation.\nSpecifically, it contains 4,565 objective questions and 1,632 subjective\nquestions spanning 12 pediatric disease groups. It adopts an integrated scoring\ncriterion based on different difficulty levels to thoroughly assess the\nproficiency of an LLM in instruction following, knowledge understanding,\nclinical case analysis, etc. Finally, we validate the effectiveness of\nPediaBench with extensive experiments on 20 open-source and commercial LLMs.\nThrough an in-depth analysis of experimental results, we offer insights into\nthe ability of LLMs to answer pediatric questions in the Chinese context,\nhighlighting their limitations for further improvements. Our code and data are\npublished at https://github.com/ACMISLab/PediaBench."
                },
                "authors": [
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Panfeng Chen"
                    },
                    {
                        "name": "Jiali Li"
                    },
                    {
                        "name": "Linkun Feng"
                    },
                    {
                        "name": "Shuyu Liu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Yanhao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanhao Wang"
                },
                "author": "Yanhao Wang",
                "arxiv_comment": "21 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04139v2",
                "updated": "2024-12-09T07:49:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    49,
                    1,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-05T13:06:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    6,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "Monet: Mixture of Monosemantic Experts for Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monet: Mixture of Monosemantic Experts for Transformers"
                },
                "summary": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet."
                },
                "authors": [
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Young Jin Ahn"
                    },
                    {
                        "name": "Kee-Eung Kim"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06272v1",
                "updated": "2024-12-09T07:46:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    46,
                    14,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T07:46:14Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    46,
                    14,
                    0,
                    344,
                    0
                ],
                "title": "Methods for Legal Citation Prediction in the Age of LLMs: An Australian\n  Law Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods for Legal Citation Prediction in the Age of LLMs: An Australian\n  Law Case Study"
                },
                "summary": "In recent years, Large Language Models (LLMs) have shown great potential\nacross a wide range of legal tasks. Despite these advances, mitigating\nhallucination remains a significant challenge, with state-of-the-art LLMs still\nfrequently generating incorrect legal references. In this paper, we focus on\nthe problem of legal citation prediction within the Australian law context,\nwhere correctly identifying and citing relevant legislations or precedents is\ncritical. We compare several approaches: prompting general purpose and\nlaw-specialised LLMs, retrieval-only pipelines with both generic and\ndomain-specific embeddings, task-specific instruction-tuning of LLMs, and\nhybrid strategies that combine LLMs with retrieval augmentation, query\nexpansion, or voting ensembles. Our findings indicate that domain-specific\npre-training alone is insufficient for achieving satisfactory citation accuracy\neven after law-specialised pre-training. In contrast, instruction tuning on our\ntask-specific dataset dramatically boosts performance reaching the best results\nacross all settings. We also highlight that database granularity along with the\ntype of embeddings play a critical role in the performance of retrieval\nsystems. Among retrieval-based approaches, hybrid methods consistently\noutperform retrieval-only setups, and among these, ensemble voting delivers the\nbest result by combining the predictive quality of instruction-tuned LLMs with\nthe retrieval system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have shown great potential\nacross a wide range of legal tasks. Despite these advances, mitigating\nhallucination remains a significant challenge, with state-of-the-art LLMs still\nfrequently generating incorrect legal references. In this paper, we focus on\nthe problem of legal citation prediction within the Australian law context,\nwhere correctly identifying and citing relevant legislations or precedents is\ncritical. We compare several approaches: prompting general purpose and\nlaw-specialised LLMs, retrieval-only pipelines with both generic and\ndomain-specific embeddings, task-specific instruction-tuning of LLMs, and\nhybrid strategies that combine LLMs with retrieval augmentation, query\nexpansion, or voting ensembles. Our findings indicate that domain-specific\npre-training alone is insufficient for achieving satisfactory citation accuracy\neven after law-specialised pre-training. In contrast, instruction tuning on our\ntask-specific dataset dramatically boosts performance reaching the best results\nacross all settings. We also highlight that database granularity along with the\ntype of embeddings play a critical role in the performance of retrieval\nsystems. Among retrieval-based approaches, hybrid methods consistently\noutperform retrieval-only setups, and among these, ensemble voting delivers the\nbest result by combining the predictive quality of instruction-tuned LLMs with\nthe retrieval system."
                },
                "authors": [
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Jiuzhou Han"
                    },
                    {
                        "name": "Paul Burgess"
                    }
                ],
                "author_detail": {
                    "name": "Paul Burgess"
                },
                "author": "Paul Burgess",
                "arxiv_comment": "For code, data, and models see https://auslawbench.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06263v1",
                "updated": "2024-12-09T07:22:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    22,
                    19,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T07:22:19Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    22,
                    19,
                    0,
                    344,
                    0
                ],
                "title": "iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large\n  Multimodal Models"
                },
                "summary": "In this paper, we introduce iLLaVA, a simple method that can be seamlessly\ndeployed upon current Large Vision-Language Models (LVLMs) to greatly increase\nthe throughput with nearly lossless model performance, without a further\nrequirement to train. iLLaVA achieves this by finding and gradually merging the\nredundant tokens with an accurate and fast algorithm, which can merge hundreds\nof tokens within only one step. While some previous methods have explored\ndirectly pruning or merging tokens in the inference stage to accelerate models,\nour method excels in both performance and throughput by two key designs. First,\nwhile most previous methods only try to save the computations of Large Language\nModels (LLMs), our method accelerates the forward pass of both image encoders\nand LLMs in LVLMs, which both occupy a significant part of time during\ninference. Second, our method recycles the beneficial information from the\npruned tokens into existing tokens, which avoids directly dropping context\ntokens like previous methods to cause performance loss. iLLaVA can nearly\n2$\\times$ the throughput, and reduce the memory costs by half with only a 0.2\\%\n- 0.5\\% performance drop across models of different scales including 7B, 13B\nand 34B. On tasks across different domains including single-image, multi-images\nand videos, iLLaVA demonstrates strong generalizability with consistently\npromising efficiency. We finally offer abundant visualizations to show the\nmerging processes of iLLaVA in each step, which show insights into the\ndistribution of computing resources in LVLMs. Code is available at\nhttps://github.com/hulianyuyy/iLLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce iLLaVA, a simple method that can be seamlessly\ndeployed upon current Large Vision-Language Models (LVLMs) to greatly increase\nthe throughput with nearly lossless model performance, without a further\nrequirement to train. iLLaVA achieves this by finding and gradually merging the\nredundant tokens with an accurate and fast algorithm, which can merge hundreds\nof tokens within only one step. While some previous methods have explored\ndirectly pruning or merging tokens in the inference stage to accelerate models,\nour method excels in both performance and throughput by two key designs. First,\nwhile most previous methods only try to save the computations of Large Language\nModels (LLMs), our method accelerates the forward pass of both image encoders\nand LLMs in LVLMs, which both occupy a significant part of time during\ninference. Second, our method recycles the beneficial information from the\npruned tokens into existing tokens, which avoids directly dropping context\ntokens like previous methods to cause performance loss. iLLaVA can nearly\n2$\\times$ the throughput, and reduce the memory costs by half with only a 0.2\\%\n- 0.5\\% performance drop across models of different scales including 7B, 13B\nand 34B. On tasks across different domains including single-image, multi-images\nand videos, iLLaVA demonstrates strong generalizability with consistently\npromising efficiency. We finally offer abundant visualizations to show the\nmerging processes of iLLaVA in each step, which show insights into the\ndistribution of computing resources in LVLMs. Code is available at\nhttps://github.com/hulianyuyy/iLLaVA."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Liang Wan"
                    },
                    {
                        "name": "Wei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Feng"
                },
                "author": "Wei Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06262v1",
                "updated": "2024-12-09T07:21:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    21,
                    27,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T07:21:27Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    21,
                    27,
                    0,
                    344,
                    0
                ],
                "title": "A Lightweight U-like Network Utilizing Neural Memory Ordinary\n  Differential Equations for Slimming the Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight U-like Network Utilizing Neural Memory Ordinary\n  Differential Equations for Slimming the Decoder"
                },
                "summary": "In recent years, advanced U-like networks have demonstrated remarkable\nperformance in medical image segmentation tasks. However, their drawbacks,\nincluding excessive parameters, high computational complexity, and slow\ninference speed, pose challenges for practical implementation in scenarios with\nlimited computational resources. Existing lightweight U-like networks have\nalleviated some of these problems, but they often have pre-designed structures\nand consist of inseparable modules, limiting their application scenarios. In\nthis paper, we propose three plug-and-play decoders by employing different\ndiscretization methods of the neural memory Ordinary Differential Equations\n(nmODEs). These decoders integrate features at various levels of abstraction by\nprocessing information from skip connections and performing numerical\noperations on upward path. Through experiments on the PH2, ISIC2017, and\nISIC2018 datasets, we embed these decoders into different U-like networks,\ndemonstrating their effectiveness in significantly reducing the number of\nparameters and FLOPs while maintaining performance. In summary, the proposed\ndiscretized nmODEs decoders are capable of reducing the number of parameters by\nabout 20% ~ 50% and FLOPs by up to 74%, while possessing the potential to adapt\nto all U-like networks. Our code is available at\nhttps://github.com/nayutayuki/Lightweight-nmODE-Decoders-For-U-like-networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, advanced U-like networks have demonstrated remarkable\nperformance in medical image segmentation tasks. However, their drawbacks,\nincluding excessive parameters, high computational complexity, and slow\ninference speed, pose challenges for practical implementation in scenarios with\nlimited computational resources. Existing lightweight U-like networks have\nalleviated some of these problems, but they often have pre-designed structures\nand consist of inseparable modules, limiting their application scenarios. In\nthis paper, we propose three plug-and-play decoders by employing different\ndiscretization methods of the neural memory Ordinary Differential Equations\n(nmODEs). These decoders integrate features at various levels of abstraction by\nprocessing information from skip connections and performing numerical\noperations on upward path. Through experiments on the PH2, ISIC2017, and\nISIC2018 datasets, we embed these decoders into different U-like networks,\ndemonstrating their effectiveness in significantly reducing the number of\nparameters and FLOPs while maintaining performance. In summary, the proposed\ndiscretized nmODEs decoders are capable of reducing the number of parameters by\nabout 20% ~ 50% and FLOPs by up to 74%, while possessing the potential to adapt\nto all U-like networks. Our code is available at\nhttps://github.com/nayutayuki/Lightweight-nmODE-Decoders-For-U-like-networks."
                },
                "authors": [
                    {
                        "name": "Quansong He"
                    },
                    {
                        "name": "Xiaojun Yao"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Zhang Yi"
                    },
                    {
                        "name": "Tao He"
                    }
                ],
                "author_detail": {
                    "name": "Tao He"
                },
                "author": "Tao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20362v2",
                "updated": "2024-12-09T07:17:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    17,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2024-10-27T07:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    7,
                    38,
                    39,
                    6,
                    301,
                    0
                ],
                "title": "Rethinking Data Synthesis: A Teacher Model Training Recipe with\n  Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Data Synthesis: A Teacher Model Training Recipe with\n  Interpretation"
                },
                "summary": "Recent advances in large language model (LLM) training have highlighted the\nneed for diverse, high-quality instruction data. Recently, many works are\nexploring synthetic data generation using LLMs. However, they primarily focus\non prompt engineering with standard supervised instruction-finetuned models,\nwhich contains a fundamental limitation: these models are optimized for general\nquestion-answering/problem-solving rather than data generation. We propose a\nparadigm shift named \\textbf{NOMAD} by investigating how to specifically train\nmodels for data generation, demonstrating that this task differs significantly\nfrom training a classical LM. We identify two key factors: no-prompt-masked\ntraining and proper training set size selection. Our method, NOMAD, shows\nsubstantial improvements over baselines, achieving >4\\% gains in TriviaQA and\n>2\\% in GSM8K with limited training data. Finally, we offer new insights by\ninterpreting synthetic data through the lenses of \"relevance\" and \"novelty\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) training have highlighted the\nneed for diverse, high-quality instruction data. Recently, many works are\nexploring synthetic data generation using LLMs. However, they primarily focus\non prompt engineering with standard supervised instruction-finetuned models,\nwhich contains a fundamental limitation: these models are optimized for general\nquestion-answering/problem-solving rather than data generation. We propose a\nparadigm shift named \\textbf{NOMAD} by investigating how to specifically train\nmodels for data generation, demonstrating that this task differs significantly\nfrom training a classical LM. We identify two key factors: no-prompt-masked\ntraining and proper training set size selection. Our method, NOMAD, shows\nsubstantial improvements over baselines, achieving >4\\% gains in TriviaQA and\n>2\\% in GSM8K with limited training data. Finally, we offer new insights by\ninterpreting synthetic data through the lenses of \"relevance\" and \"novelty\"."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "David Zhu"
                    },
                    {
                        "name": "Simon Du"
                    },
                    {
                        "name": "Kevin Jamieson"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06245v1",
                "updated": "2024-12-09T06:37:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    6,
                    37,
                    35,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T06:37:35Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    6,
                    37,
                    35,
                    0,
                    344,
                    0
                ],
                "title": "A Comparative Study of Learning Paradigms in Large Language Models via\n  Intrinsic Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of Learning Paradigms in Large Language Models via\n  Intrinsic Dimension"
                },
                "summary": "The performance of Large Language Models (LLMs) on natural language tasks can\nbe improved through both supervised fine-tuning (SFT) and in-context learning\n(ICL), which operate via distinct mechanisms. Supervised fine-tuning updates\nthe model's weights by minimizing loss on training data, whereas in-context\nlearning leverages task demonstrations embedded in the prompt, without changing\nthe model's parameters. This study investigates the effects of these learning\nparadigms on the hidden representations of LLMs using Intrinsic Dimension (ID).\nWe use ID to estimate the number of degrees of freedom between representations\nextracted from LLMs as they perform specific natural language tasks. We first\nexplore how the ID of LLM representations evolves during SFT and how it varies\ndue to the number of demonstrations in ICL. We then compare the IDs induced by\nSFT and ICL and find that ICL consistently induces a higher ID compared to SFT,\nsuggesting that representations generated during ICL reside in higher\ndimensional manifolds in the embedding space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) on natural language tasks can\nbe improved through both supervised fine-tuning (SFT) and in-context learning\n(ICL), which operate via distinct mechanisms. Supervised fine-tuning updates\nthe model's weights by minimizing loss on training data, whereas in-context\nlearning leverages task demonstrations embedded in the prompt, without changing\nthe model's parameters. This study investigates the effects of these learning\nparadigms on the hidden representations of LLMs using Intrinsic Dimension (ID).\nWe use ID to estimate the number of degrees of freedom between representations\nextracted from LLMs as they perform specific natural language tasks. We first\nexplore how the ID of LLM representations evolves during SFT and how it varies\ndue to the number of demonstrations in ICL. We then compare the IDs induced by\nSFT and ICL and find that ICL consistently induces a higher ID compared to SFT,\nsuggesting that representations generated during ICL reside in higher\ndimensional manifolds in the embedding space."
                },
                "authors": [
                    {
                        "name": "Saahith Janapati"
                    },
                    {
                        "name": "Yangfeng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Yangfeng Ji"
                },
                "author": "Yangfeng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06233v1",
                "updated": "2024-12-09T06:14:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    6,
                    14,
                    47,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T06:14:47Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    6,
                    14,
                    47,
                    0,
                    344,
                    0
                ],
                "title": "Representational Transfer Learning for Matrix Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representational Transfer Learning for Matrix Completion"
                },
                "summary": "We propose to transfer representational knowledge from multiple sources to a\ntarget noisy matrix completion task by aggregating singular subspaces\ninformation. Under our representational similarity framework, we first\nintegrate linear representation information by solving a two-way principal\ncomponent analysis problem based on a properly debiased matrix-valued dataset.\nAfter acquiring better column and row representation estimators from the\nsources, the original high-dimensional target matrix completion problem is then\ntransformed into a low-dimensional linear regression, of which the statistical\nefficiency is guaranteed. A variety of extensional arguments, including\npost-transfer statistical inference and robustness against negative transfer,\nare also discussed alongside. Finally, extensive simulation results and a\nnumber of real data cases are reported to support our claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to transfer representational knowledge from multiple sources to a\ntarget noisy matrix completion task by aggregating singular subspaces\ninformation. Under our representational similarity framework, we first\nintegrate linear representation information by solving a two-way principal\ncomponent analysis problem based on a properly debiased matrix-valued dataset.\nAfter acquiring better column and row representation estimators from the\nsources, the original high-dimensional target matrix completion problem is then\ntransformed into a low-dimensional linear regression, of which the statistical\nefficiency is guaranteed. A variety of extensional arguments, including\npost-transfer statistical inference and robustness against negative transfer,\nare also discussed alongside. Finally, extensive simulation results and a\nnumber of real data cases are reported to support our claims."
                },
                "authors": [
                    {
                        "name": "Yong He"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Kangxiang Qin"
                    },
                    {
                        "name": "Jiahui Xie"
                    }
                ],
                "author_detail": {
                    "name": "Jiahui Xie"
                },
                "author": "Jiahui Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.06769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06769v1",
                "updated": "2024-12-09T18:55:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    55,
                    56,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:55:56Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    55,
                    56,
                    0,
                    344,
                    0
                ],
                "title": "Training Large Language Models to Reason in a Continuous Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models to Reason in a Continuous Latent Space"
                },
                "summary": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research."
                },
                "authors": [
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Yuandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuandong Tian"
                },
                "author": "Yuandong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06757v1",
                "updated": "2024-12-09T18:47:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    47,
                    31,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:47:31Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    47,
                    31,
                    0,
                    344,
                    0
                ],
                "title": "Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating\n  Usage and Reliance on ChatGPT-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating\n  Usage and Reliance on ChatGPT-Generated Code"
                },
                "summary": "Large language models (LLMs) like ChatGPT have shown the potential to assist\ndevelopers with coding and debugging tasks. However, their role in\ncollaborative issue resolution is underexplored. In this study, we analyzed\n1,152 Developer-ChatGPT conversations across 1,012 issues in GitHub to examine\nthe diverse usage of ChatGPT and reliance on its generated code. Our\ncontributions are fourfold. First, we manually analyzed 289 conversations to\nunderstand ChatGPT's usage in the GitHub Issues. Our analysis revealed that\nChatGPT is primarily utilized for ideation, whereas its usage for validation\n(e.g., code documentation accuracy) is minimal. Second, we applied BERTopic\nmodeling to identify key areas of engagement on the entire dataset. We found\nthat backend issues (e.g., API management) dominate conversations, while\ntesting is surprisingly less covered. Third, we utilized the CPD clone\ndetection tool to check if the code generated by ChatGPT was used to address\nissues. Our findings revealed that ChatGPT-generated code was used as-is to\nresolve only 5.83\\% of the issues. Fourth, we estimated sentiment using a\nRoBERTa-based sentiment analysis model to determine developers' satisfaction\nwith different usages and engagement areas. We found positive sentiment (i.e.,\nhigh satisfaction) about using ChatGPT for refactoring and addressing data\nanalytics (e.g., categorizing table data) issues. On the contrary, we observed\nnegative sentiment when using ChatGPT to debug issues and address automation\ntasks (e.g., GUI interactions). Our findings show the unmet needs and growing\ndissatisfaction among developers. Researchers and ChatGPT developers should\nfocus on developing task-specific solutions that help resolve diverse issues,\nimproving user satisfaction and problem-solving efficiency in software\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like ChatGPT have shown the potential to assist\ndevelopers with coding and debugging tasks. However, their role in\ncollaborative issue resolution is underexplored. In this study, we analyzed\n1,152 Developer-ChatGPT conversations across 1,012 issues in GitHub to examine\nthe diverse usage of ChatGPT and reliance on its generated code. Our\ncontributions are fourfold. First, we manually analyzed 289 conversations to\nunderstand ChatGPT's usage in the GitHub Issues. Our analysis revealed that\nChatGPT is primarily utilized for ideation, whereas its usage for validation\n(e.g., code documentation accuracy) is minimal. Second, we applied BERTopic\nmodeling to identify key areas of engagement on the entire dataset. We found\nthat backend issues (e.g., API management) dominate conversations, while\ntesting is surprisingly less covered. Third, we utilized the CPD clone\ndetection tool to check if the code generated by ChatGPT was used to address\nissues. Our findings revealed that ChatGPT-generated code was used as-is to\nresolve only 5.83\\% of the issues. Fourth, we estimated sentiment using a\nRoBERTa-based sentiment analysis model to determine developers' satisfaction\nwith different usages and engagement areas. We found positive sentiment (i.e.,\nhigh satisfaction) about using ChatGPT for refactoring and addressing data\nanalytics (e.g., categorizing table data) issues. On the contrary, we observed\nnegative sentiment when using ChatGPT to debug issues and address automation\ntasks (e.g., GUI interactions). Our findings show the unmet needs and growing\ndissatisfaction among developers. Researchers and ChatGPT developers should\nfocus on developing task-specific solutions that help resolve diverse issues,\nimproving user satisfaction and problem-solving efficiency in software\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Joy Krishan Das"
                    },
                    {
                        "name": "Saikat Mondal"
                    },
                    {
                        "name": "Chanchal K. Roy"
                    }
                ],
                "author_detail": {
                    "name": "Chanchal K. Roy"
                },
                "author": "Chanchal K. Roy",
                "arxiv_comment": "Accepted in SANER 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14279v3",
                "updated": "2024-12-09T18:41:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    41,
                    35,
                    0,
                    344,
                    0
                ],
                "published": "2024-01-25T16:10:33Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    10,
                    33,
                    3,
                    25,
                    0
                ],
                "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code\n  Snippets using LLMs"
                },
                "summary": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1."
                },
                "authors": [
                    {
                        "name": "Azmain Kabir"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    },
                    {
                        "name": "Muhammad Asaduzzaman"
                    },
                    {
                        "name": "Wenbin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Zhang"
                },
                "author": "Wenbin Zhang",
                "arxiv_doi": "10.1145/3702979",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3702979",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted and published in ACM Transactions on\n  Software Engineering and Methodology (TOSEM), [2024],\n  [https://dl.acm.org/doi/10.1145/3702979]",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06745v1",
                "updated": "2024-12-09T18:37:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    37,
                    14,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:37:14Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    37,
                    14,
                    0,
                    344,
                    0
                ],
                "title": "ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended\n  Capabilities"
                },
                "summary": "Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models."
                },
                "authors": [
                    {
                        "name": "Adhiraj Ghosh"
                    },
                    {
                        "name": "Sebastian Dziadzio"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Vishaal Udandarao"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Matthias Bethge"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Bethge"
                },
                "author": "Matthias Bethge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11796v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11796v4",
                "updated": "2024-12-09T18:31:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    31,
                    1,
                    0,
                    344,
                    0
                ],
                "published": "2024-08-21T17:38:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    38,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pruning and Distillation in Practice: The Minitron Approach"
                },
                "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license."
                },
                "authors": [
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Chenhan Yu"
                    },
                    {
                        "name": "Wei-Chun Chen"
                    },
                    {
                        "name": "Hayley Ross"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "arxiv_comment": "v4: Update author order",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11796v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11796v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06738v1",
                "updated": "2024-12-09T18:27:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    27,
                    32,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:27:32Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    27,
                    32,
                    0,
                    344,
                    0
                ],
                "title": "JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset\n  Generation with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset\n  Generation with LLM"
                },
                "summary": "Recently some studies have highlighted the potential of Large Language Models\n(LLMs) as effective generators of supervised training data, offering advantages\nsuch as enhanced inference efficiency and reduced costs associated with data\ncollection. However, these studies have predominantly focused on English\nlanguage tasks. In this paper, we address the fundamental research question:\nCan LLMs serve as proficient training data generators for other language tasks?\nSpecifically, we leverage LLMs to synthesize supervised training data under\nfew-shot and zero-shot learning scenarios across six diverse Japanese\ndownstream tasks. Subsequently, we utilize this synthesized data to train\ncompact models (e.g., BERT). This novel methodology is termed JAPAGEN. Our\nexperimental findings underscore that JAPAGEN achieves robust performance in\nclassification tasks that necessitate formal text inputs, demonstrating\ncompetitive results compared to conventional LLM prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently some studies have highlighted the potential of Large Language Models\n(LLMs) as effective generators of supervised training data, offering advantages\nsuch as enhanced inference efficiency and reduced costs associated with data\ncollection. However, these studies have predominantly focused on English\nlanguage tasks. In this paper, we address the fundamental research question:\nCan LLMs serve as proficient training data generators for other language tasks?\nSpecifically, we leverage LLMs to synthesize supervised training data under\nfew-shot and zero-shot learning scenarios across six diverse Japanese\ndownstream tasks. Subsequently, we utilize this synthesized data to train\ncompact models (e.g., BERT). This novel methodology is termed JAPAGEN. Our\nexperimental findings underscore that JAPAGEN achieves robust performance in\nclassification tasks that necessitate formal text inputs, demonstrating\ncompetitive results compared to conventional LLM prompting strategies."
                },
                "authors": [
                    {
                        "name": "Takuro Fujii"
                    },
                    {
                        "name": "Satoru Katsumata"
                    }
                ],
                "author_detail": {
                    "name": "Satoru Katsumata"
                },
                "author": "Satoru Katsumata",
                "arxiv_comment": "Accepted by PACLIC38 (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03657v2",
                "updated": "2024-12-09T18:19:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    19,
                    5,
                    0,
                    344,
                    0
                ],
                "published": "2024-04-04T17:59:58Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    17,
                    59,
                    58,
                    3,
                    95,
                    0
                ],
                "title": "OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and\n  Captioning"
                },
                "summary": "We propose the new task 'open-world video instance segmentation and\ncaptioning'. It requires to detect, segment, track and describe with rich\ncaptions never before seen objects. This challenging task can be addressed by\ndeveloping \"abstractors\" which connect a vision model and a language foundation\nmodel. Concretely, we connect a multi-scale visual feature extractor and a\nlarge language model (LLM) by developing an object abstractor and an\nobject-to-text abstractor. The object abstractor, consisting of a prompt\nencoder and transformer blocks, introduces spatially-diverse open-world object\nqueries to discover never before seen objects in videos. An inter-query\ncontrastive loss further encourages the diversity of object queries. The\nobject-to-text abstractor is augmented with masked cross-attention and acts as\na bridge between the object queries and a frozen LLM to generate rich and\ndescriptive object-centric captions for each detected object. Our generalized\napproach surpasses the baseline that jointly addresses the tasks of open-world\nvideo instance segmentation and dense video object captioning by 13% on never\nbefore seen objects, and by 10% on object-centric captions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose the new task 'open-world video instance segmentation and\ncaptioning'. It requires to detect, segment, track and describe with rich\ncaptions never before seen objects. This challenging task can be addressed by\ndeveloping \"abstractors\" which connect a vision model and a language foundation\nmodel. Concretely, we connect a multi-scale visual feature extractor and a\nlarge language model (LLM) by developing an object abstractor and an\nobject-to-text abstractor. The object abstractor, consisting of a prompt\nencoder and transformer blocks, introduces spatially-diverse open-world object\nqueries to discover never before seen objects in videos. An inter-query\ncontrastive loss further encourages the diversity of object queries. The\nobject-to-text abstractor is augmented with masked cross-attention and acts as\na bridge between the object queries and a frozen LLM to generate rich and\ndescriptive object-centric captions for each detected object. Our generalized\napproach surpasses the baseline that jointly addresses the tasks of open-world\nvideo instance segmentation and dense video object captioning by 13% on never\nbefore seen objects, and by 10% on object-centric captions."
                },
                "authors": [
                    {
                        "name": "Anwesa Choudhuri"
                    },
                    {
                        "name": "Girish Chowdhary"
                    },
                    {
                        "name": "Alexander G. Schwing"
                    }
                ],
                "author_detail": {
                    "name": "Alexander G. Schwing"
                },
                "author": "Alexander G. Schwing",
                "arxiv_comment": "Project page: https://anwesachoudhuri.github.io/OpenWorldVISCap/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06724v1",
                "updated": "2024-12-09T18:13:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    13,
                    27,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:13:27Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    13,
                    27,
                    0,
                    344,
                    0
                ],
                "title": "AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and\n  Benchmark"
                },
                "summary": "We investigate the reasoning capabilities of large language models (LLMs) for\nautomatically generating data-cleaning workflows. To evaluate LLMs' ability to\ncomplete data-cleaning tasks, we implemented a pipeline for LLM-based Auto Data\nCleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operations\nto repair three types of data quality issues: duplicates, missing values, and\ninconsistent data formats. Given a dirty table and a purpose (expressed as a\nquery), this pipeline generates a minimal, clean table sufficient to address\nthe purpose and the data cleaning workflow used to produce the table. The\nplanning process involves three main LLM-driven components: (1) Select Target\nColumns: Identifies a set of target columns related to the purpose. (2) Inspect\nColumn Quality: Assesses the data quality for each target column and generates\na Data Quality Report as operation objectives. (3) Generate Operation &\nArguments: Predicts the next operation and arguments based on the data quality\nreport results. Additionally, we propose a data cleaning benchmark to evaluate\nthe capability of LLM agents to automatically generate workflows that address\ndata cleaning purposes of varying difficulty levels. The benchmark comprises\nthe annotated datasets as a collection of purpose, raw table, clean table, data\ncleaning workflow, and answer set. In our experiments, we evaluated three LLMs\nthat auto-generate purpose-driven data cleaning workflows. The results indicate\nthat LLMs perform well in planning and generating data-cleaning workflows\nwithout the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the reasoning capabilities of large language models (LLMs) for\nautomatically generating data-cleaning workflows. To evaluate LLMs' ability to\ncomplete data-cleaning tasks, we implemented a pipeline for LLM-based Auto Data\nCleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operations\nto repair three types of data quality issues: duplicates, missing values, and\ninconsistent data formats. Given a dirty table and a purpose (expressed as a\nquery), this pipeline generates a minimal, clean table sufficient to address\nthe purpose and the data cleaning workflow used to produce the table. The\nplanning process involves three main LLM-driven components: (1) Select Target\nColumns: Identifies a set of target columns related to the purpose. (2) Inspect\nColumn Quality: Assesses the data quality for each target column and generates\na Data Quality Report as operation objectives. (3) Generate Operation &\nArguments: Predicts the next operation and arguments based on the data quality\nreport results. Additionally, we propose a data cleaning benchmark to evaluate\nthe capability of LLM agents to automatically generate workflows that address\ndata cleaning purposes of varying difficulty levels. The benchmark comprises\nthe annotated datasets as a collection of purpose, raw table, clean table, data\ncleaning workflow, and answer set. In our experiments, we evaluated three LLMs\nthat auto-generate purpose-driven data cleaning workflows. The results indicate\nthat LLMs perform well in planning and generating data-cleaning workflows\nwithout the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Lan Li"
                    },
                    {
                        "name": "Liri Fang"
                    },
                    {
                        "name": "Vetle I. Torvik"
                    }
                ],
                "author_detail": {
                    "name": "Vetle I. Torvik"
                },
                "author": "Vetle I. Torvik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06712v1",
                "updated": "2024-12-09T18:01:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    1,
                    13,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T18:01:13Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    1,
                    13,
                    0,
                    344,
                    0
                ],
                "title": "How to Merge Your Multimodal Models Over Time?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Merge Your Multimodal Models Over Time?"
                },
                "summary": "Model merging combines multiple expert models - finetuned from a base\nfoundation model on diverse tasks and domains - into a single, more capable\nmodel. However, most existing model merging approaches assume that all experts\nare available simultaneously. In reality, new tasks and domains emerge\nprogressively over time, requiring strategies to integrate the knowledge of\nexpert models as they become available: a process we call temporal model\nmerging. The temporal dimension introduces unique challenges not addressed in\nprior work, raising new questions such as: when training for a new task, should\nthe expert model start from the merged past experts or from the original base\nmodel? Should we merge all models at each time step? Which merging techniques\nare best suited for temporal merging? Should different strategies be used to\ninitialize the training and deploy the model? To answer these questions, we\npropose a unified framework called TIME - Temporal Integration of Model\nExpertise - which defines temporal model merging across three axes: (1)\nInitialization Phase, (2) Deployment Phase, and (3) Merging Technique. Using\nTIME, we study temporal model merging across model sizes, compute budgets, and\nlearning horizons on the FoMo-in-Flux benchmark. Our comprehensive suite of\nexperiments across TIME allows us to uncover key insights for temporal model\nmerging, offering a better understanding of current challenges and best\npractices for effective temporal model merging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging combines multiple expert models - finetuned from a base\nfoundation model on diverse tasks and domains - into a single, more capable\nmodel. However, most existing model merging approaches assume that all experts\nare available simultaneously. In reality, new tasks and domains emerge\nprogressively over time, requiring strategies to integrate the knowledge of\nexpert models as they become available: a process we call temporal model\nmerging. The temporal dimension introduces unique challenges not addressed in\nprior work, raising new questions such as: when training for a new task, should\nthe expert model start from the merged past experts or from the original base\nmodel? Should we merge all models at each time step? Which merging techniques\nare best suited for temporal merging? Should different strategies be used to\ninitialize the training and deploy the model? To answer these questions, we\npropose a unified framework called TIME - Temporal Integration of Model\nExpertise - which defines temporal model merging across three axes: (1)\nInitialization Phase, (2) Deployment Phase, and (3) Merging Technique. Using\nTIME, we study temporal model merging across model sizes, compute budgets, and\nlearning horizons on the FoMo-in-Flux benchmark. Our comprehensive suite of\nexperiments across TIME allows us to uncover key insights for temporal model\nmerging, offering a better understanding of current challenges and best\npractices for effective temporal model merging."
                },
                "authors": [
                    {
                        "name": "Sebastian Dziadzio"
                    },
                    {
                        "name": "Vishaal Udandarao"
                    },
                    {
                        "name": "Karsten Roth"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Zeynep Akata"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Matthias Bethge"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Bethge"
                },
                "author": "Matthias Bethge",
                "arxiv_comment": "Technical Report. Code at\n  https://github.com/ExplainableML/fomo_in_flux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06693v1",
                "updated": "2024-12-09T17:39:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    39,
                    43,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:39:43Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    39,
                    43,
                    0,
                    344,
                    0
                ],
                "title": "OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large\n  Language Model and its Omni-Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large\n  Language Model and its Omni-Extensions"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) have significantly\nexpanded their applications, ranging from multilingual support to\ndomain-specific tasks and multimodal integration. In this paper, we present\nOmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and their\nomni-extensions across multilingual, multidomain, and multimodal capabilities.\nUnlike existing benchmarks that often focus on a single aspect, OmniEvalKit\nprovides a modular, lightweight, and automated evaluation system. It is\nstructured with a modular architecture comprising a Static Builder and Dynamic\nData Flow, promoting the seamless integration of new models and datasets.\nOmniEvalKit supports over 100 LLMs and 50 evaluation datasets, covering\ncomprehensive evaluations across thousands of model-dataset combinations.\nOmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployable\nevaluation framework, making downstream applications more convenient and\nversatile for the AI community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) have significantly\nexpanded their applications, ranging from multilingual support to\ndomain-specific tasks and multimodal integration. In this paper, we present\nOmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and their\nomni-extensions across multilingual, multidomain, and multimodal capabilities.\nUnlike existing benchmarks that often focus on a single aspect, OmniEvalKit\nprovides a modular, lightweight, and automated evaluation system. It is\nstructured with a modular architecture comprising a Static Builder and Dynamic\nData Flow, promoting the seamless integration of new models and datasets.\nOmniEvalKit supports over 100 LLMs and 50 evaluation datasets, covering\ncomprehensive evaluations across thousands of model-dataset combinations.\nOmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployable\nevaluation framework, making downstream applications more convenient and\nversatile for the AI community."
                },
                "authors": [
                    {
                        "name": "Yi-Kai Zhang"
                    },
                    {
                        "name": "Xu-Xiang Zhong"
                    },
                    {
                        "name": "Shiyin Lu"
                    },
                    {
                        "name": "Qing-Guo Chen"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00535v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00535v3",
                "updated": "2024-12-09T17:31:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    31,
                    54,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-30T16:58:42Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    16,
                    58,
                    42,
                    5,
                    335,
                    0
                ],
                "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullStack Bench: Evaluating LLMs as Full Stack Coders"
                },
                "summary": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of code large language models (LLMs) continue to expand,\ntheir applications across diverse code intelligence domains are rapidly\nincreasing. However, most existing datasets only evaluate limited application\ndomains. To address this gap, we have developed a comprehensive code evaluation\ndataset FullStack Bench focusing on full-stack programming, which encompasses a\nwide range of application domains (e.g., basic programming, data analysis,\nsoftware engineering, mathematics, and machine learning). Besides, to assess\nmultilingual programming capabilities, in FullStack Bench, we design real-world\ninstructions and corresponding unit test cases from 16 widely-used programming\nlanguages to reflect real-world usage scenarios rather than simple\ntranslations. Moreover, we also release an effective code sandbox execution\ntool (i.e., SandboxFusion) supporting various programming languages and\npackages to evaluate the performance of our FullStack Bench efficiently.\nComprehensive experimental results on our FullStack Bench demonstrate the\nnecessity and effectiveness of our FullStack Bench and SandboxFusion."
                },
                "authors": [
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Jerry Liu"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Aoyan Li"
                    },
                    {
                        "name": "Rui Long"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Jinxiang Xia"
                    },
                    {
                        "name": "Z. Y. Peng"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jing Mai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00535v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00535v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06684v1",
                "updated": "2024-12-09T17:27:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    27,
                    4,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:27:04Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    27,
                    4,
                    0,
                    344,
                    0
                ],
                "title": "Exploring Critical Testing Scenarios for Decision-Making Policies: An\n  LLM Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Critical Testing Scenarios for Decision-Making Policies: An\n  LLM Approach"
                },
                "summary": "Recent years have witnessed surprising achievements of decision-making\npolicies across various fields, such as autonomous driving and robotics.\nTesting for decision-making policies is crucial with the existence of critical\nscenarios that may threaten their reliability. Numerous research efforts have\nbeen dedicated to testing these policies. However, there are still significant\nchallenges, such as low testing efficiency and diversity due to the complexity\nof the policies and environments under test. Inspired by the remarkable\ncapabilities of large language models (LLMs), in this paper, we propose an\nLLM-driven online testing framework for efficiently testing decision-making\npolicies. The main idea is to employ an LLM-based test scenario generator to\nintelligently generate challenging test cases through contemplation and\nreasoning. Specifically, we first design a \"generate-test-feedback\" pipeline\nand apply templated prompt engineering to fully leverage the knowledge and\nreasoning abilities of LLMs. Then, we introduce a multi-scale scenario\ngeneration strategy to address the inherent challenges LLMs face in making fine\nadjustments, further enhancing testing efficiency. Finally, we evaluate the\nLLM-driven approach on five widely used benchmarks. The experimental results\ndemonstrate that our method significantly outperforms baseline approaches in\nuncovering both critical and diverse scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed surprising achievements of decision-making\npolicies across various fields, such as autonomous driving and robotics.\nTesting for decision-making policies is crucial with the existence of critical\nscenarios that may threaten their reliability. Numerous research efforts have\nbeen dedicated to testing these policies. However, there are still significant\nchallenges, such as low testing efficiency and diversity due to the complexity\nof the policies and environments under test. Inspired by the remarkable\ncapabilities of large language models (LLMs), in this paper, we propose an\nLLM-driven online testing framework for efficiently testing decision-making\npolicies. The main idea is to employ an LLM-based test scenario generator to\nintelligently generate challenging test cases through contemplation and\nreasoning. Specifically, we first design a \"generate-test-feedback\" pipeline\nand apply templated prompt engineering to fully leverage the knowledge and\nreasoning abilities of LLMs. Then, we introduce a multi-scale scenario\ngeneration strategy to address the inherent challenges LLMs face in making fine\nadjustments, further enhancing testing efficiency. Finally, we evaluate the\nLLM-driven approach on five widely used benchmarks. The experimental results\ndemonstrate that our method significantly outperforms baseline approaches in\nuncovering both critical and diverse scenarios."
                },
                "authors": [
                    {
                        "name": "Weichao Xu"
                    },
                    {
                        "name": "Huaxin Pei"
                    },
                    {
                        "name": "Jingxuan Yang"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhang"
                },
                "author": "Yi Zhang",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06681v1",
                "updated": "2024-12-09T17:24:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    24,
                    41,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:24:41Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    24,
                    41,
                    0,
                    344,
                    0
                ],
                "title": "Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual\n  Framework"
                },
                "summary": "In transportation system demand modeling and simulation, agent-based models\nand microsimulations are current state-of-the-art approaches. However, existing\nagent-based models still have some limitations on behavioral realism and\nresource demand that limit their applicability. In this study, leveraging the\nemerging technology of large language models (LLMs) and LLM-based agents, we\npropose a general LLM-agent-based modeling framework for transportation\nsystems. We argue that LLM agents not only possess the essential capabilities\nto function as agents but also offer promising solutions to overcome some\nlimitations of existing agent-based models. Our conceptual framework design\nclosely replicates the decision-making and interaction processes and traits of\nhuman travelers within transportation networks, and we demonstrate that the\nproposed systems can meet critical behavioral criteria for decision-making and\nlearning behaviors using related studies and a demonstrative example of LLM\nagents' learning and adjustment in the bottleneck setting. Although further\nrefinement of the LLM-agent-based modeling framework is necessary, we believe\nthat this approach has the potential to improve transportation system modeling\nand simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In transportation system demand modeling and simulation, agent-based models\nand microsimulations are current state-of-the-art approaches. However, existing\nagent-based models still have some limitations on behavioral realism and\nresource demand that limit their applicability. In this study, leveraging the\nemerging technology of large language models (LLMs) and LLM-based agents, we\npropose a general LLM-agent-based modeling framework for transportation\nsystems. We argue that LLM agents not only possess the essential capabilities\nto function as agents but also offer promising solutions to overcome some\nlimitations of existing agent-based models. Our conceptual framework design\nclosely replicates the decision-making and interaction processes and traits of\nhuman travelers within transportation networks, and we demonstrate that the\nproposed systems can meet critical behavioral criteria for decision-making and\nlearning behaviors using related studies and a demonstrative example of LLM\nagents' learning and adjustment in the bottleneck setting. Although further\nrefinement of the LLM-agent-based modeling framework is necessary, we believe\nthat this approach has the potential to improve transportation system modeling\nand simulation."
                },
                "authors": [
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Jirong Yang"
                    },
                    {
                        "name": "Yafeng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yafeng Yin"
                },
                "author": "Yafeng Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06673v1",
                "updated": "2024-12-09T17:11:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    11,
                    50,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:11:50Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    11,
                    50,
                    0,
                    344,
                    0
                ],
                "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance"
                },
                "summary": "In this paper, we introduce ILLUME, a unified multimodal large language model\n(MLLM) that seamlessly integrates multimodal understanding and generation\ncapabilities within a single large language model through a unified next-token\nprediction formulation. To address the large dataset size typically required\nfor image-text alignment, we propose to enhance data efficiency through the\ndesign of a vision tokenizer that incorporates semantic information and a\nprogressive multi-stage training procedure. This approach reduces the dataset\nsize to just 15M for pretraining -- over four times fewer than what is\ntypically needed -- while achieving competitive or even superior performance\nwith existing unified MLLMs, such as Janus. Additionally, to promote\nsynergistic enhancement between understanding and generation capabilities,\nwhich is under-explored in previous works, we introduce a novel self-enhancing\nmultimodal alignment scheme. This scheme supervises the MLLM to self-assess the\nconsistency between text descriptions and self-generated images, facilitating\nthe model to interpret images more accurately and avoid unrealistic and\nincorrect predictions caused by misalignment in image generation. Based on\nextensive experiments, our proposed ILLUME stands out and competes with\nstate-of-the-art unified MLLMs and specialized models across various benchmarks\nfor multimodal understanding, generation, and editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce ILLUME, a unified multimodal large language model\n(MLLM) that seamlessly integrates multimodal understanding and generation\ncapabilities within a single large language model through a unified next-token\nprediction formulation. To address the large dataset size typically required\nfor image-text alignment, we propose to enhance data efficiency through the\ndesign of a vision tokenizer that incorporates semantic information and a\nprogressive multi-stage training procedure. This approach reduces the dataset\nsize to just 15M for pretraining -- over four times fewer than what is\ntypically needed -- while achieving competitive or even superior performance\nwith existing unified MLLMs, such as Janus. Additionally, to promote\nsynergistic enhancement between understanding and generation capabilities,\nwhich is under-explored in previous works, we introduce a novel self-enhancing\nmultimodal alignment scheme. This scheme supervises the MLLM to self-assess the\nconsistency between text descriptions and self-generated images, facilitating\nthe model to interpret images more accurately and avoid unrealistic and\nincorrect predictions caused by misalignment in image generation. Based on\nextensive experiments, our proposed ILLUME stands out and competes with\nstate-of-the-art unified MLLMs and specialized models across various benchmarks\nfor multimodal understanding, generation, and editing."
                },
                "authors": [
                    {
                        "name": "Chunwei Wang"
                    },
                    {
                        "name": "Guansong Lu"
                    },
                    {
                        "name": "Junwei Yang"
                    },
                    {
                        "name": "Runhui Huang"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Hang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Xu"
                },
                "author": "Hang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06661v1",
                "updated": "2024-12-09T17:00:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    0,
                    20,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T17:00:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    0,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion"
                },
                "summary": "Text-to-image generation of Stable Diffusion models has achieved notable\nsuccess due to its remarkable generation ability. However, the repetitive\ndenoising process is computationally intensive during inference, which renders\nDiffusion models less suitable for real-world applications that require low\nlatency and scalability. Recent studies have employed post-training\nquantization (PTQ) and quantization-aware training (QAT) methods to compress\nDiffusion models. Nevertheless, prior research has often neglected to examine\nthe consistency between results generated by quantized models and those from\nfloating-point models. This consistency is crucial in fields such as content\ncreation, design, and edge deployment, as it can significantly enhance both\nefficiency and system stability for practitioners. To ensure that quantized\nmodels generate high-quality and consistent images, we propose an efficient\nquantization framework for Stable Diffusion models. Our approach features a\nSerial-to-Parallel calibration pipeline that addresses the consistency of both\nthe calibration and inference processes, as well as ensuring training\nstability. Based on this pipeline, we further introduce a mix-precision\nquantization strategy, multi-timestep activation quantization, and time\ninformation precalculation techniques to ensure high-fidelity generation in\ncomparison to floating-point models. Through extensive experiments with Stable\nDiffusion v1-4, v2-1, and XL 1.0, we have demonstrated that our method\noutperforms the current state-of-the-art techniques when tested on prompts from\nthe COCO validation dataset and the Stable-Diffusion-Prompts dataset. Under\nW4A8 quantization settings, our approach enhances both distribution similarity\nand visual similarity by 45%-60%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation of Stable Diffusion models has achieved notable\nsuccess due to its remarkable generation ability. However, the repetitive\ndenoising process is computationally intensive during inference, which renders\nDiffusion models less suitable for real-world applications that require low\nlatency and scalability. Recent studies have employed post-training\nquantization (PTQ) and quantization-aware training (QAT) methods to compress\nDiffusion models. Nevertheless, prior research has often neglected to examine\nthe consistency between results generated by quantized models and those from\nfloating-point models. This consistency is crucial in fields such as content\ncreation, design, and edge deployment, as it can significantly enhance both\nefficiency and system stability for practitioners. To ensure that quantized\nmodels generate high-quality and consistent images, we propose an efficient\nquantization framework for Stable Diffusion models. Our approach features a\nSerial-to-Parallel calibration pipeline that addresses the consistency of both\nthe calibration and inference processes, as well as ensuring training\nstability. Based on this pipeline, we further introduce a mix-precision\nquantization strategy, multi-timestep activation quantization, and time\ninformation precalculation techniques to ensure high-fidelity generation in\ncomparison to floating-point models. Through extensive experiments with Stable\nDiffusion v1-4, v2-1, and XL 1.0, we have demonstrated that our method\noutperforms the current state-of-the-art techniques when tested on prompts from\nthe COCO validation dataset and the Stable-Diffusion-Prompts dataset. Under\nW4A8 quantization settings, our approach enhances both distribution similarity\nand visual similarity by 45%-60%."
                },
                "authors": [
                    {
                        "name": "Shuaiting Li"
                    },
                    {
                        "name": "Juncan Deng"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Kedong Xu"
                    },
                    {
                        "name": "Haibin Shen"
                    },
                    {
                        "name": "Kejie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kejie Huang"
                },
                "author": "Kejie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06654v1",
                "updated": "2024-12-09T16:54:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    54,
                    54,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T16:54:54Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    54,
                    54,
                    0,
                    344,
                    0
                ],
                "title": "GEAR: A Simple GENERATE, EMBED, AVERAGE AND RANK Approach for\n  Unsupervised Reverse Dictionary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: A Simple GENERATE, EMBED, AVERAGE AND RANK Approach for\n  Unsupervised Reverse Dictionary"
                },
                "summary": "Reverse Dictionary (RD) is the task of obtaining the most relevant word or\nset of words given a textual description or dictionary definition. Effective RD\nmethods have applications in accessibility, translation or writing support\nsystems. Moreover, in NLP research we find RD to be used to benchmark text\nencoders at various granularities, as it often requires word, definition and\nsentence embeddings. In this paper, we propose a simple approach to RD that\nleverages LLMs in combination with embedding models. Despite its simplicity,\nthis approach outperforms supervised baselines in well studied RD datasets,\nwhile also showing less over-fitting. We also conduct a number of experiments\non different dictionaries and analyze how different styles, registers and\ntarget audiences impact the quality of RD systems. We conclude that, on\naverage, untuned embeddings alone fare way below an LLM-only baseline (although\nthey are competitive in highly technical dictionaries), but are crucial for\nboosting performance in combined methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Dictionary (RD) is the task of obtaining the most relevant word or\nset of words given a textual description or dictionary definition. Effective RD\nmethods have applications in accessibility, translation or writing support\nsystems. Moreover, in NLP research we find RD to be used to benchmark text\nencoders at various granularities, as it often requires word, definition and\nsentence embeddings. In this paper, we propose a simple approach to RD that\nleverages LLMs in combination with embedding models. Despite its simplicity,\nthis approach outperforms supervised baselines in well studied RD datasets,\nwhile also showing less over-fitting. We also conduct a number of experiments\non different dictionaries and analyze how different styles, registers and\ntarget audiences impact the quality of RD systems. We conclude that, on\naverage, untuned embeddings alone fare way below an LLM-only baseline (although\nthey are competitive in highly technical dictionaries), but are crucial for\nboosting performance in combined methods."
                },
                "authors": [
                    {
                        "name": "Fatemah Almeman"
                    },
                    {
                        "name": "Luis Espinosa-Anke"
                    }
                ],
                "author_detail": {
                    "name": "Luis Espinosa-Anke"
                },
                "author": "Luis Espinosa-Anke",
                "arxiv_comment": "9 pages, Accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06651v1",
                "updated": "2024-12-09T16:50:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T16:50:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben"
                },
                "summary": "This study examines the AI-powered grading tool \"AI Grading Assistant\" by the\nGerman company Fobizz, designed to support teachers in evaluating and providing\nfeedback on student assignments. Against the societal backdrop of an\noverburdened education system and rising expectations for artificial\nintelligence as a solution to these challenges, the investigation evaluates the\ntool's functional suitability through two test series. The results reveal\nsignificant shortcomings: The tool's numerical grades and qualitative feedback\nare often random and do not improve even when its suggestions are incorporated.\nThe highest ratings are achievable only with texts generated by ChatGPT. False\nclaims and nonsensical submissions frequently go undetected, while the\nimplementation of some grading criteria is unreliable and opaque. Since these\ndeficiencies stem from the inherent limitations of large language models\n(LLMs), fundamental improvements to this or similar tools are not immediately\nforeseeable. The study critiques the broader trend of adopting AI as a quick\nfix for systemic problems in education, concluding that Fobizz's marketing of\nthe tool as an objective and time-saving solution is misleading and\nirresponsible. Finally, the study calls for systematic evaluation and\nsubject-specific pedagogical scrutiny of the use of AI tools in educational\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the AI-powered grading tool \"AI Grading Assistant\" by the\nGerman company Fobizz, designed to support teachers in evaluating and providing\nfeedback on student assignments. Against the societal backdrop of an\noverburdened education system and rising expectations for artificial\nintelligence as a solution to these challenges, the investigation evaluates the\ntool's functional suitability through two test series. The results reveal\nsignificant shortcomings: The tool's numerical grades and qualitative feedback\nare often random and do not improve even when its suggestions are incorporated.\nThe highest ratings are achievable only with texts generated by ChatGPT. False\nclaims and nonsensical submissions frequently go undetected, while the\nimplementation of some grading criteria is unreliable and opaque. Since these\ndeficiencies stem from the inherent limitations of large language models\n(LLMs), fundamental improvements to this or similar tools are not immediately\nforeseeable. The study critiques the broader trend of adopting AI as a quick\nfix for systemic problems in education, concluding that Fobizz's marketing of\nthe tool as an objective and time-saving solution is misleading and\nirresponsible. Finally, the study calls for systematic evaluation and\nsubject-specific pedagogical scrutiny of the use of AI tools in educational\ncontexts."
                },
                "authors": [
                    {
                        "name": "Rainer Mhlhoff"
                    },
                    {
                        "name": "Marte Henningsen"
                    }
                ],
                "author_detail": {
                    "name": "Marte Henningsen"
                },
                "author": "Marte Henningsen",
                "arxiv_comment": "32 pages, in German language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "97B10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16851v2",
                "updated": "2024-12-09T16:42:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    42,
                    25,
                    0,
                    344,
                    0
                ],
                "published": "2024-03-25T15:15:09Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    15,
                    15,
                    9,
                    0,
                    85,
                    0
                ],
                "title": "Can tweets predict article retractions? A comparison between human and\n  LLM labelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can tweets predict article retractions? A comparison between human and\n  LLM labelling"
                },
                "summary": "Quickly detecting problematic research articles is crucial to safeguarding\nthe integrity of scientific research. This study explores whether Twitter\nmentions of retracted articles can signal potential problems with the articles\nprior to their retraction, potentially serving as an early warning system for\nscholars. To investigate this, we analysed a dataset of 4,354 Twitter mentions\nassociated with 504 retracted articles. The effectiveness of Twitter mentions\nin predicting article retractions was evaluated by both manual and Large\nLanguage Model (LLM) labelling. Manual labelling results indicated that 25.7%\nof tweets signalled problems before retraction. Using the manual labelling\nresults as the baseline, we found that LLMs (GPT-4o-mini, Gemini 1.5 Flash, and\nClaude-3.5-Haiku) outperformed lexicon-based sentiment analysis tools (e.g.,\nTextBlob) in detecting potential problems, suggesting that automatic detection\nof problematic articles from social media using LLMs is technically feasible.\nNevertheless, since only a small proportion of retracted articles (11.1%) were\ncriticised on Twitter prior to retraction, such automatic systems would detect\nonly a minority of problematic articles. Overall, this study offers insights\ninto how social media data, coupled with emerging generative AI techniques, can\nsupport research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickly detecting problematic research articles is crucial to safeguarding\nthe integrity of scientific research. This study explores whether Twitter\nmentions of retracted articles can signal potential problems with the articles\nprior to their retraction, potentially serving as an early warning system for\nscholars. To investigate this, we analysed a dataset of 4,354 Twitter mentions\nassociated with 504 retracted articles. The effectiveness of Twitter mentions\nin predicting article retractions was evaluated by both manual and Large\nLanguage Model (LLM) labelling. Manual labelling results indicated that 25.7%\nof tweets signalled problems before retraction. Using the manual labelling\nresults as the baseline, we found that LLMs (GPT-4o-mini, Gemini 1.5 Flash, and\nClaude-3.5-Haiku) outperformed lexicon-based sentiment analysis tools (e.g.,\nTextBlob) in detecting potential problems, suggesting that automatic detection\nof problematic articles from social media using LLMs is technically feasible.\nNevertheless, since only a small proportion of retracted articles (11.1%) were\ncriticised on Twitter prior to retraction, such automatic systems would detect\nonly a minority of problematic articles. Overall, this study offers insights\ninto how social media data, coupled with emerging generative AI techniques, can\nsupport research integrity."
                },
                "authors": [
                    {
                        "name": "Er-Te Zheng"
                    },
                    {
                        "name": "Hui-Zhen Fu"
                    },
                    {
                        "name": "Mike Thelwall"
                    },
                    {
                        "name": "Zhichao Fang"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Fang"
                },
                "author": "Zhichao Fang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04105v3",
                "updated": "2024-12-09T16:36:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    36,
                    34,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-06T18:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    35,
                    32,
                    2,
                    311,
                    0
                ],
                "title": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis"
                },
                "summary": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve. We perform our study on two\nfronts. First, we pursue an understanding of precisely how a three-layer\ntransformer, trained from scratch and attains perfect test accuracy, solves\nthis problem. We are able to identify certain \"planning\" and \"reasoning\"\nmechanisms in the network that necessitate cooperation between the attention\nblocks to implement the desired logic. Second, we study how pretrained LLMs,\nnamely Mistral-7B and Gemma-2-9B, solve this problem. We characterize their\nreasoning circuits through causal intervention experiments, providing necessity\nand sufficiency evidence for the circuits. We find evidence suggesting that the\ntwo models' latent reasoning strategies are surprisingly similar, and\nhuman-like. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve. We perform our study on two\nfronts. First, we pursue an understanding of precisely how a three-layer\ntransformer, trained from scratch and attains perfect test accuracy, solves\nthis problem. We are able to identify certain \"planning\" and \"reasoning\"\nmechanisms in the network that necessitate cooperation between the attention\nblocks to implement the desired logic. Second, we study how pretrained LLMs,\nnamely Mistral-7B and Gemma-2-9B, solve this problem. We characterize their\nreasoning circuits through causal intervention experiments, providing necessity\nand sufficiency evidence for the circuits. We find evidence suggesting that the\ntwo models' latent reasoning strategies are surprisingly similar, and\nhuman-like. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason."
                },
                "authors": [
                    {
                        "name": "Guan Zhe Hong"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Enming Luo"
                    },
                    {
                        "name": "Cyrus Rashtchian"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06638v1",
                "updated": "2024-12-09T16:33:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    33,
                    6,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T16:33:06Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    33,
                    6,
                    0,
                    344,
                    0
                ],
                "title": "The potential impact of large-scale wind clusters on the local weather\n  patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential impact of large-scale wind clusters on the local weather\n  patterns"
                },
                "summary": "To decarbonise the electricity sector and achieve renewable energy targets, a\nrapidly growing number of wind farms have been authorised, constructed, and\ncommissioned in the UK and EU in recent years. For instance, the UK Government\naims to expand offshore wind capacity to 60 GW by 2030, while the EU has set a\ntarget of 120 GW of offshore renewable energy by the same year. Given these\nsubstantial projected capacities, it is crucial to thoroughly investigate the\npotential impacts of large-scale wind clusters on local weather patterns to\nprevent unintended consequences prior to deployment. In this paper, we use the\nWRF model to simulate four scenarios with varying wind energy capacities in the\nNorth Sea, assessing the potential effects of these wind clusters on the local\nweather patterns over mainland UK. Please note that the simulations of Case 3\nand Case 4 are still ongoing, while all analyses in the current version of\nmanuscript are all based on Case 1 and Case 2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To decarbonise the electricity sector and achieve renewable energy targets, a\nrapidly growing number of wind farms have been authorised, constructed, and\ncommissioned in the UK and EU in recent years. For instance, the UK Government\naims to expand offshore wind capacity to 60 GW by 2030, while the EU has set a\ntarget of 120 GW of offshore renewable energy by the same year. Given these\nsubstantial projected capacities, it is crucial to thoroughly investigate the\npotential impacts of large-scale wind clusters on local weather patterns to\nprevent unintended consequences prior to deployment. In this paper, we use the\nWRF model to simulate four scenarios with varying wind energy capacities in the\nNorth Sea, assessing the potential effects of these wind clusters on the local\nweather patterns over mainland UK. Please note that the simulations of Case 3\nand Case 4 are still ongoing, while all analyses in the current version of\nmanuscript are all based on Case 1 and Case 2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jincheng Zhang"
                    },
                    {
                        "name": "Xiaowei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Zhao"
                },
                "author": "Xiaowei Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02830v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02830v3",
                "updated": "2024-12-09T16:26:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    26,
                    9,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-03T20:52:35Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    52,
                    35,
                    1,
                    338,
                    0
                ],
                "title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language\n  Models"
                },
                "summary": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical."
                },
                "authors": [
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Junda Wang"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "24 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02830v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02830v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11255v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11255v5",
                "updated": "2024-12-09T16:15:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    15,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2023-11-19T06:50:52Z",
                "published_parsed": [
                    2023,
                    11,
                    19,
                    6,
                    50,
                    52,
                    6,
                    323,
                    0
                ],
                "title": "M$^{2}$UGen: Multi-modal Music Understanding and Generation with the\n  Power of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M$^{2}$UGen: Multi-modal Music Understanding and Generation with the\n  Power of Large Language Models"
                },
                "summary": "The current landscape of research leveraging large language models (LLMs) is\nexperiencing a surge. Many works harness the powerful reasoning capabilities of\nthese models to comprehend various modalities, such as text, speech, images,\nvideos, etc. They also utilize LLMs to understand human intention and generate\ndesired outputs like images, videos, and music. However, research that combines\nboth understanding and generation using LLMs is still limited and in its\nnascent stage. To address this gap, we introduce a Multi-modal Music\nUnderstanding and Generation (M$^{2}$UGen) framework that integrates LLM's\nabilities to comprehend and generate music for different modalities. The\nM$^{2}$UGen framework is purpose-built to unlock creative potential from\ndiverse sources of inspiration, encompassing music, image, and video through\nthe use of pretrained MERT, ViT, and ViViT models, respectively. To enable\nmusic generation, we explore the use of AudioLDM 2 and MusicGen. Bridging\nmulti-modal understanding and music generation is accomplished through the\nintegration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA\nmodel to generate extensive datasets that support text/image/video-to-music\ngeneration, facilitating the training of our M$^{2}$UGen framework. We conduct\na thorough evaluation of our proposed framework. The experimental results\ndemonstrate that our model achieves or surpasses the performance of the current\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current landscape of research leveraging large language models (LLMs) is\nexperiencing a surge. Many works harness the powerful reasoning capabilities of\nthese models to comprehend various modalities, such as text, speech, images,\nvideos, etc. They also utilize LLMs to understand human intention and generate\ndesired outputs like images, videos, and music. However, research that combines\nboth understanding and generation using LLMs is still limited and in its\nnascent stage. To address this gap, we introduce a Multi-modal Music\nUnderstanding and Generation (M$^{2}$UGen) framework that integrates LLM's\nabilities to comprehend and generate music for different modalities. The\nM$^{2}$UGen framework is purpose-built to unlock creative potential from\ndiverse sources of inspiration, encompassing music, image, and video through\nthe use of pretrained MERT, ViT, and ViViT models, respectively. To enable\nmusic generation, we explore the use of AudioLDM 2 and MusicGen. Bridging\nmulti-modal understanding and music generation is accomplished through the\nintegration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA\nmodel to generate extensive datasets that support text/image/video-to-music\ngeneration, facilitating the training of our M$^{2}$UGen framework. We conduct\na thorough evaluation of our proposed framework. The experimental results\ndemonstrate that our model achieves or surpasses the performance of the current\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Shansong Liu"
                    },
                    {
                        "name": "Atin Sakkeer Hussain"
                    },
                    {
                        "name": "Qilong Wu"
                    },
                    {
                        "name": "Chenshuo Sun"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11255v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11255v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06617v1",
                "updated": "2024-12-09T16:09:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    9,
                    44,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T16:09:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    9,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "AI TrackMate: Finally, Someone Who Will Give Your Music More Than Just\n  \"Sounds Great!\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI TrackMate: Finally, Someone Who Will Give Your Music More Than Just\n  \"Sounds Great!\""
                },
                "summary": "The rise of \"bedroom producers\" has democratized music creation, while\nchallenging producers to objectively evaluate their work. To address this, we\npresent AI TrackMate, an LLM-based music chatbot designed to provide\nconstructive feedback on music productions. By combining LLMs' inherent musical\nknowledge with direct audio track analysis, AI TrackMate offers\nproduction-specific insights, distinguishing it from text-only approaches. Our\nframework integrates a Music Analysis Module, an LLM-Readable Music Report, and\nMusic Production-Oriented Feedback Instruction, creating a plug-and-play,\ntraining-free system compatible with various LLMs and adaptable to future\nadvancements. We demonstrate AI TrackMate's capabilities through an interactive\nweb interface and present findings from a pilot study with a music producer. By\nbridging AI capabilities with the needs of independent producers, AI TrackMate\noffers on-demand analytical feedback, potentially supporting the creative\nprocess and skill development in music production. This system addresses the\ngrowing demand for objective self-assessment tools in the evolving landscape of\nindependent music production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of \"bedroom producers\" has democratized music creation, while\nchallenging producers to objectively evaluate their work. To address this, we\npresent AI TrackMate, an LLM-based music chatbot designed to provide\nconstructive feedback on music productions. By combining LLMs' inherent musical\nknowledge with direct audio track analysis, AI TrackMate offers\nproduction-specific insights, distinguishing it from text-only approaches. Our\nframework integrates a Music Analysis Module, an LLM-Readable Music Report, and\nMusic Production-Oriented Feedback Instruction, creating a plug-and-play,\ntraining-free system compatible with various LLMs and adaptable to future\nadvancements. We demonstrate AI TrackMate's capabilities through an interactive\nweb interface and present findings from a pilot study with a music producer. By\nbridging AI capabilities with the needs of independent producers, AI TrackMate\noffers on-demand analytical feedback, potentially supporting the creative\nprocess and skill development in music production. This system addresses the\ngrowing demand for objective self-assessment tools in the evolving landscape of\nindependent music production."
                },
                "authors": [
                    {
                        "name": "Yi-Lin Jiang"
                    },
                    {
                        "name": "Chia-Ho Hsiung"
                    },
                    {
                        "name": "Yen-Tung Yeh"
                    },
                    {
                        "name": "Lu-Rong Chen"
                    },
                    {
                        "name": "Bo-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Bo-Yu Chen"
                },
                "author": "Bo-Yu Chen",
                "arxiv_comment": "Accepted for the NeurIPS 2024 Creative AI Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05270v2",
                "updated": "2024-12-09T16:01:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    1,
                    0,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-06T18:55:34Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    55,
                    34,
                    4,
                    341,
                    0
                ],
                "title": "APOLLO: SGD-like Memory, AdamW-level Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APOLLO: SGD-like Memory, AdamW-level Performance"
                },
                "summary": "Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are notoriously memory-intensive during\ntraining, particularly with the popular AdamW optimizer. This memory burden\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\ntraining scalability and throughput. To address this, various memory-efficient\noptimizers have been proposed to reduce optimizer memory usage. However, they\nface critical challenges: (i) reliance on costly SVD operations; (ii)\nsignificant performance trade-offs compared to AdamW; and (iii) still\nsubstantial optimizer memory overhead to maintain competitive performance.\n  In this work, we identify that AdamW's learning rate adaptation rule can be\neffectively coarsened as a structured learning rate update. Based on this\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\nOptimization (APOLLO), which approximates learning rate scaling using an\nauxiliary low-rank optimizer state based on pure random projection. This\nstructured learning rate update rule makes APOLLO highly tolerant to further\nmemory reductions while delivering comparable pre-training performance. Even\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\ncompared to AdamW with SGD-level memory costs.\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\nor better than AdamW, while achieving greater memory savings by nearly\neliminating the optimization states of AdamW. These savings provide significant\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\nquantization."
                },
                "authors": [
                    {
                        "name": "Hanqing Zhu"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Wenyan Cong"
                    },
                    {
                        "name": "Xi Liu"
                    },
                    {
                        "name": "Sem Park"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Bo Long"
                    },
                    {
                        "name": "David Z. Pan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Jinwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinwon Lee"
                },
                "author": "Jinwon Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06603v1",
                "updated": "2024-12-09T15:53:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    53,
                    0,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:53:00Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    53,
                    0,
                    0,
                    344,
                    0
                ],
                "title": "Examining the Use and Impact of an AI Code Assistant on Developer\n  Productivity and Experience in the Enterprise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Use and Impact of an AI Code Assistant on Developer\n  Productivity and Experience in the Enterprise"
                },
                "summary": "AI assistants are being created to help software engineers conduct a variety\nof coding-related tasks, such as writing, documenting, and testing code. We\ndescribe the use of the watsonx Code Assistant (WCA), an LLM-powered coding\nassistant deployed internally within IBM. Through surveys of two user cohorts\n(N=669) and unmoderated usability testing (N=15), we examined developers'\nexperiences with WCA and its impact on their productivity. We learned about\ntheir motivations for using (or not using) WCA, we examined their expectations\nof its speed and quality, and we identified new considerations regarding\nownership of and responsibility for generated code. Our case study\ncharacterizes the impact of an LLM-powered assistant on developers' perceptions\nof productivity and it shows that although such tools do often provide net\nproductivity increases, these benefits may not always be experienced by all\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI assistants are being created to help software engineers conduct a variety\nof coding-related tasks, such as writing, documenting, and testing code. We\ndescribe the use of the watsonx Code Assistant (WCA), an LLM-powered coding\nassistant deployed internally within IBM. Through surveys of two user cohorts\n(N=669) and unmoderated usability testing (N=15), we examined developers'\nexperiences with WCA and its impact on their productivity. We learned about\ntheir motivations for using (or not using) WCA, we examined their expectations\nof its speed and quality, and we identified new considerations regarding\nownership of and responsibility for generated code. Our case study\ncharacterizes the impact of an LLM-powered assistant on developers' perceptions\nof productivity and it shows that although such tools do often provide net\nproductivity increases, these benefits may not always be experienced by all\nusers."
                },
                "authors": [
                    {
                        "name": "Justin D. Weisz"
                    },
                    {
                        "name": "Shraddha Kumar"
                    },
                    {
                        "name": "Michael Muller"
                    },
                    {
                        "name": "Karen-Ellen Browne"
                    },
                    {
                        "name": "Arielle Goldberg"
                    },
                    {
                        "name": "Ellice Heintze"
                    },
                    {
                        "name": "Shagun Bajpai"
                    }
                ],
                "author_detail": {
                    "name": "Shagun Bajpai"
                },
                "author": "Shagun Bajpai",
                "arxiv_comment": "21 pages, 3 figures. To be published in CHI EA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06593v1",
                "updated": "2024-12-09T15:45:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    45,
                    3,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:45:03Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    45,
                    3,
                    0,
                    344,
                    0
                ],
                "title": "Anchoring Bias in Large Language Models: An Experimental Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchoring Bias in Large Language Models: An Experimental Study"
                },
                "summary": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly\nadvanced artificial intelligence by enabling machines to generate and\ncomprehend human-like text. Despite their impressive capabilities, LLMs are not\nimmune to limitations, including various biases. While much research has\nexplored demographic biases, the cognitive biases in LLMs have not been equally\nscrutinized. This study delves into anchoring bias, a cognitive bias where\ninitial information disproportionately influences judgment. Utilizing an\nexperimental dataset, we examine how anchoring bias manifests in LLMs and\nverify the effectiveness of various mitigation strategies. Our findings\nhighlight the sensitivity of LLM responses to biased hints. At the same time,\nour experiments show that, to mitigate anchoring bias, one needs to collect\nhints from comprehensive angles to prevent the LLMs from being anchored to\nindividual pieces of information, while simple algorithms such as\nChain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection\nare not sufficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly\nadvanced artificial intelligence by enabling machines to generate and\ncomprehend human-like text. Despite their impressive capabilities, LLMs are not\nimmune to limitations, including various biases. While much research has\nexplored demographic biases, the cognitive biases in LLMs have not been equally\nscrutinized. This study delves into anchoring bias, a cognitive bias where\ninitial information disproportionately influences judgment. Utilizing an\nexperimental dataset, we examine how anchoring bias manifests in LLMs and\nverify the effectiveness of various mitigation strategies. Our findings\nhighlight the sensitivity of LLM responses to biased hints. At the same time,\nour experiments show that, to mitigate anchoring bias, one needs to collect\nhints from comprehensive angles to prevent the LLMs from being anchored to\nindividual pieces of information, while simple algorithms such as\nChain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection\nare not sufficient."
                },
                "authors": [
                    {
                        "name": "Jiaxu Lou"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxu Lou"
                },
                "author": "Jiaxu Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05374v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05374v4",
                "updated": "2024-12-09T15:39:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    39,
                    30,
                    0,
                    344,
                    0
                ],
                "published": "2024-02-08T03:12:25Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    3,
                    12,
                    25,
                    3,
                    39,
                    0
                ],
                "title": "CIC: A Framework for Culturally-Aware Image Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIC: A Framework for Culturally-Aware Image Captioning"
                },
                "summary": "Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, Culturally-aware Image Captioning (CIC), that\ngenerates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs.\nResources can be found at https://shane3606.github.io/cic..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, Culturally-aware Image Captioning (CIC), that\ngenerates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs.\nResources can be found at https://shane3606.github.io/cic.."
                },
                "authors": [
                    {
                        "name": "Youngsik Yun"
                    },
                    {
                        "name": "Jihie Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihie Kim"
                },
                "author": "Jihie Kim",
                "arxiv_doi": "10.24963/ijcai.2024/180",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/180",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.05374v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05374v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in IJCAI 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06575v1",
                "updated": "2024-12-09T15:28:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    28,
                    39,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:28:39Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    28,
                    39,
                    0,
                    344,
                    0
                ],
                "title": "Data Quality Enhancement on the Basis of Diversity with Large Language\n  Models for Text Classification: Uncovered, Difficult, and Noisy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Quality Enhancement on the Basis of Diversity with Large Language\n  Models for Text Classification: Uncovered, Difficult, and Noisy"
                },
                "summary": "In recent years, the use of large language models (LLMs) for text\nclassification has attracted widespread attention. Despite this, the\nclassification accuracy of LLMs has not yet universally surpassed that of\nsmaller models. LLMs can enhance their performance in text classification\nthrough fine-tuning. However, existing data quality research based on LLMs is\nchallenging to apply directly to solve text classification problems. To further\nimprove the performance of LLMs in classification tasks, this paper proposes a\ndata quality enhancement (DQE) method for text classification based on LLMs.\nThis method starts by using a greedy algorithm to select data, dividing the\ndataset into sampled and unsampled subsets, and then performing fine-tuning of\nthe LLMs using the sampled data. Subsequently, this model is used to predict\nthe outcomes for the unsampled data, categorizing incorrectly predicted data\ninto uncovered, difficult, and noisy data. Experimental results demonstrate\nthat our method effectively enhances the performance of LLMs in text\nclassification tasks and significantly improves training efficiency, saving\nnearly half of the training time. Our method has achieved state-of-the-art\nperformance in several open-source classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the use of large language models (LLMs) for text\nclassification has attracted widespread attention. Despite this, the\nclassification accuracy of LLMs has not yet universally surpassed that of\nsmaller models. LLMs can enhance their performance in text classification\nthrough fine-tuning. However, existing data quality research based on LLMs is\nchallenging to apply directly to solve text classification problems. To further\nimprove the performance of LLMs in classification tasks, this paper proposes a\ndata quality enhancement (DQE) method for text classification based on LLMs.\nThis method starts by using a greedy algorithm to select data, dividing the\ndataset into sampled and unsampled subsets, and then performing fine-tuning of\nthe LLMs using the sampled data. Subsequently, this model is used to predict\nthe outcomes for the unsampled data, categorizing incorrectly predicted data\ninto uncovered, difficult, and noisy data. Experimental results demonstrate\nthat our method effectively enhances the performance of LLMs in text\nclassification tasks and significantly improves training efficiency, saving\nnearly half of the training time. Our method has achieved state-of-the-art\nperformance in several open-source classification tasks."
                },
                "authors": [
                    {
                        "name": "Min Zeng"
                    },
                    {
                        "name": "Caiquan Liu"
                    },
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Li Xie"
                    },
                    {
                        "name": "Chen Sang"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "Accepted by COLING 2025(main, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06564v1",
                "updated": "2024-12-09T15:17:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    17,
                    36,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T15:17:36Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    17,
                    36,
                    0,
                    344,
                    0
                ],
                "title": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering"
                },
                "summary": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research."
                },
                "authors": [
                    {
                        "name": "Matheus de Morais Lea"
                    },
                    {
                        "name": "Lucas Valena"
                    },
                    {
                        "name": "Reydne Santos"
                    },
                    {
                        "name": "Ronnie de Souza Santos"
                    }
                ],
                "author_detail": {
                    "name": "Ronnie de Souza Santos"
                },
                "author": "Ronnie de Souza Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06543v1",
                "updated": "2024-12-09T14:54:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    54,
                    44,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T14:54:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    54,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "Challenges and Opportunities for Visual Analytics in Jurisprudence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges and Opportunities for Visual Analytics in Jurisprudence"
                },
                "summary": "Exploring, analyzing, and interpreting law can be tedious and challenging,\neven for legal scholars, since legal texts contain domain-specific language,\nrequire knowledge of tacit legal concepts, and are sometimes intentionally\nambiguous. In related, text-based domains, Visual Analytics (VA) and large\nlanguage models (LLMs) have become essential for working with documents as they\nsupport data navigation, knowledge representation, and analytical reasoning.\nHowever, legal scholars must simultaneously manage hierarchical information\nsources, leverage implicit domain knowledge, and document complex reasoning\nprocesses, which are neither adequately accessible through existing VA designs\nnor sufficiently supported by current LLMs. To address the needs of legal\nscholars, we identify previously unexamined challenges and opportunities when\napplying VA to jurisprudence. We conducted semi-structured interviews with nine\nexperts from the legal domain and found that they lacked the ability to\narticulate their tacit domain knowledge as explicit, machine-interpretable\nknowledge. Hence, we propose leveraging interactive visualization for this\narticulation, teaching the machine relevant semantic relationships between\nlegal documents. These relationships inform the predictions of VA and LLMs,\nfacilitating the navigation between the hierarchies of legal document\ncollections. The enhanced navigation can uncover additional relevant legal\ndocuments, reinforcing the legal reasoning process by generating legal insights\nthat reflect internalized, tacit domain knowledge. In summary, we provide a\nhuman-is-the-loop VA workflow for jurisprudence that recognizes tacit domain\nknowledge as essential for deriving legal insights. More broadly, we compare\nthis workflow with related text-based research practices, revealing research\ngaps and guiding visualization researchers in knowledge-assisted VA for law and\nbeyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring, analyzing, and interpreting law can be tedious and challenging,\neven for legal scholars, since legal texts contain domain-specific language,\nrequire knowledge of tacit legal concepts, and are sometimes intentionally\nambiguous. In related, text-based domains, Visual Analytics (VA) and large\nlanguage models (LLMs) have become essential for working with documents as they\nsupport data navigation, knowledge representation, and analytical reasoning.\nHowever, legal scholars must simultaneously manage hierarchical information\nsources, leverage implicit domain knowledge, and document complex reasoning\nprocesses, which are neither adequately accessible through existing VA designs\nnor sufficiently supported by current LLMs. To address the needs of legal\nscholars, we identify previously unexamined challenges and opportunities when\napplying VA to jurisprudence. We conducted semi-structured interviews with nine\nexperts from the legal domain and found that they lacked the ability to\narticulate their tacit domain knowledge as explicit, machine-interpretable\nknowledge. Hence, we propose leveraging interactive visualization for this\narticulation, teaching the machine relevant semantic relationships between\nlegal documents. These relationships inform the predictions of VA and LLMs,\nfacilitating the navigation between the hierarchies of legal document\ncollections. The enhanced navigation can uncover additional relevant legal\ndocuments, reinforcing the legal reasoning process by generating legal insights\nthat reflect internalized, tacit domain knowledge. In summary, we provide a\nhuman-is-the-loop VA workflow for jurisprudence that recognizes tacit domain\nknowledge as essential for deriving legal insights. More broadly, we compare\nthis workflow with related text-based research practices, revealing research\ngaps and guiding visualization researchers in knowledge-assisted VA for law and\nbeyond."
                },
                "authors": [
                    {
                        "name": "Daniel Frst"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "name": "Daniel A. Keim"
                    },
                    {
                        "name": "Maximilian T. Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian T. Fischer"
                },
                "author": "Maximilian T. Fischer",
                "arxiv_comment": "13 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06542v1",
                "updated": "2024-12-09T14:54:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    54,
                    28,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T14:54:28Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    54,
                    28,
                    0,
                    344,
                    0
                ],
                "title": "Sequential Printed MLP Circuits for Super TinyML Multi-Sensory\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Printed MLP Circuits for Super TinyML Multi-Sensory\n  Applications"
                },
                "summary": "Super-TinyML aims to optimize machine learning models for deployment on\nultra-low-power application domains such as wearable technologies and implants.\nSuch domains also require conformality, flexibility, and non-toxicity which\ntraditional silicon-based systems cannot fulfill. Printed Electronics (PE)\noffers not only these characteristics, but also cost-effective and on-demand\nfabrication. However, Neural Networks (NN) with hundreds of features -- often\nnecessary for target applications -- have not been feasible in PE because of\nits restrictions such as limited device count due to its large feature sizes.\nIn contrast to the state of the art using fully parallel architectures and\nlimited to smaller classifiers, in this work we implement a super-TinyML\narchitecture for bespoke (application-specific) NNs that surpasses the previous\nlimits of state of the art and enables NNs with large number of parameters.\nWith the introduction of super-TinyML into PE technology, we address the area\nand power limitations through resource sharing with multi-cycle operation and\nneuron approximation. This enables, for the first time, the implementation of\nNNs with up to $35.9\\times$ more features and $65.4\\times$ more coefficients\nthan the state of the art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-TinyML aims to optimize machine learning models for deployment on\nultra-low-power application domains such as wearable technologies and implants.\nSuch domains also require conformality, flexibility, and non-toxicity which\ntraditional silicon-based systems cannot fulfill. Printed Electronics (PE)\noffers not only these characteristics, but also cost-effective and on-demand\nfabrication. However, Neural Networks (NN) with hundreds of features -- often\nnecessary for target applications -- have not been feasible in PE because of\nits restrictions such as limited device count due to its large feature sizes.\nIn contrast to the state of the art using fully parallel architectures and\nlimited to smaller classifiers, in this work we implement a super-TinyML\narchitecture for bespoke (application-specific) NNs that surpasses the previous\nlimits of state of the art and enables NNs with large number of parameters.\nWith the introduction of super-TinyML into PE technology, we address the area\nand power limitations through resource sharing with multi-cycle operation and\nneuron approximation. This enables, for the first time, the implementation of\nNNs with up to $35.9\\times$ more features and $65.4\\times$ more coefficients\nthan the state of the art solutions."
                },
                "authors": [
                    {
                        "name": "Gurol Saglam"
                    },
                    {
                        "name": "Florentia Afentaki"
                    },
                    {
                        "name": "Georgios Zervakis"
                    },
                    {
                        "name": "Mehdi B. Tahoori"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi B. Tahoori"
                },
                "author": "Mehdi B. Tahoori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06540v1",
                "updated": "2024-12-09T14:51:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    51,
                    26,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T14:51:26Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    51,
                    26,
                    0,
                    344,
                    0
                ],
                "title": "Sloth: scaling laws for LLM skills to predict multi-benchmark\n  performance across families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sloth: scaling laws for LLM skills to predict multi-benchmark\n  performance across families"
                },
                "summary": "Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training\nconfigurations and data processing across model families lead to significant\nvariations in benchmark performance, making it difficult for a single scaling\nlaw to generalize across all LLMs. On the other hand, training family-specific\nscaling laws requires training models of varying sizes for every family. In\nthis work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a\nnovel scaling law that leverages publicly available benchmark data and assumes\nLLM performance is driven by low-dimensional latent skills, such as reasoning\nand instruction following. These latent skills are influenced by computational\nresources like model size and training tokens but with varying efficiencies\nacross model families. Sloth exploits correlations across benchmarks to provide\nmore accurate and interpretable predictions while alleviating the need to train\nmultiple LLMs per family. We present both theoretical results on parameter\nidentification and empirical evaluations on 12 prominent benchmarks, from Open\nLLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance\nefficiently and offers insights into scaling behaviors for downstream tasks\nsuch as coding and emotional intelligence applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training\nconfigurations and data processing across model families lead to significant\nvariations in benchmark performance, making it difficult for a single scaling\nlaw to generalize across all LLMs. On the other hand, training family-specific\nscaling laws requires training models of varying sizes for every family. In\nthis work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a\nnovel scaling law that leverages publicly available benchmark data and assumes\nLLM performance is driven by low-dimensional latent skills, such as reasoning\nand instruction following. These latent skills are influenced by computational\nresources like model size and training tokens but with varying efficiencies\nacross model families. Sloth exploits correlations across benchmarks to provide\nmore accurate and interpretable predictions while alleviating the need to train\nmultiple LLMs per family. We present both theoretical results on parameter\nidentification and empirical evaluations on 12 prominent benchmarks, from Open\nLLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance\nefficiently and offers insights into scaling behaviors for downstream tasks\nsuch as coding and emotional intelligence applications."
                },
                "authors": [
                    {
                        "name": "Felipe Maia Polo"
                    },
                    {
                        "name": "Seamus Somerstep"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Yuekai Sun"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18659v5",
                "updated": "2024-12-09T14:41:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    41,
                    4,
                    0,
                    344,
                    0
                ],
                "published": "2024-02-28T19:09:08Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    9,
                    8,
                    2,
                    59,
                    0
                ],
                "title": "Large Language Models and Games: A Survey and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Games: A Survey and Roadmap"
                },
                "summary": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field."
                },
                "authors": [
                    {
                        "name": "Roberto Gallotta"
                    },
                    {
                        "name": "Graham Todd"
                    },
                    {
                        "name": "Marvin Zammit"
                    },
                    {
                        "name": "Sam Earle"
                    },
                    {
                        "name": "Antonios Liapis"
                    },
                    {
                        "name": "Julian Togelius"
                    },
                    {
                        "name": "Georgios N. Yannakakis"
                    }
                ],
                "author_detail": {
                    "name": "Georgios N. Yannakakis"
                },
                "author": "Georgios N. Yannakakis",
                "arxiv_doi": "10.1109/TG.2024.3461510",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TG.2024.3461510",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.18659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the IEEE Transactions on Games (19 pages,\n  6 figures)",
                "arxiv_journal_ref": "IEEE Transactions on Games, 2024 (early access)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14229v2",
                "updated": "2024-12-09T14:40:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    40,
                    51,
                    0,
                    344,
                    0
                ],
                "published": "2024-07-19T11:57:34Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    11,
                    57,
                    34,
                    4,
                    201,
                    0
                ],
                "title": "Words2Contact: Identifying Support Contacts from Verbal Instructions\n  Using Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words2Contact: Identifying Support Contacts from Verbal Instructions\n  Using Foundation Models"
                },
                "summary": "This paper presents Words2Contact, a language-guided multi-contact placement\npipeline leveraging large language models and vision language models. Our\nmethod is a key component for language-assisted teleoperation and human-robot\ncooperation, where human operators can instruct the robots where to place their\nsupport contacts before whole-body reaching or manipulation using natural\nlanguage. Words2Contact transforms the verbal instructions of a human operator\ninto contact placement predictions; it also deals with iterative corrections,\nuntil the human is satisfied with the contact location identified in the\nrobot's field of view. We benchmark state-of-the-art LLMs and VLMs for size and\nperformance in contact prediction. We demonstrate the effectiveness of the\niterative correction process, showing that users, even naive, quickly learn how\nto instruct the system to obtain accurate locations. Finally, we validate\nWords2Contact in real-world experiments with the Talos humanoid robot,\ninstructed by human operators to place support contacts on different locations\nand surfaces to avoid falling when reaching for distant objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Words2Contact, a language-guided multi-contact placement\npipeline leveraging large language models and vision language models. Our\nmethod is a key component for language-assisted teleoperation and human-robot\ncooperation, where human operators can instruct the robots where to place their\nsupport contacts before whole-body reaching or manipulation using natural\nlanguage. Words2Contact transforms the verbal instructions of a human operator\ninto contact placement predictions; it also deals with iterative corrections,\nuntil the human is satisfied with the contact location identified in the\nrobot's field of view. We benchmark state-of-the-art LLMs and VLMs for size and\nperformance in contact prediction. We demonstrate the effectiveness of the\niterative correction process, showing that users, even naive, quickly learn how\nto instruct the system to obtain accurate locations. Finally, we validate\nWords2Contact in real-world experiments with the Talos humanoid robot,\ninstructed by human operators to place support contacts on different locations\nand surfaces to avoid falling when reaching for distant objects."
                },
                "authors": [
                    {
                        "name": "Dionis Totsila"
                    },
                    {
                        "name": "Quentin Rouxel"
                    },
                    {
                        "name": "Jean-Baptiste Mouret"
                    },
                    {
                        "name": "Serena Ivaldi"
                    }
                ],
                "author_detail": {
                    "name": "Serena Ivaldi"
                },
                "author": "Serena Ivaldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00929v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00929v3",
                "updated": "2024-12-09T14:30:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    30,
                    11,
                    0,
                    344,
                    0
                ],
                "published": "2024-04-01T05:13:56Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    5,
                    13,
                    56,
                    0,
                    92,
                    0
                ],
                "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and\n  Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and\n  Bias"
                },
                "summary": "Based on the foundation of Large Language Models (LLMs), Multilingual LLMs\n(MLLMs) have been developed to address the challenges faced in multilingual\nnatural language processing, hoping to achieve knowledge transfer from\nhigh-resource languages to low-resource languages. However, significant\nlimitations and challenges still exist, such as language imbalance,\nmultilingual alignment, and inherent bias. In this paper, we aim to provide a\ncomprehensive analysis of MLLMs, delving deeply into discussions surrounding\nthese critical issues. First of all, we start by presenting an overview of\nMLLMs, covering their evolutions, key techniques, and multilingual capacities.\nSecondly, we explore the multilingual training corpora of MLLMs and the\nmultilingual datasets oriented for downstream tasks that are crucial to enhance\nthe cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art\nstudies of multilingual representations and investigate whether the current\nMLLMs can learn a universal language representation. Fourthly, we discuss bias\non MLLMs, including its categories, evaluation metrics, and debiasing\ntechniques. Finally, we discuss existing challenges and point out promising\nresearch directions of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the foundation of Large Language Models (LLMs), Multilingual LLMs\n(MLLMs) have been developed to address the challenges faced in multilingual\nnatural language processing, hoping to achieve knowledge transfer from\nhigh-resource languages to low-resource languages. However, significant\nlimitations and challenges still exist, such as language imbalance,\nmultilingual alignment, and inherent bias. In this paper, we aim to provide a\ncomprehensive analysis of MLLMs, delving deeply into discussions surrounding\nthese critical issues. First of all, we start by presenting an overview of\nMLLMs, covering their evolutions, key techniques, and multilingual capacities.\nSecondly, we explore the multilingual training corpora of MLLMs and the\nmultilingual datasets oriented for downstream tasks that are crucial to enhance\nthe cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art\nstudies of multilingual representations and investigate whether the current\nMLLMs can learn a universal language representation. Fourthly, we discuss bias\non MLLMs, including its categories, evaluation metrics, and debiasing\ntechniques. Finally, we discuss existing challenges and point out promising\nresearch directions of MLLMs."
                },
                "authors": [
                    {
                        "name": "Yuemei Xu"
                    },
                    {
                        "name": "Ling Hu"
                    },
                    {
                        "name": "Jiayi Zhao"
                    },
                    {
                        "name": "Zihan Qiu"
                    },
                    {
                        "name": "Kexin XU"
                    },
                    {
                        "name": "Yuqi Ye"
                    },
                    {
                        "name": "Hanwen Gu"
                    }
                ],
                "author_detail": {
                    "name": "Hanwen Gu"
                },
                "author": "Hanwen Gu",
                "arxiv_doi": "10.1007/s11704-024-40579-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11704-024-40579-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.00929v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00929v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-024-40579-4}",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01369v2",
                "updated": "2024-12-09T14:26:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    26,
                    42,
                    0,
                    344,
                    0
                ],
                "published": "2024-09-02T16:48:57Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    16,
                    48,
                    57,
                    0,
                    246,
                    0
                ],
                "title": "Imitating Language via Scalable Inverse Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitating Language via Scalable Inverse Reinforcement Learning"
                },
                "summary": "The majority of language model training builds on imitation learning. It\ncovers pretraining, supervised fine-tuning, and affects the starting conditions\nfor reinforcement learning from human feedback (RLHF). The simplicity and\nscalability of maximum likelihood estimation (MLE) for next token prediction\nled to its role as predominant paradigm. However, the broader field of\nimitation learning can more effectively utilize the sequential structure\nunderlying autoregressive generation. We focus on investigating the inverse\nreinforcement learning (IRL) perspective to imitation, extracting rewards and\ndirectly optimizing sequences instead of individual token likelihoods and\nevaluate its benefits for fine-tuning large language models. We provide a new\nangle, reformulating inverse soft-Q-learning as a temporal difference\nregularized extension of MLE. This creates a principled connection between MLE\nand IRL and allows trading off added complexity with increased performance and\ndiversity of generations in the supervised fine-tuning (SFT) setting. We find\nclear advantages for IRL-based imitation, in particular for retaining diversity\nwhile maximizing task performance, rendering IRL a strong alternative on fixed\nSFT datasets even without online data generation. Our analysis of IRL-extracted\nreward functions further indicates benefits for more robust reward functions\nvia tighter integration of supervised and preference-based LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of language model training builds on imitation learning. It\ncovers pretraining, supervised fine-tuning, and affects the starting conditions\nfor reinforcement learning from human feedback (RLHF). The simplicity and\nscalability of maximum likelihood estimation (MLE) for next token prediction\nled to its role as predominant paradigm. However, the broader field of\nimitation learning can more effectively utilize the sequential structure\nunderlying autoregressive generation. We focus on investigating the inverse\nreinforcement learning (IRL) perspective to imitation, extracting rewards and\ndirectly optimizing sequences instead of individual token likelihoods and\nevaluate its benefits for fine-tuning large language models. We provide a new\nangle, reformulating inverse soft-Q-learning as a temporal difference\nregularized extension of MLE. This creates a principled connection between MLE\nand IRL and allows trading off added complexity with increased performance and\ndiversity of generations in the supervised fine-tuning (SFT) setting. We find\nclear advantages for IRL-based imitation, in particular for retaining diversity\nwhile maximizing task performance, rendering IRL a strong alternative on fixed\nSFT datasets even without online data generation. Our analysis of IRL-extracted\nreward functions further indicates benefits for more robust reward functions\nvia tighter integration of supervised and preference-based LLM post-training."
                },
                "authors": [
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Michael Bloesch"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Arun Ahuja"
                    },
                    {
                        "name": "Jorg Bornschein"
                    },
                    {
                        "name": "Sandy Huang"
                    },
                    {
                        "name": "Artem Sokolov"
                    },
                    {
                        "name": "Matt Barnes"
                    },
                    {
                        "name": "Guillaume Desjardins"
                    },
                    {
                        "name": "Alex Bewley"
                    },
                    {
                        "name": "Sarah Maria Elisabeth Bechtle"
                    },
                    {
                        "name": "Jost Tobias Springenberg"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Matthieu Geist"
                    },
                    {
                        "name": "Martin Riedmiller"
                    }
                ],
                "author_detail": {
                    "name": "Martin Riedmiller"
                },
                "author": "Martin Riedmiller",
                "arxiv_comment": "Published at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09259v2",
                "updated": "2024-12-09T14:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    22,
                    14,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-14T07:51:51Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    51,
                    51,
                    3,
                    319,
                    0
                ],
                "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A\n  Survey"
                },
                "summary": "The rapid evolution of multimodal foundation models has led to significant\nadvancements in cross-modal understanding and generation across diverse\nmodalities, including text, images, audio, and video. However, these models\nremain susceptible to jailbreak attacks, which can bypass built-in safety\nmechanisms and induce the production of potentially harmful content.\nConsequently, understanding the methods of jailbreak attacks and existing\ndefense mechanisms is essential to ensure the safe deployment of multimodal\ngenerative models in real-world scenarios, particularly in security-sensitive\napplications. To provide comprehensive insight into this topic, this survey\nreviews jailbreak and defense in multimodal generative models. First, given the\ngeneralized lifecycle of multimodal jailbreak, we systematically explore\nattacks and corresponding defense strategies across four levels: input,\nencoder, generator, and output. Based on this analysis, we present a detailed\ntaxonomy of attack methods, defense mechanisms, and evaluation frameworks\nspecific to multimodal generative models. Additionally, we cover a wide range\nof input-output configurations, including modalities such as Any-to-Text,\nAny-to-Vision, and Any-to-Any within generative systems. Finally, we highlight\ncurrent research challenges and propose potential directions for future\nresearch. The open-source repository corresponding to this work can be found at\nhttps://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of multimodal foundation models has led to significant\nadvancements in cross-modal understanding and generation across diverse\nmodalities, including text, images, audio, and video. However, these models\nremain susceptible to jailbreak attacks, which can bypass built-in safety\nmechanisms and induce the production of potentially harmful content.\nConsequently, understanding the methods of jailbreak attacks and existing\ndefense mechanisms is essential to ensure the safe deployment of multimodal\ngenerative models in real-world scenarios, particularly in security-sensitive\napplications. To provide comprehensive insight into this topic, this survey\nreviews jailbreak and defense in multimodal generative models. First, given the\ngeneralized lifecycle of multimodal jailbreak, we systematically explore\nattacks and corresponding defense strategies across four levels: input,\nencoder, generator, and output. Based on this analysis, we present a detailed\ntaxonomy of attack methods, defense mechanisms, and evaluation frameworks\nspecific to multimodal generative models. Additionally, we cover a wide range\nof input-output configurations, including modalities such as Any-to-Text,\nAny-to-Vision, and Any-to-Any within generative systems. Finally, we highlight\ncurrent research challenges and propose potential directions for future\nresearch. The open-source repository corresponding to this work can be found at\nhttps://github.com/liuxuannan/Awesome-Multimodal-Jailbreak."
                },
                "authors": [
                    {
                        "name": "Xuannan Liu"
                    },
                    {
                        "name": "Xing Cui"
                    },
                    {
                        "name": "Peipei Li"
                    },
                    {
                        "name": "Zekun Li"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Shuhan Xia"
                    },
                    {
                        "name": "Miaoxuan Zhang"
                    },
                    {
                        "name": "Yueying Zou"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06512v1",
                "updated": "2024-12-09T14:14:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    14,
                    21,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T14:14:21Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    14,
                    14,
                    21,
                    0,
                    344,
                    0
                ],
                "title": "The Fusion of Large Language Models and Formal Methods for Trustworthy\n  AI Agents: A Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fusion of Large Language Models and Formal Methods for Trustworthy\n  AI Agents: A Roadmap"
                },
                "summary": "Large Language Models (LLMs) have emerged as a transformative AI paradigm,\nprofoundly influencing daily life through their exceptional language\nunderstanding and contextual generation capabilities. Despite their remarkable\nperformance, LLMs face a critical challenge: the propensity to produce\nunreliable outputs due to the inherent limitations of their learning-based\nnature. Formal methods (FMs), on the other hand, are a well-established\ncomputation paradigm that provides mathematically rigorous techniques for\nmodeling, specifying, and verifying the correctness of systems. FMs have been\nextensively applied in mission-critical software engineering, embedded systems,\nand cybersecurity. However, the primary challenge impeding the deployment of\nFMs in real-world settings lies in their steep learning curves, the absence of\nuser-friendly interfaces, and issues with efficiency and adaptability.\n  This position paper outlines a roadmap for advancing the next generation of\ntrustworthy AI systems by leveraging the mutual enhancement of LLMs and FMs.\nFirst, we illustrate how FMs, including reasoning and certification techniques,\ncan help LLMs generate more reliable and formally certified outputs.\nSubsequently, we highlight how the advanced learning capabilities and\nadaptability of LLMs can significantly enhance the usability, efficiency, and\nscalability of existing FM tools. Finally, we show that unifying these two\ncomputation paradigms -- integrating the flexibility and intelligence of LLMs\nwith the rigorous reasoning abilities of FMs -- has transformative potential\nfor the development of trustworthy AI software systems. We acknowledge that\nthis integration has the potential to enhance both the trustworthiness and\nefficiency of software engineering practices while fostering the development of\nintelligent FM tools capable of addressing complex yet real-world challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a transformative AI paradigm,\nprofoundly influencing daily life through their exceptional language\nunderstanding and contextual generation capabilities. Despite their remarkable\nperformance, LLMs face a critical challenge: the propensity to produce\nunreliable outputs due to the inherent limitations of their learning-based\nnature. Formal methods (FMs), on the other hand, are a well-established\ncomputation paradigm that provides mathematically rigorous techniques for\nmodeling, specifying, and verifying the correctness of systems. FMs have been\nextensively applied in mission-critical software engineering, embedded systems,\nand cybersecurity. However, the primary challenge impeding the deployment of\nFMs in real-world settings lies in their steep learning curves, the absence of\nuser-friendly interfaces, and issues with efficiency and adaptability.\n  This position paper outlines a roadmap for advancing the next generation of\ntrustworthy AI systems by leveraging the mutual enhancement of LLMs and FMs.\nFirst, we illustrate how FMs, including reasoning and certification techniques,\ncan help LLMs generate more reliable and formally certified outputs.\nSubsequently, we highlight how the advanced learning capabilities and\nadaptability of LLMs can significantly enhance the usability, efficiency, and\nscalability of existing FM tools. Finally, we show that unifying these two\ncomputation paradigms -- integrating the flexibility and intelligence of LLMs\nwith the rigorous reasoning abilities of FMs -- has transformative potential\nfor the development of trustworthy AI software systems. We acknowledge that\nthis integration has the potential to enhance both the trustworthiness and\nefficiency of software engineering practices while fostering the development of\nintelligent FM tools capable of addressing complex yet real-world challenges."
                },
                "authors": [
                    {
                        "name": "Yedi Zhang"
                    },
                    {
                        "name": "Yufan Cai"
                    },
                    {
                        "name": "Xinyue Zuo"
                    },
                    {
                        "name": "Xiaokun Luan"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Zhe Hou"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Wei"
                    },
                    {
                        "name": "Meng Sun"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Jin Song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin Song Dong"
                },
                "author": "Jin Song Dong",
                "arxiv_comment": "24 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06494v1",
                "updated": "2024-12-09T13:50:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    50,
                    52,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T13:50:52Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    50,
                    52,
                    0,
                    344,
                    0
                ],
                "title": "A cautionary tale on the cost-effectiveness of collaborative AI in\n  real-world medical applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cautionary tale on the cost-effectiveness of collaborative AI in\n  real-world medical applications"
                },
                "summary": "Background. Federated learning (FL) has gained wide popularity as a\ncollaborative learning paradigm enabling collaborative AI in sensitive\nhealthcare applications. Nevertheless, the practical implementation of FL\npresents technical and organizational challenges, as it generally requires\ncomplex communication infrastructures. In this context, consensus-based\nlearning (CBL) may represent a promising collaborative learning alternative,\nthanks to the ability of combining local knowledge into a federated decision\nsystem, while potentially reducing deployment overhead. Methods. In this work\nwe propose an extensive benchmark of the accuracy and cost-effectiveness of a\npanel of FL and CBL methods in a wide range of collaborative medical data\nanalysis scenarios. The benchmark includes 7 different medical datasets,\nencompassing 3 machine learning tasks, 8 different data modalities, and\nmulti-centric settings involving 3 to 23 clients. Findings. Our results reveal\nthat CBL is a cost-effective alternative to FL. When compared across the panel\nof medical dataset in the considered benchmark, CBL methods provide equivalent\naccuracy to the one achieved by FL.Nonetheless, CBL significantly reduces\ntraining time and communication cost (resp. 15 fold and 60 fold decrease) (p <\n0.05). Interpretation. This study opens a novel perspective on the deployment\nof collaborative AI in real-world applications, whereas the adoption of\ncost-effective methods is instrumental to achieve sustainability and\ndemocratisation of AI by alleviating the need for extensive computational\nresources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background. Federated learning (FL) has gained wide popularity as a\ncollaborative learning paradigm enabling collaborative AI in sensitive\nhealthcare applications. Nevertheless, the practical implementation of FL\npresents technical and organizational challenges, as it generally requires\ncomplex communication infrastructures. In this context, consensus-based\nlearning (CBL) may represent a promising collaborative learning alternative,\nthanks to the ability of combining local knowledge into a federated decision\nsystem, while potentially reducing deployment overhead. Methods. In this work\nwe propose an extensive benchmark of the accuracy and cost-effectiveness of a\npanel of FL and CBL methods in a wide range of collaborative medical data\nanalysis scenarios. The benchmark includes 7 different medical datasets,\nencompassing 3 machine learning tasks, 8 different data modalities, and\nmulti-centric settings involving 3 to 23 clients. Findings. Our results reveal\nthat CBL is a cost-effective alternative to FL. When compared across the panel\nof medical dataset in the considered benchmark, CBL methods provide equivalent\naccuracy to the one achieved by FL.Nonetheless, CBL significantly reduces\ntraining time and communication cost (resp. 15 fold and 60 fold decrease) (p <\n0.05). Interpretation. This study opens a novel perspective on the deployment\nof collaborative AI in real-world applications, whereas the adoption of\ncost-effective methods is instrumental to achieve sustainability and\ndemocratisation of AI by alleviating the need for extensive computational\nresources."
                },
                "authors": [
                    {
                        "name": "Francesco Cremonesi"
                    },
                    {
                        "name": "Lucia Innocenti"
                    },
                    {
                        "name": "Sebastien Ourselin"
                    },
                    {
                        "name": "Vicky Goh"
                    },
                    {
                        "name": "Michela Antonelli"
                    },
                    {
                        "name": "Marco Lorenzi"
                    }
                ],
                "author_detail": {
                    "name": "Marco Lorenzi"
                },
                "author": "Marco Lorenzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01159v2",
                "updated": "2024-12-09T13:41:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    41,
                    31,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-02T10:28:52Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    10,
                    28,
                    52,
                    3,
                    123,
                    0
                ],
                "title": "TartuNLP at EvaLatin 2024: Emotion Polarity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TartuNLP at EvaLatin 2024: Emotion Polarity Detection"
                },
                "summary": "This paper presents the TartuNLP team submission to EvaLatin 2024 shared task\nof the emotion polarity detection for historical Latin texts. Our system relies\non two distinct approaches to annotating training data for supervised learning:\n1) creating heuristics-based labels by adopting the polarity lexicon provided\nby the organizers and 2) generating labels with GPT4. We employed parameter\nefficient fine-tuning using the adapters framework and experimented with both\nmonolingual and cross-lingual knowledge transfer for training language and task\nadapters. Our submission with the LLM-generated labels achieved the overall\nfirst place in the emotion polarity detection task. Our results show that\nLLM-based annotations show promising results on texts in Latin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the TartuNLP team submission to EvaLatin 2024 shared task\nof the emotion polarity detection for historical Latin texts. Our system relies\non two distinct approaches to annotating training data for supervised learning:\n1) creating heuristics-based labels by adopting the polarity lexicon provided\nby the organizers and 2) generating labels with GPT4. We employed parameter\nefficient fine-tuning using the adapters framework and experimented with both\nmonolingual and cross-lingual knowledge transfer for training language and task\nadapters. Our submission with the LLM-generated labels achieved the overall\nfirst place in the emotion polarity detection task. Our results show that\nLLM-based annotations show promising results on texts in Latin."
                },
                "authors": [
                    {
                        "name": "Aleksei Dorkin"
                    },
                    {
                        "name": "Kairit Sirts"
                    }
                ],
                "author_detail": {
                    "name": "Kairit Sirts"
                },
                "author": "Kairit Sirts",
                "arxiv_comment": "Added Acknowledgments section",
                "arxiv_journal_ref": "Proceedings of the Third Workshop on Language Technologies for\n  Historical and Ancient Languages (LT4HALA) @ LREC-COLING-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06483v1",
                "updated": "2024-12-09T13:31:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    31,
                    46,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T13:31:46Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    31,
                    46,
                    0,
                    344,
                    0
                ],
                "title": "SafeWorld: Geo-Diverse Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeWorld: Geo-Diverse Safety Alignment"
                },
                "summary": "In the rapidly evolving field of Large Language Models (LLMs), ensuring\nsafety is a crucial and widely discussed topic. However, existing works often\noverlook the geo-diversity of cultural and legal standards across the world. To\ndemonstrate the challenges posed by geo-diverse safety standards, we introduce\nSafeWorld, a novel benchmark specifically designed to evaluate LLMs' ability to\ngenerate responses that are not only helpful but also culturally sensitive and\nlegally compliant across diverse global contexts. SafeWorld encompasses 2,342\ntest user queries, each grounded in high-quality, human-verified cultural norms\nand legal policies from 50 countries and 493 regions/races. On top of it, we\npropose a multi-dimensional automatic safety evaluation framework that assesses\nthe contextual appropriateness, accuracy, and comprehensiveness of responses.\nOur evaluations reveal that current LLMs struggle to meet these criteria. To\nenhance LLMs' alignment with geo-diverse safety standards, we synthesize\nhelpful preference pairs for Direct Preference Optimization (DPO) alignment\ntraining. The preference pair construction aims to encourage LLMs to behave\nappropriately and provide precise references to relevant cultural norms and\npolicies when necessary. Our trained SafeWorldLM outperforms all competing\nmodels, including GPT-4o on all three evaluation dimensions by a large margin.\nGlobal human evaluators also note a nearly 20% higher winning rate in\nhelpfulness and harmfulness evaluation. Our code and data can be found here:\nhttps://github.com/PlusLabNLP/SafeWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving field of Large Language Models (LLMs), ensuring\nsafety is a crucial and widely discussed topic. However, existing works often\noverlook the geo-diversity of cultural and legal standards across the world. To\ndemonstrate the challenges posed by geo-diverse safety standards, we introduce\nSafeWorld, a novel benchmark specifically designed to evaluate LLMs' ability to\ngenerate responses that are not only helpful but also culturally sensitive and\nlegally compliant across diverse global contexts. SafeWorld encompasses 2,342\ntest user queries, each grounded in high-quality, human-verified cultural norms\nand legal policies from 50 countries and 493 regions/races. On top of it, we\npropose a multi-dimensional automatic safety evaluation framework that assesses\nthe contextual appropriateness, accuracy, and comprehensiveness of responses.\nOur evaluations reveal that current LLMs struggle to meet these criteria. To\nenhance LLMs' alignment with geo-diverse safety standards, we synthesize\nhelpful preference pairs for Direct Preference Optimization (DPO) alignment\ntraining. The preference pair construction aims to encourage LLMs to behave\nappropriately and provide precise references to relevant cultural norms and\npolicies when necessary. Our trained SafeWorldLM outperforms all competing\nmodels, including GPT-4o on all three evaluation dimensions by a large margin.\nGlobal human evaluators also note a nearly 20% higher winning rate in\nhelpfulness and harmfulness evaluation. Our code and data can be found here:\nhttps://github.com/PlusLabNLP/SafeWorld."
                },
                "authors": [
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08582v2",
                "updated": "2024-12-09T13:26:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    26,
                    37,
                    0,
                    344,
                    0
                ],
                "published": "2024-07-11T15:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    15,
                    7,
                    26,
                    3,
                    193,
                    0
                ],
                "title": "On the Universal Truthfulness Hyperplane Inside LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Universal Truthfulness Hyperplane Inside LLMs"
                },
                "summary": "While large language models (LLMs) have demonstrated remarkable abilities\nacross various fields, hallucination remains a significant challenge. Recent\nstudies have explored hallucinations through the lens of internal\nrepresentations, proposing mechanisms to decipher LLMs' adherence to facts.\nHowever, these approaches often fail to generalize to out-of-distribution data,\nleading to concerns about whether internal representation patterns reflect\nfundamental factual awareness, or only overfit spurious correlations on the\nspecific datasets. In this work, we investigate whether a universal\ntruthfulness hyperplane that distinguishes the model's factually correct and\nincorrect outputs exists within the model. To this end, we scale up the number\nof training datasets and conduct an extensive evaluation -- we train the\ntruthfulness hyperplane on a diverse collection of over 40 datasets and examine\nits cross-task, cross-domain, and in-domain generalization. Our results\nindicate that increasing the diversity of the training datasets significantly\nenhances the performance in all scenarios, while the volume of data samples\nplays a less critical role. This finding supports the optimistic hypothesis\nthat a universal truthfulness hyperplane may indeed exist within the model,\noffering promising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated remarkable abilities\nacross various fields, hallucination remains a significant challenge. Recent\nstudies have explored hallucinations through the lens of internal\nrepresentations, proposing mechanisms to decipher LLMs' adherence to facts.\nHowever, these approaches often fail to generalize to out-of-distribution data,\nleading to concerns about whether internal representation patterns reflect\nfundamental factual awareness, or only overfit spurious correlations on the\nspecific datasets. In this work, we investigate whether a universal\ntruthfulness hyperplane that distinguishes the model's factually correct and\nincorrect outputs exists within the model. To this end, we scale up the number\nof training datasets and conduct an extensive evaluation -- we train the\ntruthfulness hyperplane on a diverse collection of over 40 datasets and examine\nits cross-task, cross-domain, and in-domain generalization. Our results\nindicate that increasing the diversity of the training datasets significantly\nenhances the performance in all scenarios, while the volume of data samples\nplays a less critical role. This finding supports the optimistic hypothesis\nthat a universal truthfulness hyperplane may indeed exist within the model,\noffering promising directions for future research."
                },
                "authors": [
                    {
                        "name": "Junteng Liu"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05592v2",
                "updated": "2024-12-09T13:07:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    7,
                    1,
                    0,
                    344,
                    0
                ],
                "published": "2024-07-08T04:14:52Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    4,
                    14,
                    52,
                    0,
                    190,
                    0
                ],
                "title": "Transfer or Self-Supervised? Bridging the Performance Gap in Medical\n  Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer or Self-Supervised? Bridging the Performance Gap in Medical\n  Imaging"
                },
                "summary": "Recently, transfer learning and self-supervised learning have gained\nsignificant attention within the medical field due to their ability to mitigate\nthe challenges posed by limited data availability, improve model\ngeneralisation, and reduce computational expenses. Transfer learning and\nself-supervised learning hold immense potential for advancing medical research.\nHowever, it is crucial to recognise that transfer learning and self-supervised\nlearning architectures exhibit distinct advantages and limitations, manifesting\nvariations in accuracy, training speed, and robustness. This paper compares the\nperformance and robustness of transfer learning and self-supervised learning in\nthe medical field. Specifically, we pre-trained two models using the same\nsource domain datasets with different pre-training methods and evaluated them\non small-sized medical datasets to identify the factors influencing their final\nperformance. We tested data with several common issues in medical domains, such\nas data imbalance, data scarcity, and domain mismatch, through comparison\nexperiments to understand their impact on specific pre-trained models. Finally,\nwe provide recommendations to help users apply transfer learning and\nself-supervised learning methods in medical areas, and build more convenient\nand efficient deployment strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, transfer learning and self-supervised learning have gained\nsignificant attention within the medical field due to their ability to mitigate\nthe challenges posed by limited data availability, improve model\ngeneralisation, and reduce computational expenses. Transfer learning and\nself-supervised learning hold immense potential for advancing medical research.\nHowever, it is crucial to recognise that transfer learning and self-supervised\nlearning architectures exhibit distinct advantages and limitations, manifesting\nvariations in accuracy, training speed, and robustness. This paper compares the\nperformance and robustness of transfer learning and self-supervised learning in\nthe medical field. Specifically, we pre-trained two models using the same\nsource domain datasets with different pre-training methods and evaluated them\non small-sized medical datasets to identify the factors influencing their final\nperformance. We tested data with several common issues in medical domains, such\nas data imbalance, data scarcity, and domain mismatch, through comparison\nexperiments to understand their impact on specific pre-trained models. Finally,\nwe provide recommendations to help users apply transfer learning and\nself-supervised learning methods in medical areas, and build more convenient\nand efficient deployment strategies."
                },
                "authors": [
                    {
                        "name": "Zehui Zhao"
                    },
                    {
                        "name": "Laith Alzubaidi"
                    },
                    {
                        "name": "Jinglan Zhang"
                    },
                    {
                        "name": "Ye Duan"
                    },
                    {
                        "name": "Usman Naseem"
                    },
                    {
                        "name": "Yuantong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Yuantong Gu"
                },
                "author": "Yuantong Gu",
                "arxiv_comment": "37 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06456v1",
                "updated": "2024-12-09T12:56:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    56,
                    50,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T12:56:50Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    56,
                    50,
                    0,
                    344,
                    0
                ],
                "title": "UAV Virtual Antenna Array Deployment for Uplink Interference Mitigation\n  in Data Collection Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV Virtual Antenna Array Deployment for Uplink Interference Mitigation\n  in Data Collection Networks"
                },
                "summary": "Unmanned aerial vehicles (UAVs) have gained considerable attention as a\nplatform for establishing aerial wireless networks and communications. However,\nthe line-of-sight dominance in air-to-ground communications often leads to\nsignificant interference with terrestrial networks, reducing communication\nefficiency among terrestrial terminals. This paper explores a novel uplink\ninterference mitigation approach based on the collaborative beamforming (CB)\nmethod in multi-UAV network systems. Specifically, the UAV swarm forms a\nUAV-enabled virtual antenna array (VAA) to achieve the transmissions of\ngathered data to multiple base stations (BSs) for data backup and distributed\nprocessing. However, there is a trade-off between the effectiveness of CB-based\ninterference mitigation and the energy conservation of UAVs. Thus, by jointly\noptimizing the excitation current weights and hover position of UAVs as well as\nthe sequence of data transmission to various BSs, we formulate an uplink\ninterference mitigation multi-objective optimization problem (MOOP) to decrease\ninterference affection, enhance transmission efficiency, and improve energy\nefficiency, simultaneously. In response to the computational demands of the\nformulated problem, we introduce an evolutionary computation method, namely\nchaotic non-dominated sorting genetic algorithm II (CNSGA-II) with multiple\nimproved operators. The proposed CNSGA-II efficiently addresses the formulated\nMOOP, outperforming several other comparative algorithms, as evidenced by the\noutcomes of the simulations. Moreover, the proposed CB-based uplink\ninterference mitigation approach can significantly reduce the interference\ncaused by UAVs to non-receiving BSs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) have gained considerable attention as a\nplatform for establishing aerial wireless networks and communications. However,\nthe line-of-sight dominance in air-to-ground communications often leads to\nsignificant interference with terrestrial networks, reducing communication\nefficiency among terrestrial terminals. This paper explores a novel uplink\ninterference mitigation approach based on the collaborative beamforming (CB)\nmethod in multi-UAV network systems. Specifically, the UAV swarm forms a\nUAV-enabled virtual antenna array (VAA) to achieve the transmissions of\ngathered data to multiple base stations (BSs) for data backup and distributed\nprocessing. However, there is a trade-off between the effectiveness of CB-based\ninterference mitigation and the energy conservation of UAVs. Thus, by jointly\noptimizing the excitation current weights and hover position of UAVs as well as\nthe sequence of data transmission to various BSs, we formulate an uplink\ninterference mitigation multi-objective optimization problem (MOOP) to decrease\ninterference affection, enhance transmission efficiency, and improve energy\nefficiency, simultaneously. In response to the computational demands of the\nformulated problem, we introduce an evolutionary computation method, namely\nchaotic non-dominated sorting genetic algorithm II (CNSGA-II) with multiple\nimproved operators. The proposed CNSGA-II efficiently addresses the formulated\nMOOP, outperforming several other comparative algorithms, as evidenced by the\noutcomes of the simulations. Moreover, the proposed CB-based uplink\ninterference mitigation approach can significantly reduce the interference\ncaused by UAVs to non-receiving BSs."
                },
                "authors": [
                    {
                        "name": "Hongjuan Li"
                    },
                    {
                        "name": "Hui Kang"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06438v1",
                "updated": "2024-12-09T12:27:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    27,
                    21,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T12:27:21Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    27,
                    21,
                    0,
                    344,
                    0
                ],
                "title": "Can foundation models actively gather information in interactive\n  environments to test hypotheses?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can foundation models actively gather information in interactive\n  environments to test hypotheses?"
                },
                "summary": "While problem solving is a standard evaluation task for foundation models, a\ncrucial component of problem solving -- actively and strategically gathering\ninformation to test hypotheses -- has not been closely investigated. To assess\nthe information gathering abilities of foundation models in interactive\nenvironments, we introduce a framework in which a model must determine the\nfactors influencing a hidden reward function by iteratively reasoning about its\npreviously gathered information and proposing its next exploratory action to\nmaximize information gain at each step. We implement this framework in both a\ntext-based environment, which offers a tightly controlled setting and enables\nhigh-throughput parameter sweeps, and in an embodied 3D environment, which\nrequires addressing complexities of multi-modal interaction more relevant to\nreal-world applications. We further investigate whether approaches such as\nself-correction and increased inference time improve information gathering\nefficiency. In a relatively simple task that requires identifying a single\nrewarding feature, we find that LLM's information gathering capability is close\nto optimal. However, when the model must identify a conjunction of rewarding\nfeatures, performance is suboptimal. The hit in performance is due partly to\nthe model translating task description to a policy and partly to the model's\neffectiveness in using its in-context memory. Performance is comparable in both\ntext and 3D embodied environments, although imperfect visual object recognition\nreduces its accuracy in drawing conclusions from gathered information in the 3D\nembodied case. For single-feature-based rewards, we find that smaller models\ncuriously perform better; for conjunction-based rewards, incorporating self\ncorrection into the model improves performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While problem solving is a standard evaluation task for foundation models, a\ncrucial component of problem solving -- actively and strategically gathering\ninformation to test hypotheses -- has not been closely investigated. To assess\nthe information gathering abilities of foundation models in interactive\nenvironments, we introduce a framework in which a model must determine the\nfactors influencing a hidden reward function by iteratively reasoning about its\npreviously gathered information and proposing its next exploratory action to\nmaximize information gain at each step. We implement this framework in both a\ntext-based environment, which offers a tightly controlled setting and enables\nhigh-throughput parameter sweeps, and in an embodied 3D environment, which\nrequires addressing complexities of multi-modal interaction more relevant to\nreal-world applications. We further investigate whether approaches such as\nself-correction and increased inference time improve information gathering\nefficiency. In a relatively simple task that requires identifying a single\nrewarding feature, we find that LLM's information gathering capability is close\nto optimal. However, when the model must identify a conjunction of rewarding\nfeatures, performance is suboptimal. The hit in performance is due partly to\nthe model translating task description to a policy and partly to the model's\neffectiveness in using its in-context memory. Performance is comparable in both\ntext and 3D embodied environments, although imperfect visual object recognition\nreduces its accuracy in drawing conclusions from gathered information in the 3D\nembodied case. For single-feature-based rewards, we find that smaller models\ncuriously perform better; for conjunction-based rewards, incorporating self\ncorrection into the model improves performance."
                },
                "authors": [
                    {
                        "name": "Nan Rosemary Ke"
                    },
                    {
                        "name": "Danny P. Sawyer"
                    },
                    {
                        "name": "Hubert Soyer"
                    },
                    {
                        "name": "Martin Engelcke"
                    },
                    {
                        "name": "David P Reichert"
                    },
                    {
                        "name": "Drew A. Hudson"
                    },
                    {
                        "name": "John Reid"
                    },
                    {
                        "name": "Alexander Lerchner"
                    },
                    {
                        "name": "Danilo Jimenez Rezende"
                    },
                    {
                        "name": "Timothy P Lillicrap"
                    },
                    {
                        "name": "Michael Mozer"
                    },
                    {
                        "name": "Jane X Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jane X Wang"
                },
                "author": "Jane X Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06435v1",
                "updated": "2024-12-09T12:21:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    21,
                    20,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T12:21:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    21,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Simulating Human-like Daily Activities with Desire-driven Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Human-like Daily Activities with Desire-driven Autonomy"
                },
                "summary": "Existing task-oriented AI agents often depend on explicit instructions or\nexternal rewards, limiting their ability to be driven by intrinsic motivations\nlike humans. In this paper, we present a desire-driven autonomy framework to\nguide a Large Language Model-based (LLM-based) agent to simulate human-like\ndaily activities. In contrast to previous agents, our Desire-driven Autonomous\nAgent (D2A) operates on the principle of intrinsic desire, allowing it to\npropose and select tasks that fulfill its motivational framework autonomously.\nInspired by the Theory of Needs, the motivational framework incorporates an\nunderstanding of human-like desires, such as the need for social interaction,\npersonal fulfillment, and self-care. Utilizing a desire-driven task generation\nmechanism, the agent evaluates its current state and takes a sequence of\nactivities aligned with its intrinsic motivations. Through simulations, we\ndemonstrate that our Desire-driven Autonomous Agent (D2A) generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based frameworks demonstrates that our approach significantly enhances the\nrationality of the simulated activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing task-oriented AI agents often depend on explicit instructions or\nexternal rewards, limiting their ability to be driven by intrinsic motivations\nlike humans. In this paper, we present a desire-driven autonomy framework to\nguide a Large Language Model-based (LLM-based) agent to simulate human-like\ndaily activities. In contrast to previous agents, our Desire-driven Autonomous\nAgent (D2A) operates on the principle of intrinsic desire, allowing it to\npropose and select tasks that fulfill its motivational framework autonomously.\nInspired by the Theory of Needs, the motivational framework incorporates an\nunderstanding of human-like desires, such as the need for social interaction,\npersonal fulfillment, and self-care. Utilizing a desire-driven task generation\nmechanism, the agent evaluates its current state and takes a sequence of\nactivities aligned with its intrinsic motivations. Through simulations, we\ndemonstrate that our Desire-driven Autonomous Agent (D2A) generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based frameworks demonstrates that our approach significantly enhances the\nrationality of the simulated activities."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06432v1",
                "updated": "2024-12-09T12:20:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    20,
                    33,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T12:20:33Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    20,
                    33,
                    0,
                    344,
                    0
                ],
                "title": "Integrating Expert Labels into LLM-based Emission Goal Detection:\n  Example Selection vs Automatic Prompt Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Expert Labels into LLM-based Emission Goal Detection:\n  Example Selection vs Automatic Prompt Design"
                },
                "summary": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task."
                },
                "authors": [
                    {
                        "name": "Marco Wrzalik"
                    },
                    {
                        "name": "Adrian Ulges"
                    },
                    {
                        "name": "Anne Uersfeld"
                    },
                    {
                        "name": "Florian Faust"
                    }
                ],
                "author_detail": {
                    "name": "Florian Faust"
                },
                "author": "Florian Faust",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08299v2",
                "updated": "2024-12-09T12:12:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    12,
                    14,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-13T02:41:02Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    2,
                    41,
                    2,
                    2,
                    318,
                    0
                ],
                "title": "DNN Task Assignment in UAV Networks: A Generative AI Enhanced\n  Multi-Agent Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNN Task Assignment in UAV Networks: A Generative AI Enhanced\n  Multi-Agent Reinforcement Learning Approach"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) possess high mobility and flexible deployment\ncapabilities, prompting the development of UAVs for various application\nscenarios within the Internet of Things (IoT). The unique capabilities of UAVs\ngive rise to increasingly critical and complex tasks in uncertain and\npotentially harsh environments. The substantial amount of data generated from\nthese applications necessitates processing and analysis through deep neural\nnetworks (DNNs). However, UAVs encounter challenges due to their limited\ncomputing resources when managing DNN models. This paper presents a joint\napproach that combines multiple-agent reinforcement learning (MARL) and\ngenerative diffusion models (GDM) for assigning DNN tasks to a UAV swarm, aimed\nat reducing latency from task capture to result output. To address these\nchallenges, we first consider the task size of the target area to be inspected\nand the shortest flying path as optimization constraints, employing a greedy\nalgorithm to resolve the subproblem with a focus on minimizing the UAV's flying\npath and the overall system cost. In the second stage, we introduce a novel DNN\ntask assignment algorithm, termed GDM-MADDPG, which utilizes the reverse\ndenoising process of GDM to replace the actor network in multi-agent deep\ndeterministic policy gradient (MADDPG). This approach generates specific DNN\ntask assignment actions based on agents' observations in a dynamic environment.\nSimulation results indicate that our algorithm performs favorably compared to\nbenchmarks in terms of path planning, Age of Information (AoI), energy\nconsumption, and task load balancing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) possess high mobility and flexible deployment\ncapabilities, prompting the development of UAVs for various application\nscenarios within the Internet of Things (IoT). The unique capabilities of UAVs\ngive rise to increasingly critical and complex tasks in uncertain and\npotentially harsh environments. The substantial amount of data generated from\nthese applications necessitates processing and analysis through deep neural\nnetworks (DNNs). However, UAVs encounter challenges due to their limited\ncomputing resources when managing DNN models. This paper presents a joint\napproach that combines multiple-agent reinforcement learning (MARL) and\ngenerative diffusion models (GDM) for assigning DNN tasks to a UAV swarm, aimed\nat reducing latency from task capture to result output. To address these\nchallenges, we first consider the task size of the target area to be inspected\nand the shortest flying path as optimization constraints, employing a greedy\nalgorithm to resolve the subproblem with a focus on minimizing the UAV's flying\npath and the overall system cost. In the second stage, we introduce a novel DNN\ntask assignment algorithm, termed GDM-MADDPG, which utilizes the reverse\ndenoising process of GDM to replace the actor network in multi-agent deep\ndeterministic policy gradient (MADDPG). This approach generates specific DNN\ntask assignment actions based on agents' observations in a dynamic environment.\nSimulation results indicate that our algorithm performs favorably compared to\nbenchmarks in terms of path planning, Age of Information (AoI), energy\nconsumption, and task load balancing."
                },
                "authors": [
                    {
                        "name": "Xin Tang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wenjie Weng"
                    },
                    {
                        "name": "Binhan Liao"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Xianbin Cao"
                    },
                    {
                        "name": "Xiaohuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohuan Li"
                },
                "author": "Xiaohuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06419v1",
                "updated": "2024-12-09T11:57:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    57,
                    16,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T11:57:16Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    57,
                    16,
                    0,
                    344,
                    0
                ],
                "title": "LLM-BIP: Structured Pruning for Large Language Models with Block-Wise\n  Forward Importance Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-BIP: Structured Pruning for Large Language Models with Block-Wise\n  Forward Importance Propagation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious language tasks, but their widespread deployment is impeded by their\nlarge size and high computational costs. Structural pruning is a prevailing\ntechnique used to introduce sparsity into pre-trained models and facilitate\ndirect hardware acceleration during inference by removing redundant connections\n(structurally-grouped parameters), such as channels and attention heads.\nExisting structural pruning approaches often employ either global or layer-wise\npruning criteria; however, they are hindered by ineffectiveness stemming from\ninaccurate evaluation of connection importance. Global pruning methods\ntypically assess component importance using near-zero and unreliable gradients,\nwhile layer-wise pruning approaches encounter significant pruning error\naccumulation issues. To this end, we propose a more accurate pruning metric\nbased on the block-wise importance score propagation, termed LLM-BIP.\nSpecifically, LLM-BIP precisely evaluates connection importance by gauging its\ninfluence on the respective transformer block output, which can be efficiently\napproximated in a single forward pass through an upper bound derived from the\nassumption of Lipschitz continuity. We evaluate the proposed method using\nLLaMA-7B, Vicuna-7B, and LLaMA-13B across common zero-shot tasks. The results\ndemonstrate that our approach achieves an average of 3.26% increase in accuracy\nfor common reasoning tasks compared to previous best baselines. It also reduces\nperplexity by 14.09 and 68.76 on average for the WikiText2 dataset and PTB\ndataset, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious language tasks, but their widespread deployment is impeded by their\nlarge size and high computational costs. Structural pruning is a prevailing\ntechnique used to introduce sparsity into pre-trained models and facilitate\ndirect hardware acceleration during inference by removing redundant connections\n(structurally-grouped parameters), such as channels and attention heads.\nExisting structural pruning approaches often employ either global or layer-wise\npruning criteria; however, they are hindered by ineffectiveness stemming from\ninaccurate evaluation of connection importance. Global pruning methods\ntypically assess component importance using near-zero and unreliable gradients,\nwhile layer-wise pruning approaches encounter significant pruning error\naccumulation issues. To this end, we propose a more accurate pruning metric\nbased on the block-wise importance score propagation, termed LLM-BIP.\nSpecifically, LLM-BIP precisely evaluates connection importance by gauging its\ninfluence on the respective transformer block output, which can be efficiently\napproximated in a single forward pass through an upper bound derived from the\nassumption of Lipschitz continuity. We evaluate the proposed method using\nLLaMA-7B, Vicuna-7B, and LLaMA-13B across common zero-shot tasks. The results\ndemonstrate that our approach achieves an average of 3.26% increase in accuracy\nfor common reasoning tasks compared to previous best baselines. It also reduces\nperplexity by 14.09 and 68.76 on average for the WikiText2 dataset and PTB\ndataset, respectively."
                },
                "authors": [
                    {
                        "name": "Haihang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Haihang Wu"
                },
                "author": "Haihang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06412v1",
                "updated": "2024-12-09T11:40:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    40,
                    6,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T11:40:06Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    40,
                    6,
                    0,
                    344,
                    0
                ],
                "title": "StarWhisper Telescope: Agent-Based Observation Assistant System to\n  Approach AI Astrophysicist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarWhisper Telescope: Agent-Based Observation Assistant System to\n  Approach AI Astrophysicist"
                },
                "summary": "With the rapid advancements in Large Language Models (LLMs), LLM-based agents\nhave introduced convenient and user-friendly methods for leveraging tools\nacross various domains. In the field of astronomical observation, the\nconstruction of new telescopes has significantly increased astronomers'\nworkload. Deploying LLM-powered agents can effectively alleviate this burden\nand reduce the costs associated with training personnel. Within the Nearby\nGalaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes\nacross three observation sites, aiming to find the transients from the galaxies\nin 50 mpc, we have developed the \\textbf{StarWhisper Telescope System} to\nmanage the entire observation process. This system automates tasks such as\ngenerating observation lists, conducting observations, analyzing data, and\nproviding feedback to the observer. Observation lists are customized for\ndifferent sites and strategies to ensure comprehensive coverage of celestial\nobjects. After manual verification, these lists are uploaded to the telescopes\nvia the agents in the system, which initiates observations upon neutral\nlanguage. The observed images are analyzed in real-time, and the transients are\npromptly communicated to the observer. The agent modifies them into a real-time\nfollow-up observation proposal and send to the Xinglong observatory group chat,\nthen add them to the next-day observation lists. Additionally, the integration\nof AI agents within the system provides online accessibility, saving\nastronomers' time and encouraging greater participation from amateur\nastronomers in the NGSS project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancements in Large Language Models (LLMs), LLM-based agents\nhave introduced convenient and user-friendly methods for leveraging tools\nacross various domains. In the field of astronomical observation, the\nconstruction of new telescopes has significantly increased astronomers'\nworkload. Deploying LLM-powered agents can effectively alleviate this burden\nand reduce the costs associated with training personnel. Within the Nearby\nGalaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes\nacross three observation sites, aiming to find the transients from the galaxies\nin 50 mpc, we have developed the \\textbf{StarWhisper Telescope System} to\nmanage the entire observation process. This system automates tasks such as\ngenerating observation lists, conducting observations, analyzing data, and\nproviding feedback to the observer. Observation lists are customized for\ndifferent sites and strategies to ensure comprehensive coverage of celestial\nobjects. After manual verification, these lists are uploaded to the telescopes\nvia the agents in the system, which initiates observations upon neutral\nlanguage. The observed images are analyzed in real-time, and the transients are\npromptly communicated to the observer. The agent modifies them into a real-time\nfollow-up observation proposal and send to the Xinglong observatory group chat,\nthen add them to the next-day observation lists. Additionally, the integration\nof AI agents within the system provides online accessibility, saving\nastronomers' time and encouraging greater participation from amateur\nastronomers in the NGSS project."
                },
                "authors": [
                    {
                        "name": "Cunshi Wang"
                    },
                    {
                        "name": "Xinjie Hu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xunhao Chen"
                    },
                    {
                        "name": "Pengliang Du"
                    },
                    {
                        "name": "Yiming Mao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Ying Wu"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Yansong Li"
                    },
                    {
                        "name": "Beichuan Wang"
                    },
                    {
                        "name": "Haiyang Mu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Jianfeng Tian"
                    },
                    {
                        "name": "Liang Ge"
                    },
                    {
                        "name": "Yongna Mao"
                    },
                    {
                        "name": "Shengming Li"
                    },
                    {
                        "name": "Xiaomeng Lu"
                    },
                    {
                        "name": "Jinhang Zou"
                    },
                    {
                        "name": "Yang Huang"
                    },
                    {
                        "name": "Ningchen Sun"
                    },
                    {
                        "name": "Jie Zheng"
                    },
                    {
                        "name": "Min He"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Junjie Jin"
                    },
                    {
                        "name": "Hong Wu"
                    },
                    {
                        "name": "Chaohui Shang"
                    },
                    {
                        "name": "Jifeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Liu"
                },
                "author": "Jifeng Liu",
                "arxiv_comment": "21 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06394v1",
                "updated": "2024-12-09T11:22:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    22,
                    59,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T11:22:59Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    22,
                    59,
                    0,
                    344,
                    0
                ],
                "title": "GameArena: Evaluating LLM Reasoning through Live Computer Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GameArena: Evaluating LLM Reasoning through Live Computer Games"
                },
                "summary": "Evaluating the reasoning abilities of large language models (LLMs) is\nchallenging. Existing benchmarks often depend on static datasets, which are\nvulnerable to data contamination and may get saturated over time, or on binary\nlive human feedback that conflates reasoning with other abilities. As the most\nprominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in\nreal-world settings, but lacks the granularity in assessing specific reasoning\ncapabilities. We introduce GameArena, a dynamic benchmark designed to evaluate\nLLM reasoning capabilities through interactive gameplay with humans. GameArena\nconsists of three games designed to test specific reasoning capabilities (e.g.,\ndeductive and inductive reasoning), while keeping participants entertained and\nengaged. We analyze the gaming data retrospectively to uncover the underlying\nreasoning processes of LLMs and measure their fine-grained reasoning\ncapabilities. We collect over 2000 game sessions and provide detailed\nassessments of various reasoning capabilities for five state-of-the-art LLMs.\nOur user study with 100 participants suggests that GameArena improves user\nengagement compared to Chatbot Arena. For the first time, GameArena enables the\ncollection of step-by-step LLM reasoning data in the wild.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the reasoning abilities of large language models (LLMs) is\nchallenging. Existing benchmarks often depend on static datasets, which are\nvulnerable to data contamination and may get saturated over time, or on binary\nlive human feedback that conflates reasoning with other abilities. As the most\nprominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in\nreal-world settings, but lacks the granularity in assessing specific reasoning\ncapabilities. We introduce GameArena, a dynamic benchmark designed to evaluate\nLLM reasoning capabilities through interactive gameplay with humans. GameArena\nconsists of three games designed to test specific reasoning capabilities (e.g.,\ndeductive and inductive reasoning), while keeping participants entertained and\nengaged. We analyze the gaming data retrospectively to uncover the underlying\nreasoning processes of LLMs and measure their fine-grained reasoning\ncapabilities. We collect over 2000 game sessions and provide detailed\nassessments of various reasoning capabilities for five state-of-the-art LLMs.\nOur user study with 100 participants suggests that GameArena improves user\nengagement compared to Chatbot Arena. For the first time, GameArena enables the\ncollection of step-by-step LLM reasoning data in the wild."
                },
                "authors": [
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Qiyu Li"
                    },
                    {
                        "name": "Anze Xie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Haojian Jin"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06370v1",
                "updated": "2024-12-09T10:44:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    10,
                    44,
                    47,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T10:44:47Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    10,
                    44,
                    47,
                    0,
                    344,
                    0
                ],
                "title": "Exploring Memorization and Copyright Violation in Frontier LLMs: A Study\n  of the New York Times v. OpenAI 2023 Lawsuit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Memorization and Copyright Violation in Frontier LLMs: A Study\n  of the New York Times v. OpenAI 2023 Lawsuit"
                },
                "summary": "Copyright infringement in frontier LLMs has received much attention recently\ndue to the New York Times v. OpenAI lawsuit, filed in December 2023. The New\nYork Times claims that GPT-4 has infringed its copyrights by reproducing\narticles for use in LLM training and by memorizing the inputs, thereby publicly\ndisplaying them in LLM outputs. Our work aims to measure the propensity of\nOpenAI's LLMs to exhibit verbatim memorization in its outputs relative to other\nLLMs, specifically focusing on news articles. We discover that both GPT and\nClaude models use refusal training and output filters to prevent verbatim\noutput of the memorized articles. We apply a basic prompt template to bypass\nthe refusal training and show that OpenAI models are currently less prone to\nmemorization elicitation than models from Meta, Mistral, and Anthropic. We find\nthat as models increase in size, especially beyond 100 billion parameters, they\ndemonstrate significantly greater capacity for memorization. Our findings have\npractical implications for training: more attention must be placed on\npreventing verbatim memorization in very large models. Our findings also have\nlegal significance: in assessing the relative memorization capacity of OpenAI's\nLLMs, we probe the strength of The New York Times's copyright infringement\nclaims and OpenAI's legal defenses, while underscoring issues at the\nintersection of generative AI, law, and policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copyright infringement in frontier LLMs has received much attention recently\ndue to the New York Times v. OpenAI lawsuit, filed in December 2023. The New\nYork Times claims that GPT-4 has infringed its copyrights by reproducing\narticles for use in LLM training and by memorizing the inputs, thereby publicly\ndisplaying them in LLM outputs. Our work aims to measure the propensity of\nOpenAI's LLMs to exhibit verbatim memorization in its outputs relative to other\nLLMs, specifically focusing on news articles. We discover that both GPT and\nClaude models use refusal training and output filters to prevent verbatim\noutput of the memorized articles. We apply a basic prompt template to bypass\nthe refusal training and show that OpenAI models are currently less prone to\nmemorization elicitation than models from Meta, Mistral, and Anthropic. We find\nthat as models increase in size, especially beyond 100 billion parameters, they\ndemonstrate significantly greater capacity for memorization. Our findings have\npractical implications for training: more attention must be placed on\npreventing verbatim memorization in very large models. Our findings also have\nlegal significance: in assessing the relative memorization capacity of OpenAI's\nLLMs, we probe the strength of The New York Times's copyright infringement\nclaims and OpenAI's legal defenses, while underscoring issues at the\nintersection of generative AI, law, and policy."
                },
                "authors": [
                    {
                        "name": "Joshua Freeman"
                    },
                    {
                        "name": "Chloe Rippe"
                    },
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ],
                "author_detail": {
                    "name": "Maksym Andriushchenko"
                },
                "author": "Maksym Andriushchenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21348v2",
                "updated": "2024-12-09T10:11:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    10,
                    11,
                    22,
                    0,
                    344,
                    0
                ],
                "published": "2024-10-28T11:07:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    7,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Large Language Model Benchmarks in Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Benchmarks in Medical Tasks"
                },
                "summary": "With the increasing application of large language models (LLMs) in the\nmedical domain, evaluating these models' performance using benchmark datasets\nhas become crucial. This paper presents a comprehensive survey of various\nbenchmark datasets employed in medical LLM tasks. These datasets span multiple\nmodalities including text, image, and multimodal benchmarks, focusing on\ndifferent aspects of medical knowledge such as electronic health records\n(EHRs), doctor-patient dialogues, medical question-answering, and medical image\ncaptioning. The survey categorizes the datasets by modality, discussing their\nsignificance, data structure, and impact on the development of LLMs for\nclinical tasks such as diagnosis, report generation, and predictive decision\nsupport. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and\nCheXpert, which have facilitated advancements in tasks like medical report\ngeneration, clinical summarization, and synthetic data generation. The paper\nsummarizes the challenges and opportunities in leveraging these benchmarks for\nadvancing multimodal medical intelligence, emphasizing the need for datasets\nwith a greater degree of language diversity, structured omics data, and\ninnovative approaches to synthesis. This work also provides a foundation for\nfuture research in the application of LLMs in medicine, contributing to the\nevolving field of medical artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing application of large language models (LLMs) in the\nmedical domain, evaluating these models' performance using benchmark datasets\nhas become crucial. This paper presents a comprehensive survey of various\nbenchmark datasets employed in medical LLM tasks. These datasets span multiple\nmodalities including text, image, and multimodal benchmarks, focusing on\ndifferent aspects of medical knowledge such as electronic health records\n(EHRs), doctor-patient dialogues, medical question-answering, and medical image\ncaptioning. The survey categorizes the datasets by modality, discussing their\nsignificance, data structure, and impact on the development of LLMs for\nclinical tasks such as diagnosis, report generation, and predictive decision\nsupport. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and\nCheXpert, which have facilitated advancements in tasks like medical report\ngeneration, clinical summarization, and synthetic data generation. The paper\nsummarizes the challenges and opportunities in leveraging these benchmarks for\nadvancing multimodal medical intelligence, emphasizing the need for datasets\nwith a greater degree of language diversity, structured omics data, and\ninnovative approaches to synthesis. This work also provides a foundation for\nfuture research in the application of LLMs in medicine, contributing to the\nevolving field of medical artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Lawrence K. Q. Yan"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Cheng Fei"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Yunze Wang"
                    },
                    {
                        "name": "Silin Chen"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Junyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Junyu Liu"
                },
                "author": "Junyu Liu",
                "arxiv_comment": "25 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10957v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10957v4",
                "updated": "2024-12-09T09:57:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    57,
                    5,
                    0,
                    344,
                    0
                ],
                "published": "2024-06-16T14:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    14,
                    24,
                    30,
                    6,
                    168,
                    0
                ],
                "title": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliminating Biased Length Reliance of Direct Preference Optimization via\n  Down-Sampled KL Divergence"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for\nthe direct and robust alignment of Large Language Models (LLMs) with human\npreferences, offering a more straightforward alternative to the complex\nReinforcement Learning from Human Feedback (RLHF). Despite its promising\nefficacy, DPO faces a notable drawback: \"verbosity\", a common over-optimization\nphenomenon also observed in RLHF. While previous studies mainly attributed\nverbosity to biased labels within the data, we propose that the issue also\nstems from an inherent algorithmic length reliance in DPO. Specifically, we\nsuggest that the discrepancy between sequence-level Kullback-Leibler (KL)\ndivergences between chosen and rejected sequences, used in DPO, results in\noverestimated or underestimated rewards due to varying token lengths.\nEmpirically, we utilize datasets with different label lengths to demonstrate\nthe presence of biased rewards. We then introduce an effective downsampling\napproach, named SamPO, to eliminate potential length reliance. Our experimental\nevaluations, conducted across three LLMs of varying scales and a diverse array\nof conditional and open-ended benchmarks, highlight the efficacy of SamPO in\nmitigating verbosity, achieving improvements of 5% to 12% over DPO through\ndebaised rewards. Our codes can be accessed at:\nhttps://github.com/LuJunru/SamPO/."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "EMNLP 2024 Main, Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10957v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10957v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.08339v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.08339v5",
                "updated": "2024-12-09T09:56:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    56,
                    59,
                    0,
                    344,
                    0
                ],
                "published": "2023-05-15T04:10:13Z",
                "published_parsed": [
                    2023,
                    5,
                    15,
                    4,
                    10,
                    13,
                    0,
                    135,
                    0
                ],
                "title": "Assessing the potential of LLM-assisted annotation for corpus-based\n  pragmatics and discourse analysis: The case of apology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the potential of LLM-assisted annotation for corpus-based\n  pragmatics and discourse analysis: The case of apology"
                },
                "summary": "Certain forms of linguistic annotation, like part of speech and semantic\ntagging, can be automated with high accuracy. However, manual annotation is\nstill necessary for complex pragmatic and discursive features that lack a\ndirect mapping to lexical forms. This manual process is time-consuming and\nerror-prone, limiting the scalability of function-to-form approaches in corpus\nlinguistics. To address this, our study explores the possibility of using large\nlanguage models (LLMs) to automate pragma-discursive corpus annotation. We\ncompare GPT-3.5 (the model behind the free-to-use version of ChatGPT), GPT-4\n(the model underpinning the precise mode of Bing chatbot), and a human coder in\nannotating apology components in English based on the local grammar framework.\nWe find that GPT-4 outperformed GPT-3.5, with accuracy approaching that of a\nhuman coder. These results suggest that LLMs can be successfully deployed to\naid pragma-discursive corpus annotation, making the process more efficient,\nscalable and accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certain forms of linguistic annotation, like part of speech and semantic\ntagging, can be automated with high accuracy. However, manual annotation is\nstill necessary for complex pragmatic and discursive features that lack a\ndirect mapping to lexical forms. This manual process is time-consuming and\nerror-prone, limiting the scalability of function-to-form approaches in corpus\nlinguistics. To address this, our study explores the possibility of using large\nlanguage models (LLMs) to automate pragma-discursive corpus annotation. We\ncompare GPT-3.5 (the model behind the free-to-use version of ChatGPT), GPT-4\n(the model underpinning the precise mode of Bing chatbot), and a human coder in\nannotating apology components in English based on the local grammar framework.\nWe find that GPT-4 outperformed GPT-3.5, with accuracy approaching that of a\nhuman coder. These results suggest that LLMs can be successfully deployed to\naid pragma-discursive corpus annotation, making the process more efficient,\nscalable and accessible."
                },
                "authors": [
                    {
                        "name": "Danni Yu"
                    },
                    {
                        "name": "Luyang Li"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Matteo Fuoli"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Fuoli"
                },
                "author": "Matteo Fuoli",
                "arxiv_doi": "10.1075/ijcl.23087.yu",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1075/ijcl.23087.yu",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.08339v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.08339v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 2 figures, 3 tablels",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11811v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11811v4",
                "updated": "2024-12-09T09:53:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    53,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2024-02-19T03:56:44Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    3,
                    56,
                    44,
                    0,
                    50,
                    0
                ],
                "title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema"
                },
                "summary": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and six testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and six testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "COLING 2025, Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11811v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11811v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01432v3",
                "updated": "2024-12-09T09:39:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    39,
                    12,
                    0,
                    344,
                    0
                ],
                "published": "2023-09-29T14:38:58Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    14,
                    38,
                    58,
                    4,
                    272,
                    0
                ],
                "title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split and Merge: Aligning Position Biases in LLM-based Evaluators"
                },
                "summary": "Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications."
                },
                "authors": [
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Accepted by EMNLP 2024. Please cite the conference version of this\n  paper, e.g., \"Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai\n  Wang, Cuiyun Gao, and Yang Liu. 2024. Split and Merge: Aligning Position\n  Biases in LLM-based Evaluators. (EMNLP 2024)\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09335v2",
                "updated": "2024-12-09T09:31:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    31,
                    39,
                    0,
                    344,
                    0
                ],
                "published": "2024-10-12T02:48:34Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    48,
                    34,
                    5,
                    286,
                    0
                ],
                "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You\n  Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Data Selection at Scale: Random Selection is Almost All You\n  Need"
                },
                "summary": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models\n(LLMs) with human instructions. The primary goal during SFT is to select a\nsmall yet representative subset of training data from the larger pool, such\nthat fine-tuning with this subset achieves results comparable to or even\nexceeding those obtained using the entire dataset. However, most existing data\nselection techniques are designed for small-scale data pools, which fail to\nmeet the demands of real-world SFT scenarios. In this paper, we replicated\nseveral self-scoring methods those that do not rely on external model\nassistance on two million scale datasets, and found that nearly all methods\nstruggled to significantly outperform random selection when dealing with such\nlarge-scale data pools. Moreover, our comparisons suggest that, during SFT,\ndiversity in data selection is more critical than simply focusing on high\nquality data. We also analyzed the limitations of several current approaches,\nexplaining why they perform poorly on large-scale datasets and why they are\nunsuitable for such contexts. Finally, we found that filtering data by token\nlength offers a stable and efficient method for improving results. This\napproach, particularly when training on long text data, proves highly\nbeneficial for relatively weaker base models, such as Llama3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models\n(LLMs) with human instructions. The primary goal during SFT is to select a\nsmall yet representative subset of training data from the larger pool, such\nthat fine-tuning with this subset achieves results comparable to or even\nexceeding those obtained using the entire dataset. However, most existing data\nselection techniques are designed for small-scale data pools, which fail to\nmeet the demands of real-world SFT scenarios. In this paper, we replicated\nseveral self-scoring methods those that do not rely on external model\nassistance on two million scale datasets, and found that nearly all methods\nstruggled to significantly outperform random selection when dealing with such\nlarge-scale data pools. Moreover, our comparisons suggest that, during SFT,\ndiversity in data selection is more critical than simply focusing on high\nquality data. We also analyzed the limitations of several current approaches,\nexplaining why they perform poorly on large-scale datasets and why they are\nunsuitable for such contexts. Finally, we found that filtering data by token\nlength offers a stable and efficient method for improving results. This\napproach, particularly when training on long text data, proves highly\nbeneficial for relatively weaker base models, such as Llama3."
                },
                "authors": [
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Yuan Wu"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01004v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01004v3",
                "updated": "2024-12-09T09:25:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    25,
                    36,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-01T23:41:42Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    23,
                    41,
                    42,
                    6,
                    336,
                    0
                ],
                "title": "Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual\n  Learning Vision-Language Models with Dynamic Rank-Selective LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual\n  Learning Vision-Language Models with Dynamic Rank-Selective LoRA"
                },
                "summary": "We investigate whether the pre-trained knowledge of vision-language models\n(VLMs), such as CLIP, can be retained or even enhanced during continual\nlearning (CL) while absorbing knowledge from a data stream. Existing methods\noften rely on additional reference data, isolated components for distribution\nor domain predictions, leading to high training costs, increased inference\ncomplexity, and limited improvement potential for pre-trained models. To\naddress these challenges, we first comprehensively analyze the effects of\nparameter update locations and ranks on downstream adaptation and knowledge\nretention. Based on these insights, we propose Dynamic Rank-Selective Low Rank\nAdaptation (LoRA), a universal and efficient CL approach that adaptively\nassigns ranks to LoRA modules based on their relevance to the current data.\nUnlike prior methods, our approach continually enhances the pre-trained VLM by\nretaining both the pre-trained knowledge and the knowledge acquired during CL.\nOur approach eliminates the need for explicit domain or distribution prediction\nand additional reference data, enabling seamless integration of new tasks while\npreserving pre-trained capabilities. It also maintains the original\narchitecture and deployment pipeline of the pre-trained model without incurring\nany additional inference overhead. Extensive experiments and analyses\ndemonstrate that our method outperforms state-of-the-art approaches in\ncontinually absorbing knowledge of downstream tasks while retaining pre-trained\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the pre-trained knowledge of vision-language models\n(VLMs), such as CLIP, can be retained or even enhanced during continual\nlearning (CL) while absorbing knowledge from a data stream. Existing methods\noften rely on additional reference data, isolated components for distribution\nor domain predictions, leading to high training costs, increased inference\ncomplexity, and limited improvement potential for pre-trained models. To\naddress these challenges, we first comprehensively analyze the effects of\nparameter update locations and ranks on downstream adaptation and knowledge\nretention. Based on these insights, we propose Dynamic Rank-Selective Low Rank\nAdaptation (LoRA), a universal and efficient CL approach that adaptively\nassigns ranks to LoRA modules based on their relevance to the current data.\nUnlike prior methods, our approach continually enhances the pre-trained VLM by\nretaining both the pre-trained knowledge and the knowledge acquired during CL.\nOur approach eliminates the need for explicit domain or distribution prediction\nand additional reference data, enabling seamless integration of new tasks while\npreserving pre-trained capabilities. It also maintains the original\narchitecture and deployment pipeline of the pre-trained model without incurring\nany additional inference overhead. Extensive experiments and analyses\ndemonstrate that our method outperforms state-of-the-art approaches in\ncontinually absorbing knowledge of downstream tasks while retaining pre-trained\nknowledge."
                },
                "authors": [
                    {
                        "name": "Haodong Lu"
                    },
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Jason Xue"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01004v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01004v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05248v2",
                "updated": "2024-12-09T09:21:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    21,
                    49,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-06T18:27:15Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    18,
                    27,
                    15,
                    4,
                    341,
                    0
                ],
                "title": "Enhancing FKG.in: automating Indian food composition analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing FKG.in: automating Indian food composition analysis"
                },
                "summary": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG.in and iteratively supplement\nfood composition data from verified knowledge bases. Additionally, this paper\nhighlights the challenges of representing Indian food and accessing food\ncomposition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG.in and iteratively supplement\nfood composition data from verified knowledge bases. Additionally, this paper\nhighlights the challenges of representing Indian food and accessing food\ncomposition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain."
                },
                "authors": [
                    {
                        "name": "Saransh Kumar Gupta"
                    },
                    {
                        "name": "Lipika Dey"
                    },
                    {
                        "name": "Partha Pratim Das"
                    },
                    {
                        "name": "Geeta Trilok-Kumar"
                    },
                    {
                        "name": "Ramesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Jain"
                },
                "author": "Ramesh Jain",
                "arxiv_comment": "15 pages, 5 figures, 30 references, International Conference on\n  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03817v3",
                "updated": "2024-12-09T09:20:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    9,
                    20,
                    11,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-06T10:35:11Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    35,
                    11,
                    2,
                    311,
                    0
                ],
                "title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning"
                },
                "summary": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Zhirui Deng"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06308v1",
                "updated": "2024-12-09T08:55:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    55,
                    48,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:55:48Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    55,
                    48,
                    0,
                    344,
                    0
                ],
                "title": "PRECISE: Pre-training Sequential Recommenders with Collaborative and\n  Semantic Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRECISE: Pre-training Sequential Recommenders with Collaborative and\n  Semantic Information"
                },
                "summary": "Real-world recommendation systems commonly offer diverse content scenarios\nfor users to interact with. Considering the enormous number of users in\nindustrial platforms, it is infeasible to utilize a single unified\nrecommendation model to meet the requirements of all scenarios. Usually,\nseparate recommendation pipelines are established for each distinct scenario.\nThis practice leads to challenges in comprehensively grasping users' interests.\nRecent research endeavors have been made to tackle this problem by pre-training\nmodels to encapsulate the overall interests of users. Traditional pre-trained\nrecommendation models mainly capture user interests by leveraging collaborative\nsignals. Nevertheless, a prevalent drawback of these systems is their\nincapacity to handle long-tail items and cold-start scenarios. With the recent\nadvent of large language models, there has been a significant increase in\nresearch efforts focused on exploiting LLMs to extract semantic information for\nusers and items. However, text-based recommendations highly rely on elaborate\nfeature engineering and frequently fail to capture collaborative similarities.\nTo overcome these limitations, we propose a novel pre-training framework for\nsequential recommendation, termed PRECISE. This framework combines\ncollaborative signals with semantic information. Moreover, PRECISE employs a\nlearning framework that initially models users' comprehensive interests across\nall recommendation scenarios and subsequently concentrates on the specific\ninterests of target-scene behaviors. We demonstrate that PRECISE precisely\ncaptures the entire range of user interests and effectively transfers them to\nthe target interests. Empirical findings reveal that the PRECISE framework\nattains outstanding performance on both public and industrial datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world recommendation systems commonly offer diverse content scenarios\nfor users to interact with. Considering the enormous number of users in\nindustrial platforms, it is infeasible to utilize a single unified\nrecommendation model to meet the requirements of all scenarios. Usually,\nseparate recommendation pipelines are established for each distinct scenario.\nThis practice leads to challenges in comprehensively grasping users' interests.\nRecent research endeavors have been made to tackle this problem by pre-training\nmodels to encapsulate the overall interests of users. Traditional pre-trained\nrecommendation models mainly capture user interests by leveraging collaborative\nsignals. Nevertheless, a prevalent drawback of these systems is their\nincapacity to handle long-tail items and cold-start scenarios. With the recent\nadvent of large language models, there has been a significant increase in\nresearch efforts focused on exploiting LLMs to extract semantic information for\nusers and items. However, text-based recommendations highly rely on elaborate\nfeature engineering and frequently fail to capture collaborative similarities.\nTo overcome these limitations, we propose a novel pre-training framework for\nsequential recommendation, termed PRECISE. This framework combines\ncollaborative signals with semantic information. Moreover, PRECISE employs a\nlearning framework that initially models users' comprehensive interests across\nall recommendation scenarios and subsequently concentrates on the specific\ninterests of target-scene behaviors. We demonstrate that PRECISE precisely\ncaptures the entire range of user interests and effectively transfers them to\nthe target interests. Empirical findings reveal that the PRECISE framework\nattains outstanding performance on both public and industrial datasets."
                },
                "authors": [
                    {
                        "name": "Chonggang Song"
                    },
                    {
                        "name": "Chunxu Shen"
                    },
                    {
                        "name": "Hao Gu"
                    },
                    {
                        "name": "Yaoming Wu"
                    },
                    {
                        "name": "Lingling Yi"
                    },
                    {
                        "name": "Jie Wen"
                    },
                    {
                        "name": "Chuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Chen"
                },
                "author": "Chuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06303v1",
                "updated": "2024-12-09T08:47:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    47,
                    5,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:47:05Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    47,
                    5,
                    0,
                    344,
                    0
                ],
                "title": "DSAI: Unbiased and Interpretable Latent Feature Extraction for\n  Data-Centric AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSAI: Unbiased and Interpretable Latent Feature Extraction for\n  Data-Centric AI"
                },
                "summary": "Large language models (LLMs) often struggle to objectively identify latent\ncharacteristics in large datasets due to their reliance on pre-trained\nknowledge rather than actual data patterns. To address this data grounding\nissue, we propose Data Scientist AI (DSAI), a framework that enables unbiased\nand interpretable feature extraction through a multi-stage pipeline with\nquantifiable prominence metrics for evaluating extracted features. On synthetic\ndatasets with known ground-truth features, DSAI demonstrates high recall in\nidentifying expert-defined features while faithfully reflecting the underlying\ndata. Applications on real-world datasets illustrate the framework's practical\nutility in uncovering meaningful patterns with minimal expert oversight,\nsupporting use cases such as interpretable classification.\n  The title of our paper is chosen from multiple candidates based on\nDSAI-generated criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often struggle to objectively identify latent\ncharacteristics in large datasets due to their reliance on pre-trained\nknowledge rather than actual data patterns. To address this data grounding\nissue, we propose Data Scientist AI (DSAI), a framework that enables unbiased\nand interpretable feature extraction through a multi-stage pipeline with\nquantifiable prominence metrics for evaluating extracted features. On synthetic\ndatasets with known ground-truth features, DSAI demonstrates high recall in\nidentifying expert-defined features while faithfully reflecting the underlying\ndata. Applications on real-world datasets illustrate the framework's practical\nutility in uncovering meaningful patterns with minimal expert oversight,\nsupporting use cases such as interpretable classification.\n  The title of our paper is chosen from multiple candidates based on\nDSAI-generated criteria."
                },
                "authors": [
                    {
                        "name": "Hyowon Cho"
                    },
                    {
                        "name": "Soonwon Ka"
                    },
                    {
                        "name": "Daechul Park"
                    },
                    {
                        "name": "Jaewook Kang"
                    },
                    {
                        "name": "Minjoon Seo"
                    },
                    {
                        "name": "Bokyung Son"
                    }
                ],
                "author_detail": {
                    "name": "Bokyung Son"
                },
                "author": "Bokyung Son",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06294v1",
                "updated": "2024-12-09T08:37:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    37,
                    6,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:37:06Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    37,
                    6,
                    0,
                    344,
                    0
                ],
                "title": "Beyond pip install: Evaluating LLM Agents for the Automated Installation\n  of Python Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond pip install: Evaluating LLM Agents for the Automated Installation\n  of Python Projects"
                },
                "summary": "Many works have recently proposed the use of Large Language Model (LLM) based\nagents for performing `repository level' tasks, loosely defined as a set of\ntasks whose scopes are greater than a single file. This has led to speculation\nthat the orchestration of these repository-level tasks could lead to software\nengineering agents capable of performing almost independently of human\nintervention. However, of the suite of tasks that would need to be performed by\nthis autonomous software engineering agent, we argue that one important task is\nmissing, which is to fulfil project level dependency by installing other\nrepositories. To investigate the feasibility of this repository level\ninstallation task, we introduce a benchmark of of repository installation tasks\ncurated from 40 open source Python projects, which includes a ground truth\ninstallation process for each target repository. Further, we propose\nInstallamatic, an agent which aims to perform and verify the installation of a\ngiven repository by searching for relevant instructions from documentation in\nthe repository. Empirical experiments reveal that that 55% of the studied\nrepositories can be automatically installed by our agent at least one out of\nten times. Through further analysis, we identify the common causes for our\nagent's inability to install a repository, discuss the challenges faced in the\ndesign and implementation of such an agent and consider the implications that\nsuch an agent could have for developers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many works have recently proposed the use of Large Language Model (LLM) based\nagents for performing `repository level' tasks, loosely defined as a set of\ntasks whose scopes are greater than a single file. This has led to speculation\nthat the orchestration of these repository-level tasks could lead to software\nengineering agents capable of performing almost independently of human\nintervention. However, of the suite of tasks that would need to be performed by\nthis autonomous software engineering agent, we argue that one important task is\nmissing, which is to fulfil project level dependency by installing other\nrepositories. To investigate the feasibility of this repository level\ninstallation task, we introduce a benchmark of of repository installation tasks\ncurated from 40 open source Python projects, which includes a ground truth\ninstallation process for each target repository. Further, we propose\nInstallamatic, an agent which aims to perform and verify the installation of a\ngiven repository by searching for relevant instructions from documentation in\nthe repository. Empirical experiments reveal that that 55% of the studied\nrepositories can be automatically installed by our agent at least one out of\nten times. Through further analysis, we identify the common causes for our\nagent's inability to install a repository, discuss the challenges faced in the\ndesign and implementation of such an agent and consider the implications that\nsuch an agent could have for developers."
                },
                "authors": [
                    {
                        "name": "Louis Milliken"
                    },
                    {
                        "name": "Sungmin Kang"
                    },
                    {
                        "name": "Shin Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shin Yoo"
                },
                "author": "Shin Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00627v2",
                "updated": "2024-12-09T08:27:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    27,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-01T00:52:51Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    0,
                    52,
                    51,
                    6,
                    336,
                    0
                ],
                "title": "ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by\n  Multimodal Gemini LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by\n  Multimodal Gemini LLM"
                },
                "summary": "Cooking meals can be difficult, causing many to resort to cookbooks and\nonline recipes. However, relying on these traditional methods of cooking often\nresults in missing ingredients, nutritional hazards, and unsatisfactory meals.\nUsing Augmented Reality (AR) can address these issues; however, current AR\ncooking applications have poor user interfaces and limited accessibility. This\npaper proposes a prototype of an iOS application that integrates AR and\nComputer Vision (CV) into the cooking process. We leverage Google's Gemini\nLarge Language Model (LLM) to identify ingredients in the camera's field of\nvision and generate recipe choices with detailed nutritional information.\nAdditionally, this application uses Apple's ARKit to create an AR user\ninterface compatible with iOS devices. Users can personalize their meal\nsuggestions by inputting their dietary preferences and rating each meal. The\napplication's effectiveness is evaluated through three rounds of user\nexperience surveys. This application advances the field of accessible cooking\nassistance technologies, aiming to reduce food wastage and improve the meal\nplanning experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooking meals can be difficult, causing many to resort to cookbooks and\nonline recipes. However, relying on these traditional methods of cooking often\nresults in missing ingredients, nutritional hazards, and unsatisfactory meals.\nUsing Augmented Reality (AR) can address these issues; however, current AR\ncooking applications have poor user interfaces and limited accessibility. This\npaper proposes a prototype of an iOS application that integrates AR and\nComputer Vision (CV) into the cooking process. We leverage Google's Gemini\nLarge Language Model (LLM) to identify ingredients in the camera's field of\nvision and generate recipe choices with detailed nutritional information.\nAdditionally, this application uses Apple's ARKit to create an AR user\ninterface compatible with iOS devices. Users can personalize their meal\nsuggestions by inputting their dietary preferences and rating each meal. The\napplication's effectiveness is evaluated through three rounds of user\nexperience surveys. This application advances the field of accessible cooking\nassistance technologies, aiming to reduce food wastage and improve the meal\nplanning experience."
                },
                "authors": [
                    {
                        "name": "Rithik Vir"
                    },
                    {
                        "name": "Parsa Madinei"
                    }
                ],
                "author_detail": {
                    "name": "Parsa Madinei"
                },
                "author": "Parsa Madinei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06289v1",
                "updated": "2024-12-09T08:24:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    24,
                    11,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:24:11Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    24,
                    11,
                    0,
                    344,
                    0
                ],
                "title": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity"
                },
                "summary": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents overfitting and\nforgetting, delivers SOTA performance on both commonsense and arithmetic\nreasoning with 4.6% and 1.3% average improvements compared to LoRA, and\nsurpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents overfitting and\nforgetting, delivers SOTA performance on both commonsense and arithmetic\nreasoning with 4.6% and 1.3% average improvements compared to LoRA, and\nsurpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Jixuan Leng"
                    },
                    {
                        "name": "Geyang Guo"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Ryumei Nakada"
                    },
                    {
                        "name": "Linjun Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06287v1",
                "updated": "2024-12-09T08:19:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    19,
                    28,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T08:19:28Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    19,
                    28,
                    0,
                    344,
                    0
                ],
                "title": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking\n  Large Language Models"
                },
                "summary": "The emergence of Large Language Models (LLMs) in the medical domain has\nstressed a compelling need for standard datasets to evaluate their\nquestion-answering (QA) performance. Although there have been several benchmark\ndatasets for medical QA, they either cover common knowledge across different\ndepartments or are specific to another department rather than pediatrics.\nMoreover, some of them are limited to objective questions and do not measure\nthe generation capacity of LLMs. Therefore, they cannot comprehensively assess\nthe QA ability of LLMs in pediatrics. To fill this gap, we construct\nPediaBench, the first Chinese pediatric dataset for LLM evaluation.\nSpecifically, it contains 4,565 objective questions and 1,632 subjective\nquestions spanning 12 pediatric disease groups. It adopts an integrated scoring\ncriterion based on different difficulty levels to thoroughly assess the\nproficiency of an LLM in instruction following, knowledge understanding,\nclinical case analysis, etc. Finally, we validate the effectiveness of\nPediaBench with extensive experiments on 20 open-source and commercial LLMs.\nThrough an in-depth analysis of experimental results, we offer insights into\nthe ability of LLMs to answer pediatric questions in the Chinese context,\nhighlighting their limitations for further improvements. Our code and data are\npublished at https://github.com/ACMISLab/PediaBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) in the medical domain has\nstressed a compelling need for standard datasets to evaluate their\nquestion-answering (QA) performance. Although there have been several benchmark\ndatasets for medical QA, they either cover common knowledge across different\ndepartments or are specific to another department rather than pediatrics.\nMoreover, some of them are limited to objective questions and do not measure\nthe generation capacity of LLMs. Therefore, they cannot comprehensively assess\nthe QA ability of LLMs in pediatrics. To fill this gap, we construct\nPediaBench, the first Chinese pediatric dataset for LLM evaluation.\nSpecifically, it contains 4,565 objective questions and 1,632 subjective\nquestions spanning 12 pediatric disease groups. It adopts an integrated scoring\ncriterion based on different difficulty levels to thoroughly assess the\nproficiency of an LLM in instruction following, knowledge understanding,\nclinical case analysis, etc. Finally, we validate the effectiveness of\nPediaBench with extensive experiments on 20 open-source and commercial LLMs.\nThrough an in-depth analysis of experimental results, we offer insights into\nthe ability of LLMs to answer pediatric questions in the Chinese context,\nhighlighting their limitations for further improvements. Our code and data are\npublished at https://github.com/ACMISLab/PediaBench."
                },
                "authors": [
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Panfeng Chen"
                    },
                    {
                        "name": "Jiali Li"
                    },
                    {
                        "name": "Linkun Feng"
                    },
                    {
                        "name": "Shuyu Liu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Yanhao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanhao Wang"
                },
                "author": "Yanhao Wang",
                "arxiv_comment": "21 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08535v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08535v3",
                "updated": "2024-12-09T08:01:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    1,
                    13,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-13T11:32:37Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    11,
                    32,
                    37,
                    2,
                    318,
                    0
                ],
                "title": "The EU AI Act is a good start but falls short",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EU AI Act is a good start but falls short"
                },
                "summary": "The EU AI Act was created to ensure ethical and safe Artificial Intelligence\n(AI) development and deployment across the EU. This study aims to identify key\nchallenges and strategies for helping enterprises focus on resources\neffectively. To achieve this aim, we conducted a Multivocal Literature Review\n(MLR) to explore the sentiments of both the industry and the academia. From 130\narticles, 56 met the criteria. Our key findings are three-fold. First,\nliability. Second, discrimination. Third, tool adequacy. Additionally, some\nnegative sentiments were expressed by industry and academia regarding\nregulatory interpretations, specific requirements, and transparency issues.\nNext, our findings are three essential themes for enterprises. First,\nrisk-based regulatory compliance. Second, ethical frameworks and principles in\ntechnology development. Third, policies and systems for regulatory risk\nmanagement. These results identify the key challenges and strategies and\nprovide less commonly discussed themes, enabling enterprises to align with the\nrequirements and minimize their distance from the EU market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EU AI Act was created to ensure ethical and safe Artificial Intelligence\n(AI) development and deployment across the EU. This study aims to identify key\nchallenges and strategies for helping enterprises focus on resources\neffectively. To achieve this aim, we conducted a Multivocal Literature Review\n(MLR) to explore the sentiments of both the industry and the academia. From 130\narticles, 56 met the criteria. Our key findings are three-fold. First,\nliability. Second, discrimination. Third, tool adequacy. Additionally, some\nnegative sentiments were expressed by industry and academia regarding\nregulatory interpretations, specific requirements, and transparency issues.\nNext, our findings are three essential themes for enterprises. First,\nrisk-based regulatory compliance. Second, ethical frameworks and principles in\ntechnology development. Third, policies and systems for regulatory risk\nmanagement. These results identify the key challenges and strategies and\nprovide less commonly discussed themes, enabling enterprises to align with the\nrequirements and minimize their distance from the EU market."
                },
                "authors": [
                    {
                        "name": "Chalisa Veesommai Sillberg"
                    },
                    {
                        "name": "Jose Siqueira De Cerqueira"
                    },
                    {
                        "name": "Pekka Sillberg"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "This is a preprint version of an article accepted to the 15th\n  International Conference on Software Business (ICSOB 2024). The final version\n  accepted for publishing the Springer Lecture Notes in Business Information\n  Processing (LNBIP) which contains 19 pages, 4 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08535v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08535v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04139v2",
                "updated": "2024-12-09T07:49:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    49,
                    1,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-05T13:06:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    13,
                    6,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "Monet: Mixture of Monosemantic Experts for Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monet: Mixture of Monosemantic Experts for Transformers"
                },
                "summary": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet."
                },
                "authors": [
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Young Jin Ahn"
                    },
                    {
                        "name": "Kee-Eung Kim"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06272v1",
                "updated": "2024-12-09T07:46:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    46,
                    14,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T07:46:14Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    46,
                    14,
                    0,
                    344,
                    0
                ],
                "title": "Methods for Legal Citation Prediction in the Age of LLMs: An Australian\n  Law Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods for Legal Citation Prediction in the Age of LLMs: An Australian\n  Law Case Study"
                },
                "summary": "In recent years, Large Language Models (LLMs) have shown great potential\nacross a wide range of legal tasks. Despite these advances, mitigating\nhallucination remains a significant challenge, with state-of-the-art LLMs still\nfrequently generating incorrect legal references. In this paper, we focus on\nthe problem of legal citation prediction within the Australian law context,\nwhere correctly identifying and citing relevant legislations or precedents is\ncritical. We compare several approaches: prompting general purpose and\nlaw-specialised LLMs, retrieval-only pipelines with both generic and\ndomain-specific embeddings, task-specific instruction-tuning of LLMs, and\nhybrid strategies that combine LLMs with retrieval augmentation, query\nexpansion, or voting ensembles. Our findings indicate that domain-specific\npre-training alone is insufficient for achieving satisfactory citation accuracy\neven after law-specialised pre-training. In contrast, instruction tuning on our\ntask-specific dataset dramatically boosts performance reaching the best results\nacross all settings. We also highlight that database granularity along with the\ntype of embeddings play a critical role in the performance of retrieval\nsystems. Among retrieval-based approaches, hybrid methods consistently\noutperform retrieval-only setups, and among these, ensemble voting delivers the\nbest result by combining the predictive quality of instruction-tuned LLMs with\nthe retrieval system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have shown great potential\nacross a wide range of legal tasks. Despite these advances, mitigating\nhallucination remains a significant challenge, with state-of-the-art LLMs still\nfrequently generating incorrect legal references. In this paper, we focus on\nthe problem of legal citation prediction within the Australian law context,\nwhere correctly identifying and citing relevant legislations or precedents is\ncritical. We compare several approaches: prompting general purpose and\nlaw-specialised LLMs, retrieval-only pipelines with both generic and\ndomain-specific embeddings, task-specific instruction-tuning of LLMs, and\nhybrid strategies that combine LLMs with retrieval augmentation, query\nexpansion, or voting ensembles. Our findings indicate that domain-specific\npre-training alone is insufficient for achieving satisfactory citation accuracy\neven after law-specialised pre-training. In contrast, instruction tuning on our\ntask-specific dataset dramatically boosts performance reaching the best results\nacross all settings. We also highlight that database granularity along with the\ntype of embeddings play a critical role in the performance of retrieval\nsystems. Among retrieval-based approaches, hybrid methods consistently\noutperform retrieval-only setups, and among these, ensemble voting delivers the\nbest result by combining the predictive quality of instruction-tuned LLMs with\nthe retrieval system."
                },
                "authors": [
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Jiuzhou Han"
                    },
                    {
                        "name": "Paul Burgess"
                    }
                ],
                "author_detail": {
                    "name": "Paul Burgess"
                },
                "author": "Paul Burgess",
                "arxiv_comment": "For code, data, and models see https://auslawbench.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06263v1",
                "updated": "2024-12-09T07:22:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    22,
                    19,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T07:22:19Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    22,
                    19,
                    0,
                    344,
                    0
                ],
                "title": "iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large\n  Multimodal Models"
                },
                "summary": "In this paper, we introduce iLLaVA, a simple method that can be seamlessly\ndeployed upon current Large Vision-Language Models (LVLMs) to greatly increase\nthe throughput with nearly lossless model performance, without a further\nrequirement to train. iLLaVA achieves this by finding and gradually merging the\nredundant tokens with an accurate and fast algorithm, which can merge hundreds\nof tokens within only one step. While some previous methods have explored\ndirectly pruning or merging tokens in the inference stage to accelerate models,\nour method excels in both performance and throughput by two key designs. First,\nwhile most previous methods only try to save the computations of Large Language\nModels (LLMs), our method accelerates the forward pass of both image encoders\nand LLMs in LVLMs, which both occupy a significant part of time during\ninference. Second, our method recycles the beneficial information from the\npruned tokens into existing tokens, which avoids directly dropping context\ntokens like previous methods to cause performance loss. iLLaVA can nearly\n2$\\times$ the throughput, and reduce the memory costs by half with only a 0.2\\%\n- 0.5\\% performance drop across models of different scales including 7B, 13B\nand 34B. On tasks across different domains including single-image, multi-images\nand videos, iLLaVA demonstrates strong generalizability with consistently\npromising efficiency. We finally offer abundant visualizations to show the\nmerging processes of iLLaVA in each step, which show insights into the\ndistribution of computing resources in LVLMs. Code is available at\nhttps://github.com/hulianyuyy/iLLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce iLLaVA, a simple method that can be seamlessly\ndeployed upon current Large Vision-Language Models (LVLMs) to greatly increase\nthe throughput with nearly lossless model performance, without a further\nrequirement to train. iLLaVA achieves this by finding and gradually merging the\nredundant tokens with an accurate and fast algorithm, which can merge hundreds\nof tokens within only one step. While some previous methods have explored\ndirectly pruning or merging tokens in the inference stage to accelerate models,\nour method excels in both performance and throughput by two key designs. First,\nwhile most previous methods only try to save the computations of Large Language\nModels (LLMs), our method accelerates the forward pass of both image encoders\nand LLMs in LVLMs, which both occupy a significant part of time during\ninference. Second, our method recycles the beneficial information from the\npruned tokens into existing tokens, which avoids directly dropping context\ntokens like previous methods to cause performance loss. iLLaVA can nearly\n2$\\times$ the throughput, and reduce the memory costs by half with only a 0.2\\%\n- 0.5\\% performance drop across models of different scales including 7B, 13B\nand 34B. On tasks across different domains including single-image, multi-images\nand videos, iLLaVA demonstrates strong generalizability with consistently\npromising efficiency. We finally offer abundant visualizations to show the\nmerging processes of iLLaVA in each step, which show insights into the\ndistribution of computing resources in LVLMs. Code is available at\nhttps://github.com/hulianyuyy/iLLaVA."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Liang Wan"
                    },
                    {
                        "name": "Wei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Feng"
                },
                "author": "Wei Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20362v2",
                "updated": "2024-12-09T07:17:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    7,
                    17,
                    7,
                    0,
                    344,
                    0
                ],
                "published": "2024-10-27T07:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    7,
                    38,
                    39,
                    6,
                    301,
                    0
                ],
                "title": "Rethinking Data Synthesis: A Teacher Model Training Recipe with\n  Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Data Synthesis: A Teacher Model Training Recipe with\n  Interpretation"
                },
                "summary": "Recent advances in large language model (LLM) training have highlighted the\nneed for diverse, high-quality instruction data. Recently, many works are\nexploring synthetic data generation using LLMs. However, they primarily focus\non prompt engineering with standard supervised instruction-finetuned models,\nwhich contains a fundamental limitation: these models are optimized for general\nquestion-answering/problem-solving rather than data generation. We propose a\nparadigm shift named \\textbf{NOMAD} by investigating how to specifically train\nmodels for data generation, demonstrating that this task differs significantly\nfrom training a classical LM. We identify two key factors: no-prompt-masked\ntraining and proper training set size selection. Our method, NOMAD, shows\nsubstantial improvements over baselines, achieving >4\\% gains in TriviaQA and\n>2\\% in GSM8K with limited training data. Finally, we offer new insights by\ninterpreting synthetic data through the lenses of \"relevance\" and \"novelty\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) training have highlighted the\nneed for diverse, high-quality instruction data. Recently, many works are\nexploring synthetic data generation using LLMs. However, they primarily focus\non prompt engineering with standard supervised instruction-finetuned models,\nwhich contains a fundamental limitation: these models are optimized for general\nquestion-answering/problem-solving rather than data generation. We propose a\nparadigm shift named \\textbf{NOMAD} by investigating how to specifically train\nmodels for data generation, demonstrating that this task differs significantly\nfrom training a classical LM. We identify two key factors: no-prompt-masked\ntraining and proper training set size selection. Our method, NOMAD, shows\nsubstantial improvements over baselines, achieving >4\\% gains in TriviaQA and\n>2\\% in GSM8K with limited training data. Finally, we offer new insights by\ninterpreting synthetic data through the lenses of \"relevance\" and \"novelty\"."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "David Zhu"
                    },
                    {
                        "name": "Simon Du"
                    },
                    {
                        "name": "Kevin Jamieson"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06245v1",
                "updated": "2024-12-09T06:37:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    6,
                    37,
                    35,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T06:37:35Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    6,
                    37,
                    35,
                    0,
                    344,
                    0
                ],
                "title": "A Comparative Study of Learning Paradigms in Large Language Models via\n  Intrinsic Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of Learning Paradigms in Large Language Models via\n  Intrinsic Dimension"
                },
                "summary": "The performance of Large Language Models (LLMs) on natural language tasks can\nbe improved through both supervised fine-tuning (SFT) and in-context learning\n(ICL), which operate via distinct mechanisms. Supervised fine-tuning updates\nthe model's weights by minimizing loss on training data, whereas in-context\nlearning leverages task demonstrations embedded in the prompt, without changing\nthe model's parameters. This study investigates the effects of these learning\nparadigms on the hidden representations of LLMs using Intrinsic Dimension (ID).\nWe use ID to estimate the number of degrees of freedom between representations\nextracted from LLMs as they perform specific natural language tasks. We first\nexplore how the ID of LLM representations evolves during SFT and how it varies\ndue to the number of demonstrations in ICL. We then compare the IDs induced by\nSFT and ICL and find that ICL consistently induces a higher ID compared to SFT,\nsuggesting that representations generated during ICL reside in higher\ndimensional manifolds in the embedding space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) on natural language tasks can\nbe improved through both supervised fine-tuning (SFT) and in-context learning\n(ICL), which operate via distinct mechanisms. Supervised fine-tuning updates\nthe model's weights by minimizing loss on training data, whereas in-context\nlearning leverages task demonstrations embedded in the prompt, without changing\nthe model's parameters. This study investigates the effects of these learning\nparadigms on the hidden representations of LLMs using Intrinsic Dimension (ID).\nWe use ID to estimate the number of degrees of freedom between representations\nextracted from LLMs as they perform specific natural language tasks. We first\nexplore how the ID of LLM representations evolves during SFT and how it varies\ndue to the number of demonstrations in ICL. We then compare the IDs induced by\nSFT and ICL and find that ICL consistently induces a higher ID compared to SFT,\nsuggesting that representations generated during ICL reside in higher\ndimensional manifolds in the embedding space."
                },
                "authors": [
                    {
                        "name": "Saahith Janapati"
                    },
                    {
                        "name": "Yangfeng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Yangfeng Ji"
                },
                "author": "Yangfeng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06229v1",
                "updated": "2024-12-09T06:03:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    6,
                    3,
                    48,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T06:03:48Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    6,
                    3,
                    48,
                    0,
                    344,
                    0
                ],
                "title": "LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial\n  Search for Adaptive Arguments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial\n  Search for Adaptive Arguments"
                },
                "summary": "This paper introduces DebateBrawl, an innovative AI-powered debate platform\nthat integrates Large Language Models (LLMs), Genetic Algorithms (GA), and\nAdversarial Search (AS) to create an adaptive and engaging debating experience.\nDebateBrawl addresses the limitations of traditional LLMs in strategic planning\nby incorporating evolutionary optimization and game-theoretic techniques. The\nsystem demonstrates remarkable performance in generating coherent, contextually\nrelevant arguments while adapting its strategy in real-time. Experimental\nresults involving 23 debates show balanced outcomes between AI and human\nparticipants, with the AI system achieving an average score of 2.72 compared to\nthe human average of 2.67 out of 10. User feedback indicates significant\nimprovements in debating skills and a highly satisfactory learning experience,\nwith 85% of users reporting improved debating abilities and 78% finding the AI\nopponent appropriately challenging. The system's ability to maintain high\nfactual accuracy (92% compared to 78% in human-only debates) while generating\ndiverse arguments addresses critical concerns in AI-assisted discourse.\nDebateBrawl not only serves as an effective educational tool but also\ncontributes to the broader goal of improving public discourse through\nAI-assisted argumentation. The paper discusses the ethical implications of AI\nin persuasive contexts and outlines the measures implemented to ensure\nresponsible development and deployment of the system, including robust\nfact-checking mechanisms and transparency in decision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces DebateBrawl, an innovative AI-powered debate platform\nthat integrates Large Language Models (LLMs), Genetic Algorithms (GA), and\nAdversarial Search (AS) to create an adaptive and engaging debating experience.\nDebateBrawl addresses the limitations of traditional LLMs in strategic planning\nby incorporating evolutionary optimization and game-theoretic techniques. The\nsystem demonstrates remarkable performance in generating coherent, contextually\nrelevant arguments while adapting its strategy in real-time. Experimental\nresults involving 23 debates show balanced outcomes between AI and human\nparticipants, with the AI system achieving an average score of 2.72 compared to\nthe human average of 2.67 out of 10. User feedback indicates significant\nimprovements in debating skills and a highly satisfactory learning experience,\nwith 85% of users reporting improved debating abilities and 78% finding the AI\nopponent appropriately challenging. The system's ability to maintain high\nfactual accuracy (92% compared to 78% in human-only debates) while generating\ndiverse arguments addresses critical concerns in AI-assisted discourse.\nDebateBrawl not only serves as an effective educational tool but also\ncontributes to the broader goal of improving public discourse through\nAI-assisted argumentation. The paper discusses the ethical implications of AI\nin persuasive contexts and outlines the measures implemented to ensure\nresponsible development and deployment of the system, including robust\nfact-checking mechanisms and transparency in decision-making processes."
                },
                "authors": [
                    {
                        "name": "Prakash Aryan"
                    }
                ],
                "author_detail": {
                    "name": "Prakash Aryan"
                },
                "author": "Prakash Aryan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16153v2",
                "updated": "2024-12-09T04:47:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    4,
                    47,
                    26,
                    0,
                    344,
                    0
                ],
                "published": "2024-10-21T16:19:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    19,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages"
                },
                "summary": "Despite recent advances in multimodal large language models (MLLMs), their\ndevelopment has predominantly focused on English- and western-centric datasets\nand tasks, leaving most of the world's languages and diverse cultural contexts\nunderrepresented. This paper introduces Pangea, a multilingual multimodal LLM\ntrained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages.\nPangeaIns features: 1) high-quality English instructions, 2) carefully\nmachine-translated instructions, and 3) culturally relevant multimodal tasks to\nensure cross-cultural coverage. To rigorously assess models' capabilities, we\nintroduce PangeaBench, a holistic evaluation suite encompassing 14 datasets\ncovering 47 languages. Results show that Pangea significantly outperforms\nexisting open-source models in multilingual settings and diverse cultural\ncontexts. Ablation studies further reveal the importance of English data\nproportions, language popularity, and the number of multimodal training samples\non overall performance. We fully open-source our data, code, and trained\ncheckpoints, to facilitate the development of inclusive and robust multilingual\nMLLMs, promoting equity and accessibility across a broader linguistic and\ncultural spectrum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in multimodal large language models (MLLMs), their\ndevelopment has predominantly focused on English- and western-centric datasets\nand tasks, leaving most of the world's languages and diverse cultural contexts\nunderrepresented. This paper introduces Pangea, a multilingual multimodal LLM\ntrained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages.\nPangeaIns features: 1) high-quality English instructions, 2) carefully\nmachine-translated instructions, and 3) culturally relevant multimodal tasks to\nensure cross-cultural coverage. To rigorously assess models' capabilities, we\nintroduce PangeaBench, a holistic evaluation suite encompassing 14 datasets\ncovering 47 languages. Results show that Pangea significantly outperforms\nexisting open-source models in multilingual settings and diverse cultural\ncontexts. Ablation studies further reveal the importance of English data\nproportions, language popularity, and the number of multimodal training samples\non overall performance. We fully open-source our data, code, and trained\ncheckpoints, to facilitate the development of inclusive and robust multilingual\nMLLMs, promoting equity and accessibility across a broader linguistic and\ncultural spectrum."
                },
                "authors": [
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Yueqi Song"
                    },
                    {
                        "name": "Akari Asai"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Jean de Dieu Nyandwi"
                    },
                    {
                        "name": "Simran Khanuja"
                    },
                    {
                        "name": "Anjali Kantharuban"
                    },
                    {
                        "name": "Lintang Sutawika"
                    },
                    {
                        "name": "Sathyanarayanan Ramamoorthy"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "arxiv_comment": "54 pages, 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06198v1",
                "updated": "2024-12-09T04:27:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    4,
                    27,
                    3,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T04:27:03Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    4,
                    27,
                    3,
                    0,
                    344,
                    0
                ],
                "title": "SparseAccelerate: Efficient Long-Context Inference for Mid-Range GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseAccelerate: Efficient Long-Context Inference for Mid-Range GPUs"
                },
                "summary": "As Large Language Models (LLMs) scale to longer context windows, the\ncomputational cost of attention mechanisms, which traditionally grows\nquadratically with input length, presents a critical challenge for real-time\nand memory-constrained deployments. Existing sparse attention techniques have\nsought to reduce this complexity, but they often incur significant overhead or\ncompromise accuracy, making them less practical for large contexts on mid-range\nhardware. In this paper, we introduce SparseAccelerate, a dynamic sparse\nattention method that adapts its sparsity patterns based on input\ncharacteristics, effectively flattening the attention complexity curve. Our\napproach is effective for input lengths starting at 16K tokens and scales\nefficiently up to 128K tokens on dual NVIDIA A5000 GPUs (24GB each).\nExperimental results show that SparseAccelerate achieves up to a 1.04x\nreduction in Time-To-First-Token (TTFT) latency at 32K tokens, while also\nproviding substantial memory savings. These improvements yield practical gains\nfor memory-intensive applications and long-context tasks that were previously\ninfeasible with standard attention. Beyond latency reductions, SparseAccelerate\nfundamentally shifts the scaling trend, demonstrating the smallest TTFT growth\ngradient relative to context length among competing methods. Ongoing\nevaluations on diverse benchmarks confirm its scalability, positioning\nSparseAccelerate as a critical advancement toward efficient, real-time, and\nlarge-context LLM inference on accessible hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale to longer context windows, the\ncomputational cost of attention mechanisms, which traditionally grows\nquadratically with input length, presents a critical challenge for real-time\nand memory-constrained deployments. Existing sparse attention techniques have\nsought to reduce this complexity, but they often incur significant overhead or\ncompromise accuracy, making them less practical for large contexts on mid-range\nhardware. In this paper, we introduce SparseAccelerate, a dynamic sparse\nattention method that adapts its sparsity patterns based on input\ncharacteristics, effectively flattening the attention complexity curve. Our\napproach is effective for input lengths starting at 16K tokens and scales\nefficiently up to 128K tokens on dual NVIDIA A5000 GPUs (24GB each).\nExperimental results show that SparseAccelerate achieves up to a 1.04x\nreduction in Time-To-First-Token (TTFT) latency at 32K tokens, while also\nproviding substantial memory savings. These improvements yield practical gains\nfor memory-intensive applications and long-context tasks that were previously\ninfeasible with standard attention. Beyond latency reductions, SparseAccelerate\nfundamentally shifts the scaling trend, demonstrating the smallest TTFT growth\ngradient relative to context length among competing methods. Ongoing\nevaluations on diverse benchmarks confirm its scalability, positioning\nSparseAccelerate as a critical advancement toward efficient, real-time, and\nlarge-context LLM inference on accessible hardware."
                },
                "authors": [
                    {
                        "name": "James Vo"
                    }
                ],
                "author_detail": {
                    "name": "James Vo"
                },
                "author": "James Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05133v3",
                "updated": "2024-12-09T04:21:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    4,
                    21,
                    8,
                    0,
                    344,
                    0
                ],
                "published": "2024-02-06T04:18:58Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    4,
                    18,
                    58,
                    1,
                    37,
                    0
                ],
                "title": "Personalized Language Modeling from Personalized Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Language Modeling from Personalized Human Feedback"
                },
                "summary": "Personalized large language models (LLMs) are designed to tailor responses to\nindividual user preferences. While Reinforcement Learning from Human Feedback\n(RLHF) is a commonly used framework for aligning LLMs with human preferences,\nvanilla RLHF assumes that all human preferences share the same distribution,\npreventing fine-tuned LLMs from generating personalized content when user\npreferences are diverse. In this work, we propose Personalized-RLHF (P-RLHF),\nan efficient framework that utilizes a lightweight user model to capture\nindividual user preferences and jointly learns the user model and the\npersonalized LLM from human feedback. P-RLHF exhibits the following three\ncharacteristics: (1) It enables an LLM to generate personalized content and\nscale efficiently with growing number of users. (2) It handles both explicit\nuser preferences described as textual input and implicit user preferences\nencoded in the feedback data. (3) It eliminates the need for users to fully\narticulate their preferences, which are normally needed for prompting LLMs to\ngenerate personalized content yet are often impractical to obtain in real-world\nscenarios. Our experimental results show that personalized LLMs trained using\nP-RLHF generate responses that are more closely aligned with individual user\npreferences, outperforming vanilla, non-personalized RLHF and prompting-based\npersonalization approaches across different tasks. We opensource our code at\nhttps://github.com/HumainLab/Personalized_RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized large language models (LLMs) are designed to tailor responses to\nindividual user preferences. While Reinforcement Learning from Human Feedback\n(RLHF) is a commonly used framework for aligning LLMs with human preferences,\nvanilla RLHF assumes that all human preferences share the same distribution,\npreventing fine-tuned LLMs from generating personalized content when user\npreferences are diverse. In this work, we propose Personalized-RLHF (P-RLHF),\nan efficient framework that utilizes a lightweight user model to capture\nindividual user preferences and jointly learns the user model and the\npersonalized LLM from human feedback. P-RLHF exhibits the following three\ncharacteristics: (1) It enables an LLM to generate personalized content and\nscale efficiently with growing number of users. (2) It handles both explicit\nuser preferences described as textual input and implicit user preferences\nencoded in the feedback data. (3) It eliminates the need for users to fully\narticulate their preferences, which are normally needed for prompting LLMs to\ngenerate personalized content yet are often impractical to obtain in real-world\nscenarios. Our experimental results show that personalized LLMs trained using\nP-RLHF generate responses that are more closely aligned with individual user\npreferences, outperforming vanilla, non-personalized RLHF and prompting-based\npersonalization approaches across different tasks. We opensource our code at\nhttps://github.com/HumainLab/Personalized_RLHF."
                },
                "authors": [
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Ruiyang Zhou"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    },
                    {
                        "name": "Liu Leqi"
                    }
                ],
                "author_detail": {
                    "name": "Liu Leqi"
                },
                "author": "Liu Leqi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06190v1",
                "updated": "2024-12-09T04:00:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    4,
                    0,
                    18,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T04:00:18Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    4,
                    0,
                    18,
                    0,
                    344,
                    0
                ],
                "title": "Category-Adaptive Cross-Modal Semantic Refinement and Transfer for\n  Open-Vocabulary Multi-Label Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-Adaptive Cross-Modal Semantic Refinement and Transfer for\n  Open-Vocabulary Multi-Label Recognition"
                },
                "summary": "Benefiting from the generalization capability of CLIP, recent vision language\npre-training (VLP) models have demonstrated an impressive ability to capture\nvirtually any visual concept in daily images. However, due to the presence of\nunseen categories in open-vocabulary settings, existing algorithms struggle to\neffectively capture strong semantic correlations between categories, resulting\nin sub-optimal performance on the open-vocabulary multi-label recognition\n(OV-MLR). Furthermore, the substantial variation in the number of\ndiscriminative areas across diverse object categories is misaligned with the\nfixed-number patch matching used in current methods, introducing noisy visual\ncues that hinder the accurate capture of target semantics. To tackle these\nchallenges, we propose a novel category-adaptive cross-modal semantic\nrefinement and transfer (C$^2$SRT) framework to explore the semantic\ncorrelation both within each category and across different categories, in a\ncategory-adaptive manner. The proposed framework consists of two complementary\nmodules, i.e., intra-category semantic refinement (ISR) module and\ninter-category semantic transfer (IST) module. Specifically, the ISR module\nleverages the cross-modal knowledge of the VLP model to adaptively find a set\nof local discriminative regions that best represent the semantics of the target\ncategory. The IST module adaptively discovers a set of most correlated\ncategories for a target category by utilizing the commonsense capabilities of\nLLMs to construct a category-adaptive correlation graph and transfers semantic\nknowledge from the correlated seen categories to unseen ones. Extensive\nexperiments on OV-MLR benchmarks clearly demonstrate that the proposed C$^2$SRT\nframework outperforms current state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the generalization capability of CLIP, recent vision language\npre-training (VLP) models have demonstrated an impressive ability to capture\nvirtually any visual concept in daily images. However, due to the presence of\nunseen categories in open-vocabulary settings, existing algorithms struggle to\neffectively capture strong semantic correlations between categories, resulting\nin sub-optimal performance on the open-vocabulary multi-label recognition\n(OV-MLR). Furthermore, the substantial variation in the number of\ndiscriminative areas across diverse object categories is misaligned with the\nfixed-number patch matching used in current methods, introducing noisy visual\ncues that hinder the accurate capture of target semantics. To tackle these\nchallenges, we propose a novel category-adaptive cross-modal semantic\nrefinement and transfer (C$^2$SRT) framework to explore the semantic\ncorrelation both within each category and across different categories, in a\ncategory-adaptive manner. The proposed framework consists of two complementary\nmodules, i.e., intra-category semantic refinement (ISR) module and\ninter-category semantic transfer (IST) module. Specifically, the ISR module\nleverages the cross-modal knowledge of the VLP model to adaptively find a set\nof local discriminative regions that best represent the semantics of the target\ncategory. The IST module adaptively discovers a set of most correlated\ncategories for a target category by utilizing the commonsense capabilities of\nLLMs to construct a category-adaptive correlation graph and transfers semantic\nknowledge from the correlated seen categories to unseen ones. Extensive\nexperiments on OV-MLR benchmarks clearly demonstrate that the proposed C$^2$SRT\nframework outperforms current state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Tao Pu"
                    },
                    {
                        "name": "Hefeng Wu"
                    },
                    {
                        "name": "Keze Wang"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10053v2",
                "updated": "2024-12-09T03:53:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    53,
                    19,
                    0,
                    344,
                    0
                ],
                "published": "2024-09-16T07:29:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    29,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in\n  LLMs with Direction-Magnitude Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in\n  LLMs with Direction-Magnitude Perspective"
                },
                "summary": "Activation Editing, which involves directly editting the internal\nrepresentations of large language models (LLMs) to alter their behaviors and\nachieve desired properties, has emerged as a promising area of research.\nExisting works primarily treat LLMs' activations as points in space and modify\nthem by adding steering vectors. However, this approach is limited in its\nability to achieve greater performance improvement while maintaining the\nnecessary consistency of activation magnitudes. To overcome these issues, we\npropose a novel editing method that views activations in terms of their\ndirections and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),\nmimics the rotation transformation, thus preserving activation norms and\nresulting in an improved performance on various safety benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Editing, which involves directly editting the internal\nrepresentations of large language models (LLMs) to alter their behaviors and\nachieve desired properties, has emerged as a promising area of research.\nExisting works primarily treat LLMs' activations as points in space and modify\nthem by adding steering vectors. However, this approach is limited in its\nability to achieve greater performance improvement while maintaining the\nnecessary consistency of activation magnitudes. To overcome these issues, we\npropose a novel editing method that views activations in terms of their\ndirections and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),\nmimics the rotation transformation, thus preserving activation norms and\nresulting in an improved performance on various safety benchmarks."
                },
                "authors": [
                    {
                        "name": "Van-Cuong Pham"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10805v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10805v6",
                "updated": "2024-12-09T03:39:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    39,
                    0,
                    0,
                    344,
                    0
                ],
                "published": "2024-07-15T15:20:40Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    15,
                    20,
                    40,
                    0,
                    197,
                    0
                ],
                "title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has improved large language models\n(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.\nHowever, current RAG methods often fall short of ensuring the depth and\ncompleteness of retrieved information, which is necessary for complex reasoning\ntasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG\nframework that iteratively retrieves information from both unstructured and\nstructured knowledge sources in a tight-coupling manner. Specifically, ToG-2\nleverages knowledge graphs (KGs) to link documents via entities, facilitating\ndeep and knowledge-guided context retrieval. Simultaneously, it utilizes\ndocuments as entity contexts to achieve precise and efficient graph retrieval.\nToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate answers. We\nconduct a series of well-designed experiments to highlight the following\nadvantages of ToG-2: 1) ToG-2 tightly couples the processes of context\nretrieval and graph retrieval, deepening context retrieval via the KG while\nenabling reliable graph retrieval based on contexts; 2) it achieves deep and\nfaithful reasoning in LLMs through an iterative knowledge retrieval process of\ncollaboration between contexts and the KG; and 3) ToG-2 is training-free and\nplug-and-play compatible with various LLMs. Extensive experiments demonstrate\nthat ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7\nknowledge-intensive datasets with GPT-3.5, and can elevate the performance of\nsmaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\nThe source code is available on https://github.com/IDEA-FinAI/ToG-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has improved large language models\n(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.\nHowever, current RAG methods often fall short of ensuring the depth and\ncompleteness of retrieved information, which is necessary for complex reasoning\ntasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG\nframework that iteratively retrieves information from both unstructured and\nstructured knowledge sources in a tight-coupling manner. Specifically, ToG-2\nleverages knowledge graphs (KGs) to link documents via entities, facilitating\ndeep and knowledge-guided context retrieval. Simultaneously, it utilizes\ndocuments as entity contexts to achieve precise and efficient graph retrieval.\nToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate answers. We\nconduct a series of well-designed experiments to highlight the following\nadvantages of ToG-2: 1) ToG-2 tightly couples the processes of context\nretrieval and graph retrieval, deepening context retrieval via the KG while\nenabling reliable graph retrieval based on contexts; 2) it achieves deep and\nfaithful reasoning in LLMs through an iterative knowledge retrieval process of\ncollaboration between contexts and the KG; and 3) ToG-2 is training-free and\nplug-and-play compatible with various LLMs. Extensive experiments demonstrate\nthat ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7\nknowledge-intensive datasets with GPT-3.5, and can elevate the performance of\nsmaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\nThe source code is available on https://github.com/IDEA-FinAI/ToG-2."
                },
                "authors": [
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Muzhi Li"
                    },
                    {
                        "name": "Huaren Qu"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10805v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10805v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06181v1",
                "updated": "2024-12-09T03:34:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    34,
                    49,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T03:34:49Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    34,
                    49,
                    0,
                    344,
                    0
                ],
                "title": "Enhancing Adversarial Resistance in LLMs with Recursion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Adversarial Resistance in LLMs with Recursion"
                },
                "summary": "The increasing integration of Large Language Models (LLMs) into society\nnecessitates robust defenses against vulnerabilities from jailbreaking and\nadversarial prompts. This project proposes a recursive framework for enhancing\nthe resistance of LLMs to manipulation through the use of prompt simplification\ntechniques. By increasing the transparency of complex and confusing adversarial\nprompts, the proposed method enables more reliable detection and prevention of\nmalicious inputs. Our findings attempt to address a critical problem in AI\nsafety and security, providing a foundation for the development of systems able\nto distinguish harmless inputs from prompts containing malicious intent. As\nLLMs continue to be used in diverse applications, the importance of such\nsafeguards will only grow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of Large Language Models (LLMs) into society\nnecessitates robust defenses against vulnerabilities from jailbreaking and\nadversarial prompts. This project proposes a recursive framework for enhancing\nthe resistance of LLMs to manipulation through the use of prompt simplification\ntechniques. By increasing the transparency of complex and confusing adversarial\nprompts, the proposed method enables more reliable detection and prevention of\nmalicious inputs. Our findings attempt to address a critical problem in AI\nsafety and security, providing a foundation for the development of systems able\nto distinguish harmless inputs from prompts containing malicious intent. As\nLLMs continue to be used in diverse applications, the importance of such\nsafeguards will only grow."
                },
                "authors": [
                    {
                        "name": "Bryan Li"
                    },
                    {
                        "name": "Sounak Bagchi"
                    },
                    {
                        "name": "Zizhan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zizhan Wang"
                },
                "author": "Zizhan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07870v4",
                "updated": "2024-12-09T03:25:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    25,
                    55,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-12T15:26:17Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    15,
                    26,
                    17,
                    1,
                    317,
                    0
                ],
                "title": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders"
                },
                "summary": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Jaya Krishna Mandivarapu"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Krishna Mandivarapu"
                },
                "author": "Jaya Krishna Mandivarapu",
                "arxiv_doi": "10.18653/v1/2024.customnlp4u-1.13",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.customnlp4u-1.13",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.07870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EMNLP CustomNLP4U 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06176v1",
                "updated": "2024-12-09T03:22:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    22,
                    35,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T03:22:35Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    22,
                    35,
                    0,
                    344,
                    0
                ],
                "title": "AlphaVerus: Bootstrapping Formally Verified Code Generation through\n  Self-Improving Translation and Treefinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaVerus: Bootstrapping Formally Verified Code Generation through\n  Self-Improving Translation and Treefinement"
                },
                "summary": "Automated code generation with large language models has gained significant\ntraction, but there remains no guarantee on the correctness of generated code.\nWe aim to use formal verification to provide mathematical guarantees that the\ngenerated code is correct. However, generating formally verified code with LLMs\nis hindered by the scarcity of training data and the complexity of formal\nproofs. To tackle this challenge, we introduce AlphaVerus, a self-improving\nframework that bootstraps formally verified code generation by iteratively\ntranslating programs from a higher-resource language and leveraging feedback\nfrom a verifier. AlphaVerus operates in three phases: exploration of candidate\ntranslations, Treefinement -- a novel tree search algorithm for program\nrefinement using verifier feedback, and filtering misaligned specifications and\nprograms to prevent reward hacking. Through this iterative process, AlphaVerus\nenables a LLaMA-3.1-70B model to generate verified code without human\nintervention or model finetuning. AlphaVerus shows an ability to generate\nformally verified solutions for HumanEval and MBPP, laying the groundwork for\ntruly trustworthy code-generation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code generation with large language models has gained significant\ntraction, but there remains no guarantee on the correctness of generated code.\nWe aim to use formal verification to provide mathematical guarantees that the\ngenerated code is correct. However, generating formally verified code with LLMs\nis hindered by the scarcity of training data and the complexity of formal\nproofs. To tackle this challenge, we introduce AlphaVerus, a self-improving\nframework that bootstraps formally verified code generation by iteratively\ntranslating programs from a higher-resource language and leveraging feedback\nfrom a verifier. AlphaVerus operates in three phases: exploration of candidate\ntranslations, Treefinement -- a novel tree search algorithm for program\nrefinement using verifier feedback, and filtering misaligned specifications and\nprograms to prevent reward hacking. Through this iterative process, AlphaVerus\nenables a LLaMA-3.1-70B model to generate verified code without human\nintervention or model finetuning. AlphaVerus shows an ability to generate\nformally verified solutions for HumanEval and MBPP, laying the groundwork for\ntruly trustworthy code-generation agents."
                },
                "authors": [
                    {
                        "name": "Pranjal Aggarwal"
                    },
                    {
                        "name": "Bryan Parno"
                    },
                    {
                        "name": "Sean Welleck"
                    }
                ],
                "author_detail": {
                    "name": "Sean Welleck"
                },
                "author": "Sean Welleck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06171v1",
                "updated": "2024-12-09T03:05:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    5,
                    34,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T03:05:34Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    5,
                    34,
                    0,
                    344,
                    0
                ],
                "title": "Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any\n  Granularity"
                },
                "summary": "How can we enable models to comprehend video anomalies occurring over varying\ntemporal scales and contexts? Traditional Video Anomaly Understanding (VAU)\nmethods focus on frame-level anomaly prediction, often missing the\ninterpretability of complex and diverse real-world anomalies. Recent multimodal\napproaches leverage visual and textual data but lack hierarchical annotations\nthat capture both short-term and long-term anomalies. To address this\nchallenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical\nvideo anomaly understanding across any granularity. We develop a semi-automated\nannotation engine that efficiently scales high-quality annotations by combining\nmanual video segmentation with recursive free-text annotation using large\nlanguage models (LLMs). This results in over 70,000 multi-granular annotations\norganized at clip-level, event-level, and video-level segments. For efficient\nanomaly detection in long videos, we propose the Anomaly-focused Temporal\nSampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to\nadaptively select frames based on anomaly scores, ensuring that the multimodal\nLLM concentrates on anomaly-rich regions, which significantly enhances both\nefficiency and accuracy. Extensive experiments demonstrate that our\nhierarchical instruction data markedly improves anomaly comprehension. The\nintegrated ATS and visual-language model outperform traditional methods in\nprocessing long videos. Our benchmark and model are publicly available at\nhttps://github.com/pipixin321/HolmesVAU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we enable models to comprehend video anomalies occurring over varying\ntemporal scales and contexts? Traditional Video Anomaly Understanding (VAU)\nmethods focus on frame-level anomaly prediction, often missing the\ninterpretability of complex and diverse real-world anomalies. Recent multimodal\napproaches leverage visual and textual data but lack hierarchical annotations\nthat capture both short-term and long-term anomalies. To address this\nchallenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical\nvideo anomaly understanding across any granularity. We develop a semi-automated\nannotation engine that efficiently scales high-quality annotations by combining\nmanual video segmentation with recursive free-text annotation using large\nlanguage models (LLMs). This results in over 70,000 multi-granular annotations\norganized at clip-level, event-level, and video-level segments. For efficient\nanomaly detection in long videos, we propose the Anomaly-focused Temporal\nSampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to\nadaptively select frames based on anomaly scores, ensuring that the multimodal\nLLM concentrates on anomaly-rich regions, which significantly enhances both\nefficiency and accuracy. Extensive experiments demonstrate that our\nhierarchical instruction data markedly improves anomaly comprehension. The\nintegrated ATS and visual-language model outperform traditional methods in\nprocessing long videos. Our benchmark and model are publicly available at\nhttps://github.com/pipixin321/HolmesVAU."
                },
                "authors": [
                    {
                        "name": "Huaxin Zhang"
                    },
                    {
                        "name": "Xiaohao Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Jialong Zuo"
                    },
                    {
                        "name": "Xiaonan Huang"
                    },
                    {
                        "name": "Changxin Gao"
                    },
                    {
                        "name": "Shanjun Zhang"
                    },
                    {
                        "name": "Li Yu"
                    },
                    {
                        "name": "Nong Sang"
                    }
                ],
                "author_detail": {
                    "name": "Nong Sang"
                },
                "author": "Nong Sang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06168v1",
                "updated": "2024-12-09T03:01:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    1,
                    47,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T03:01:47Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    1,
                    47,
                    0,
                    344,
                    0
                ],
                "title": "Out-of-Distribution Detection with Overlap Index",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution Detection with Overlap Index"
                },
                "summary": "Out-of-distribution (OOD) detection is crucial for the deployment of machine\nlearning models in the open world. While existing OOD detectors are effective\nin identifying OOD samples that deviate significantly from in-distribution (ID)\ndata, they often come with trade-offs. For instance, deep OOD detectors usually\nsuffer from high computational costs, require tuning hyperparameters, and have\nlimited interpretability, whereas traditional OOD detectors may have a low\naccuracy on large high-dimensional datasets. To address these limitations, we\npropose a novel effective OOD detection approach that employs an overlap index\n(OI)-based confidence score function to evaluate the likelihood of a given\ninput belonging to the same distribution as the available ID samples. The\nproposed OI-based confidence score function is non-parametric, lightweight, and\neasy to interpret, hence providing strong flexibility and generality. Extensive\nempirical evaluations indicate that our OI-based OOD detector is competitive\nwith state-of-the-art OOD detectors in terms of detection accuracy on a wide\nrange of datasets while requiring less computation and memory costs. Lastly, we\nshow that the proposed OI-based confidence score function inherits nice\nproperties from OI (e.g., insensitivity to small distributional variations and\nrobustness against Huber $\\epsilon$-contamination) and is a versatile tool for\nestimating OI and model accuracy in specific contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is crucial for the deployment of machine\nlearning models in the open world. While existing OOD detectors are effective\nin identifying OOD samples that deviate significantly from in-distribution (ID)\ndata, they often come with trade-offs. For instance, deep OOD detectors usually\nsuffer from high computational costs, require tuning hyperparameters, and have\nlimited interpretability, whereas traditional OOD detectors may have a low\naccuracy on large high-dimensional datasets. To address these limitations, we\npropose a novel effective OOD detection approach that employs an overlap index\n(OI)-based confidence score function to evaluate the likelihood of a given\ninput belonging to the same distribution as the available ID samples. The\nproposed OI-based confidence score function is non-parametric, lightweight, and\neasy to interpret, hence providing strong flexibility and generality. Extensive\nempirical evaluations indicate that our OI-based OOD detector is competitive\nwith state-of-the-art OOD detectors in terms of detection accuracy on a wide\nrange of datasets while requiring less computation and memory costs. Lastly, we\nshow that the proposed OI-based confidence score function inherits nice\nproperties from OI (e.g., insensitivity to small distributional variations and\nrobustness against Huber $\\epsilon$-contamination) and is a versatile tool for\nestimating OI and model accuracy in specific contexts."
                },
                "authors": [
                    {
                        "name": "Hao Fu"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Farshad Khorrami"
                    }
                ],
                "author_detail": {
                    "name": "Farshad Khorrami"
                },
                "author": "Farshad Khorrami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10980v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10980v5",
                "updated": "2024-12-09T03:01:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    1,
                    35,
                    0,
                    344,
                    0
                ],
                "published": "2024-02-15T21:33:07Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    21,
                    33,
                    7,
                    3,
                    46,
                    0
                ],
                "title": "ChemReasoner: Heuristic Search over a Large Language Model's Knowledge\n  Space using Quantum-Chemical Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemReasoner: Heuristic Search over a Large Language Model's Knowledge\n  Space using Quantum-Chemical Feedback"
                },
                "summary": "The discovery of new catalysts is essential for the design of new and more\nefficient chemical processes in order to transition to a sustainable future. We\nintroduce an AI-guided computational screening framework unifying linguistic\nreasoning with quantum-chemistry based feedback from 3D atomistic\nrepresentations. Our approach formulates catalyst discovery as an uncertain\nenvironment where an agent actively searches for highly effective catalysts via\nthe iterative combination of large language model (LLM)-derived hypotheses and\natomistic graph neural network (GNN)-derived feedback. Identified catalysts in\nintermediate search steps undergo structural evaluation based on spatial\norientation, reaction pathways, and stability. Scoring functions based on\nadsorption energies and reaction energy barriers steer the exploration in the\nLLM's knowledge space toward energetically favorable, high-efficiency\ncatalysts. We introduce planning methods that automatically guide the\nexploration without human input, providing competitive performance against\nexpert-enumerated chemical descriptor-based implementations. By integrating\nlanguage-guided reasoning with computational chemistry feedback, our work\npioneers AI-accelerated, trustworthy catalyst discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of new catalysts is essential for the design of new and more\nefficient chemical processes in order to transition to a sustainable future. We\nintroduce an AI-guided computational screening framework unifying linguistic\nreasoning with quantum-chemistry based feedback from 3D atomistic\nrepresentations. Our approach formulates catalyst discovery as an uncertain\nenvironment where an agent actively searches for highly effective catalysts via\nthe iterative combination of large language model (LLM)-derived hypotheses and\natomistic graph neural network (GNN)-derived feedback. Identified catalysts in\nintermediate search steps undergo structural evaluation based on spatial\norientation, reaction pathways, and stability. Scoring functions based on\nadsorption energies and reaction energy barriers steer the exploration in the\nLLM's knowledge space toward energetically favorable, high-efficiency\ncatalysts. We introduce planning methods that automatically guide the\nexploration without human input, providing competitive performance against\nexpert-enumerated chemical descriptor-based implementations. By integrating\nlanguage-guided reasoning with computational chemistry feedback, our work\npioneers AI-accelerated, trustworthy catalyst discovery."
                },
                "authors": [
                    {
                        "name": "Henry W. Sprueill"
                    },
                    {
                        "name": "Carl Edwards"
                    },
                    {
                        "name": "Khushbu Agarwal"
                    },
                    {
                        "name": "Mariefel V. Olarte"
                    },
                    {
                        "name": "Udishnu Sanyal"
                    },
                    {
                        "name": "Conrad Johnston"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Sutanay Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Sutanay Choudhury"
                },
                "author": "Sutanay Choudhury",
                "arxiv_comment": "9 pages, accepted by ICML 2024, final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10980v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10980v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04497v2",
                "updated": "2024-12-09T03:00:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    0,
                    42,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-30T00:10:56Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    0,
                    10,
                    56,
                    5,
                    335,
                    0
                ],
                "title": "Opportunities and Challenges of Large Language Models for Low-Resource\n  Languages in Humanities Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opportunities and Challenges of Large Language Models for Low-Resource\n  Languages in Humanities Research"
                },
                "summary": "Low-resource languages serve as invaluable repositories of human history,\nembodying cultural evolution and intellectual diversity. Despite their\nsignificance, these languages face critical challenges, including data scarcity\nand technological limitations, which hinder their comprehensive study and\npreservation. Recent advancements in large language models (LLMs) offer\ntransformative opportunities for addressing these challenges, enabling\ninnovative methodologies in linguistic, historical, and cultural research. This\nstudy systematically evaluates the applications of LLMs in low-resource\nlanguage research, encompassing linguistic variation, historical documentation,\ncultural expressions, and literary analysis. By analyzing technical frameworks,\ncurrent methodologies, and ethical considerations, this paper identifies key\nchallenges such as data accessibility, model adaptability, and cultural\nsensitivity. Given the cultural, historical, and linguistic richness inherent\nin low-resource languages, this work emphasizes interdisciplinary collaboration\nand the development of customized models as promising avenues for advancing\nresearch in this domain. By underscoring the potential of integrating\nartificial intelligence with the humanities to preserve and study humanity's\nlinguistic and cultural heritage, this study fosters global efforts towards\nsafeguarding intellectual diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource languages serve as invaluable repositories of human history,\nembodying cultural evolution and intellectual diversity. Despite their\nsignificance, these languages face critical challenges, including data scarcity\nand technological limitations, which hinder their comprehensive study and\npreservation. Recent advancements in large language models (LLMs) offer\ntransformative opportunities for addressing these challenges, enabling\ninnovative methodologies in linguistic, historical, and cultural research. This\nstudy systematically evaluates the applications of LLMs in low-resource\nlanguage research, encompassing linguistic variation, historical documentation,\ncultural expressions, and literary analysis. By analyzing technical frameworks,\ncurrent methodologies, and ethical considerations, this paper identifies key\nchallenges such as data accessibility, model adaptability, and cultural\nsensitivity. Given the cultural, historical, and linguistic richness inherent\nin low-resource languages, this work emphasizes interdisciplinary collaboration\nand the development of customized models as promising avenues for advancing\nresearch in this domain. By underscoring the potential of integrating\nartificial intelligence with the humanities to preserve and study humanity's\nlinguistic and cultural heritage, this study fosters global efforts towards\nsafeguarding intellectual diversity."
                },
                "authors": [
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Zhenyuan Yang"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Ruidong Zhang"
                    },
                    {
                        "name": "Yiheng Liu"
                    },
                    {
                        "name": "Haiyang Sun"
                    },
                    {
                        "name": "Yi Pan"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06162v1",
                "updated": "2024-12-09T02:51:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    2,
                    51,
                    21,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T02:51:21Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    2,
                    51,
                    21,
                    0,
                    344,
                    0
                ],
                "title": "Query-Efficient Planning with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Efficient Planning with Language Models"
                },
                "summary": "Planning in complex environments requires an agent to efficiently query a\nworld model to find a feasible sequence of actions from start to goal. Recent\nwork has shown that Large Language Models (LLMs), with their rich prior\nknowledge and reasoning capabilities, can potentially help with planning by\nsearching over promising states and adapting to feedback from the world. In\nthis paper, we propose and study two fundamentally competing frameworks that\nleverage LLMs for query-efficient planning. The first uses LLMs as a heuristic\nwithin a search-based planner to select promising nodes to expand and propose\npromising actions. The second uses LLMs as a generative planner to propose an\nentire sequence of actions from start to goal, query a world model, and adapt\nbased on feedback. We show that while both approaches improve upon comparable\nbaselines, using an LLM as a generative planner results in significantly fewer\ninteractions. Our key finding is that the LLM as a planner can more rapidly\nadapt its planning strategies based on immediate feedback than LLM as a\nheuristic. We present evaluations and ablations on Robotouille and PDDL\nplanning benchmarks and discuss connections to existing theory on\nquery-efficient planning algorithms. Code is available at\nhttps://github.com/portal-cornell/llms-for-planning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning in complex environments requires an agent to efficiently query a\nworld model to find a feasible sequence of actions from start to goal. Recent\nwork has shown that Large Language Models (LLMs), with their rich prior\nknowledge and reasoning capabilities, can potentially help with planning by\nsearching over promising states and adapting to feedback from the world. In\nthis paper, we propose and study two fundamentally competing frameworks that\nleverage LLMs for query-efficient planning. The first uses LLMs as a heuristic\nwithin a search-based planner to select promising nodes to expand and propose\npromising actions. The second uses LLMs as a generative planner to propose an\nentire sequence of actions from start to goal, query a world model, and adapt\nbased on feedback. We show that while both approaches improve upon comparable\nbaselines, using an LLM as a generative planner results in significantly fewer\ninteractions. Our key finding is that the LLM as a planner can more rapidly\nadapt its planning strategies based on immediate feedback than LLM as a\nheuristic. We present evaluations and ablations on Robotouille and PDDL\nplanning benchmarks and discuss connections to existing theory on\nquery-efficient planning algorithms. Code is available at\nhttps://github.com/portal-cornell/llms-for-planning"
                },
                "authors": [
                    {
                        "name": "Gonzalo Gonzalez-Pumariega"
                    },
                    {
                        "name": "Wayne Chen"
                    },
                    {
                        "name": "Kushal Kedia"
                    },
                    {
                        "name": "Sanjiban Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiban Choudhury"
                },
                "author": "Sanjiban Choudhury",
                "arxiv_comment": "11 pages (not including references or appendix); 13 figures (9 main\n  paper, 4 appendix); (v1) preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00978v2",
                "updated": "2024-12-09T02:28:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    2,
                    28,
                    1,
                    0,
                    344,
                    0
                ],
                "published": "2024-07-01T05:28:40Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    5,
                    28,
                    40,
                    0,
                    183,
                    0
                ],
                "title": "Hybrid RAG-empowered Multi-modal LLM for Secure Data Management in\n  Internet of Medical Things: A Diffusion-based Contract Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid RAG-empowered Multi-modal LLM for Secure Data Management in\n  Internet of Medical Things: A Diffusion-based Contract Approach"
                },
                "summary": "Secure data management and effective data sharing have become paramount in\nthe rapidly evolving healthcare landscape, especially with the growing\nintegration of the Internet of Medical Things (IoMT). The rise of generative\nartificial intelligence has further elevated Multi-modal Large Language Models\n(MLLMs) as essential tools for managing and optimizing healthcare data in IoMT.\nMLLMs can support multi-modal inputs and generate diverse types of content by\nleveraging large-scale training on vast amounts of multi-modal data. However,\ncritical challenges persist in developing medical MLLMs, including security and\nfreshness issues of healthcare data, affecting the output quality of MLLMs. To\nthis end, in this paper, we propose a hybrid Retrieval-Augmented Generation\n(RAG)-empowered medical MLLM framework for healthcare data management. This\nframework leverages a hierarchical cross-chain architecture to facilitate\nsecure data training. Moreover, it enhances the output quality of MLLMs through\nhybrid RAG, which employs multi-modal metrics to filter various unimodal RAG\nresults and incorporates these retrieval results as additional inputs to MLLMs.\nAdditionally, we employ age of information to indirectly evaluate the data\nfreshness impact of MLLMs and utilize contract theory to incentivize healthcare\ndata holders to share their fresh data, mitigating information asymmetry during\ndata sharing. Finally, we utilize a generative diffusion model-based deep\nreinforcement learning algorithm to identify the optimal contract for efficient\ndata sharing. Numerical results demonstrate the effectiveness of the proposed\nschemes, which achieve secure and efficient healthcare data management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure data management and effective data sharing have become paramount in\nthe rapidly evolving healthcare landscape, especially with the growing\nintegration of the Internet of Medical Things (IoMT). The rise of generative\nartificial intelligence has further elevated Multi-modal Large Language Models\n(MLLMs) as essential tools for managing and optimizing healthcare data in IoMT.\nMLLMs can support multi-modal inputs and generate diverse types of content by\nleveraging large-scale training on vast amounts of multi-modal data. However,\ncritical challenges persist in developing medical MLLMs, including security and\nfreshness issues of healthcare data, affecting the output quality of MLLMs. To\nthis end, in this paper, we propose a hybrid Retrieval-Augmented Generation\n(RAG)-empowered medical MLLM framework for healthcare data management. This\nframework leverages a hierarchical cross-chain architecture to facilitate\nsecure data training. Moreover, it enhances the output quality of MLLMs through\nhybrid RAG, which employs multi-modal metrics to filter various unimodal RAG\nresults and incorporates these retrieval results as additional inputs to MLLMs.\nAdditionally, we employ age of information to indirectly evaluate the data\nfreshness impact of MLLMs and utilize contract theory to incentivize healthcare\ndata holders to share their fresh data, mitigating information asymmetry during\ndata sharing. Finally, we utilize a generative diffusion model-based deep\nreinforcement learning algorithm to identify the optimal contract for efficient\ndata sharing. Numerical results demonstrate the effectiveness of the proposed\nschemes, which achieve secure and efficient healthcare data management."
                },
                "authors": [
                    {
                        "name": "Cheng Su"
                    },
                    {
                        "name": "Jinbo Wen"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Yonghua Wang"
                    },
                    {
                        "name": "Yuanjia Su"
                    },
                    {
                        "name": "Hudan Pan"
                    },
                    {
                        "name": "Zishao Zhong"
                    },
                    {
                        "name": "M. Shamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Hossain"
                },
                "author": "M. Shamim Hossain",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03712v2",
                "updated": "2024-12-09T02:14:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    2,
                    14,
                    8,
                    0,
                    344,
                    0
                ],
                "published": "2024-06-06T03:15:13Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    3,
                    15,
                    13,
                    3,
                    158,
                    0
                ],
                "title": "A Survey on Medical Large Language Models: Technology, Application,\n  Trustworthiness, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Medical Large Language Models: Technology, Application,\n  Trustworthiness, and Future Directions"
                },
                "summary": "With the advent of Large Language Models (LLMs), medical artificial\nintelligence (AI) has experienced substantial technological progress and\nparadigm shifts, highlighting the potential of LLMs to streamline healthcare\ndelivery and improve patient outcomes. Considering this rapid technical\nprogress, in this survey, we trace the recent advances of Medical Large\nLanguage Models (Med-LLMs), including the background, key findings, and\nmainstream techniques, especially for the evolution from general-purpose models\nto medical-specialized applications. Firstly, we delve into the foundational\ntechnology of Med-LLMs, indicating how general models can be progressively\nadapted and refined for the complicated medical tasks. Secondly, the\nwide-ranging applications of Med-LLMs are investigated across various\nhealthcare domains, as well as an up-to-date review of existing Med-LLMs. The\ntransformative impact of these models on daily medical practice is evident\nthrough their ability to assist clinicians, educators, and patients.\nRecognizing the importance of responsible innovation, we discuss the challenges\nassociated with ensuring fairness, accountability, privacy, and robustness.\nEthical considerations, rigorous evaluation methodologies, and the\nestablishment of regulatory frameworks are crucial for building trustworthiness\nin the real-world system. We emphasize the need for ongoing scrutiny and\ndevelopment to maintain high standards of safety and reliability. Finally, we\nanticipate possible future trajectories for Med-LLMs, identifying key avenues\nfor prudent expansion. By consolidating these insights, our review aims to\nprovide professionals and researchers with a thorough understanding of the\nstrengths and limitations of Med-LLMs, fostering a balanced and ethical\napproach to their integration into the healthcare ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Large Language Models (LLMs), medical artificial\nintelligence (AI) has experienced substantial technological progress and\nparadigm shifts, highlighting the potential of LLMs to streamline healthcare\ndelivery and improve patient outcomes. Considering this rapid technical\nprogress, in this survey, we trace the recent advances of Medical Large\nLanguage Models (Med-LLMs), including the background, key findings, and\nmainstream techniques, especially for the evolution from general-purpose models\nto medical-specialized applications. Firstly, we delve into the foundational\ntechnology of Med-LLMs, indicating how general models can be progressively\nadapted and refined for the complicated medical tasks. Secondly, the\nwide-ranging applications of Med-LLMs are investigated across various\nhealthcare domains, as well as an up-to-date review of existing Med-LLMs. The\ntransformative impact of these models on daily medical practice is evident\nthrough their ability to assist clinicians, educators, and patients.\nRecognizing the importance of responsible innovation, we discuss the challenges\nassociated with ensuring fairness, accountability, privacy, and robustness.\nEthical considerations, rigorous evaluation methodologies, and the\nestablishment of regulatory frameworks are crucial for building trustworthiness\nin the real-world system. We emphasize the need for ongoing scrutiny and\ndevelopment to maintain high standards of safety and reliability. Finally, we\nanticipate possible future trajectories for Med-LLMs, identifying key avenues\nfor prudent expansion. By consolidating these insights, our review aims to\nprovide professionals and researchers with a thorough understanding of the\nstrengths and limitations of Med-LLMs, fostering a balanced and ethical\napproach to their integration into the healthcare ecosystem."
                },
                "authors": [
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Xiaoyan Yang"
                    },
                    {
                        "name": "Junchi Lei"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Zhixuan Chu"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06142v1",
                "updated": "2024-12-09T01:51:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    51,
                    18,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T01:51:18Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    51,
                    18,
                    0,
                    344,
                    0
                ],
                "title": "AgentAlign: Misalignment-Adapted Multi-Agent Perception for Resilient\n  Inter-Agent Sensor Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentAlign: Misalignment-Adapted Multi-Agent Perception for Resilient\n  Inter-Agent Sensor Correlations"
                },
                "summary": "Cooperative perception has attracted wide attention given its capability to\nleverage shared information across connected automated vehicles (CAVs) and\nsmart infrastructures to address sensing occlusion and range limitation issues.\nHowever, existing research overlooks the fragile multi-sensor correlations in\nmulti-agent settings, as the heterogeneous agent sensor measurements are highly\nsusceptible to environmental factors, leading to weakened inter-agent sensor\ninteractions. The varying operational conditions and other real-world factors\ninevitably introduce multifactorial noise and consequentially lead to\nmulti-sensor misalignment, making the deployment of multi-agent multi-modality\nperception particularly challenging in the real world. In this paper, we\npropose AgentAlign, a real-world heterogeneous agent cross-modality feature\nalignment framework, to effectively address these multi-modality misalignment\nissues. Our method introduces a cross-modality feature alignment space (CFAS)\nand heterogeneous agent feature alignment (HAFA) mechanism to harmonize\nmulti-modality features across various agents dynamically. Additionally, we\npresent a novel V2XSet-noise dataset that simulates realistic sensor\nimperfections under diverse environmental conditions, facilitating a systematic\nevaluation of our approach's robustness. Extensive experiments on the V2X-Real\nand V2XSet-Noise benchmarks demonstrate that our framework achieves\nstate-of-the-art performance, underscoring its potential for real-world\napplications in cooperative autonomous driving. The controllable V2XSet-Noise\ndataset and generation pipeline will be released in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative perception has attracted wide attention given its capability to\nleverage shared information across connected automated vehicles (CAVs) and\nsmart infrastructures to address sensing occlusion and range limitation issues.\nHowever, existing research overlooks the fragile multi-sensor correlations in\nmulti-agent settings, as the heterogeneous agent sensor measurements are highly\nsusceptible to environmental factors, leading to weakened inter-agent sensor\ninteractions. The varying operational conditions and other real-world factors\ninevitably introduce multifactorial noise and consequentially lead to\nmulti-sensor misalignment, making the deployment of multi-agent multi-modality\nperception particularly challenging in the real world. In this paper, we\npropose AgentAlign, a real-world heterogeneous agent cross-modality feature\nalignment framework, to effectively address these multi-modality misalignment\nissues. Our method introduces a cross-modality feature alignment space (CFAS)\nand heterogeneous agent feature alignment (HAFA) mechanism to harmonize\nmulti-modality features across various agents dynamically. Additionally, we\npresent a novel V2XSet-noise dataset that simulates realistic sensor\nimperfections under diverse environmental conditions, facilitating a systematic\nevaluation of our approach's robustness. Extensive experiments on the V2X-Real\nand V2XSet-Noise benchmarks demonstrate that our framework achieves\nstate-of-the-art performance, underscoring its potential for real-world\napplications in cooperative autonomous driving. The controllable V2XSet-Noise\ndataset and generation pipeline will be released in the future."
                },
                "authors": [
                    {
                        "name": "Zonglin Meng"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Zhaoliang Zheng"
                    },
                    {
                        "name": "Zhihao Zhao"
                    },
                    {
                        "name": "Jiaqi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Ma"
                },
                "author": "Jiaqi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06141v1",
                "updated": "2024-12-09T01:50:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    50,
                    39,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T01:50:39Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    50,
                    39,
                    0,
                    344,
                    0
                ],
                "title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware\n  Multimodal Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware\n  Multimodal Preference Optimization"
                },
                "summary": "The advancement of Large Vision-Language Models (LVLMs) has propelled their\napplication in the medical field. However, Medical LVLMs (Med-LVLMs) encounter\nfactuality challenges due to modality misalignment, where the models prioritize\ntextual knowledge over visual input, leading to hallucinations that contradict\ninformation in medical images. Previous attempts to enhance modality alignment\nin Med-LVLMs through preference optimization have inadequately mitigated\nclinical relevance in preference data, making these samples easily\ndistinguishable and reducing alignment effectiveness. To address this\nchallenge, we propose MMedPO, a novel multimodal medical preference\noptimization approach that considers the clinical relevance of preference\nsamples to enhance Med-LVLM alignment. MMedPO curates multimodal preference\ndata by introducing two types of dispreference: (1) plausible hallucinations\ninjected through target Med-LVLMs or GPT-4o to produce medically inaccurate\nresponses, and (2) lesion region neglect achieved through local lesion-noising,\ndisrupting visual understanding of critical areas. We then calculate clinical\nrelevance for each sample based on scores from multiple Med-LLMs and visual\ntools, and integrate these scores into the preference optimization process as\nweights, enabling effective alignment. Our experiments demonstrate that MMedPO\nsignificantly enhances factual accuracy in Med-LVLMs, achieving substantial\nimprovements over existing preference optimization methods by averaging 14.2%\nand 51.7% across the Med-VQA and report generation tasks. Our code are\navailable in https://github.com/aiming-lab/MMedPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of Large Vision-Language Models (LVLMs) has propelled their\napplication in the medical field. However, Medical LVLMs (Med-LVLMs) encounter\nfactuality challenges due to modality misalignment, where the models prioritize\ntextual knowledge over visual input, leading to hallucinations that contradict\ninformation in medical images. Previous attempts to enhance modality alignment\nin Med-LVLMs through preference optimization have inadequately mitigated\nclinical relevance in preference data, making these samples easily\ndistinguishable and reducing alignment effectiveness. To address this\nchallenge, we propose MMedPO, a novel multimodal medical preference\noptimization approach that considers the clinical relevance of preference\nsamples to enhance Med-LVLM alignment. MMedPO curates multimodal preference\ndata by introducing two types of dispreference: (1) plausible hallucinations\ninjected through target Med-LVLMs or GPT-4o to produce medically inaccurate\nresponses, and (2) lesion region neglect achieved through local lesion-noising,\ndisrupting visual understanding of critical areas. We then calculate clinical\nrelevance for each sample based on scores from multiple Med-LLMs and visual\ntools, and integrate these scores into the preference optimization process as\nweights, enabling effective alignment. Our experiments demonstrate that MMedPO\nsignificantly enhances factual accuracy in Med-LVLMs, achieving substantial\nimprovements over existing preference optimization methods by averaging 14.2%\nand 51.7% across the Med-VQA and report generation tasks. Our code are\navailable in https://github.com/aiming-lab/MMedPO."
                },
                "authors": [
                    {
                        "name": "Kangyu Zhu"
                    },
                    {
                        "name": "Peng Xia"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Hongtu Zhu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06136v1",
                "updated": "2024-12-09T01:39:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    16,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T01:39:16Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    16,
                    0,
                    344,
                    0
                ],
                "title": "AIDE: Task-Specific Fine Tuning with Attribute Guided Multi-Hop Data\n  Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIDE: Task-Specific Fine Tuning with Attribute Guided Multi-Hop Data\n  Expansion"
                },
                "summary": "Fine-tuning large language models (LLMs) for specific tasks requires\nhigh-quality, diverse training data relevant to the task. Recent research has\nleveraged LLMs to synthesize training data, but existing approaches either\ndepend on large seed datasets or struggle to ensure both task relevance and\ndata diversity in the generated outputs. To address these challenges, we\npropose AIDE, a novel data synthesis framework that uses a multi-hop process to\nexpand 10 seed data points while ensuring diversity and task relevance. AIDE\nextracts the main topic and key knowledge attributes from the seed data to\nguide the synthesis process. In each subsequent hop, it extracts the topic and\nattributes from the newly generated data and continues guided synthesis. This\nprocess repeats for a total of K hops. To prevent irrelevant data generation as\nthe hop depth increases, AIDE incorporates a residual connection mechanism and\nuses self-reflection to improve data quality. Our empirical results demonstrate\nthat fine-tuning Mistral-7B, Llama-3.1-8B and Llama-3.2-3B with AIDE achieves\nmore than 10% accuracy improvements over the base models across 13 tasks from 5\ndifferent benchmarks, while outperforming the models fine-tuned with\nstate-of-the-art data synthesis methods like Evol-Instruct, DataTune and\nPrompt2Model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) for specific tasks requires\nhigh-quality, diverse training data relevant to the task. Recent research has\nleveraged LLMs to synthesize training data, but existing approaches either\ndepend on large seed datasets or struggle to ensure both task relevance and\ndata diversity in the generated outputs. To address these challenges, we\npropose AIDE, a novel data synthesis framework that uses a multi-hop process to\nexpand 10 seed data points while ensuring diversity and task relevance. AIDE\nextracts the main topic and key knowledge attributes from the seed data to\nguide the synthesis process. In each subsequent hop, it extracts the topic and\nattributes from the newly generated data and continues guided synthesis. This\nprocess repeats for a total of K hops. To prevent irrelevant data generation as\nthe hop depth increases, AIDE incorporates a residual connection mechanism and\nuses self-reflection to improve data quality. Our empirical results demonstrate\nthat fine-tuning Mistral-7B, Llama-3.1-8B and Llama-3.2-3B with AIDE achieves\nmore than 10% accuracy improvements over the base models across 13 tasks from 5\ndifferent benchmarks, while outperforming the models fine-tuned with\nstate-of-the-art data synthesis methods like Evol-Instruct, DataTune and\nPrompt2Model."
                },
                "authors": [
                    {
                        "name": "Jiayu Li"
                    },
                    {
                        "name": "Xuan Zhu"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Yanjun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Qi"
                },
                "author": "Yanjun Qi",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06134v1",
                "updated": "2024-12-09T01:29:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    29,
                    47,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T01:29:47Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    29,
                    47,
                    0,
                    344,
                    0
                ],
                "title": "Evaluating and Mitigating Social Bias for Large Language Models in\n  Open-ended Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Mitigating Social Bias for Large Language Models in\n  Open-ended Settings"
                },
                "summary": "Current social bias benchmarks for Large Language Models (LLMs) primarily\nrely on pre-defined question formats like multiple-choice, limiting their\nability to reflect the complexity and open-ended nature of real-world\ninteractions. To address this gap, we extend an existing BBQ dataset introduced\nby incorporating fill-in-the-blank and short-answer question types, designed to\nevaluate biases in an open-ended setting. Our finding reveals that LLMs tend to\nproduce responses that are more biased against certain protected attributes,\nlike age and socio-economic status. On the other hand, these biased outputs\nproduced by LLMs can serve as valuable contexts and chains of thought for\ndebiasing. Our debiasing approach combined zero-shot, few-shot, and\nchain-of-thought could significantly reduce the level of bias to almost 0. We\nopen-source our evaluation and debiasing code hoping to encourage further\nmeasurements and mitigation of bias and stereotype in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current social bias benchmarks for Large Language Models (LLMs) primarily\nrely on pre-defined question formats like multiple-choice, limiting their\nability to reflect the complexity and open-ended nature of real-world\ninteractions. To address this gap, we extend an existing BBQ dataset introduced\nby incorporating fill-in-the-blank and short-answer question types, designed to\nevaluate biases in an open-ended setting. Our finding reveals that LLMs tend to\nproduce responses that are more biased against certain protected attributes,\nlike age and socio-economic status. On the other hand, these biased outputs\nproduced by LLMs can serve as valuable contexts and chains of thought for\ndebiasing. Our debiasing approach combined zero-shot, few-shot, and\nchain-of-thought could significantly reduce the level of bias to almost 0. We\nopen-source our evaluation and debiasing code hoping to encourage further\nmeasurements and mitigation of bias and stereotype in LLMs."
                },
                "authors": [
                    {
                        "name": "Zhao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Liu"
                },
                "author": "Zhao Liu",
                "arxiv_comment": "12 panges",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06113v1",
                "updated": "2024-12-09T00:24:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    0,
                    24,
                    9,
                    0,
                    344,
                    0
                ],
                "published": "2024-12-09T00:24:09Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    0,
                    24,
                    9,
                    0,
                    344,
                    0
                ],
                "title": "Privacy-Preserving Large Language Models: Mechanisms, Applications, and\n  Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Large Language Models: Mechanisms, Applications, and\n  Future Directions"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nnatural language processing, enabling applications in diverse domains such as\nhealthcare, finance and education. However, the growing reliance on extensive\ndata for training and inference has raised significant privacy concerns,\nranging from data leakage to adversarial attacks. This survey comprehensively\nexplores the landscape of privacy-preserving mechanisms tailored for LLMs,\nincluding differential privacy, federated learning, cryptographic protocols,\nand trusted execution environments. We examine their efficacy in addressing key\nprivacy challenges, such as membership inference and model inversion attacks,\nwhile balancing trade-offs between privacy and model utility. Furthermore, we\nanalyze privacy-preserving applications of LLMs in privacy-sensitive domains,\nhighlighting successful implementations and inherent limitations. Finally, this\nsurvey identifies emerging research directions, emphasizing the need for novel\nframeworks that integrate privacy by design into the lifecycle of LLMs. By\nsynthesizing state-of-the-art approaches and future trends, this paper provides\na foundation for developing robust, privacy-preserving large language models\nthat safeguard sensitive information without compromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nnatural language processing, enabling applications in diverse domains such as\nhealthcare, finance and education. However, the growing reliance on extensive\ndata for training and inference has raised significant privacy concerns,\nranging from data leakage to adversarial attacks. This survey comprehensively\nexplores the landscape of privacy-preserving mechanisms tailored for LLMs,\nincluding differential privacy, federated learning, cryptographic protocols,\nand trusted execution environments. We examine their efficacy in addressing key\nprivacy challenges, such as membership inference and model inversion attacks,\nwhile balancing trade-offs between privacy and model utility. Furthermore, we\nanalyze privacy-preserving applications of LLMs in privacy-sensitive domains,\nhighlighting successful implementations and inherent limitations. Finally, this\nsurvey identifies emerging research directions, emphasizing the need for novel\nframeworks that integrate privacy by design into the lifecycle of LLMs. By\nsynthesizing state-of-the-art approaches and future trends, this paper provides\na foundation for developing robust, privacy-preserving large language models\nthat safeguard sensitive information without compromising performance."
                },
                "authors": [
                    {
                        "name": "Guoshenghui Zhao"
                    },
                    {
                        "name": "Eric Song"
                    }
                ],
                "author_detail": {
                    "name": "Eric Song"
                },
                "author": "Eric Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06106v1",
                "updated": "2024-12-08T23:41:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    23,
                    41,
                    38,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T23:41:38Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    23,
                    41,
                    38,
                    6,
                    343,
                    0
                ],
                "title": "Enhanced Computationally Efficient Long LoRA Inspired Perceiver\n  Architectures for Auto-Regressive Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Computationally Efficient Long LoRA Inspired Perceiver\n  Architectures for Auto-Regressive Language Modeling"
                },
                "summary": "The Transformer architecture has revolutionized the Natural Language\nProcessing field and is the backbone of Large Language Models (LLMs). The\nTransformer uses the attention mechanism that computes the pair-wise similarity\nbetween its input tokens to produce latent vectors that are able to understand\nthe semantic meaning of the input text. One of the challenges in the\nTransformer architecture is the quadratic complexity of the attention mechanism\nthat prohibits the efficient processing of long sequence lengths. While many\nrecent research works have attempted to provide a reduction from $O(n^2)$ time\ncomplexity of attention to semi-linear complexity, it remains an unsolved\nproblem in the sense of maintaining a high performance when such complexity is\nreduced. One of the important works in this respect is the Perceiver class of\narchitectures that have demonstrated excellent performance while reducing the\ncomputation complexity. In this paper, we use the PerceiverAR that was proposed\nfor Auto-Regressive modeling as a baseline, and provide three different\narchitectural enhancements to it with varying computation overhead tradeoffs.\nInspired by the recently proposed efficient attention computation approach of\nLong-LoRA, we then present an equally efficient Perceiver-based architecture\n(termed as Long LoRA Pereceiver - LLP) that can be used as the base\narchitecture in LLMs instead of just a fine-tuning add-on. Our results on\ndifferent benchmarks indicate impressive improvements compared to recent\nTransformer based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has revolutionized the Natural Language\nProcessing field and is the backbone of Large Language Models (LLMs). The\nTransformer uses the attention mechanism that computes the pair-wise similarity\nbetween its input tokens to produce latent vectors that are able to understand\nthe semantic meaning of the input text. One of the challenges in the\nTransformer architecture is the quadratic complexity of the attention mechanism\nthat prohibits the efficient processing of long sequence lengths. While many\nrecent research works have attempted to provide a reduction from $O(n^2)$ time\ncomplexity of attention to semi-linear complexity, it remains an unsolved\nproblem in the sense of maintaining a high performance when such complexity is\nreduced. One of the important works in this respect is the Perceiver class of\narchitectures that have demonstrated excellent performance while reducing the\ncomputation complexity. In this paper, we use the PerceiverAR that was proposed\nfor Auto-Regressive modeling as a baseline, and provide three different\narchitectural enhancements to it with varying computation overhead tradeoffs.\nInspired by the recently proposed efficient attention computation approach of\nLong-LoRA, we then present an equally efficient Perceiver-based architecture\n(termed as Long LoRA Pereceiver - LLP) that can be used as the base\narchitecture in LLMs instead of just a fine-tuning add-on. Our results on\ndifferent benchmarks indicate impressive improvements compared to recent\nTransformer based models."
                },
                "authors": [
                    {
                        "name": "Kaleel Mahmood"
                    },
                    {
                        "name": "Shaoyi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shaoyi Huang"
                },
                "author": "Shaoyi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06090v1",
                "updated": "2024-12-08T22:46:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    22,
                    46,
                    30,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T22:46:30Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    22,
                    46,
                    30,
                    6,
                    343,
                    0
                ],
                "title": "Trust No AI: Prompt Injection Along The CIA Security Triad",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust No AI: Prompt Injection Along The CIA Security Triad"
                },
                "summary": "The CIA security triad - Confidentiality, Integrity, and Availability - is a\ncornerstone of data and cybersecurity. With the emergence of large language\nmodel (LLM) applications, a new class of threat, known as prompt injection, was\nfirst identified in 2022. Since then, numerous real-world vulnerabilities and\nexploits have been documented in production LLM systems, including those from\nleading vendors like OpenAI, Microsoft, Anthropic and Google. This paper\ncompiles real-world exploits and proof-of concept examples, based on the\nresearch conducted and publicly documented by the author, demonstrating how\nprompt injection undermines the CIA triad and poses ongoing risks to\ncybersecurity and AI systems at large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CIA security triad - Confidentiality, Integrity, and Availability - is a\ncornerstone of data and cybersecurity. With the emergence of large language\nmodel (LLM) applications, a new class of threat, known as prompt injection, was\nfirst identified in 2022. Since then, numerous real-world vulnerabilities and\nexploits have been documented in production LLM systems, including those from\nleading vendors like OpenAI, Microsoft, Anthropic and Google. This paper\ncompiles real-world exploits and proof-of concept examples, based on the\nresearch conducted and publicly documented by the author, demonstrating how\nprompt injection undermines the CIA triad and poses ongoing risks to\ncybersecurity and AI systems at large."
                },
                "authors": [
                    {
                        "name": "Johann Rehberger"
                    }
                ],
                "author_detail": {
                    "name": "Johann Rehberger"
                },
                "arxiv_affiliation": "Independent Researcher, Embrace The Red",
                "author": "Johann Rehberger",
                "arxiv_comment": "Based on research presented at Black Hat Europe 2024, Microsoft\n  Bluehat 2024 and publications from embracethered.com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06089v1",
                "updated": "2024-12-08T22:29:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    22,
                    29,
                    56,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T22:29:56Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    22,
                    29,
                    56,
                    6,
                    343,
                    0
                ],
                "title": "GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis"
                },
                "summary": "Text-to-image (T2I) generation has seen significant progress with diffusion\nmodels, enabling generation of photo-realistic images from text prompts.\nDespite this progress, existing methods still face challenges in following\ncomplex text prompts, especially those requiring compositional and multi-step\nreasoning. Given such complex instructions, SOTA models often make mistakes in\nfaithfully modeling object attributes, and relationships among them. In this\nwork, we present an alternate paradigm for T2I synthesis, decomposing the task\nof complex multi-step generation into three steps, (a) Generate: we first\ngenerate an image using existing diffusion models (b) Plan: we make use of\nMulti-Modal LLMs (MLLMs) to identify the mistakes in the generated image\nexpressed in terms of individual objects and their properties, and produce a\nsequence of corrective steps required in the form of an edit-plan. (c) Edit: we\nmake use of an existing text-guided image editing models to sequentially\nexecute our edit-plan over the generated image to get the desired image which\nis faithful to the original instruction. Our approach derives its strength from\nthe fact that it is modular in nature, is training free, and can be applied\nover any combination of image generation and editing models. As an added\ncontribution, we also develop a model capable of compositional editing, which\nfurther helps improve the overall accuracy of our proposed approach. Our method\nflexibly trades inference time compute with performance on compositional text\nprompts. We perform extensive experimental evaluation across 3 benchmarks and\n10 T2I models including DALLE-3 and the latest -- SD-3.5-Large. Our approach\nnot only improves the performance of the SOTA models, by upto 3 points, it also\nreduces the performance gap between weaker and stronger models.\n$\\href{https://dair-iitd.github.io/GraPE/}{https://dair-iitd.github.io/GraPE/}$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation has seen significant progress with diffusion\nmodels, enabling generation of photo-realistic images from text prompts.\nDespite this progress, existing methods still face challenges in following\ncomplex text prompts, especially those requiring compositional and multi-step\nreasoning. Given such complex instructions, SOTA models often make mistakes in\nfaithfully modeling object attributes, and relationships among them. In this\nwork, we present an alternate paradigm for T2I synthesis, decomposing the task\nof complex multi-step generation into three steps, (a) Generate: we first\ngenerate an image using existing diffusion models (b) Plan: we make use of\nMulti-Modal LLMs (MLLMs) to identify the mistakes in the generated image\nexpressed in terms of individual objects and their properties, and produce a\nsequence of corrective steps required in the form of an edit-plan. (c) Edit: we\nmake use of an existing text-guided image editing models to sequentially\nexecute our edit-plan over the generated image to get the desired image which\nis faithful to the original instruction. Our approach derives its strength from\nthe fact that it is modular in nature, is training free, and can be applied\nover any combination of image generation and editing models. As an added\ncontribution, we also develop a model capable of compositional editing, which\nfurther helps improve the overall accuracy of our proposed approach. Our method\nflexibly trades inference time compute with performance on compositional text\nprompts. We perform extensive experimental evaluation across 3 benchmarks and\n10 T2I models including DALLE-3 and the latest -- SD-3.5-Large. Our approach\nnot only improves the performance of the SOTA models, by upto 3 points, it also\nreduces the performance gap between weaker and stronger models.\n$\\href{https://dair-iitd.github.io/GraPE/}{https://dair-iitd.github.io/GraPE/}$"
                },
                "authors": [
                    {
                        "name": "Ashish Goswami"
                    },
                    {
                        "name": "Satyam Kumar Modi"
                    },
                    {
                        "name": "Santhosh Rishi Deshineni"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Prathosh A. P"
                    },
                    {
                        "name": "Parag Singla"
                    }
                ],
                "author_detail": {
                    "name": "Parag Singla"
                },
                "author": "Parag Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06082v1",
                "updated": "2024-12-08T22:05:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    22,
                    5,
                    38,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T22:05:38Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    22,
                    5,
                    38,
                    6,
                    343,
                    0
                ],
                "title": "Are foundation models for computer vision good conformal predictors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are foundation models for computer vision good conformal predictors?"
                },
                "summary": "Recent advances in self-supervision and constrastive learning have brought\nthe performance of foundation models to unprecedented levels in a variety of\ntasks. Fueled by this progress, these models are becoming the prevailing\napproach for a wide array of real-world vision problems, including\nrisk-sensitive and high-stakes applications. However, ensuring safe deployment\nin these scenarios requires a more comprehensive understanding of their\nuncertainty modeling capabilities, which has been barely explored. In this\nwork, we delve into the behavior of vision and vision-language foundation\nmodels under Conformal Prediction (CP), a statistical framework that provides\ntheoretical guarantees of marginal coverage of the true class. Across extensive\nexperiments including popular vision classification benchmarks, well-known\nfoundation vision models, and three CP methods, our findings reveal that\nfoundation models are well-suited for conformalization procedures, particularly\nthose integrating Vision Transformers. Furthermore, we show that calibrating\nthe confidence predictions of these models leads to efficiency degradation of\nthe conformal set on adaptive CP methods. In contrast, few-shot adaptation to\ndownstream tasks generally enhances conformal scores, where we identify\nAdapters as a better conformable alternative compared to Prompt Learning\nstrategies. Our empirical study identifies APS as particularly promising in the\ncontext of vision foundation models, as it does not violate the marginal\ncoverage property across multiple challenging, yet realistic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in self-supervision and constrastive learning have brought\nthe performance of foundation models to unprecedented levels in a variety of\ntasks. Fueled by this progress, these models are becoming the prevailing\napproach for a wide array of real-world vision problems, including\nrisk-sensitive and high-stakes applications. However, ensuring safe deployment\nin these scenarios requires a more comprehensive understanding of their\nuncertainty modeling capabilities, which has been barely explored. In this\nwork, we delve into the behavior of vision and vision-language foundation\nmodels under Conformal Prediction (CP), a statistical framework that provides\ntheoretical guarantees of marginal coverage of the true class. Across extensive\nexperiments including popular vision classification benchmarks, well-known\nfoundation vision models, and three CP methods, our findings reveal that\nfoundation models are well-suited for conformalization procedures, particularly\nthose integrating Vision Transformers. Furthermore, we show that calibrating\nthe confidence predictions of these models leads to efficiency degradation of\nthe conformal set on adaptive CP methods. In contrast, few-shot adaptation to\ndownstream tasks generally enhances conformal scores, where we identify\nAdapters as a better conformable alternative compared to Prompt Learning\nstrategies. Our empirical study identifies APS as particularly promising in the\ncontext of vision foundation models, as it does not violate the marginal\ncoverage property across multiple challenging, yet realistic scenarios."
                },
                "authors": [
                    {
                        "name": "Leo Fillioux"
                    },
                    {
                        "name": "Julio Silva-Rodrguez"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    },
                    {
                        "name": "Paul-Henry Cournde"
                    },
                    {
                        "name": "Maria Vakalopoulou"
                    },
                    {
                        "name": "Stergios Christodoulidis"
                    },
                    {
                        "name": "Jose Dolz"
                    }
                ],
                "author_detail": {
                    "name": "Jose Dolz"
                },
                "author": "Jose Dolz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]