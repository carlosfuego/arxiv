[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v2",
                "updated": "2025-03-19T04:49:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    49,
                    8,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v1",
                "updated": "2025-03-17T23:38:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v1",
                "updated": "2025-03-17T11:10:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v4",
                "updated": "2025-03-16T16:25:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    16,
                    25,
                    31,
                    6,
                    75,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v1",
                "updated": "2025-03-15T14:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v1",
                "updated": "2025-03-14T06:01:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers"
                },
                "summary": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v1",
                "updated": "2025-03-13T03:36:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08879v1",
                "updated": "2025-03-11T20:45:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T20:45:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference"
                },
                "summary": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
                },
                "authors": [
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Changran Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07518v1",
                "updated": "2025-03-10T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "TokenButler: Token Importance is Predictable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenButler: Token Importance is Predictable"
                },
                "summary": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler"
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Ahmed F AbouElhamayed"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.15478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15478v1",
                "updated": "2025-03-19T17:55:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    55,
                    8,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:55:08Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    55,
                    8,
                    2,
                    78,
                    0
                ],
                "title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks"
                },
                "summary": "Large language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existing multi-turn RL algorithms for optimizing LLM\nagents fail to perform effective credit assignment over multiple turns while\nleveraging the generalization capabilities of LLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,\nColBench, where an LLM agent interacts with a human collaborator over multiple\nturns to solve realistic tasks in backend programming and frontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation from Training-time information), that uses a carefully\ndesigned optimization objective to train a critic model with access to\nadditional training-time information. The critic provides step-level rewards\nfor improving the policy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success and win rates on ColBench\ncompared to other state-of-the-art multi-turn RL algorithms, enabling\nLlama-3.1-8B to match or exceed the performance of GPT4-o in realistic\ncollaborative content creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existing multi-turn RL algorithms for optimizing LLM\nagents fail to perform effective credit assignment over multiple turns while\nleveraging the generalization capabilities of LLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,\nColBench, where an LLM agent interacts with a human collaborator over multiple\nturns to solve realistic tasks in backend programming and frontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation from Training-time information), that uses a carefully\ndesigned optimization objective to train a critic model with access to\nadditional training-time information. The critic provides step-level rewards\nfor improving the policy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success and win rates on ColBench\ncompared to other state-of-the-art multi-turn RL algorithms, enabling\nLlama-3.1-8B to match or exceed the performance of GPT4-o in realistic\ncollaborative content creation."
                },
                "authors": [
                    {
                        "name": "Yifei Zhou"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Xian Li"
                    }
                ],
                "author_detail": {
                    "name": "Xian Li"
                },
                "author": "Xian Li",
                "arxiv_comment": "29 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15475v1",
                "updated": "2025-03-19T17:52:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    52,
                    17,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:52:17Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    52,
                    17,
                    2,
                    78,
                    0
                ],
                "title": "Cube: A Roblox View of 3D Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cube: A Roblox View of 3D Intelligence"
                },
                "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence."
                },
                "authors": [
                    {
                        "name": "Foundation AI Team"
                    },
                    {
                        "name": "Kiran Bhat"
                    },
                    {
                        "name": "Nishchaie Khanna"
                    },
                    {
                        "name": "Karun Channa"
                    },
                    {
                        "name": "Tinghui Zhou"
                    },
                    {
                        "name": "Yiheng Zhu"
                    },
                    {
                        "name": "Xiaoxia Sun"
                    },
                    {
                        "name": "Charles Shang"
                    },
                    {
                        "name": "Anirudh Sudarshan"
                    },
                    {
                        "name": "Maurice Chu"
                    },
                    {
                        "name": "Daiqing Li"
                    },
                    {
                        "name": "Kangle Deng"
                    },
                    {
                        "name": "Jean-Philippe Fauconnier"
                    },
                    {
                        "name": "Tijmen Verhulsdonck"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Kayvon Fatahalian"
                    },
                    {
                        "name": "Alexander Weiss"
                    },
                    {
                        "name": "Christian Reiser"
                    },
                    {
                        "name": "Ravi Kiran Chirravuri"
                    },
                    {
                        "name": "Ravali Kandur"
                    },
                    {
                        "name": "Alejandro Pelaez"
                    },
                    {
                        "name": "Akash Garg"
                    },
                    {
                        "name": "Michael Palleschi"
                    },
                    {
                        "name": "Jessica Wang"
                    },
                    {
                        "name": "Skylar Litz"
                    },
                    {
                        "name": "Leon Liu"
                    },
                    {
                        "name": "Anying Li"
                    },
                    {
                        "name": "David Harmon"
                    },
                    {
                        "name": "Derek Liu"
                    },
                    {
                        "name": "Liangjun Feng"
                    },
                    {
                        "name": "Denis Goupil"
                    },
                    {
                        "name": "Lukas Kuczynski"
                    },
                    {
                        "name": "Jihyun Yoon"
                    },
                    {
                        "name": "Naveen Marri"
                    },
                    {
                        "name": "Peiye Zhuang"
                    },
                    {
                        "name": "Yinan Zhang"
                    },
                    {
                        "name": "Brian Yin"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Marcel van Workum"
                    },
                    {
                        "name": "Thomas Lane"
                    },
                    {
                        "name": "Bryce Erickson"
                    },
                    {
                        "name": "Salil Pathare"
                    },
                    {
                        "name": "Kyle Price"
                    },
                    {
                        "name": "Anupam Singh"
                    },
                    {
                        "name": "David Baszucki"
                    }
                ],
                "author_detail": {
                    "name": "David Baszucki"
                },
                "author": "David Baszucki",
                "arxiv_comment": "Our code and model weights can be found at:\n  https://github.com/Roblox/cube",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15463v1",
                "updated": "2025-03-19T17:41:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    41,
                    46,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:41:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    41,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment"
                },
                "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15457v1",
                "updated": "2025-03-19T17:36:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    36,
                    54,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:36:54Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    36,
                    54,
                    2,
                    78,
                    0
                ],
                "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step\n  Generator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step\n  Generator"
                },
                "summary": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling."
                },
                "authors": [
                    {
                        "name": "Yuanzhi Zhu"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Stéphane Lathuilière"
                    },
                    {
                        "name": "Vicky Kalogeiton"
                    }
                ],
                "author_detail": {
                    "name": "Vicky Kalogeiton"
                },
                "author": "Vicky Kalogeiton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15454v1",
                "updated": "2025-03-19T17:36:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    36,
                    35,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:36:35Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    36,
                    35,
                    2,
                    78,
                    0
                ],
                "title": "Evaluating Bias in Retrieval-Augmented Medical Question-Answering\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Bias in Retrieval-Augmented Medical Question-Answering\n  Systems"
                },
                "summary": "Medical QA systems powered by Retrieval-Augmented Generation (RAG) models\nsupport clinical decision-making but may introduce biases related to race,\ngender, and social determinants of health. We systematically evaluate biases in\nRAG-based LLM by examining demographic-sensitive queries and measuring\nretrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze\nretrieval overlap and correctness disparities. Our findings reveal substantial\ndemographic disparities within RAG pipelines, emphasizing the critical need for\nretrieval methods that explicitly account for fairness to ensure equitable\nclinical decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical QA systems powered by Retrieval-Augmented Generation (RAG) models\nsupport clinical decision-making but may introduce biases related to race,\ngender, and social determinants of health. We systematically evaluate biases in\nRAG-based LLM by examining demographic-sensitive queries and measuring\nretrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze\nretrieval overlap and correctness disparities. Our findings reveal substantial\ndemographic disparities within RAG pipelines, emphasizing the critical need for\nretrieval methods that explicitly account for fairness to ensure equitable\nclinical decision-making."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanshan Wang"
                },
                "author": "Yanshan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15450v1",
                "updated": "2025-03-19T17:31:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    31,
                    15,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:31:15Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    31,
                    15,
                    2,
                    78,
                    0
                ],
                "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling"
                },
                "summary": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder."
                },
                "authors": [
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "22 pages. Accepted to ICLR 2025 Workshop on Open Science for\n  Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14478v2",
                "updated": "2025-03-19T17:03:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    3,
                    25,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-18T17:51:34Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    34,
                    1,
                    77,
                    0
                ],
                "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM"
                },
                "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench."
                },
                "authors": [
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Zhijian Chen"
                    },
                    {
                        "name": "Kai Lan"
                    },
                    {
                        "name": "Lixin Ma"
                    },
                    {
                        "name": "Shengyuan Ding"
                    },
                    {
                        "name": "Yingji Liang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Farong Wen"
                    },
                    {
                        "name": "Zicheng Zhang"
                    },
                    {
                        "name": "Guofeng Zhang"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "Evaluation Code and dataset see\n  https://github.com/open-compass/Creation-MMBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15422v1",
                "updated": "2025-03-19T17:01:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    1,
                    40,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:01:40Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    1,
                    40,
                    2,
                    78,
                    0
                ],
                "title": "Limits on the Ejecta Mass During the Search for Kilonovae Associated\n  with Neutron Star-Black Hole Mergers: A case study of S230518h, GW230529,\n  S230627c and the Low-Significance Candidate S240422ed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits on the Ejecta Mass During the Search for Kilonovae Associated\n  with Neutron Star-Black Hole Mergers: A case study of S230518h, GW230529,\n  S230627c and the Low-Significance Candidate S240422ed"
                },
                "summary": "Neutron star-black hole (NSBH) mergers, detectable via their\ngravitational-wave (GW) emission, are expected to produce kilonovae (KNe). Four\nNSBH candidates have been identified and followed-up by more than fifty\ninstruments since the start of the fourth GW Observing Run (O4), in May 2023,\nup to July 2024; however, no confirmed associated KN has been detected. This\nstudy evaluates ejecta properties from multi-messenger observations to\nunderstand the absence of detectable KN: we use GW public information and joint\nobservations taken from 05.2023 to 07.2024 (LVK, ATLAS, DECam, GECKO, GOTO,\nGRANDMA, SAGUARO, TESS, WINTER, ZTF). First, our analysis on follow-up\nobservation strategies shows that, on average, more than 50% of the simulated\nKNe associated with NSBH mergers reach their peak luminosity around one day\nafter merger in the $g,r,i$- bands, which is not necessarily covered for each\nNSBH GW candidate. We also analyze the trade-off between observation efficiency\nand the intrinsic properties of the KN emission, to understand the impact on\nhow these constraints affect our ability to detect the KN, and underlying\nejecta properties for each GW candidate. In particular, we can only confirm the\nkilonova was not missed for 1% of the GW230529 and S230627c sky localization\nregion, given the large sky localization error of GW230529 and the large\ndistance for S230627c and, their respective KN faint luminosities. More\nconstraining, for S230518h, we infer the dynamical ejecta and post-merger disk\nwind ejecta $m_{dyn}, m_{wind}$ $<$ $0.03$ $M_\\odot$ and the viewing angle\n$\\theta>25^\\circ$. Similarly, the non-astrophysical origin of S240422ed is\nlikely further confirmed by the fact that we would have detected even a faint\nKN at the time and presumed distance of the S240422ed event candidate, within a\nminimum 45% credible region of the sky area, that can be larger depending on\nthe KN scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron star-black hole (NSBH) mergers, detectable via their\ngravitational-wave (GW) emission, are expected to produce kilonovae (KNe). Four\nNSBH candidates have been identified and followed-up by more than fifty\ninstruments since the start of the fourth GW Observing Run (O4), in May 2023,\nup to July 2024; however, no confirmed associated KN has been detected. This\nstudy evaluates ejecta properties from multi-messenger observations to\nunderstand the absence of detectable KN: we use GW public information and joint\nobservations taken from 05.2023 to 07.2024 (LVK, ATLAS, DECam, GECKO, GOTO,\nGRANDMA, SAGUARO, TESS, WINTER, ZTF). First, our analysis on follow-up\nobservation strategies shows that, on average, more than 50% of the simulated\nKNe associated with NSBH mergers reach their peak luminosity around one day\nafter merger in the $g,r,i$- bands, which is not necessarily covered for each\nNSBH GW candidate. We also analyze the trade-off between observation efficiency\nand the intrinsic properties of the KN emission, to understand the impact on\nhow these constraints affect our ability to detect the KN, and underlying\nejecta properties for each GW candidate. In particular, we can only confirm the\nkilonova was not missed for 1% of the GW230529 and S230627c sky localization\nregion, given the large sky localization error of GW230529 and the large\ndistance for S230627c and, their respective KN faint luminosities. More\nconstraining, for S230518h, we infer the dynamical ejecta and post-merger disk\nwind ejecta $m_{dyn}, m_{wind}$ $<$ $0.03$ $M_\\odot$ and the viewing angle\n$\\theta>25^\\circ$. Similarly, the non-astrophysical origin of S240422ed is\nlikely further confirmed by the fact that we would have detected even a faint\nKN at the time and presumed distance of the S240422ed event candidate, within a\nminimum 45% credible region of the sky area, that can be larger depending on\nthe KN scenario."
                },
                "authors": [
                    {
                        "name": "M. Pillas"
                    },
                    {
                        "name": "S. Antier"
                    },
                    {
                        "name": "K. Ackley"
                    },
                    {
                        "name": "T. Ahumada"
                    },
                    {
                        "name": "D. Akl"
                    },
                    {
                        "name": "L. de Almeida"
                    },
                    {
                        "name": "S. Anand"
                    },
                    {
                        "name": "C. Andrade"
                    },
                    {
                        "name": "I. Andreoni"
                    },
                    {
                        "name": "K. A. Bostroem"
                    },
                    {
                        "name": "M. Bulla"
                    },
                    {
                        "name": "E. Burns"
                    },
                    {
                        "name": "T. Cabrera"
                    },
                    {
                        "name": "S. Chang"
                    },
                    {
                        "name": "H. Choi"
                    },
                    {
                        "name": "B. O'Connor"
                    },
                    {
                        "name": "M. W. Coughlin"
                    },
                    {
                        "name": "W. Corradi"
                    },
                    {
                        "name": "A. R. Gibbs"
                    },
                    {
                        "name": "T. Dietrich"
                    },
                    {
                        "name": "D. Dornic"
                    },
                    {
                        "name": "J. -G. Ducoin"
                    },
                    {
                        "name": "P. -A. Duverne"
                    },
                    {
                        "name": "M. Dyer"
                    },
                    {
                        "name": "H. -B. Eggenstein"
                    },
                    {
                        "name": "M. Freeberg"
                    },
                    {
                        "name": "M. Fausnaugh"
                    },
                    {
                        "name": "W. Fong"
                    },
                    {
                        "name": "F. Foucart"
                    },
                    {
                        "name": "D. Frostig"
                    },
                    {
                        "name": "N. Guessoum"
                    },
                    {
                        "name": "V. Gupta"
                    },
                    {
                        "name": "P. Hello"
                    },
                    {
                        "name": "G. Hosseinzadeh"
                    },
                    {
                        "name": "L. Hu"
                    },
                    {
                        "name": "T. Hussenot-Desenonges"
                    },
                    {
                        "name": "M. Im"
                    },
                    {
                        "name": "R. Jayaraman"
                    },
                    {
                        "name": "M. Jeong"
                    },
                    {
                        "name": "V. Karambelkar"
                    },
                    {
                        "name": "S. Karpov"
                    },
                    {
                        "name": "M. Kasliwal"
                    },
                    {
                        "name": "C. D. Kilpatrick"
                    },
                    {
                        "name": "S. Kim"
                    },
                    {
                        "name": "N. Kochiashvili"
                    },
                    {
                        "name": "K. Kunnumkai"
                    },
                    {
                        "name": "M. Lamoureux"
                    },
                    {
                        "name": "C. U. Lee"
                    },
                    {
                        "name": "N. Lourie"
                    },
                    {
                        "name": "J. Lyman"
                    },
                    {
                        "name": "F. Magnani"
                    },
                    {
                        "name": "M. Masek"
                    },
                    {
                        "name": "G. Mo"
                    },
                    {
                        "name": "M. Molham"
                    },
                    {
                        "name": "F. Navarete"
                    },
                    {
                        "name": "D. O'Neill"
                    },
                    {
                        "name": "M. Nicholl"
                    },
                    {
                        "name": "A. H. Nitz"
                    },
                    {
                        "name": "K. Noysena"
                    },
                    {
                        "name": "G. S. H. Paek"
                    },
                    {
                        "name": "A. Palmese"
                    },
                    {
                        "name": "R. Poggiani"
                    },
                    {
                        "name": "T. Pradier"
                    },
                    {
                        "name": "O. Pyshna"
                    },
                    {
                        "name": "Y. Rajabov"
                    },
                    {
                        "name": "J. C. Rastinejad"
                    },
                    {
                        "name": "D. J. Sand"
                    },
                    {
                        "name": "P. Shawhan"
                    },
                    {
                        "name": "M. Shrestha"
                    },
                    {
                        "name": "R. Simcoe"
                    },
                    {
                        "name": "S. J. Smartt"
                    },
                    {
                        "name": "D. Steeghs"
                    },
                    {
                        "name": "R. Stein"
                    },
                    {
                        "name": "H. F. Stevance"
                    },
                    {
                        "name": "M. Sun"
                    },
                    {
                        "name": "A. Takey"
                    },
                    {
                        "name": "A. Toivonen"
                    },
                    {
                        "name": "D. Turpin"
                    },
                    {
                        "name": "K. Ulaczyk"
                    },
                    {
                        "name": "A. Wold"
                    },
                    {
                        "name": "T. Wouters"
                    }
                ],
                "author_detail": {
                    "name": "T. Wouters"
                },
                "author": "T. Wouters",
                "arxiv_comment": "47 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15421v1",
                "updated": "2025-03-19T17:01:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    1,
                    15,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:01:15Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    1,
                    15,
                    2,
                    78,
                    0
                ],
                "title": "Probing the topology of the space of tokens with structured prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the topology of the space of tokens with structured prompts"
                },
                "summary": "This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes."
                },
                "authors": [
                    {
                        "name": "Michael Robinson"
                    },
                    {
                        "name": "Sourya Dey"
                    },
                    {
                        "name": "Taisa Kushner"
                    }
                ],
                "author_detail": {
                    "name": "Taisa Kushner"
                },
                "author": "Taisa Kushner",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53Z50, 58Z05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15420v1",
                "updated": "2025-03-19T17:00:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    0,
                    58,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:00:58Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    0,
                    58,
                    2,
                    78,
                    0
                ],
                "title": "LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding"
                },
                "summary": "Implicit Neural Representations (INRs) are proving to be a powerful paradigm\nin unifying task modeling across diverse data domains, offering key advantages\nsuch as memory efficiency and resolution independence. Conventional deep\nlearning models are typically modality-dependent, often requiring custom\narchitectures and objectives for different types of signals. However, existing\nINR frameworks frequently rely on global latent vectors or exhibit\ncomputational inefficiencies that limit their broader applicability. We\nintroduce LIFT, a novel, high-performance framework that addresses these\nchallenges by capturing multiscale information through meta-learning. LIFT\nleverages multiple parallel localized implicit functions alongside a\nhierarchical latent generator to produce unified latent representations that\nspan local, intermediate, and global features. This architecture facilitates\nsmooth transitions across local regions, enhancing expressivity while\nmaintaining inference efficiency. Additionally, we introduce ReLIFT, an\nenhanced variant of LIFT that incorporates residual connections and expressive\nfrequency encodings. With this straightforward approach, ReLIFT effectively\naddresses the convergence-capacity gap found in comparable methods, providing\nan efficient yet powerful solution to improve capacity and speed up\nconvergence. Empirical results show that LIFT achieves state-of-the-art (SOTA)\nperformance in generative modeling and classification tasks, with notable\nreductions in computational costs. Moreover, in single-task settings, the\nstreamlined ReLIFT architecture proves effective in signal representations and\ninverse problem tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Neural Representations (INRs) are proving to be a powerful paradigm\nin unifying task modeling across diverse data domains, offering key advantages\nsuch as memory efficiency and resolution independence. Conventional deep\nlearning models are typically modality-dependent, often requiring custom\narchitectures and objectives for different types of signals. However, existing\nINR frameworks frequently rely on global latent vectors or exhibit\ncomputational inefficiencies that limit their broader applicability. We\nintroduce LIFT, a novel, high-performance framework that addresses these\nchallenges by capturing multiscale information through meta-learning. LIFT\nleverages multiple parallel localized implicit functions alongside a\nhierarchical latent generator to produce unified latent representations that\nspan local, intermediate, and global features. This architecture facilitates\nsmooth transitions across local regions, enhancing expressivity while\nmaintaining inference efficiency. Additionally, we introduce ReLIFT, an\nenhanced variant of LIFT that incorporates residual connections and expressive\nfrequency encodings. With this straightforward approach, ReLIFT effectively\naddresses the convergence-capacity gap found in comparable methods, providing\nan efficient yet powerful solution to improve capacity and speed up\nconvergence. Empirical results show that LIFT achieves state-of-the-art (SOTA)\nperformance in generative modeling and classification tasks, with notable\nreductions in computational costs. Moreover, in single-task settings, the\nstreamlined ReLIFT architecture proves effective in signal representations and\ninverse problem tasks."
                },
                "authors": [
                    {
                        "name": "Amirhossein Kazerouni"
                    },
                    {
                        "name": "Soroush Mehraban"
                    },
                    {
                        "name": "Michael Brudno"
                    },
                    {
                        "name": "Babak Taati"
                    }
                ],
                "author_detail": {
                    "name": "Babak Taati"
                },
                "author": "Babak Taati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13213v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13213v3",
                "updated": "2025-03-19T16:57:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    57,
                    23,
                    2,
                    78,
                    0
                ],
                "published": "2024-02-20T18:24:47Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    18,
                    24,
                    47,
                    1,
                    51,
                    0
                ],
                "title": "Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A"
                },
                "summary": "We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold."
                },
                "authors": [
                    {
                        "name": "Benjamin Plaut"
                    },
                    {
                        "name": "Nguyen X. Khanh"
                    },
                    {
                        "name": "Tu Trinh"
                    }
                ],
                "author_detail": {
                    "name": "Tu Trinh"
                },
                "author": "Tu Trinh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13213v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13213v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15395v1",
                "updated": "2025-03-19T16:35:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    35,
                    5,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T16:35:05Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    35,
                    5,
                    2,
                    78,
                    0
                ],
                "title": "Making Online Polls More Accurate: Statistical Methods Explained",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Online Polls More Accurate: Statistical Methods Explained"
                },
                "summary": "Online data has the potential to transform how researchers and companies\nproduce election forecasts. Social media surveys, online panels and even\ncomments scraped from the internet can offer valuable insights into political\npreferences. However, such data is often affected by significant selection\nbias, as online respondents may not be representative of the overall\npopulation. At the same time, traditional data collection methods are becoming\nincreasingly cost-prohibitive. In this scenario, scientists need instruments to\nbe able to draw the most accurate estimate possible from samples drawn online.\nThis paper provides an introduction to key statistical methods for mitigating\nbias and improving inference in such cases, with a focus on electoral polling.\nSpecifically, it presents the main statistical techniques, categorized into\nweighting, modeling and other approaches. It also offers practical\nrecommendations for drawing estimates with measures of uncertainty. Designed\nfor both researchers and industry practitioners, this introduction takes a\nhands-on approach, with code available for implementing the main methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online data has the potential to transform how researchers and companies\nproduce election forecasts. Social media surveys, online panels and even\ncomments scraped from the internet can offer valuable insights into political\npreferences. However, such data is often affected by significant selection\nbias, as online respondents may not be representative of the overall\npopulation. At the same time, traditional data collection methods are becoming\nincreasingly cost-prohibitive. In this scenario, scientists need instruments to\nbe able to draw the most accurate estimate possible from samples drawn online.\nThis paper provides an introduction to key statistical methods for mitigating\nbias and improving inference in such cases, with a focus on electoral polling.\nSpecifically, it presents the main statistical techniques, categorized into\nweighting, modeling and other approaches. It also offers practical\nrecommendations for drawing estimates with measures of uncertainty. Designed\nfor both researchers and industry practitioners, this introduction takes a\nhands-on approach, with code available for implementing the main methods."
                },
                "authors": [
                    {
                        "name": "Alberto Arletti"
                    },
                    {
                        "name": "Maria Letizia Tanturri"
                    },
                    {
                        "name": "Omar Paccagnella"
                    }
                ],
                "author_detail": {
                    "name": "Omar Paccagnella"
                },
                "author": "Omar Paccagnella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11197v3",
                "updated": "2025-03-19T16:33:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    33,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-14T08:43:53Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    43,
                    53,
                    4,
                    73,
                    0
                ],
                "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering"
                },
                "summary": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa."
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Jizhong Liu"
                    },
                    {
                        "name": "Heinrich Dinkel"
                    },
                    {
                        "name": "Yadong Niu"
                    },
                    {
                        "name": "Junbo Zhang"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15393v1",
                "updated": "2025-03-19T16:33:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    33,
                    8,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T16:33:08Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    33,
                    8,
                    2,
                    78,
                    0
                ],
                "title": "Orbital eccentricity in a neutron star - black hole binary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orbital eccentricity in a neutron star - black hole binary"
                },
                "summary": "The observation of gravitational waves from merging black holes and neutron\nstars provides a unique opportunity to discern information about their\nastrophysical environment. Two signatures that are considered powerful tracers\nto distinguish between different binary formation channels are\ngeneral-relativistic spin-induced orbital precession and orbital eccentricity.\nBoth effects leave characteristic imprints in the gravitational-wave signal\nthat can be extracted from observations. To date, neither precession nor\neccentricity have been discerned in neutron star - black hole binaries. Here we\nreport the first measurement of orbital eccentricity in a neutron star - black\nhole binary. Using, for the first time, a waveform model that incorporates\nprecession and eccentricity, we perform Bayesian inference on the event\nGW200105 and infer a median orbital eccentricity of $e_{20}\\sim 0.145$ at an\norbital period of $0.1$s, excluding zero at more than $99\\%$ confidence. We\nfind inconclusive evidence for the presence of precession, consistent with\nprevious, non-eccentric results. Our result implies a fraction of these\nbinaries will exhibit orbital eccentricity even at small separations,\nsuggesting formation through mechanisms involving dynamical interactions beyond\nisolated binary evolution. Future observations will reveal the contribution of\neccentric neutron star - black hole binaries to the total merger rate across\ncosmic time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The observation of gravitational waves from merging black holes and neutron\nstars provides a unique opportunity to discern information about their\nastrophysical environment. Two signatures that are considered powerful tracers\nto distinguish between different binary formation channels are\ngeneral-relativistic spin-induced orbital precession and orbital eccentricity.\nBoth effects leave characteristic imprints in the gravitational-wave signal\nthat can be extracted from observations. To date, neither precession nor\neccentricity have been discerned in neutron star - black hole binaries. Here we\nreport the first measurement of orbital eccentricity in a neutron star - black\nhole binary. Using, for the first time, a waveform model that incorporates\nprecession and eccentricity, we perform Bayesian inference on the event\nGW200105 and infer a median orbital eccentricity of $e_{20}\\sim 0.145$ at an\norbital period of $0.1$s, excluding zero at more than $99\\%$ confidence. We\nfind inconclusive evidence for the presence of precession, consistent with\nprevious, non-eccentric results. Our result implies a fraction of these\nbinaries will exhibit orbital eccentricity even at small separations,\nsuggesting formation through mechanisms involving dynamical interactions beyond\nisolated binary evolution. Future observations will reveal the contribution of\neccentric neutron star - black hole binaries to the total merger rate across\ncosmic time."
                },
                "authors": [
                    {
                        "name": "Gonzalo Morras"
                    },
                    {
                        "name": "Geraint Pratten"
                    },
                    {
                        "name": "Patricia Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Schmidt"
                },
                "author": "Patricia Schmidt",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04715v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04715v4",
                "updated": "2025-03-19T16:28:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    28,
                    25,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-06T18:58:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.09% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.09% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/"
                },
                "authors": [
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shijie Xuyang"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04715v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04715v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08020v2",
                "updated": "2025-03-19T16:26:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    26,
                    10,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-11T23:40:53Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    23,
                    40,
                    53,
                    1,
                    42,
                    0
                ],
                "title": "Speculate, then Collaborate: Fusing Knowledge of Language Models during\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate, then Collaborate: Fusing Knowledge of Language Models during\n  Decoding"
                },
                "summary": "Large Language Models (LLMs) often excel in specific domains but fall short\nin others due to the limitations of their training. Thus, enabling LLMs to\nsolve problems collaboratively by integrating their complementary knowledge\npromises to improve their performance across domains. To realize this\npotential, we introduce a novel Collaborative Speculative Decoding (CoSD)\nalgorithm that enables efficient LLM knowledge fusion at test time without\nrequiring additional model training. CoSD employs a draft model to generate\ninitial sequences and an easy-to-learn rule or decision tree to decide when to\ninvoke an assistant model to improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference efficiency, is transferable across\ndomains and models, and offers greater explainability. Experimental results\ndemonstrate that CoSD improves accuracy by up to 10\\% across benchmarks\ncompared to existing methods, providing a scalable and effective solution for\nLLM-based applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often excel in specific domains but fall short\nin others due to the limitations of their training. Thus, enabling LLMs to\nsolve problems collaboratively by integrating their complementary knowledge\npromises to improve their performance across domains. To realize this\npotential, we introduce a novel Collaborative Speculative Decoding (CoSD)\nalgorithm that enables efficient LLM knowledge fusion at test time without\nrequiring additional model training. CoSD employs a draft model to generate\ninitial sequences and an easy-to-learn rule or decision tree to decide when to\ninvoke an assistant model to improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference efficiency, is transferable across\ndomains and models, and offers greater explainability. Experimental results\ndemonstrate that CoSD improves accuracy by up to 10\\% across benchmarks\ncompared to existing methods, providing a scalable and effective solution for\nLLM-based applications"
                },
                "authors": [
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Muneeza Azmat"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15386v1",
                "updated": "2025-03-19T16:24:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    24,
                    55,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T16:24:55Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    24,
                    55,
                    2,
                    78,
                    0
                ],
                "title": "CCDP: Composition of Conditional Diffusion Policies with Guided Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCDP: Composition of Conditional Diffusion Policies with Guided Sampling"
                },
                "summary": "Imitation Learning offers a promising approach to learn directly from data\nwithout requiring explicit models, simulations, or detailed task definitions.\nDuring inference, actions are sampled from the learned distribution and\nexecuted on the robot. However, sampled actions may fail for various reasons,\nand simply repeating the sampling step until a successful action is obtained\ncan be inefficient. In this work, we propose an enhanced sampling strategy that\nrefines the sampling distribution to avoid previously unsuccessful actions. We\ndemonstrate that by solely utilizing data from successful demonstrations, our\nmethod can infer recovery actions without the need for additional exploratory\nbehavior or a high-level controller. Furthermore, we leverage the concept of\ndiffusion model decomposition to break down the primary problem (which may\nrequire long-horizon history to manage failures) into multiple smaller, more\nmanageable sub-problems in learning, data collection, and inference, thereby\nenabling the system to adapt to variable failure counts. Our approach yields a\nlow-level controller that dynamically adjusts its sampling space to improve\nefficiency when prior samples fall short. We validate our method across several\ntasks, including door opening with unknown directions, object manipulation, and\nbutton-searching scenarios, demonstrating that our approach outperforms\ntraditional baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation Learning offers a promising approach to learn directly from data\nwithout requiring explicit models, simulations, or detailed task definitions.\nDuring inference, actions are sampled from the learned distribution and\nexecuted on the robot. However, sampled actions may fail for various reasons,\nand simply repeating the sampling step until a successful action is obtained\ncan be inefficient. In this work, we propose an enhanced sampling strategy that\nrefines the sampling distribution to avoid previously unsuccessful actions. We\ndemonstrate that by solely utilizing data from successful demonstrations, our\nmethod can infer recovery actions without the need for additional exploratory\nbehavior or a high-level controller. Furthermore, we leverage the concept of\ndiffusion model decomposition to break down the primary problem (which may\nrequire long-horizon history to manage failures) into multiple smaller, more\nmanageable sub-problems in learning, data collection, and inference, thereby\nenabling the system to adapt to variable failure counts. Our approach yields a\nlow-level controller that dynamically adjusts its sampling space to improve\nefficiency when prior samples fall short. We validate our method across several\ntasks, including door opening with unknown directions, object manipulation, and\nbutton-searching scenarios, demonstrating that our approach outperforms\ntraditional baselines."
                },
                "authors": [
                    {
                        "name": "Amirreza Razmjoo"
                    },
                    {
                        "name": "Sylvain Calinon"
                    },
                    {
                        "name": "Michael Gienger"
                    },
                    {
                        "name": "Fan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Zhang"
                },
                "author": "Fan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15374v1",
                "updated": "2025-03-19T16:12:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    12,
                    11,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T16:12:11Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    12,
                    11,
                    2,
                    78,
                    0
                ],
                "title": "Real-world validation of a multimodal LLM-powered pipeline for\n  High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world validation of a multimodal LLM-powered pipeline for\n  High-Accuracy Clinical Trial Patient Matching leveraging EHR data"
                },
                "summary": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching."
                },
                "authors": [
                    {
                        "name": "Anatole Callies"
                    },
                    {
                        "name": "Quentin Bodinier"
                    },
                    {
                        "name": "Philippe Ravaud"
                    },
                    {
                        "name": "Kourosh Davarpanah"
                    }
                ],
                "author_detail": {
                    "name": "Kourosh Davarpanah"
                },
                "arxiv_affiliation": "Inato",
                "author": "Kourosh Davarpanah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05206v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05206v3",
                "updated": "2025-03-19T16:10:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    10,
                    18,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-02T05:14:22Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    5,
                    14,
                    22,
                    6,
                    33,
                    0
                ],
                "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
                },
                "summary": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models."
                },
                "authors": [
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Ye Sun"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Hengyuan Xu"
                    },
                    {
                        "name": "Yunhao Chen"
                    },
                    {
                        "name": "Yunhan Zhao"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Jingfeng Zhang"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Baoyuan Wu"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Ruoxi Jia"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Neil Gong"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Sarah Erfani"
                    },
                    {
                        "name": "Tim Baldwin"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Masashi Sugiyama"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "James Bailey"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "47 pages, 3 figures, 11 tables; GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05206v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05206v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18750v2",
                "updated": "2025-03-19T16:08:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    8,
                    36,
                    2,
                    78,
                    0
                ],
                "published": "2024-12-25T02:48:53Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    2,
                    48,
                    53,
                    2,
                    360,
                    0
                ],
                "title": "The Impact of Input Order Bias on Large Language Models for Software\n  Fault Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Input Order Bias on Large Language Models for Software\n  Fault Localization"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications."
                },
                "authors": [
                    {
                        "name": "Md Nakhla Rafi"
                    },
                    {
                        "name": "Dong Jae Kim"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    },
                    {
                        "name": "Shaowei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Wang"
                },
                "author": "Shaowei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15367v1",
                "updated": "2025-03-19T16:05:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    5,
                    52,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T16:05:52Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    5,
                    52,
                    2,
                    78,
                    0
                ],
                "title": "FedBEns: One-Shot Federated Learning based on Bayesian Ensemble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedBEns: One-Shot Federated Learning based on Bayesian Ensemble"
                },
                "summary": "One-Shot Federated Learning (FL) is a recent paradigm that enables multiple\nclients to cooperatively learn a global model in a single round of\ncommunication with a central server. In this paper, we analyze the One-Shot FL\nproblem through the lens of Bayesian inference and propose FedBEns, an\nalgorithm that leverages the inherent multimodality of local loss functions to\nfind better global models. Our algorithm leverages a mixture of Laplace\napproximations for the clients' local posteriors, which the server then\naggregates to infer the global model. We conduct extensive experiments on\nvarious datasets, demonstrating that the proposed method outperforms competing\nbaselines that typically rely on unimodal approximations of the local losses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Shot Federated Learning (FL) is a recent paradigm that enables multiple\nclients to cooperatively learn a global model in a single round of\ncommunication with a central server. In this paper, we analyze the One-Shot FL\nproblem through the lens of Bayesian inference and propose FedBEns, an\nalgorithm that leverages the inherent multimodality of local loss functions to\nfind better global models. Our algorithm leverages a mixture of Laplace\napproximations for the clients' local posteriors, which the server then\naggregates to infer the global model. We conduct extensive experiments on\nvarious datasets, demonstrating that the proposed method outperforms competing\nbaselines that typically rely on unimodal approximations of the local losses."
                },
                "authors": [
                    {
                        "name": "Jacopo Talpini"
                    },
                    {
                        "name": "Marco Savi"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15358v1",
                "updated": "2025-03-19T15:58:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    58,
                    46,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:58:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    58,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation"
                },
                "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."
                },
                "authors": [
                    {
                        "name": "Thomas Pickard"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Maggie Mi"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Dylan Phelps"
                    },
                    {
                        "name": "Carolina Scarton"
                    },
                    {
                        "name": "Marco Idiart"
                    }
                ],
                "author_detail": {
                    "name": "Marco Idiart"
                },
                "author": "Marco Idiart",
                "arxiv_comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.4.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20227v2",
                "updated": "2025-03-19T15:56:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    56,
                    49,
                    2,
                    78,
                    0
                ],
                "published": "2024-12-28T17:48:33Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    48,
                    33,
                    5,
                    363,
                    0
                ],
                "title": "LLM Reasoning Engine: Specialized Training for Enhanced Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Reasoning Engine: Specialized Training for Enhanced Mathematical\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance in various\nnatural language processing tasks but face challenges in mathematical\nreasoning, where complex problem-solving requires both linguistic understanding\nand mathematical reasoning skills. Existing approaches to address this\nchallenge often rely on ensemble methods and suffer from the problem of data\nscarcity in target domains. In this work, we present a novel method to enhance\nLLMs' capabilities in mathematical reasoning tasks. Motivated by the need to\nbridge this gap, our approach incorporates a question paraphrase strategy,\nwhich aims at diversifying the linguistic forms of mathematical questions to\nimprove generalization. Additionally, specialized training objectives are\nemployed to guide the model's learning process, focusing on enhancing its\nunderstanding of mathematical concepts and reasoning processes. We conduct\nexperiments on four datasets using different LLMs, and demonstrate the\neffectiveness of our approach in improving LLMs' performance on mathematical\nreasoning tasks. Our findings underscore the significance of our methodology in\nthe advancement of large language models and its potential implications for\nreal-world applications that require mathematical reasoning abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance in various\nnatural language processing tasks but face challenges in mathematical\nreasoning, where complex problem-solving requires both linguistic understanding\nand mathematical reasoning skills. Existing approaches to address this\nchallenge often rely on ensemble methods and suffer from the problem of data\nscarcity in target domains. In this work, we present a novel method to enhance\nLLMs' capabilities in mathematical reasoning tasks. Motivated by the need to\nbridge this gap, our approach incorporates a question paraphrase strategy,\nwhich aims at diversifying the linguistic forms of mathematical questions to\nimprove generalization. Additionally, specialized training objectives are\nemployed to guide the model's learning process, focusing on enhancing its\nunderstanding of mathematical concepts and reasoning processes. We conduct\nexperiments on four datasets using different LLMs, and demonstrate the\neffectiveness of our approach in improving LLMs' performance on mathematical\nreasoning tasks. Our findings underscore the significance of our methodology in\nthe advancement of large language models and its potential implications for\nreal-world applications that require mathematical reasoning abilities."
                },
                "authors": [
                    {
                        "name": "Shuguang Chen"
                    },
                    {
                        "name": "Guang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guang Lin"
                },
                "author": "Guang Lin",
                "arxiv_comment": "Accepted to NAACL 2025 KnowledgeNLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15352v1",
                "updated": "2025-03-19T15:51:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    51,
                    17,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:51:17Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    51,
                    17,
                    2,
                    78,
                    0
                ],
                "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for\n  Cross-modal Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for\n  Cross-modal Transfer"
                },
                "summary": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning."
                },
                "authors": [
                    {
                        "name": "Abhi Kamboj"
                    },
                    {
                        "name": "Minh N. Do"
                    }
                ],
                "author_detail": {
                    "name": "Minh N. Do"
                },
                "author": "Minh N. Do",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15351v1",
                "updated": "2025-03-19T15:48:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    48,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:48:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    48,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models"
                },
                "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals."
                },
                "authors": [
                    {
                        "name": "I-Fan Lin"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13551v2",
                "updated": "2025-03-19T15:43:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    43,
                    56,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-16T15:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    15,
                    18,
                    40,
                    6,
                    75,
                    0
                ],
                "title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models"
                },
                "summary": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate\nsteps. In this paper, we propose a novel reward model approach, Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps from fine-grained and coarse-grained level. HRM performs better in\nassessing reasoning coherence and self-reflection, particularly when the\nprevious reasoning step is incorrect. Furthermore, to address the inefficiency\nof autonomous generating PRM training data via Monte Carlo Tree Search (MCTS),\nwe introduce a lightweight and effective data augmentation strategy called\nHierarchical Node Compression (HNC) based on node merging (combining two\nconsecutive reasoning steps into one step) in the tree structure. This approach\ndiversifies MCTS results for HRM with negligible computational overhead,\nenhancing label robustness by introducing noise. Empirical results on the\nPRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves\nsuperior stability and reliability in evaluation compared to PRM. Furthermore,\ncross-domain evaluations on MATH500 and GSM8K confirm HRM's superior\ngeneralization and robustness across diverse reasoning tasks. The code for all\nexperiments will be released at https:\n//github.com/tengwang0318/hierarchial_reward_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate\nsteps. In this paper, we propose a novel reward model approach, Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps from fine-grained and coarse-grained level. HRM performs better in\nassessing reasoning coherence and self-reflection, particularly when the\nprevious reasoning step is incorrect. Furthermore, to address the inefficiency\nof autonomous generating PRM training data via Monte Carlo Tree Search (MCTS),\nwe introduce a lightweight and effective data augmentation strategy called\nHierarchical Node Compression (HNC) based on node merging (combining two\nconsecutive reasoning steps into one step) in the tree structure. This approach\ndiversifies MCTS results for HRM with negligible computational overhead,\nenhancing label robustness by introducing noise. Empirical results on the\nPRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves\nsuperior stability and reliability in evaluation compared to PRM. Furthermore,\ncross-domain evaluations on MATH500 and GSM8K confirm HRM's superior\ngeneralization and robustness across diverse reasoning tasks. The code for all\nexperiments will be released at https:\n//github.com/tengwang0318/hierarchial_reward_model."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Zhangyi Jiang"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Yanan Zheng"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Shenyang Tong"
                    },
                    {
                        "name": "Hailei Gong"
                    }
                ],
                "author_detail": {
                    "name": "Hailei Gong"
                },
                "author": "Hailei Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15342v1",
                "updated": "2025-03-19T15:41:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    41,
                    32,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:41:32Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    41,
                    32,
                    2,
                    78,
                    0
                ],
                "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TruthLens:A Training-Free Paradigm for DeepFake Detection"
                },
                "summary": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation."
                },
                "authors": [
                    {
                        "name": "Ritabrata Chakraborty"
                    },
                    {
                        "name": "Rajatsubhra Chakraborty"
                    },
                    {
                        "name": "Ali Khaleghi Rahimian"
                    },
                    {
                        "name": "Thomas MacDougall"
                    }
                ],
                "author_detail": {
                    "name": "Thomas MacDougall"
                },
                "author": "Thomas MacDougall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15341v1",
                "updated": "2025-03-19T15:40:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    40,
                    45,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:40:45Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    40,
                    45,
                    2,
                    78,
                    0
                ],
                "title": "Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has been demonstrated as an effective\ntechnique for improving the problem-solving capabilities of large language\nmodels (LLMs) in the context of code generation. However, existing CoT methods\noften exhibit a tendency toward \"overthinking\", where the LLM consistently\napplies reasoning strategies without adequately considering the task's\nunderlying complexity. This results in the LLMs allocating excessive\ncomputational resources, in terms of tokens, to relatively simple tasks or\nproblems where the correct answer is already evident. Additionally, this\noverthinking may lead LLMs down incorrect reasoning paths, resulting in\nincorrect code generation. In this paper, we introduce UnCertainty-Aware\nChain-of-Thought (UnCert-CoT), an LLM-based approach designed to enhance code\ngeneration by incorporating an uncertainty-aware CoT reasoning mechanism, which\nfocuses computational resources on targeting points where LLMs are more prone\nto error. We propose two confidence-based uncertainty measures: Entropy-based\nand Probability Differential-based methods. When uncertainty is high,\nUnCert-CoT activates CoT-decoding to generate multiple reasoning paths and\nselects the final code that exhibits the highest likelihood of correctness. In\ncontrast, LLM directly generates the code when uncertainty is low. This\nuncertainty judgment mechanism allows LLMs to prioritize complex tasks and\navoid unnecessary steps in simpler cases, thereby improving overall efficiency\nand accuracy in code generation. Our experimental results demonstrate that\nUnCert-CoT significantly enhances code generation accuracy on challenging\nbenchmark MHPP(Mostly Hard Python Problems), it achieves improvements up to\n6.1% on PassRate accuracy, particularly in situations where traditional LLMs\nare prone to errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has been demonstrated as an effective\ntechnique for improving the problem-solving capabilities of large language\nmodels (LLMs) in the context of code generation. However, existing CoT methods\noften exhibit a tendency toward \"overthinking\", where the LLM consistently\napplies reasoning strategies without adequately considering the task's\nunderlying complexity. This results in the LLMs allocating excessive\ncomputational resources, in terms of tokens, to relatively simple tasks or\nproblems where the correct answer is already evident. Additionally, this\noverthinking may lead LLMs down incorrect reasoning paths, resulting in\nincorrect code generation. In this paper, we introduce UnCertainty-Aware\nChain-of-Thought (UnCert-CoT), an LLM-based approach designed to enhance code\ngeneration by incorporating an uncertainty-aware CoT reasoning mechanism, which\nfocuses computational resources on targeting points where LLMs are more prone\nto error. We propose two confidence-based uncertainty measures: Entropy-based\nand Probability Differential-based methods. When uncertainty is high,\nUnCert-CoT activates CoT-decoding to generate multiple reasoning paths and\nselects the final code that exhibits the highest likelihood of correctness. In\ncontrast, LLM directly generates the code when uncertainty is low. This\nuncertainty judgment mechanism allows LLMs to prioritize complex tasks and\navoid unnecessary steps in simpler cases, thereby improving overall efficiency\nand accuracy in code generation. Our experimental results demonstrate that\nUnCert-CoT significantly enhances code generation accuracy on challenging\nbenchmark MHPP(Mostly Hard Python Problems), it achieves improvements up to\n6.1% on PassRate accuracy, particularly in situations where traditional LLMs\nare prone to errors."
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hong Mei"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Yihong Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Dong"
                },
                "author": "Yihong Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08415v2",
                "updated": "2025-03-19T15:40:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    40,
                    41,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-11T13:24:39Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    24,
                    39,
                    1,
                    70,
                    0
                ],
                "title": "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems"
                },
                "summary": "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Guoyang Duan"
                    },
                    {
                        "name": "Tianle Xu"
                    },
                    {
                        "name": "Junchi Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Yongqiang Yao"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "arxiv_comment": "9 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14703v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14703v3",
                "updated": "2025-03-19T15:37:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    37,
                    42,
                    2,
                    78,
                    0
                ],
                "published": "2024-06-20T19:50:56Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    19,
                    50,
                    56,
                    3,
                    172,
                    0
                ],
                "title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction."
                },
                "authors": [
                    {
                        "name": "Seungbeen Lee"
                    },
                    {
                        "name": "Seungwon Lim"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Giyeong Oh"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Beong-woo Kwak"
                    },
                    {
                        "name": "Yeonsoo Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "Accepted to NAACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14703v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14703v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17211v2",
                "updated": "2025-03-19T15:35:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    35,
                    38,
                    2,
                    78,
                    0
                ],
                "published": "2023-09-29T13:09:40Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    13,
                    9,
                    40,
                    4,
                    272,
                    0
                ],
                "title": "Data-Free Dynamic Compression of CNNs for Tractable Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Free Dynamic Compression of CNNs for Tractable Efficiency"
                },
                "summary": "To reduce the computational cost of convolutional neural networks (CNNs) on\nresource-constrained devices, structured pruning approaches have shown promise\nin lowering floating-point operations (FLOPs) without substantial drops in\naccuracy. However, most methods require fine-tuning or specific training\nprocedures to achieve a reasonable trade-off between retained accuracy and\nreduction in FLOPs, adding computational overhead and requiring training data\nto be available. To this end, we propose HASTE (Hashing for Tractable\nEfficiency), a data-free, plug-and-play convolution module that instantly\nreduces a network's test-time inference cost without training or fine-tuning.\nOur approach utilizes locality-sensitive hashing (LSH) to detect redundancies\nin the channel dimension of latent feature maps, compressing similar channels\nto reduce input and filter depth simultaneously, resulting in cheaper\nconvolutions. We demonstrate our approach on the popular vision benchmarks\nCIFAR-10 and ImageNet, where we achieve a 46.72% reduction in FLOPs with only a\n1.25% loss in accuracy by swapping the convolution modules in a ResNet34 on\nCIFAR-10 for our HASTE module.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce the computational cost of convolutional neural networks (CNNs) on\nresource-constrained devices, structured pruning approaches have shown promise\nin lowering floating-point operations (FLOPs) without substantial drops in\naccuracy. However, most methods require fine-tuning or specific training\nprocedures to achieve a reasonable trade-off between retained accuracy and\nreduction in FLOPs, adding computational overhead and requiring training data\nto be available. To this end, we propose HASTE (Hashing for Tractable\nEfficiency), a data-free, plug-and-play convolution module that instantly\nreduces a network's test-time inference cost without training or fine-tuning.\nOur approach utilizes locality-sensitive hashing (LSH) to detect redundancies\nin the channel dimension of latent feature maps, compressing similar channels\nto reduce input and filter depth simultaneously, resulting in cheaper\nconvolutions. We demonstrate our approach on the popular vision benchmarks\nCIFAR-10 and ImageNet, where we achieve a 46.72% reduction in FLOPs with only a\n1.25% loss in accuracy by swapping the convolution modules in a ResNet34 on\nCIFAR-10 for our HASTE module."
                },
                "authors": [
                    {
                        "name": "Lukas Meiner"
                    },
                    {
                        "name": "Jens Mehnert"
                    },
                    {
                        "name": "Alexandru Paul Condurache"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paul Condurache"
                },
                "author": "Alexandru Paul Condurache",
                "arxiv_doi": "10.5220/0013301000003912",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5220/0013301000003912",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.17211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at VISAPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15338v1",
                "updated": "2025-03-19T15:34:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    34,
                    21,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:34:21Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    34,
                    21,
                    2,
                    78,
                    0
                ],
                "title": "Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context"
                },
                "summary": "Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio."
                },
                "authors": [
                    {
                        "name": "Junyi Ao"
                    },
                    {
                        "name": "Dekun Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Wenjie Feng"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Zhizheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhizheng Wu"
                },
                "author": "Zhizheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15327v1",
                "updated": "2025-03-19T15:27:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    27,
                    22,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:27:22Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    27,
                    22,
                    2,
                    78,
                    0
                ],
                "title": "Euclid Quick Data Release (Q1). The Strong Lensing Discovery Engine D --\n  Double-source-plane lens candidates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid Quick Data Release (Q1). The Strong Lensing Discovery Engine D --\n  Double-source-plane lens candidates"
                },
                "summary": "Strong gravitational lensing systems with multiple source planes are powerful\ntools for probing the density profiles and dark matter substructure of the\ngalaxies. The ratio of Einstein radii is related to the dark energy equation of\nstate through the cosmological scaling factor $\\beta$. However, galaxy-scale\ndouble-source-plane lenses (DSPLs) are extremely rare. In this paper, we report\nthe discovery of four new galaxy-scale double-source-plane lens candidates in\nthe Euclid Quick Release 1 (Q1) data. These systems were initially identified\nthrough a combination of machine learning lens-finding models and subsequent\nvisual inspection from citizens and experts. We apply the widely-used {\\tt\nLensPop} lens forecasting model to predict that the full \\Euclid survey will\ndiscover 1700 DSPLs, which scales to $6 \\pm 3$ DSPLs in 63 deg$^2$, the area of\nQ1. The number of discoveries in this work is broadly consistent with this\nforecast. We present lens models for each DSPL and infer their $\\beta$ values.\nOur initial Q1 sample demonstrates the promise of \\Euclid to discover such rare\nobjects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong gravitational lensing systems with multiple source planes are powerful\ntools for probing the density profiles and dark matter substructure of the\ngalaxies. The ratio of Einstein radii is related to the dark energy equation of\nstate through the cosmological scaling factor $\\beta$. However, galaxy-scale\ndouble-source-plane lenses (DSPLs) are extremely rare. In this paper, we report\nthe discovery of four new galaxy-scale double-source-plane lens candidates in\nthe Euclid Quick Release 1 (Q1) data. These systems were initially identified\nthrough a combination of machine learning lens-finding models and subsequent\nvisual inspection from citizens and experts. We apply the widely-used {\\tt\nLensPop} lens forecasting model to predict that the full \\Euclid survey will\ndiscover 1700 DSPLs, which scales to $6 \\pm 3$ DSPLs in 63 deg$^2$, the area of\nQ1. The number of discoveries in this work is broadly consistent with this\nforecast. We present lens models for each DSPL and infer their $\\beta$ values.\nOur initial Q1 sample demonstrates the promise of \\Euclid to discover such rare\nobjects."
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "T. Li"
                    },
                    {
                        "name": "T. E. Collett"
                    },
                    {
                        "name": "M. Walmsley"
                    },
                    {
                        "name": "N. E. P. Lines"
                    },
                    {
                        "name": "K. Rojas"
                    },
                    {
                        "name": "J. W. Nightingale"
                    },
                    {
                        "name": "W. J. R. Enzi"
                    },
                    {
                        "name": "L. A. Moustakas"
                    },
                    {
                        "name": "C. Krawczyk"
                    },
                    {
                        "name": "R. Gavazzi"
                    },
                    {
                        "name": "G. Despali"
                    },
                    {
                        "name": "P. Holloway"
                    },
                    {
                        "name": "S. Schuldt"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "D. J. Ballard"
                    },
                    {
                        "name": "A. Verma"
                    },
                    {
                        "name": "B. Clément"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "A. Melo"
                    },
                    {
                        "name": "J. A. Acevedo Barroso"
                    },
                    {
                        "name": "L. Leuzzi"
                    },
                    {
                        "name": "A. Manjón-García"
                    },
                    {
                        "name": "R. Pearce-Casey"
                    },
                    {
                        "name": "D. Sluse"
                    },
                    {
                        "name": "C. Tortora"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "G. Mahler"
                    },
                    {
                        "name": "A. More"
                    },
                    {
                        "name": "N. Aghanim"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "H. Aussel"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "A. Balestra"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "R. Bender"
                    },
                    {
                        "name": "F. Bernardeau"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "A. Bonchi"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "G. Cañas-Herrera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "M. Cropper"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "A. M. Di Giorgio"
                    },
                    {
                        "name": "C. Dolding"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "C. A. J. Duncan"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "M. Fabricius"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "S. Fotopoulou"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "W. Gillard"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "P. Gómez-Alvarez"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "B. R. Granett"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "H. Hoekstra"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "I. M. Hook"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "P. Hudelot"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihänen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kümmel"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "Q. Le Boulc'h"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "D. Le Mignant"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "S. Maurogordato"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "R. C. Nichol"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "J. A. Schewtschenko"
                    },
                    {
                        "name": "M. Schirmer"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "M. Seiffert"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Crespí"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "E. A. Valentijn"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "A. Zacchei"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "F. M. Zerbi"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "V. Allevato"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "E. Bozzo"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "D. Di Ferdinando"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "M. Huertas-Company"
                    },
                    {
                        "name": "J. Martín-Fleitas"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "M. Pöntinen"
                    },
                    {
                        "name": "C. Porciani"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "S. Alvi"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "S. Anselmi"
                    },
                    {
                        "name": "M. Archidiacono"
                    },
                    {
                        "name": "F. Atrio-Barandela"
                    },
                    {
                        "name": "K. Benson"
                    },
                    {
                        "name": "P. Bergamini"
                    },
                    {
                        "name": "D. Bertacca"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "L. Blot"
                    },
                    {
                        "name": "M. L. Brown"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "A. Calabro"
                    },
                    {
                        "name": "F. Caro"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "F. Cogato"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "O. Cucciati"
                    },
                    {
                        "name": "S. Davini"
                    },
                    {
                        "name": "F. De Paolis"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "A. Díaz-Sánchez"
                    },
                    {
                        "name": "J. J. Diaz"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "P. -A. Duc"
                    },
                    {
                        "name": "A. Enia"
                    },
                    {
                        "name": "Y. Fang"
                    },
                    {
                        "name": "A. G. Ferrari"
                    },
                    {
                        "name": "P. G. Ferreira"
                    },
                    {
                        "name": "A. Finoguenov"
                    },
                    {
                        "name": "A. Fontana"
                    },
                    {
                        "name": "A. Franco"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "T. Gasparetto"
                    },
                    {
                        "name": "V. Gautard"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "F. Gianotti"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "C. M. Gutierrez"
                    },
                    {
                        "name": "A. Hall"
                    },
                    {
                        "name": "W. G. Hartley"
                    },
                    {
                        "name": "C. Hernández-Monteagudo"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "M. Jauzac"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "V. Kansal"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "K. Kiiveri"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "J. Le Graet"
                    },
                    {
                        "name": "L. Legrand"
                    },
                    {
                        "name": "M. Lembo"
                    },
                    {
                        "name": "F. Lepori"
                    },
                    {
                        "name": "G. Leroy"
                    },
                    {
                        "name": "G. F. Lesci"
                    },
                    {
                        "name": "J. Lesgourgues"
                    },
                    {
                        "name": "T. I. Liaudat"
                    },
                    {
                        "name": "A. Loureiro"
                    },
                    {
                        "name": "J. Macias-Perez"
                    },
                    {
                        "name": "G. Maggio"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "M. Migliaccio"
                    },
                    {
                        "name": "M. Miluzio"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "G. Morgante"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Nesseris"
                    },
                    {
                        "name": "F. Passalacqua"
                    },
                    {
                        "name": "K. Paterson"
                    },
                    {
                        "name": "L. Patrizii"
                    },
                    {
                        "name": "A. Pisani"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "S. Quai"
                    },
                    {
                        "name": "M. Radovich"
                    },
                    {
                        "name": "P. -F. Rocci"
                    },
                    {
                        "name": "S. Sacquegna"
                    },
                    {
                        "name": "M. Sahlén"
                    },
                    {
                        "name": "D. B. Sanders"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "C. Scarlata"
                    },
                    {
                        "name": "J. Schaye"
                    },
                    {
                        "name": "A. Schneider"
                    },
                    {
                        "name": "D. Sciotti"
                    },
                    {
                        "name": "E. Sellentin"
                    },
                    {
                        "name": "L. C. Smith"
                    },
                    {
                        "name": "K. Tanidis"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "C. Valieri"
                    },
                    {
                        "name": "A. Venhola"
                    },
                    {
                        "name": "D. Vergani"
                    },
                    {
                        "name": "G. Vernardos"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "P. Vielzeuf"
                    },
                    {
                        "name": "N. A. Walton"
                    },
                    {
                        "name": "J. Wilde"
                    },
                    {
                        "name": "D. Scott"
                    }
                ],
                "author_detail": {
                    "name": "D. Scott"
                },
                "author": "D. Scott",
                "arxiv_comment": "Paper submitted as part of the A&A Special Issue `Euclid Quick Data\n  Release (Q1), 16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15315v1",
                "updated": "2025-03-19T15:27:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    27,
                    10,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:27:10Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    27,
                    10,
                    2,
                    78,
                    0
                ],
                "title": "Euclid Quick Data Release (Q1). A probabilistic classification of\n  quenched galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid Quick Data Release (Q1). A probabilistic classification of\n  quenched galaxies"
                },
                "summary": "Investigating the drivers of the quenching of star formation in galaxies is\nkey to understanding their evolution. The Euclid mission will provide rich\nspatial and spectral data from optical to infrared wavelengths for millions of\ngalaxies, enabling precise measurements of their star formation histories.\nUsing the first Euclid Quick Data Release (Q1), we developed a probabilistic\nclassification framework, that combines the average specific star-formation\nrate ($\\rm sSFR_\\tau$) inferred over two timescales ($\\tau={10^8,10^9}$ yr), to\ncategorize galaxies as `Ageing' (secularly evolving), `Quenched' (recently\nhalted star formation), or `Retired' (dominated by old stars). We validated\nthis methodology using synthetic observations from the IllustrisTNG simulation.\nTwo classification methods were employed: a probabilistic approach, integrating\nposterior distributions, and a model-driven method optimizing sample purity and\ncompleteness using IllustrisTNG. At $z<0.1$ and $M_\\ast \\gtrsim 3\\times10^{8}\\,\nM_\\odot$, we obtain Euclid class fractions of 68-72%, 8-17%, and 14-19% for\nAgeing, Quenched, and Retired populations, respectively, consistent with\nprevious studies. The evolution with redshift shows increasing/decreasing\nfraction of Ageing/Retired galaxies. The fraction of quenched systems shows a\nweaker dependence on stellar mass and redshift, varying between 5% and 15%. We\nanalysed the mass-size-metallicity relation for each population. Ageing\ngalaxies generally exhibit disc morphologies and low metallicities. Retired\ngalaxies show compact structures and enhanced chemical enrichment, while\nQuenched galaxies form an intermediate population, more compact and chemically\nevolved than Ageing systems. This work demonstrates Euclid's great potential\nfor elucidating the physical nature of the quenching mechanisms that govern\ngalaxy evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the drivers of the quenching of star formation in galaxies is\nkey to understanding their evolution. The Euclid mission will provide rich\nspatial and spectral data from optical to infrared wavelengths for millions of\ngalaxies, enabling precise measurements of their star formation histories.\nUsing the first Euclid Quick Data Release (Q1), we developed a probabilistic\nclassification framework, that combines the average specific star-formation\nrate ($\\rm sSFR_\\tau$) inferred over two timescales ($\\tau={10^8,10^9}$ yr), to\ncategorize galaxies as `Ageing' (secularly evolving), `Quenched' (recently\nhalted star formation), or `Retired' (dominated by old stars). We validated\nthis methodology using synthetic observations from the IllustrisTNG simulation.\nTwo classification methods were employed: a probabilistic approach, integrating\nposterior distributions, and a model-driven method optimizing sample purity and\ncompleteness using IllustrisTNG. At $z<0.1$ and $M_\\ast \\gtrsim 3\\times10^{8}\\,\nM_\\odot$, we obtain Euclid class fractions of 68-72%, 8-17%, and 14-19% for\nAgeing, Quenched, and Retired populations, respectively, consistent with\nprevious studies. The evolution with redshift shows increasing/decreasing\nfraction of Ageing/Retired galaxies. The fraction of quenched systems shows a\nweaker dependence on stellar mass and redshift, varying between 5% and 15%. We\nanalysed the mass-size-metallicity relation for each population. Ageing\ngalaxies generally exhibit disc morphologies and low metallicities. Retired\ngalaxies show compact structures and enhanced chemical enrichment, while\nQuenched galaxies form an intermediate population, more compact and chemically\nevolved than Ageing systems. This work demonstrates Euclid's great potential\nfor elucidating the physical nature of the quenching mechanisms that govern\ngalaxy evolution."
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "P. Corcho-Caballero"
                    },
                    {
                        "name": "Y. Ascasibar"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "C. C. Lovell"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "C. Cleland"
                    },
                    {
                        "name": "F. Fontanot"
                    },
                    {
                        "name": "C. Tortora"
                    },
                    {
                        "name": "L. V. E. Koopmans"
                    },
                    {
                        "name": "T. Moutard"
                    },
                    {
                        "name": "C. Laigle"
                    },
                    {
                        "name": "A. Nersesian"
                    },
                    {
                        "name": "F. Shankar"
                    },
                    {
                        "name": "N. Aghanim"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "H. Aussel"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "A. Balestra"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "A. Bonchi"
                    },
                    {
                        "name": "D. Bonino"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "G. Cañas-Herrera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "A. Costille"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "M. Cropper"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "A. M. Di Giorgio"
                    },
                    {
                        "name": "C. Dolding"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "A. Ealet"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "S. Fotopoulou"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "B. R. Granett"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "L. Guzzo"
                    },
                    {
                        "name": "S. Gwyn"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "I. M. Hook"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "P. Hudelot"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "E. Keihänen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "K. Kuijken"
                    },
                    {
                        "name": "M. Kümmel"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "Q. Le Boulc'h"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "D. Le Mignant"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "S. Maurogordato"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "J. W. Nightingale"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "A. G. Sánchez"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "J. A. Schewtschenko"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "M. Scodeggio"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Crespí"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "A. Zacchei"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "F. M. Zerbi"
                    },
                    {
                        "name": "I. A. Zinchenko"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "V. Allevato"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "E. Bozzo"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "D. Di Ferdinando"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "M. Huertas-Company"
                    },
                    {
                        "name": "J. Martín-Fleitas"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "M. Pöntinen"
                    },
                    {
                        "name": "C. Porciani"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "S. Alvi"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "S. Anselmi"
                    },
                    {
                        "name": "M. Archidiacono"
                    },
                    {
                        "name": "F. Atrio-Barandela"
                    },
                    {
                        "name": "K. Benson"
                    },
                    {
                        "name": "D. Bertacca"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "L. Blot"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "M. L. Brown"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "A. Calabro"
                    },
                    {
                        "name": "F. Caro"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "F. Cogato"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "O. Cucciati"
                    },
                    {
                        "name": "S. Davini"
                    },
                    {
                        "name": "F. De Paolis"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "A. Díaz-Sánchez"
                    },
                    {
                        "name": "J. J. Diaz"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "A. Enia"
                    },
                    {
                        "name": "Y. Fang"
                    },
                    {
                        "name": "A. G. Ferrari"
                    },
                    {
                        "name": "P. G. Ferreira"
                    },
                    {
                        "name": "A. Finoguenov"
                    },
                    {
                        "name": "A. Fontana"
                    },
                    {
                        "name": "A. Franco"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "T. Gasparetto"
                    },
                    {
                        "name": "V. Gautard"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "F. Gianotti"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "C. M. Gutierrez"
                    },
                    {
                        "name": "A. Hall"
                    },
                    {
                        "name": "W. G. Hartley"
                    },
                    {
                        "name": "S. Hemmati"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "V. Kansal"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "K. Kiiveri"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "J. Le Graet"
                    },
                    {
                        "name": "L. Legrand"
                    },
                    {
                        "name": "M. Lembo"
                    },
                    {
                        "name": "F. Lepori"
                    },
                    {
                        "name": "G. Leroy"
                    },
                    {
                        "name": "G. F. Lesci"
                    },
                    {
                        "name": "J. Lesgourgues"
                    },
                    {
                        "name": "L. Leuzzi"
                    },
                    {
                        "name": "T. I. Liaudat"
                    },
                    {
                        "name": "S. J. Liu"
                    },
                    {
                        "name": "A. Loureiro"
                    },
                    {
                        "name": "J. Macias-Perez"
                    },
                    {
                        "name": "G. Maggio"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "E. A. Magnier"
                    },
                    {
                        "name": "C. Mancini"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "M. Miluzio"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "G. Morgante"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Nesseris"
                    },
                    {
                        "name": "F. Passalacqua"
                    },
                    {
                        "name": "K. Paterson"
                    },
                    {
                        "name": "L. Patrizii"
                    },
                    {
                        "name": "A. Pisani"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "S. Quai"
                    },
                    {
                        "name": "M. Radovich"
                    },
                    {
                        "name": "P. -F. Rocci"
                    },
                    {
                        "name": "S. Sacquegna"
                    },
                    {
                        "name": "M. Sahlén"
                    },
                    {
                        "name": "D. B. Sanders"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "C. Scarlata"
                    },
                    {
                        "name": "J. Schaye"
                    },
                    {
                        "name": "A. Schneider"
                    },
                    {
                        "name": "D. Sciotti"
                    },
                    {
                        "name": "E. Sellentin"
                    },
                    {
                        "name": "L. C. Smith"
                    },
                    {
                        "name": "S. A. Stanford"
                    },
                    {
                        "name": "K. Tanidis"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "C. Valieri"
                    },
                    {
                        "name": "A. Venhola"
                    },
                    {
                        "name": "D. Vergani"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "P. Vielzeuf"
                    },
                    {
                        "name": "N. A. Walton"
                    },
                    {
                        "name": "J. R. Weaver"
                    },
                    {
                        "name": "J. G. Sorce"
                    }
                ],
                "author_detail": {
                    "name": "J. G. Sorce"
                },
                "author": "J. G. Sorce",
                "arxiv_comment": "Paper submitted as part of the A&A Special Issue `Euclid Quick Data\n  Release (Q1)', 24 pages, 11 figures (+ 2 appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12167v2",
                "updated": "2025-03-19T15:23:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    23,
                    29,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-15T15:11:17Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    15,
                    11,
                    17,
                    5,
                    74,
                    0
                ],
                "title": "PLM: Efficient Peripheral Language Models Hardware-Co-Designed for\n  Ubiquitous Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLM: Efficient Peripheral Language Models Hardware-Co-Designed for\n  Ubiquitous Computing"
                },
                "summary": "While scaling laws have been continuously validated in large language models\n(LLMs) with increasing model parameters, the inherent tension between the\ninference demands of LLMs and the limited resources of edge devices poses a\ncritical challenge to the development of edge intelligence. Recently, numerous\nsmall language models have emerged, aiming to distill the capabilities of LLMs\ninto smaller footprints. However, these models often retain the fundamental\narchitectural principles of their larger counterparts, still imposing\nconsiderable strain on the storage and bandwidth capacities of edge devices. In\nthis paper, we introduce the PLM, a Peripheral Language Model, developed\nthrough a co-design process that jointly optimizes model architecture and edge\nsystem constraints. The PLM utilizes a Multi-head Latent Attention mechanism\nand employs the squared ReLU activation function to encourage sparsity, thereby\nreducing peak memory footprint during inference. During training, we collect\nand reorganize open-source datasets, implement a multi-phase training strategy,\nand empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning\nrate scheduler. Additionally, we incorporate Reinforcement Learning from Human\nFeedback (RLHF) by adopting the ARIES preference learning approach. Following a\ntwo-phase SFT process, this method yields performance gains of 2% in general\ntasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel\narchitecture, evaluation results demonstrate that PLM outperforms existing\nsmall language models trained on publicly available data while maintaining the\nlowest number of activated parameters. Furthermore, deployment across various\nedge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis,\nvalidates PLM's suitability for peripheral applications. The PLM series models\nare publicly available at https://github.com/plm-team/PLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While scaling laws have been continuously validated in large language models\n(LLMs) with increasing model parameters, the inherent tension between the\ninference demands of LLMs and the limited resources of edge devices poses a\ncritical challenge to the development of edge intelligence. Recently, numerous\nsmall language models have emerged, aiming to distill the capabilities of LLMs\ninto smaller footprints. However, these models often retain the fundamental\narchitectural principles of their larger counterparts, still imposing\nconsiderable strain on the storage and bandwidth capacities of edge devices. In\nthis paper, we introduce the PLM, a Peripheral Language Model, developed\nthrough a co-design process that jointly optimizes model architecture and edge\nsystem constraints. The PLM utilizes a Multi-head Latent Attention mechanism\nand employs the squared ReLU activation function to encourage sparsity, thereby\nreducing peak memory footprint during inference. During training, we collect\nand reorganize open-source datasets, implement a multi-phase training strategy,\nand empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning\nrate scheduler. Additionally, we incorporate Reinforcement Learning from Human\nFeedback (RLHF) by adopting the ARIES preference learning approach. Following a\ntwo-phase SFT process, this method yields performance gains of 2% in general\ntasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel\narchitecture, evaluation results demonstrate that PLM outperforms existing\nsmall language models trained on publicly available data while maintaining the\nlowest number of activated parameters. Furthermore, deployment across various\nedge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis,\nvalidates PLM's suitability for peripheral applications. The PLM series models\nare publicly available at https://github.com/plm-team/PLM."
                },
                "authors": [
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Wenxin Zhao"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel M. Ni"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09454v2",
                "updated": "2025-03-19T15:23:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    23,
                    4,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-12T14:57:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    57,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "Explicit Learning and the LLM in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Learning and the LLM in Machine Translation"
                },
                "summary": "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs."
                },
                "authors": [
                    {
                        "name": "Malik Marmonier"
                    },
                    {
                        "name": "Rachel Bawden"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15301v1",
                "updated": "2025-03-19T15:22:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    22,
                    58,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:22:58Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    22,
                    58,
                    2,
                    78,
                    0
                ],
                "title": "aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in\n  Repository-level Code Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in\n  Repository-level Code Completion"
                },
                "summary": "Repository-level code completion aims to complete code based on the long\ncontexts of the repository. Existing studies extract long contexts from the\nrepository as inputs and leverage Large Language Models (LLMs) to generate\ncode. However, we reveal a severe limitation of LLMs, i.e., LLMs may ignore the\ninformation within long contexts in code completion. In other words, even the\ncontexts contain useful information (e.g., relevant APIs or similar code), LLMs\nmay fail to utilize this information. We think this limitation is caused by an\ninherent bias in LLMs, i.e., relying on nearby contexts and ignoring long-range\ncontexts. To address this, we propose a novel fine-tuning approach named CoLT.\nThe core idea of CoLT is to provide explicit supervision signals, which\nemphasize that long-range contexts may hold relevant information. Specifically,\nCoLT proposes a reinforcement learning-based training, which explicitly\nencourages models to utilize the information within long contexts and punishes\nmodels for ignoring long contexts. To support CoLT, we release CoLT-132K, a\nlarge-scale dataset with 132k samples across four languages, each containing\nlong-context inputs. We apply CoLT to a popular LLM - aiXcoder-7B and release\naiXcoder-7B-v2. We conduct extensive experiments on CoLT-132K and a public\nbenchmark - CrossCodeEval. Our experiments yield the results: 1. Effectiveness.\nCoLT substantially improves aiXcoder-7B. aiXcoder-7B-v2 outperforms aiXcoder-7B\nby up to 44% in exact match. aiXcoder-7B-v2 becomes the state-of-the-art 7B\nmodel in code completion and even surpasses larger models. 2. Generalizability.\nThe capability learned by CoLT can generalize to new languages. Besides, CoLT\nis model-agnostic and effectively improves multiple LLMs. 3. Enhanced Context\nUtilization Capability. CoLT significantly improves the capability of LLMs in\nutilizing the relevant information within long contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code completion aims to complete code based on the long\ncontexts of the repository. Existing studies extract long contexts from the\nrepository as inputs and leverage Large Language Models (LLMs) to generate\ncode. However, we reveal a severe limitation of LLMs, i.e., LLMs may ignore the\ninformation within long contexts in code completion. In other words, even the\ncontexts contain useful information (e.g., relevant APIs or similar code), LLMs\nmay fail to utilize this information. We think this limitation is caused by an\ninherent bias in LLMs, i.e., relying on nearby contexts and ignoring long-range\ncontexts. To address this, we propose a novel fine-tuning approach named CoLT.\nThe core idea of CoLT is to provide explicit supervision signals, which\nemphasize that long-range contexts may hold relevant information. Specifically,\nCoLT proposes a reinforcement learning-based training, which explicitly\nencourages models to utilize the information within long contexts and punishes\nmodels for ignoring long contexts. To support CoLT, we release CoLT-132K, a\nlarge-scale dataset with 132k samples across four languages, each containing\nlong-context inputs. We apply CoLT to a popular LLM - aiXcoder-7B and release\naiXcoder-7B-v2. We conduct extensive experiments on CoLT-132K and a public\nbenchmark - CrossCodeEval. Our experiments yield the results: 1. Effectiveness.\nCoLT substantially improves aiXcoder-7B. aiXcoder-7B-v2 outperforms aiXcoder-7B\nby up to 44% in exact match. aiXcoder-7B-v2 becomes the state-of-the-art 7B\nmodel in code completion and even surpasses larger models. 2. Generalizability.\nThe capability learned by CoLT can generalize to new languages. Besides, CoLT\nis model-agnostic and effectively improves multiple LLMs. 3. Enhanced Context\nUtilization Capability. CoLT significantly improves the capability of LLMs in\nutilizing the relevant information within long contexts."
                },
                "authors": [
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Xianjie Shi"
                    },
                    {
                        "name": "He Zong"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Siyuan Jiang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15299v1",
                "updated": "2025-03-19T15:21:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    48,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:21:48Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    48,
                    2,
                    78,
                    0
                ],
                "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside-Out: Hidden Factual Knowledge in LLMs"
                },
                "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."
                },
                "authors": [
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Idan Szpector"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13857v2",
                "updated": "2025-03-19T15:21:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    6,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-18T03:14:23Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    3,
                    14,
                    23,
                    1,
                    77,
                    0
                ],
                "title": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations"
                },
                "summary": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate systematic incorporation of preprint\narticles in evidence-based medicine, supporting researchers in more effective\nevaluation and utilization of preprint resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate systematic incorporation of preprint\narticles in evidence-based medicine, supporting researchers in more effective\nevaluation and utilization of preprint resources."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jiayi Tong"
                    },
                    {
                        "name": "Haoyuan Wang"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Ziyang Hu"
                    },
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Christopher J. Lindsell"
                    },
                    {
                        "name": "Michael J. Pencina"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Chuan Hong"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Hong"
                },
                "author": "Chuan Hong",
                "arxiv_comment": "28 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15297v1",
                "updated": "2025-03-19T15:18:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    18,
                    5,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:18:05Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    18,
                    5,
                    2,
                    78,
                    0
                ],
                "title": "Probabilistic Delay Forecasting in 5G Using Recurrent and\n  Attention-Based Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Delay Forecasting in 5G Using Recurrent and\n  Attention-Based Architectures"
                },
                "summary": "With the emergence of new application areas such as cyber-physical systems\nand human-in-the-loop applications ensuring a specific level of end-to-end\nnetwork latency with high reliability (e.g., 99.9%) is becoming increasingly\ncritical. To align wireless links with these reliability requirements, it is\nessential to analyze and control network latency in terms of its full\nprobability distribution. However, in a wireless link, the distribution may\nvary over time, making this task particularly challenging. We propose\npredicting the latency distribution using state-of-the-art data-driven\ntechniques that leverage historical network information. Our approach tokenizes\nnetwork state information and processes it using temporal deep-learning\narchitectures-namely LSTM and Transformer models-to capture both short- and\nlong-term delay dependencies. These models output parameters for a chosen\nparametric density via a mixture density network with Gaussian mixtures,\nyielding multi-step probabilistic forecasts of future delays. To validate our\nproposed approach, we implemented and tested these methods using a\ntime-synchronized, SDR-based OpenAirInterface 5G testbed to collect and\npreprocess network-delay data. Our experiments show that the Transformer model\nachieves lower negative log-likelihood and mean absolute error than both LSTM\nand feed-forward baselines in challenging scenarios, while also providing\ninsights into model complexity and training/inference overhead. This framework\nenables more informed decision-making for adaptive scheduling and resource\nallocation, paving the way toward enhanced QoS in evolving 5G and 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of new application areas such as cyber-physical systems\nand human-in-the-loop applications ensuring a specific level of end-to-end\nnetwork latency with high reliability (e.g., 99.9%) is becoming increasingly\ncritical. To align wireless links with these reliability requirements, it is\nessential to analyze and control network latency in terms of its full\nprobability distribution. However, in a wireless link, the distribution may\nvary over time, making this task particularly challenging. We propose\npredicting the latency distribution using state-of-the-art data-driven\ntechniques that leverage historical network information. Our approach tokenizes\nnetwork state information and processes it using temporal deep-learning\narchitectures-namely LSTM and Transformer models-to capture both short- and\nlong-term delay dependencies. These models output parameters for a chosen\nparametric density via a mixture density network with Gaussian mixtures,\nyielding multi-step probabilistic forecasts of future delays. To validate our\nproposed approach, we implemented and tested these methods using a\ntime-synchronized, SDR-based OpenAirInterface 5G testbed to collect and\npreprocess network-delay data. Our experiments show that the Transformer model\nachieves lower negative log-likelihood and mean absolute error than both LSTM\nand feed-forward baselines in challenging scenarios, while also providing\ninsights into model complexity and training/inference overhead. This framework\nenables more informed decision-making for adaptive scheduling and resource\nallocation, paving the way toward enhanced QoS in evolving 5G and 6G networks."
                },
                "authors": [
                    {
                        "name": "Samie Mostafavi"
                    },
                    {
                        "name": "Gourav Prateek Sharma"
                    },
                    {
                        "name": "Ahmad Traboulsi"
                    },
                    {
                        "name": "James Gross"
                    }
                ],
                "author_detail": {
                    "name": "James Gross"
                },
                "author": "James Gross",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.13859v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.13859v3",
                "updated": "2025-03-19T15:10:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    10,
                    32,
                    2,
                    78,
                    0
                ],
                "published": "2023-07-25T23:13:21Z",
                "published_parsed": [
                    2023,
                    7,
                    25,
                    23,
                    13,
                    21,
                    1,
                    206,
                    0
                ],
                "title": "Random (Un)rounding : Vulnerabilities in Discrete Attribute Disclosure\n  in the 2021 Canadian Census",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random (Un)rounding : Vulnerabilities in Discrete Attribute Disclosure\n  in the 2021 Canadian Census"
                },
                "summary": "The 2021 Canadian census is notable for using a unique form of privacy,\nrandom rounding, which independently and probabilistically rounds discrete\nnumerical attribute values. In this work, we explore how hierarchical summative\ncorrelation between discrete variables allows for both probabilistic and exact\nsolutions to attribute values in the 2021 Canadian Census disclosure. We\ndemonstrate that, in some cases, it is possible to \"unround\" and extract the\noriginal private values before rounding, both in the presence and absence of\nprovided population invariants. Using these methods, we expose the exact value\nof 624 previously private attributes in the 2021 Canadian census disclosure. We\nalso infer the potential values of more than 1000 private attributes with a\nhigh probability of correctness. Finally, we propose how a simple solution\nbased on unbounded discrete noise can effectively negate exact unrounding while\nmaintaining high utility in the final product.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 2021 Canadian census is notable for using a unique form of privacy,\nrandom rounding, which independently and probabilistically rounds discrete\nnumerical attribute values. In this work, we explore how hierarchical summative\ncorrelation between discrete variables allows for both probabilistic and exact\nsolutions to attribute values in the 2021 Canadian Census disclosure. We\ndemonstrate that, in some cases, it is possible to \"unround\" and extract the\noriginal private values before rounding, both in the presence and absence of\nprovided population invariants. Using these methods, we expose the exact value\nof 624 previously private attributes in the 2021 Canadian census disclosure. We\nalso infer the potential values of more than 1000 private attributes with a\nhigh probability of correctness. Finally, we propose how a simple solution\nbased on unbounded discrete noise can effectively negate exact unrounding while\nmaintaining high utility in the final product."
                },
                "authors": [
                    {
                        "name": "Christopher West"
                    },
                    {
                        "name": "Vecna"
                    },
                    {
                        "name": "Raiyan Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Raiyan Chowdhury"
                },
                "author": "Raiyan Chowdhury",
                "arxiv_comment": "Small formatting revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.13859v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.13859v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15289v1",
                "updated": "2025-03-19T15:09:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    9,
                    39,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:09:39Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    9,
                    39,
                    2,
                    78,
                    0
                ],
                "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification"
                },
                "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation."
                },
                "authors": [
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Min Xiao"
                    },
                    {
                        "name": "Yining Wang"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Chengqing Zong"
                    }
                ],
                "author_detail": {
                    "name": "Chengqing Zong"
                },
                "author": "Chengqing Zong",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15282v1",
                "updated": "2025-03-19T15:02:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    2,
                    7,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:02:07Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    2,
                    7,
                    2,
                    78,
                    0
                ],
                "title": "SENAI: Towards Software Engineering Native Generative Artificial\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SENAI: Towards Software Engineering Native Generative Artificial\n  Intelligence"
                },
                "summary": "Large Language Models have significantly advanced the field of code\ngeneration, demonstrating the ability to produce functionally correct code\nsnippets. However, advancements in generative AI for code overlook foundational\nSoftware Engineering (SE) principles such as modularity, and single\nresponsibility, and concepts such as cohesion and coupling which are critical\nfor creating maintainable, scalable, and robust software systems. These\nconcepts are missing in pipelines that start with pre-training and end with the\nevaluation using benchmarks.\n  This vision paper argues for the integration of SE knowledge into LLMs to\nenhance their capability to understand, analyze, and generate code and other SE\nartifacts following established SE knowledge. The aim is to propose a new\ndirection where LLMs can move beyond mere functional accuracy to perform\ngenerative tasks that require adherence to SE principles and best practices. In\naddition, given the interactive nature of these conversational models, we\npropose using Bloom's Taxonomy as a framework to assess the extent to which\nthey internalize SE knowledge. The proposed evaluation framework offers a sound\nand more comprehensive evaluation technique compared to existing approaches\nsuch as linear probing. Software engineering native generative models will not\nonly overcome the shortcomings present in current models but also pave the way\nfor the next generation of generative models capable of handling real-world\nsoftware engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have significantly advanced the field of code\ngeneration, demonstrating the ability to produce functionally correct code\nsnippets. However, advancements in generative AI for code overlook foundational\nSoftware Engineering (SE) principles such as modularity, and single\nresponsibility, and concepts such as cohesion and coupling which are critical\nfor creating maintainable, scalable, and robust software systems. These\nconcepts are missing in pipelines that start with pre-training and end with the\nevaluation using benchmarks.\n  This vision paper argues for the integration of SE knowledge into LLMs to\nenhance their capability to understand, analyze, and generate code and other SE\nartifacts following established SE knowledge. The aim is to propose a new\ndirection where LLMs can move beyond mere functional accuracy to perform\ngenerative tasks that require adherence to SE principles and best practices. In\naddition, given the interactive nature of these conversational models, we\npropose using Bloom's Taxonomy as a framework to assess the extent to which\nthey internalize SE knowledge. The proposed evaluation framework offers a sound\nand more comprehensive evaluation technique compared to existing approaches\nsuch as linear probing. Software engineering native generative models will not\nonly overcome the shortcomings present in current models but also pave the way\nfor the next generation of generative models capable of handling real-world\nsoftware engineering."
                },
                "authors": [
                    {
                        "name": "Mootez Saad"
                    },
                    {
                        "name": "José Antonio Hernández López"
                    },
                    {
                        "name": "Boqi Chen"
                    },
                    {
                        "name": "Neil Ernst"
                    },
                    {
                        "name": "Dániel Varró"
                    },
                    {
                        "name": "Tushar Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Sharma"
                },
                "author": "Tushar Sharma",
                "arxiv_comment": "5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02191v3",
                "updated": "2025-03-19T14:54:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    54,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-04T02:01:37Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    2,
                    1,
                    37,
                    1,
                    63,
                    0
                ],
                "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Predicting Derailment in Toxic Conversations on GitHub"
                },
                "summary": "Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches."
                },
                "authors": [
                    {
                        "name": "Mia Mohammad Imran"
                    },
                    {
                        "name": "Robert Zita"
                    },
                    {
                        "name": "Rebekah Copeland"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    },
                    {
                        "name": "Rahat Rizvi Rahman"
                    },
                    {
                        "name": "Kostadin Damevski"
                    }
                ],
                "author_detail": {
                    "name": "Kostadin Damevski"
                },
                "author": "Kostadin Damevski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13393v2",
                "updated": "2025-03-19T14:49:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    49,
                    31,
                    2,
                    78,
                    0
                ],
                "published": "2024-12-18T00:10:00Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    0,
                    10,
                    0,
                    2,
                    353,
                    0
                ],
                "title": "MaskHand: Generative Masked Modeling for Robust Hand Mesh Reconstruction\n  in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaskHand: Generative Masked Modeling for Robust Hand Mesh Reconstruction\n  in the Wild"
                },
                "summary": "Reconstructing a 3D hand mesh from a single RGB image is challenging due to\ncomplex articulations, self-occlusions, and depth ambiguities. Traditional\ndiscriminative methods, which learn a deterministic mapping from a 2D image to\na single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D\nmapping. To address this challenge, we propose MaskHand, a novel generative\nmasked model for hand mesh recovery that synthesizes plausible 3D hand meshes\nby learning and sampling from the probabilistic distribution of the ambiguous\n2D-to-3D mapping process. MaskHand consists of two key components: (1) a\nVQ-MANO, which encodes 3D hand articulations as discrete pose tokens in a\nlatent space, and (2) a Context-Guided Masked Transformer that randomly masks\nout pose tokens and learns their joint distribution, conditioned on corrupted\ntoken sequence, image context, and 2D pose cues. This learned distribution\nfacilitates confidence-guided sampling during inference, producing mesh\nreconstructions with low uncertainty and high precision. Extensive evaluations\non benchmark and real-world datasets demonstrate that MaskHand achieves\nstate-of-the-art accuracy, robustness, and realism in 3D hand mesh\nreconstruction. Project website:\nhttps://m-usamasaleem.github.io/publication/MaskHand/MaskHand.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing a 3D hand mesh from a single RGB image is challenging due to\ncomplex articulations, self-occlusions, and depth ambiguities. Traditional\ndiscriminative methods, which learn a deterministic mapping from a 2D image to\na single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D\nmapping. To address this challenge, we propose MaskHand, a novel generative\nmasked model for hand mesh recovery that synthesizes plausible 3D hand meshes\nby learning and sampling from the probabilistic distribution of the ambiguous\n2D-to-3D mapping process. MaskHand consists of two key components: (1) a\nVQ-MANO, which encodes 3D hand articulations as discrete pose tokens in a\nlatent space, and (2) a Context-Guided Masked Transformer that randomly masks\nout pose tokens and learns their joint distribution, conditioned on corrupted\ntoken sequence, image context, and 2D pose cues. This learned distribution\nfacilitates confidence-guided sampling during inference, producing mesh\nreconstructions with low uncertainty and high precision. Extensive evaluations\non benchmark and real-world datasets demonstrate that MaskHand achieves\nstate-of-the-art accuracy, robustness, and realism in 3D hand mesh\nreconstruction. Project website:\nhttps://m-usamasaleem.github.io/publication/MaskHand/MaskHand.html."
                },
                "authors": [
                    {
                        "name": "Muhammad Usama Saleem"
                    },
                    {
                        "name": "Ekkasit Pinyoanuntapong"
                    },
                    {
                        "name": "Mayur Jagdishbhai Patel"
                    },
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Ahmed Helmy"
                    },
                    {
                        "name": "Srijan Das"
                    },
                    {
                        "name": "Pu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Pu Wang"
                },
                "author": "Pu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15272v1",
                "updated": "2025-03-19T14:46:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    46,
                    53,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:46:53Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    46,
                    53,
                    2,
                    78,
                    0
                ],
                "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with\n  Multi-Agent Collaboration"
                },
                "summary": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe."
                },
                "authors": [
                    {
                        "name": "David Wan"
                    },
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15268v1",
                "updated": "2025-03-19T14:44:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    44,
                    2,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:44:02Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    44,
                    2,
                    2,
                    78,
                    0
                ],
                "title": "Do Chains-of-Thoughts of Large Language Models Suffer from\n  Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Chains-of-Thoughts of Large Language Models Suffer from\n  Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?"
                },
                "summary": "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies."
                },
                "authors": [
                    {
                        "name": "Roberto Araya"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Araya"
                },
                "author": "Roberto Araya",
                "arxiv_comment": "24 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15030v2",
                "updated": "2025-03-19T14:33:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    33,
                    50,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-20T20:34:41Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    20,
                    34,
                    41,
                    3,
                    51,
                    0
                ],
                "title": "CHOIR: Chat-based Helper for Organizational Intelligence Repository",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHOIR: Chat-based Helper for Organizational Intelligence Repository"
                },
                "summary": "Modern organizations frequently rely on chat-based platforms (e.g., Slack,\nMicrosoft Teams, and Discord) for day-to-day communication and decision-making.\nAs conversations evolve, organizational knowledge can get buried, prompting\nrepeated searches and discussions. While maintaining shared documents, such as\nWiki articles for the organization, offers a partial solution, it requires\nmanual and timely efforts to keep it up to date, and it may not effectively\npreserve the social and contextual aspect of prior discussions. Moreover,\nreaching a consensus on document updates with relevant stakeholders can be\ntime-consuming and complex. To address these challenges, we introduce CHOIR\n(Chat-based Helper for Organizational Intelligence Repository), a chatbot that\nintegrates seamlessly with chat platforms. CHOIR automatically identifies and\nproposes edits to related documents, initiates discussions with relevant team\nmembers, and preserves contextual revision histories. By embedding knowledge\nmanagement directly into chat environments and leveraging LLMs, CHOIR\nsimplifies manual updates and supports consensus-driven editing based on\nmaintained context with revision histories. We plan to design, deploy, and\nevaluate CHOIR in the context of maintaining an organizational memory for a\nresearch lab. We describe the chatbot's motivation, design, and early\nimplementation to show how CHOIR streamlines collaborative document management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern organizations frequently rely on chat-based platforms (e.g., Slack,\nMicrosoft Teams, and Discord) for day-to-day communication and decision-making.\nAs conversations evolve, organizational knowledge can get buried, prompting\nrepeated searches and discussions. While maintaining shared documents, such as\nWiki articles for the organization, offers a partial solution, it requires\nmanual and timely efforts to keep it up to date, and it may not effectively\npreserve the social and contextual aspect of prior discussions. Moreover,\nreaching a consensus on document updates with relevant stakeholders can be\ntime-consuming and complex. To address these challenges, we introduce CHOIR\n(Chat-based Helper for Organizational Intelligence Repository), a chatbot that\nintegrates seamlessly with chat platforms. CHOIR automatically identifies and\nproposes edits to related documents, initiates discussions with relevant team\nmembers, and preserves contextual revision histories. By embedding knowledge\nmanagement directly into chat environments and leveraging LLMs, CHOIR\nsimplifies manual updates and supports consensus-driven editing based on\nmaintained context with revision histories. We plan to design, deploy, and\nevaluate CHOIR in the context of maintaining an organizational memory for a\nresearch lab. We describe the chatbot's motivation, design, and early\nimplementation to show how CHOIR streamlines collaborative document management."
                },
                "authors": [
                    {
                        "name": "Sangwook Lee"
                    },
                    {
                        "name": "Adnan Abbas"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Sang Won Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Won Lee"
                },
                "author": "Sang Won Lee",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05040v2",
                "updated": "2025-03-19T14:27:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    27,
                    29,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-07T16:07:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    16,
                    7,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "GaussRender: Learning 3D Occupancy with Gaussian Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaussRender: Learning 3D Occupancy with Gaussian Rendering"
                },
                "summary": "Understanding the 3D geometry and semantics of driving scenes is critical for\nsafe autonomous driving. Recent advances in 3D occupancy prediction have\nimproved scene representation but often suffer from spatial inconsistencies,\nleading to floating artifacts and poor surface localization. Existing\nvoxel-wise losses (e.g., cross-entropy) fail to enforce geometric coherence. In\nthis paper, we propose GaussRender, a module that improves 3D occupancy\nlearning by enforcing projective consistency. Our key idea is to project both\npredicted and ground-truth 3D occupancy into 2D camera views, where we apply\nsupervision. Our method penalizes 3D configurations that produce inconsistent\n2D projections, thereby enforcing a more coherent 3D structure. To achieve this\nefficiently, we leverage differentiable rendering with Gaussian splatting.\nGaussRender seamlessly integrates with existing architectures while maintaining\nefficiency and requiring no inference-time modifications. Extensive evaluations\non multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes,\nSSCBench-KITTI360) demonstrate that GaussRender significantly improves\ngeometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc,\nSymphonies), achieving state-of-the-art results, particularly on\nsurface-sensitive metrics. The code is open-sourced at\nhttps://github.com/valeoai/GaussRender.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the 3D geometry and semantics of driving scenes is critical for\nsafe autonomous driving. Recent advances in 3D occupancy prediction have\nimproved scene representation but often suffer from spatial inconsistencies,\nleading to floating artifacts and poor surface localization. Existing\nvoxel-wise losses (e.g., cross-entropy) fail to enforce geometric coherence. In\nthis paper, we propose GaussRender, a module that improves 3D occupancy\nlearning by enforcing projective consistency. Our key idea is to project both\npredicted and ground-truth 3D occupancy into 2D camera views, where we apply\nsupervision. Our method penalizes 3D configurations that produce inconsistent\n2D projections, thereby enforcing a more coherent 3D structure. To achieve this\nefficiently, we leverage differentiable rendering with Gaussian splatting.\nGaussRender seamlessly integrates with existing architectures while maintaining\nefficiency and requiring no inference-time modifications. Extensive evaluations\non multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes,\nSSCBench-KITTI360) demonstrate that GaussRender significantly improves\ngeometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc,\nSymphonies), achieving state-of-the-art results, particularly on\nsurface-sensitive metrics. The code is open-sourced at\nhttps://github.com/valeoai/GaussRender."
                },
                "authors": [
                    {
                        "name": "Loïck Chambon"
                    },
                    {
                        "name": "Eloi Zablocki"
                    },
                    {
                        "name": "Alexandre Boulch"
                    },
                    {
                        "name": "Mickaël Chen"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15252v1",
                "updated": "2025-03-19T14:26:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    26,
                    9,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:26:09Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    26,
                    9,
                    2,
                    78,
                    0
                ],
                "title": "Efficient allocation of image recognition and LLM tasks on multi-GPU\n  system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient allocation of image recognition and LLM tasks on multi-GPU\n  system"
                },
                "summary": "This work is concerned with the evaluation of the performance of\nparallelization of learning and tuning processes for image classification and\nlarge language models. For machine learning model in image recognition, various\nparallelization methods are developed based on different hardware and software\nscenarios: simple data parallelism, distributed data parallelism, and\ndistributed processing. A detailed description of presented strategies is\ngiven, highlighting the challenges and benefits of their application.\nFurthermore, the impact of different dataset types on the tuning process of\nlarge language models is investigated. Experiments show to what extent the task\ntype affects the iteration time in a multi-GPU environment, offering valuable\ninsights into the optimal data utilization strategies to improve model\nperformance. Furthermore, this study leverages the built-in parallelization\nmechanisms of PyTorch that can facilitate these tasks. Furthermore, performance\nprofiling is incorporated into the study to thoroughly evaluate the impact of\nmemory and communication operations during the training/tuning procedure. Test\nscenarios are developed and tested with numerous benchmarks on the NVIDIA H100\narchitecture showing efficiency through selected metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work is concerned with the evaluation of the performance of\nparallelization of learning and tuning processes for image classification and\nlarge language models. For machine learning model in image recognition, various\nparallelization methods are developed based on different hardware and software\nscenarios: simple data parallelism, distributed data parallelism, and\ndistributed processing. A detailed description of presented strategies is\ngiven, highlighting the challenges and benefits of their application.\nFurthermore, the impact of different dataset types on the tuning process of\nlarge language models is investigated. Experiments show to what extent the task\ntype affects the iteration time in a multi-GPU environment, offering valuable\ninsights into the optimal data utilization strategies to improve model\nperformance. Furthermore, this study leverages the built-in parallelization\nmechanisms of PyTorch that can facilitate these tasks. Furthermore, performance\nprofiling is incorporated into the study to thoroughly evaluate the impact of\nmemory and communication operations during the training/tuning procedure. Test\nscenarios are developed and tested with numerous benchmarks on the NVIDIA H100\narchitecture showing efficiency through selected metrics."
                },
                "authors": [
                    {
                        "name": "Marcin Lawenda"
                    },
                    {
                        "name": "Krzesimir Samborski"
                    },
                    {
                        "name": "Kyrylo Khloponin"
                    },
                    {
                        "name": "Łukasz Szustak"
                    }
                ],
                "author_detail": {
                    "name": "Łukasz Szustak"
                },
                "author": "Łukasz Szustak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14286v2",
                "updated": "2025-03-19T14:25:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    25,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-18T14:23:37Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    23,
                    37,
                    1,
                    77,
                    0
                ],
                "title": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs"
                },
                "summary": "We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance."
                },
                "authors": [
                    {
                        "name": "Nicolas Le Roux"
                    },
                    {
                        "name": "Marc G. Bellemare"
                    },
                    {
                        "name": "Jonathan Lebensold"
                    },
                    {
                        "name": "Arnaud Bergeron"
                    },
                    {
                        "name": "Joshua Greaves"
                    },
                    {
                        "name": "Alex Fréchette"
                    },
                    {
                        "name": "Carolyne Pelletier"
                    },
                    {
                        "name": "Eric Thibodeau-Laufer"
                    },
                    {
                        "name": "Sándor Toth"
                    },
                    {
                        "name": "Sam Work"
                    }
                ],
                "author_detail": {
                    "name": "Sam Work"
                },
                "author": "Sam Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15249v1",
                "updated": "2025-03-19T14:24:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    24,
                    19,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:24:19Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    24,
                    19,
                    2,
                    78,
                    0
                ],
                "title": "The Effects of iBGP Convergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effects of iBGP Convergence"
                },
                "summary": "Analyzing violations of forwarding properties is a classic networking\nproblem. However, existing work is either tailored to the steady state -- and\nnot to transient states during iBGP convergence -- or does analyze transient\nviolations but with inaccurate proxies, like control-plane convergence, or\nwithout precise control over the different impact factors.\n  We address this gap with a measurement framework that controllably and\naccurately measures transient violation times in realistic network deployments.\nThe framework relies on a programmable switch to flexibly emulate diverse\ntopologies and gain traffic visibility at all links -- enabling accurately\ninferring violation times of any forwarding property. Using the framework, we\nanalyze 50 network scenarios on a topology with 12 real routers, and show how\nfactors like the network configuration and BGP event affect transient violation\ntimes. Further, we shed light on less-known aspects of BGP convergence,\nincluding that transient violations can start before the trigger event, or that\nkeeping a backup route advertised at all times can increase violation times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing violations of forwarding properties is a classic networking\nproblem. However, existing work is either tailored to the steady state -- and\nnot to transient states during iBGP convergence -- or does analyze transient\nviolations but with inaccurate proxies, like control-plane convergence, or\nwithout precise control over the different impact factors.\n  We address this gap with a measurement framework that controllably and\naccurately measures transient violation times in realistic network deployments.\nThe framework relies on a programmable switch to flexibly emulate diverse\ntopologies and gain traffic visibility at all links -- enabling accurately\ninferring violation times of any forwarding property. Using the framework, we\nanalyze 50 network scenarios on a topology with 12 real routers, and show how\nfactors like the network configuration and BGP event affect transient violation\ntimes. Further, we shed light on less-known aspects of BGP convergence,\nincluding that transient violations can start before the trigger event, or that\nkeeping a backup route advertised at all times can increase violation times."
                },
                "authors": [
                    {
                        "name": "Roland Schmid"
                    },
                    {
                        "name": "Tibor Schneider"
                    },
                    {
                        "name": "Georgia Fragkouli"
                    },
                    {
                        "name": "Laurent Vanbever"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Vanbever"
                },
                "author": "Laurent Vanbever",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15248v1",
                "updated": "2025-03-19T14:23:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    23,
                    22,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:23:22Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    23,
                    22,
                    2,
                    78,
                    0
                ],
                "title": "Automated Non-Functional Requirements Generation in Software Engineering\n  with Large Language Models: A Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Non-Functional Requirements Generation in Software Engineering\n  with Large Language Models: A Comparative Study"
                },
                "summary": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering."
                },
                "authors": [
                    {
                        "name": "Jomar Thomas Almonte"
                    },
                    {
                        "name": "Santhosh Anitha Boominathan"
                    },
                    {
                        "name": "Nathalia Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Nathalia Nascimento"
                },
                "author": "Nathalia Nascimento",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15242v1",
                "updated": "2025-03-19T14:19:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    19,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:19:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    19,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?"
                },
                "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time."
                },
                "authors": [
                    {
                        "name": "Pierre Chambon"
                    },
                    {
                        "name": "Baptiste Roziere"
                    },
                    {
                        "name": "Benoit Sagot"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Synnaeve"
                },
                "author": "Gabriel Synnaeve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13413v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13413v3",
                "updated": "2025-03-19T14:18:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    18,
                    1,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-17T17:42:51Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    42,
                    51,
                    0,
                    76,
                    0
                ],
                "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO."
                },
                "authors": [
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Jingjing Chen"
                    },
                    {
                        "name": "Libo Qin"
                    }
                ],
                "author_detail": {
                    "name": "Libo Qin"
                },
                "author": "Libo Qin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13413v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13413v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12896v2",
                "updated": "2025-03-19T14:15:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    15,
                    12,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-18T14:32:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks"
                },
                "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers."
                },
                "authors": [
                    {
                        "name": "Eva Sánchez Salido"
                    },
                    {
                        "name": "Julio Gonzalo"
                    },
                    {
                        "name": "Guillermo Marco"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Marco"
                },
                "author": "Guillermo Marco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15235v1",
                "updated": "2025-03-19T14:13:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    13,
                    2,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:13:02Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    13,
                    2,
                    2,
                    78,
                    0
                ],
                "title": "Exploring Large Language Models for Word Games:Who is the Spy?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Models for Word Games:Who is the Spy?"
                },
                "summary": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy."
                },
                "authors": [
                    {
                        "name": "Chentian Wei"
                    },
                    {
                        "name": "Jiewei Chen"
                    },
                    {
                        "name": "Jinzhu Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhu Xu"
                },
                "author": "Jinzhu Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15231v1",
                "updated": "2025-03-19T14:08:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    8,
                    47,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:08:47Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    8,
                    47,
                    2,
                    78,
                    0
                ],
                "title": "When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code\n  Generation Just as It Helps Developers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code\n  Generation Just as It Helps Developers?"
                },
                "summary": "Retrieval-augmented generation (RAG) has increasingly shown its power in\nextending large language models' (LLMs') capability beyond their pre-trained\nknowledge. Existing works have shown that RAG can help with software\ndevelopment tasks such as code generation, code update, and test generation.\nYet, the effectiveness of adapting LLMs to fast-evolving or less common API\nlibraries using RAG remains unknown. To bridge this gap, we take an initial\nstep to study this unexplored yet practical setting - when developers code with\na less common library, they often refer to its API documentation; likewise,\nwhen LLMs are allowed to look up API documentation via RAG, to what extent can\nLLMs be advanced? To mimic such a setting, we select four less common\nopen-source Python libraries with a total of 1017 eligible APIs. We study the\nfactors that affect the effectiveness of using the documentation of less common\nAPI libraries as additional knowledge for retrieval and generation. Our\nintensive study yields interesting findings: (1) RAG helps improve LLMs'\nperformance by 83%-220%. (2) Example code contributes the most to advance LLMs,\ninstead of the descriptive texts and parameter lists in the API documentation.\n(3) LLMs could sometimes tolerate mild noises (typos in description or\nincorrect parameters) by referencing their pre-trained knowledge or document\ncontext. Finally, we suggest that developers pay more attention to the quality\nand diversity of the code examples in the API documentation. The study sheds\nlight on future low-code software development workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has increasingly shown its power in\nextending large language models' (LLMs') capability beyond their pre-trained\nknowledge. Existing works have shown that RAG can help with software\ndevelopment tasks such as code generation, code update, and test generation.\nYet, the effectiveness of adapting LLMs to fast-evolving or less common API\nlibraries using RAG remains unknown. To bridge this gap, we take an initial\nstep to study this unexplored yet practical setting - when developers code with\na less common library, they often refer to its API documentation; likewise,\nwhen LLMs are allowed to look up API documentation via RAG, to what extent can\nLLMs be advanced? To mimic such a setting, we select four less common\nopen-source Python libraries with a total of 1017 eligible APIs. We study the\nfactors that affect the effectiveness of using the documentation of less common\nAPI libraries as additional knowledge for retrieval and generation. Our\nintensive study yields interesting findings: (1) RAG helps improve LLMs'\nperformance by 83%-220%. (2) Example code contributes the most to advance LLMs,\ninstead of the descriptive texts and parameter lists in the API documentation.\n(3) LLMs could sometimes tolerate mild noises (typos in description or\nincorrect parameters) by referencing their pre-trained knowledge or document\ncontext. Finally, we suggest that developers pay more attention to the quality\nand diversity of the code examples in the API documentation. The study sheds\nlight on future low-code software development workflows."
                },
                "authors": [
                    {
                        "name": "Jingyi Chen"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Jiasi Shen"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19634v2",
                "updated": "2025-03-19T13:55:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    55,
                    33,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-26T23:57:34Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    23,
                    57,
                    34,
                    2,
                    57,
                    0
                ],
                "title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language\n  Models (VLMs) via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language\n  Models (VLMs) via Reinforcement Learning"
                },
                "summary": "Reasoning is a critical frontier for advancing medical image analysis, where\ntransparency and trustworthiness play a central role in both clinician trust\nand regulatory approval. Although Medical Visual Language Models (VLMs) show\npromise for radiological tasks, most existing VLMs merely produce final answers\nwithout revealing the underlying reasoning. To address this gap, we introduce\nMedVLM-R1, a medical VLM that explicitly generates natural language reasoning\nto enhance transparency and trustworthiness. Instead of relying on supervised\nfine-tuning (SFT), which often suffers from overfitting to training\ndistributions and fails to foster genuine reasoning, MedVLM-R1 employs a\nreinforcement learning framework that incentivizes the model to discover\nhuman-interpretable reasoning paths without using any reasoning references.\nDespite limited training data (600 visual question answering samples) and model\nparameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI,\nCT, and X-ray benchmarks, outperforming larger models trained on over a million\nsamples. It also demonstrates robust domain generalization under\nout-of-distribution tasks. By unifying medical image analysis with explicit\nreasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable\nAI in clinical practice. Inference model is available at:\nhttps://huggingface.co/JZPeterPan/MedVLM-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a critical frontier for advancing medical image analysis, where\ntransparency and trustworthiness play a central role in both clinician trust\nand regulatory approval. Although Medical Visual Language Models (VLMs) show\npromise for radiological tasks, most existing VLMs merely produce final answers\nwithout revealing the underlying reasoning. To address this gap, we introduce\nMedVLM-R1, a medical VLM that explicitly generates natural language reasoning\nto enhance transparency and trustworthiness. Instead of relying on supervised\nfine-tuning (SFT), which often suffers from overfitting to training\ndistributions and fails to foster genuine reasoning, MedVLM-R1 employs a\nreinforcement learning framework that incentivizes the model to discover\nhuman-interpretable reasoning paths without using any reasoning references.\nDespite limited training data (600 visual question answering samples) and model\nparameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI,\nCT, and X-ray benchmarks, outperforming larger models trained on over a million\nsamples. It also demonstrates robust domain generalization under\nout-of-distribution tasks. By unifying medical image analysis with explicit\nreasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable\nAI in clinical practice. Inference model is available at:\nhttps://huggingface.co/JZPeterPan/MedVLM-R1."
                },
                "authors": [
                    {
                        "name": "Jiazhen Pan"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Junde Wu"
                    },
                    {
                        "name": "Fenglin Liu"
                    },
                    {
                        "name": "Jiayuan Zhu"
                    },
                    {
                        "name": "Hongwei Bran Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Cheng Ouyang"
                    },
                    {
                        "name": "Daniel Rueckert"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Rueckert"
                },
                "author": "Daniel Rueckert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18423v2",
                "updated": "2025-03-19T13:45:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    45,
                    31,
                    2,
                    78,
                    0
                ],
                "published": "2025-01-30T15:25:30Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    25,
                    30,
                    3,
                    30,
                    0
                ],
                "title": "DeepExtractor: Time-domain reconstruction of signals and glitches in\n  gravitational wave data with deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepExtractor: Time-domain reconstruction of signals and glitches in\n  gravitational wave data with deep learning"
                },
                "summary": "Gravitational wave (GW) interferometers, detect faint signals from distant\nastrophysical events, such as binary black hole mergers. However, their high\nsensitivity also makes them susceptible to background noise, which can obscure\nthese signals. This noise often includes transient artifacts called \"glitches\"\nthat can mimic astrophysical signals or mask their characteristics. Fast and\naccurate reconstruction of both signals and glitches is crucial for reliable\nscientific inference. In this study, we present DeepExtractor, a deep learning\nframework designed to reconstruct signals and glitches with power exceeding\ninterferometer noise, regardless of their source. We design DeepExtractor to\nmodel the inherent noise distribution of GW interferometers, following\nconventional assumptions that the noise is Gaussian and stationary over short\ntime scales. It operates by predicting and subtracting the noise component of\nthe data, retaining only the clean reconstruction. Our approach achieves\nsuperior generalization capabilities for arbitrary signals and glitches\ncompared to methods that directly map inputs to the clean training waveforms.\nWe validate DeepExtractor's effectiveness through three experiments: (1)\nreconstructing simulated glitches injected into simulated detector noise, (2)\ncomparing performance with the state-of-the-art BayesWave algorithm, and (3)\nanalyzing real data from the Gravity Spy dataset to demonstrate effective\nglitch subtraction from LIGO strain data. DeepExtractor achieves a median\nmismatch of only 0.9% for simulated glitches, outperforming several deep\nlearning baselines. Additionally, DeepExtractor surpasses BayesWave in glitch\nrecovery, offering a dramatic computational speedup by reconstructing one\nglitch sample in approx. 0.1 seconds on a CPU, compared to BayesWave's\nprocessing time of approx. one hour per glitch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational wave (GW) interferometers, detect faint signals from distant\nastrophysical events, such as binary black hole mergers. However, their high\nsensitivity also makes them susceptible to background noise, which can obscure\nthese signals. This noise often includes transient artifacts called \"glitches\"\nthat can mimic astrophysical signals or mask their characteristics. Fast and\naccurate reconstruction of both signals and glitches is crucial for reliable\nscientific inference. In this study, we present DeepExtractor, a deep learning\nframework designed to reconstruct signals and glitches with power exceeding\ninterferometer noise, regardless of their source. We design DeepExtractor to\nmodel the inherent noise distribution of GW interferometers, following\nconventional assumptions that the noise is Gaussian and stationary over short\ntime scales. It operates by predicting and subtracting the noise component of\nthe data, retaining only the clean reconstruction. Our approach achieves\nsuperior generalization capabilities for arbitrary signals and glitches\ncompared to methods that directly map inputs to the clean training waveforms.\nWe validate DeepExtractor's effectiveness through three experiments: (1)\nreconstructing simulated glitches injected into simulated detector noise, (2)\ncomparing performance with the state-of-the-art BayesWave algorithm, and (3)\nanalyzing real data from the Gravity Spy dataset to demonstrate effective\nglitch subtraction from LIGO strain data. DeepExtractor achieves a median\nmismatch of only 0.9% for simulated glitches, outperforming several deep\nlearning baselines. Additionally, DeepExtractor surpasses BayesWave in glitch\nrecovery, offering a dramatic computational speedup by reconstructing one\nglitch sample in approx. 0.1 seconds on a CPU, compared to BayesWave's\nprocessing time of approx. one hour per glitch."
                },
                "authors": [
                    {
                        "name": "Tom Dooney"
                    },
                    {
                        "name": "Harsh Narola"
                    },
                    {
                        "name": "Stefano Bromuri"
                    },
                    {
                        "name": "R. Lyana Curier"
                    },
                    {
                        "name": "Chris Van Den Broeck"
                    },
                    {
                        "name": "Sarah Caudill"
                    },
                    {
                        "name": "Daniel Stanley Tan"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Stanley Tan"
                },
                "author": "Daniel Stanley Tan",
                "arxiv_comment": "22 pages, 16 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09283v2",
                "updated": "2025-03-19T13:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    41,
                    21,
                    2,
                    78,
                    0
                ],
                "published": "2024-07-12T14:13:59Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    14,
                    13,
                    59,
                    4,
                    194,
                    0
                ],
                "title": "DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection"
                },
                "summary": "Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog)."
                },
                "authors": [
                    {
                        "name": "Sangpil Youm"
                    },
                    {
                        "name": "Brodie Mather"
                    },
                    {
                        "name": "Chathuri Jayaweera"
                    },
                    {
                        "name": "Juliana Prada"
                    },
                    {
                        "name": "Bonnie Dorr"
                    }
                ],
                "author_detail": {
                    "name": "Bonnie Dorr"
                },
                "author": "Bonnie Dorr",
                "arxiv_doi": "10.1007/978-3-031-70239-6_29",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-70239-6_29",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.09283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 6 figures, Accepted to The 29th International Conference on\n  Natural Language & Information Systems (NLDB 2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15195v1",
                "updated": "2025-03-19T13:33:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    33,
                    29,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T13:33:29Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    33,
                    29,
                    2,
                    78,
                    0
                ],
                "title": "Benchmarking Large Language Models for Handwritten Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Handwritten Text Recognition"
                },
                "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions."
                },
                "authors": [
                    {
                        "name": "Giorgia Crosilla"
                    },
                    {
                        "name": "Lukas Klic"
                    },
                    {
                        "name": "Giovanni Colavizza"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Colavizza"
                },
                "author": "Giovanni Colavizza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15191v1",
                "updated": "2025-03-19T13:21:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    21,
                    49,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T13:21:49Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    21,
                    49,
                    2,
                    78,
                    0
                ],
                "title": "Optimizing Retrieval Strategies for Financial Question Answering\n  Documents in Retrieval-Augmented Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Retrieval Strategies for Financial Question Answering\n  Documents in Retrieval-Augmented Generation Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework to\nmitigate hallucinations in Large Language Models (LLMs), yet its overall\nperformance is dependent on the underlying retrieval system. In the finance\ndomain, documents such as 10-K reports pose distinct challenges due to\ndomain-specific vocabulary and multi-hierarchical tabular data. In this work,\nwe introduce an efficient, end-to-end RAG pipeline that enhances retrieval for\nfinancial documents through a three-phase approach: pre-retrieval, retrieval,\nand post-retrieval. In the pre-retrieval phase, various query and corpus\npreprocessing techniques are employed to enrich input data. During the\nretrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with\ndomain-specific knowledge and implemented a hybrid retrieval strategy that\ncombines dense and sparse representations. Finally, the post-retrieval phase\nleverages Direct Preference Optimization (DPO) training and document selection\nmethods to further refine the results. Evaluations on seven financial question\nanswering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA,\nand MultiHiertt-demonstrate substantial improvements in retrieval performance,\nleading to more accurate and contextually appropriate generation. These\nfindings highlight the critical role of tailored retrieval techniques in\nadvancing the effectiveness of RAG systems for financial applications. A fully\nreplicable pipeline is available on GitHub:\nhttps://github.com/seohyunwoo-0407/GAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework to\nmitigate hallucinations in Large Language Models (LLMs), yet its overall\nperformance is dependent on the underlying retrieval system. In the finance\ndomain, documents such as 10-K reports pose distinct challenges due to\ndomain-specific vocabulary and multi-hierarchical tabular data. In this work,\nwe introduce an efficient, end-to-end RAG pipeline that enhances retrieval for\nfinancial documents through a three-phase approach: pre-retrieval, retrieval,\nand post-retrieval. In the pre-retrieval phase, various query and corpus\npreprocessing techniques are employed to enrich input data. During the\nretrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with\ndomain-specific knowledge and implemented a hybrid retrieval strategy that\ncombines dense and sparse representations. Finally, the post-retrieval phase\nleverages Direct Preference Optimization (DPO) training and document selection\nmethods to further refine the results. Evaluations on seven financial question\nanswering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA,\nand MultiHiertt-demonstrate substantial improvements in retrieval performance,\nleading to more accurate and contextually appropriate generation. These\nfindings highlight the critical role of tailored retrieval techniques in\nadvancing the effectiveness of RAG systems for financial applications. A fully\nreplicable pipeline is available on GitHub:\nhttps://github.com/seohyunwoo-0407/GAR."
                },
                "authors": [
                    {
                        "name": "Sejong Kim"
                    },
                    {
                        "name": "Hyunseo Song"
                    },
                    {
                        "name": "Hyunwoo Seo"
                    },
                    {
                        "name": "Hyunjun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunjun Kim"
                },
                "author": "Hyunjun Kim",
                "arxiv_comment": "15 pages, 3 figures, 11 tables. Accepted at ICLR 2025 Workshop on\n  Advances in Financial AI. Code available at\n  https://github.com/seohyunwoo-0407/GAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2207.04053v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2207.04053v4",
                "updated": "2025-03-19T13:15:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    15,
                    56,
                    2,
                    78,
                    0
                ],
                "published": "2022-07-08T10:37:22Z",
                "published_parsed": [
                    2022,
                    7,
                    8,
                    10,
                    37,
                    22,
                    4,
                    189,
                    0
                ],
                "title": "On the Need and Applicability of Causality for Fairness: A Unified\n  Framework for AI Auditing and Legal Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Need and Applicability of Causality for Fairness: A Unified\n  Framework for AI Auditing and Legal Analysis"
                },
                "summary": "As Artificial Intelligence (AI) increasingly influences decisions in critical\nsocietal sectors, understanding and establishing causality becomes essential\nfor evaluating the fairness of automated systems. This article explores the\nsignificance of causal reasoning in addressing algorithmic discrimination,\nemphasizing both legal and societal perspectives. By reviewing landmark cases\nand regulatory frameworks, particularly within the European Union, we\nillustrate the challenges inherent in proving causal claims when confronted\nwith opaque AI decision-making processes. The discussion outlines practical\nobstacles and methodological limitations in applying causal inference to\nreal-world fairness scenarios, proposing actionable solutions to enhance\ntransparency, accountability, and fairness in algorithm-driven decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Artificial Intelligence (AI) increasingly influences decisions in critical\nsocietal sectors, understanding and establishing causality becomes essential\nfor evaluating the fairness of automated systems. This article explores the\nsignificance of causal reasoning in addressing algorithmic discrimination,\nemphasizing both legal and societal perspectives. By reviewing landmark cases\nand regulatory frameworks, particularly within the European Union, we\nillustrate the challenges inherent in proving causal claims when confronted\nwith opaque AI decision-making processes. The discussion outlines practical\nobstacles and methodological limitations in applying causal inference to\nreal-world fairness scenarios, proposing actionable solutions to enhance\ntransparency, accountability, and fairness in algorithm-driven decisions."
                },
                "authors": [
                    {
                        "name": "Ruta Binkyte"
                    },
                    {
                        "name": "Ljupcho Grozdanovski"
                    },
                    {
                        "name": "Sami Zhioua"
                    }
                ],
                "author_detail": {
                    "name": "Sami Zhioua"
                },
                "author": "Sami Zhioua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2207.04053v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2207.04053v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15176v1",
                "updated": "2025-03-19T13:02:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    2,
                    1,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T13:02:01Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    2,
                    1,
                    2,
                    78,
                    0
                ],
                "title": "A Review on Large Language Models for Visual Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review on Large Language Models for Visual Analytics"
                },
                "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration."
                },
                "authors": [
                    {
                        "name": "Navya Sonal Agarwal"
                    },
                    {
                        "name": "Sanjay Kumar Sonbhadra"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Kumar Sonbhadra"
                },
                "author": "Sanjay Kumar Sonbhadra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19464v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19464v4",
                "updated": "2025-03-20T03:32:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    3,
                    32,
                    3,
                    3,
                    79,
                    0
                ],
                "published": "2024-10-25T10:48:41Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    48,
                    41,
                    4,
                    299,
                    0
                ],
                "title": "LOCAL: Learning with Orientation Matrix to Infer Causal Structure from\n  Time Series Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOCAL: Learning with Orientation Matrix to Infer Causal Structure from\n  Time Series Data"
                },
                "summary": "Discovering the underlying Directed Acyclic Graph (DAG) from time series\nobservational data is highly challenging due to the dynamic nature and complex\nnonlinear interactions between variables. Existing methods typically search for\nthe optimal DAG by optimizing an objective function but face scalability\nchallenges, as their computational demands grow exponentially with the\ndimensional expansion of variables. To this end, we propose LOCAL, a highly\nefficient, easy-to-implement, and constraint-free method for recovering dynamic\ncausal structures. LOCAL is the first attempt to formulate a quasi-maximum\nlikelihood-based score function for learning the dynamic DAG equivalent to the\nground truth. Building on this, we introduce two adaptive modules that enhance\nthe algebraic characterization of acyclicity: Asymptotic Causal Mask Learning\n(ACML) and Dynamic Graph Parameter Learning (DGPL). ACML constructs causal\nmasks using learnable priority vectors and the Gumbel-Sigmoid function,\nensuring DAG formation while optimizing computational efficiency. DGPL\ntransforms causal learning into decomposed matrix products, capturing dynamic\ncausal structure in high-dimensional data and improving interpretability.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nLOCAL significantly outperforms existing methods and highlight LOCAL's\npotential as a robust and efficient method for dynamic causal discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering the underlying Directed Acyclic Graph (DAG) from time series\nobservational data is highly challenging due to the dynamic nature and complex\nnonlinear interactions between variables. Existing methods typically search for\nthe optimal DAG by optimizing an objective function but face scalability\nchallenges, as their computational demands grow exponentially with the\ndimensional expansion of variables. To this end, we propose LOCAL, a highly\nefficient, easy-to-implement, and constraint-free method for recovering dynamic\ncausal structures. LOCAL is the first attempt to formulate a quasi-maximum\nlikelihood-based score function for learning the dynamic DAG equivalent to the\nground truth. Building on this, we introduce two adaptive modules that enhance\nthe algebraic characterization of acyclicity: Asymptotic Causal Mask Learning\n(ACML) and Dynamic Graph Parameter Learning (DGPL). ACML constructs causal\nmasks using learnable priority vectors and the Gumbel-Sigmoid function,\nensuring DAG formation while optimizing computational efficiency. DGPL\ntransforms causal learning into decomposed matrix products, capturing dynamic\ncausal structure in high-dimensional data and improving interpretability.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nLOCAL significantly outperforms existing methods and highlight LOCAL's\npotential as a robust and efficient method for dynamic causal discovery."
                },
                "authors": [
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Boyang Qiang"
                    },
                    {
                        "name": "Xiaoyu Guo"
                    },
                    {
                        "name": "Weiwei Xing"
                    },
                    {
                        "name": "Yue Cheng"
                    },
                    {
                        "name": "Witold Pedrycz"
                    }
                ],
                "author_detail": {
                    "name": "Witold Pedrycz"
                },
                "author": "Witold Pedrycz",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19464v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19464v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15169v1",
                "updated": "2025-03-19T12:51:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    51,
                    52,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T12:51:52Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    51,
                    52,
                    2,
                    78,
                    0
                ],
                "title": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks"
                },
                "summary": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs."
                },
                "authors": [
                    {
                        "name": "Yuting Guo"
                    },
                    {
                        "name": "Abeed Sarker"
                    }
                ],
                "author_detail": {
                    "name": "Abeed Sarker"
                },
                "author": "Abeed Sarker",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15168v1",
                "updated": "2025-03-19T12:50:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    50,
                    40,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T12:50:40Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    50,
                    40,
                    2,
                    78,
                    0
                ],
                "title": "World Models in Artificial Intelligence: Sensing, Learning, and\n  Reasoning Like a Child",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World Models in Artificial Intelligence: Sensing, Learning, and\n  Reasoning Like a Child"
                },
                "summary": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Javier Del Ser"
                    },
                    {
                        "name": "Jesus L. Lobo"
                    },
                    {
                        "name": "Heimo Müller"
                    },
                    {
                        "name": "Andreas Holzinger"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Holzinger"
                },
                "author": "Andreas Holzinger",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15167v1",
                "updated": "2025-03-19T12:47:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    47,
                    50,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T12:47:50Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    47,
                    50,
                    2,
                    78,
                    0
                ],
                "title": "Volumetric Reconstruction From Partial Views for Task-Oriented Grasping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Volumetric Reconstruction From Partial Views for Task-Oriented Grasping"
                },
                "summary": "Object affordance and volumetric information are essential in devising\neffective grasping strategies under task-specific constraints. This paper\npresents an approach for inferring suitable grasping strategies from limited\npartial views of an object. To achieve this, a recurrent generative adversarial\nnetwork (R-GAN) was proposed by incorporating a recurrent generator with long\nshort-term memory (LSTM) units for it to process a variable number of depth\nscans. To determine object affordances, the AffordPose knowledge dataset is\nutilized as prior knowledge. Affordance retrieving is defined by the volume\nsimilarity measured via Chamfer Distance and action similarities. A Proximal\nPolicy Optimization (PPO) reinforcement learning model is further implemented\nto refine the retrieved grasp strategies for task-oriented grasping. The\nretrieved grasp strategies were evaluated on a dual-arm mobile manipulation\nrobot with an overall grasping accuracy of 89% for four tasks: lift, handle\ngrasp, wrap grasp, and press.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object affordance and volumetric information are essential in devising\neffective grasping strategies under task-specific constraints. This paper\npresents an approach for inferring suitable grasping strategies from limited\npartial views of an object. To achieve this, a recurrent generative adversarial\nnetwork (R-GAN) was proposed by incorporating a recurrent generator with long\nshort-term memory (LSTM) units for it to process a variable number of depth\nscans. To determine object affordances, the AffordPose knowledge dataset is\nutilized as prior knowledge. Affordance retrieving is defined by the volume\nsimilarity measured via Chamfer Distance and action similarities. A Proximal\nPolicy Optimization (PPO) reinforcement learning model is further implemented\nto refine the retrieved grasp strategies for task-oriented grasping. The\nretrieved grasp strategies were evaluated on a dual-arm mobile manipulation\nrobot with an overall grasping accuracy of 89% for four tasks: lift, handle\ngrasp, wrap grasp, and press."
                },
                "authors": [
                    {
                        "name": "Fujian Yan"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Hongsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng He"
                },
                "author": "Hongsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11509v2",
                "updated": "2025-03-19T12:42:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    42,
                    41,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-14T15:29:58Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    29,
                    58,
                    4,
                    73,
                    0
                ],
                "title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis"
                },
                "summary": "With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available."
                },
                "authors": [
                    {
                        "name": "Jonas Belouadi"
                    },
                    {
                        "name": "Eddy Ilg"
                    },
                    {
                        "name": "Margret Keuper"
                    },
                    {
                        "name": "Hideki Tanaka"
                    },
                    {
                        "name": "Masao Utiyama"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    }
                ],
                "author_detail": {
                    "name": "Simone Paolo Ponzetto"
                },
                "author": "Simone Paolo Ponzetto",
                "arxiv_comment": "Project page: https://github.com/potamides/DeTikZify",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11302v2",
                "updated": "2025-03-19T12:17:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    17,
                    11,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-14T11:11:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Are formal and functional linguistic mechanisms dissociated in language\n  models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are formal and functional linguistic mechanisms dissociated in language\n  models?"
                },
                "summary": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist."
                },
                "authors": [
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "arxiv_comment": "35 pages, 10 figures, 3 tables. Code available at\n  https://github.com/hannamw/formal-functional-dissociation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15150v1",
                "updated": "2025-03-19T12:16:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    16,
                    54,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T12:16:54Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    16,
                    54,
                    2,
                    78,
                    0
                ],
                "title": "Preference Construction: A Bayesian Interactive Preference Elicitation\n  Framework Based on Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Construction: A Bayesian Interactive Preference Elicitation\n  Framework Based on Monte Carlo Tree Search"
                },
                "summary": "We present a novel preference learning framework to capture participant\npreferences efficiently within limited interaction rounds. It involves three\nmain contributions. First, we develop a variational Bayesian approach to infer\nthe participant's preference model by estimating posterior distributions and\nmanaging uncertainty from limited information. Second, we propose an adaptive\nquestioning policy that maximizes cumulative uncertainty reduction, formulating\nquestioning as a finite Markov decision process and using Monte Carlo Tree\nSearch to prioritize promising question trajectories. By considering long-term\neffects and leveraging the efficiency of the Bayesian approach, the policy\navoids shortsightedness. Third, we apply the framework to Multiple Criteria\nDecision Aiding, with pairwise comparison as the preference information and an\nadditive value function as the preference model. We integrate the\nreparameterization trick to address high-variance issues, enhancing robustness\nand efficiency. Computational studies on real-world and synthetic datasets\ndemonstrate the framework's practical usability, outperforming baselines in\ncapturing preferences and achieving superior uncertainty reduction within\nlimited interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel preference learning framework to capture participant\npreferences efficiently within limited interaction rounds. It involves three\nmain contributions. First, we develop a variational Bayesian approach to infer\nthe participant's preference model by estimating posterior distributions and\nmanaging uncertainty from limited information. Second, we propose an adaptive\nquestioning policy that maximizes cumulative uncertainty reduction, formulating\nquestioning as a finite Markov decision process and using Monte Carlo Tree\nSearch to prioritize promising question trajectories. By considering long-term\neffects and leveraging the efficiency of the Bayesian approach, the policy\navoids shortsightedness. Third, we apply the framework to Multiple Criteria\nDecision Aiding, with pairwise comparison as the preference information and an\nadditive value function as the preference model. We integrate the\nreparameterization trick to address high-variance issues, enhancing robustness\nand efficiency. Computational studies on real-world and synthetic datasets\ndemonstrate the framework's practical usability, outperforming baselines in\ncapturing preferences and achieving superior uncertainty reduction within\nlimited interactions."
                },
                "authors": [
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jiapeng Liu"
                    },
                    {
                        "name": "Milosz Kadziński"
                    },
                    {
                        "name": "Xiuwu Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiuwu Liao"
                },
                "author": "Xiuwu Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11280v2",
                "updated": "2025-03-19T12:16:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    16,
                    42,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-14T10:39:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "High-Dimensional Interlingual Representations of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Interlingual Representations of Large Language Models"
                },
                "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."
                },
                "authors": [
                    {
                        "name": "Bryan Wilie"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v3",
                "updated": "2025-03-19T12:15:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    15,
                    10,
                    2,
                    78,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "ReAttention: Training-Free Infinite Context with Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReAttention: Training-Free Infinite Context with Finite Attention Scope"
                },
                "summary": "The long-context capability of the Large Language Models (LLM) has made\nsignificant breakthroughs, but the maximum supported context length in length\nextrapolation remains a critical bottleneck limiting their practical\napplications. The constraint of context length in LLMs arises from the\nself-attention mechanism, which cannot effectively and efficiently capture the\nsemantic relationships within infinitely long contexts via the limited\npre-trained positional information and attention scope. In this work, we\npropose ReAttention, a training-free approach enabling LLM based on the\nself-attention mechanism to support an infinite context with a finite attention\nscope under sufficient memory resources. ReAttention performs the\nposition-agnostic top-$k$ attention before the ordinary position-aware\nself-attention, freeing LLMs from the length extrapolation issue. We validate\nthe performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and\ndemonstrate that it is on par with traditional methods. Furthermore, we also\napply ReAttention on mainstream LLMs, including LLaMA3.1-8B and\nMistral-v0.3-7B, enabling them to support context lengths of at least 1M and\neven expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M\nwithout any further training in Needle-In-A-Haystack tests. We also improve the\nefficiency of ReAttention with Triton and achieve an efficient extrapolation\nwithout additional overhead. The code is available at\nhttps://github.com/OpenMOSS/ReAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long-context capability of the Large Language Models (LLM) has made\nsignificant breakthroughs, but the maximum supported context length in length\nextrapolation remains a critical bottleneck limiting their practical\napplications. The constraint of context length in LLMs arises from the\nself-attention mechanism, which cannot effectively and efficiently capture the\nsemantic relationships within infinitely long contexts via the limited\npre-trained positional information and attention scope. In this work, we\npropose ReAttention, a training-free approach enabling LLM based on the\nself-attention mechanism to support an infinite context with a finite attention\nscope under sufficient memory resources. ReAttention performs the\nposition-agnostic top-$k$ attention before the ordinary position-aware\nself-attention, freeing LLMs from the length extrapolation issue. We validate\nthe performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and\ndemonstrate that it is on par with traditional methods. Furthermore, we also\napply ReAttention on mainstream LLMs, including LLaMA3.1-8B and\nMistral-v0.3-7B, enabling them to support context lengths of at least 1M and\neven expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M\nwithout any further training in Needle-In-A-Haystack tests. We also improve the\nefficiency of ReAttention with Triton and achieve an efficient extrapolation\nwithout additional overhead. The code is available at\nhttps://github.com/OpenMOSS/ReAttention."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "21 pages, 11 figures, Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07246v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07246v4",
                "updated": "2025-03-19T11:46:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    46,
                    58,
                    2,
                    78,
                    0
                ],
                "published": "2024-08-14T01:16:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B."
                },
                "authors": [
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Xunzhi Wang"
                    },
                    {
                        "name": "Zeying Hao"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Qian Tan"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yaotian Yang"
                    },
                    {
                        "name": "Xinrui Xiong"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "11 pages, updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07246v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07246v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15129v1",
                "updated": "2025-03-19T11:44:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    44,
                    47,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:44:47Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    44,
                    47,
                    2,
                    78,
                    0
                ],
                "title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code\n  Generation by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code\n  Generation by Large Language Models"
                },
                "summary": "This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation."
                },
                "authors": [
                    {
                        "name": "Man Fai Wong"
                    },
                    {
                        "name": "Chee Wei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chee Wei Tan"
                },
                "author": "Chee Wei Tan",
                "arxiv_doi": "10.1109/TBDATA.2024.3524104",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TBDATA.2024.3524104",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.15129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15128v1",
                "updated": "2025-03-19T11:42:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    42,
                    33,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:42:33Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    42,
                    33,
                    2,
                    78,
                    0
                ],
                "title": "Increasing the Robustness of the Fine-tuned Multilingual\n  Machine-Generated Text Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing the Robustness of the Fine-tuned Multilingual\n  Machine-Generated Text Detectors"
                },
                "summary": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data."
                },
                "authors": [
                    {
                        "name": "Dominik Macko"
                    },
                    {
                        "name": "Robert Moro"
                    },
                    {
                        "name": "Ivan Srba"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Srba"
                },
                "author": "Ivan Srba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15126v1",
                "updated": "2025-03-19T11:38:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    38,
                    14,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:38:14Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    38,
                    14,
                    2,
                    78,
                    0
                ],
                "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action\n  Segmentation"
                },
                "summary": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Haoyu Ji"
                    },
                    {
                        "name": "Bowen Chen"
                    },
                    {
                        "name": "Weihong Ren"
                    },
                    {
                        "name": "Wenze Huang"
                    },
                    {
                        "name": "Zhihao Yang"
                    },
                    {
                        "name": "Zhiyong Wang"
                    },
                    {
                        "name": "Honghai Liu"
                    }
                ],
                "author_detail": {
                    "name": "Honghai Liu"
                },
                "author": "Honghai Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16457v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16457v4",
                "updated": "2025-03-19T11:37:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    37,
                    27,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-23T06:16:23Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    6,
                    16,
                    23,
                    6,
                    54,
                    0
                ],
                "title": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge"
                },
                "summary": "Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Ji Hoon Hong"
                    },
                    {
                        "name": "Dong Won Jeon"
                    },
                    {
                        "name": "Ga-Yeon Baek"
                    },
                    {
                        "name": "Gyeong-Won Kwak"
                    },
                    {
                        "name": "Dong-Hee Lee"
                    },
                    {
                        "name": "Jisu Bae"
                    },
                    {
                        "name": "Chihoon Lee"
                    },
                    {
                        "name": "Yunseo Kim"
                    },
                    {
                        "name": "Seon-Jin Choi"
                    },
                    {
                        "name": "Jin-Seong Park"
                    },
                    {
                        "name": "Sung Beom Cho"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16457v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16457v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02273v2",
                "updated": "2025-03-19T11:32:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    32,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-01-04T12:30:14Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    12,
                    30,
                    14,
                    5,
                    4,
                    0
                ],
                "title": "Blind Training for Channel-Adaptive Digital Semantic Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blind Training for Channel-Adaptive Digital Semantic Communications"
                },
                "summary": "Semantic encoders and decoders for digital semantic communication (SC) often\nstruggle to adapt to variations in unpredictable channel environments and\ndiverse system designs. To address these challenges, this paper proposes a\nnovel framework for training semantic encoders and decoders to enable\nchannel-adaptive digital SC. The core idea is to use binary symmetric channel\n(BSC) as a universal representation of generic digital communications,\neliminating the need to specify channel environments or system designs. Based\non this idea, our framework employs parallel BSCs to equivalently model the\nrelationship between the encoder's output and the decoder's input. The bit-flip\nprobabilities of these BSCs are treated as trainable parameters during\nend-to-end training, with varying levels of regularization applied to address\ndiverse requirements in practical systems. The advantage of our framework is\njustified by developing a training-aware communication strategy for the\ninference stage. This strategy makes communication bit errors align with the\npre-trained bit-flip probabilities by adaptively selecting power and modulation\nlevels based on practical requirements and channel conditions. Simulation\nresults demonstrate that the proposed framework outperforms existing training\napproaches in terms of both task performance and power consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic encoders and decoders for digital semantic communication (SC) often\nstruggle to adapt to variations in unpredictable channel environments and\ndiverse system designs. To address these challenges, this paper proposes a\nnovel framework for training semantic encoders and decoders to enable\nchannel-adaptive digital SC. The core idea is to use binary symmetric channel\n(BSC) as a universal representation of generic digital communications,\neliminating the need to specify channel environments or system designs. Based\non this idea, our framework employs parallel BSCs to equivalently model the\nrelationship between the encoder's output and the decoder's input. The bit-flip\nprobabilities of these BSCs are treated as trainable parameters during\nend-to-end training, with varying levels of regularization applied to address\ndiverse requirements in practical systems. The advantage of our framework is\njustified by developing a training-aware communication strategy for the\ninference stage. This strategy makes communication bit errors align with the\npre-trained bit-flip probabilities by adaptively selecting power and modulation\nlevels based on practical requirements and channel conditions. Simulation\nresults demonstrate that the proposed framework outperforms existing training\napproaches in terms of both task performance and power consumption."
                },
                "authors": [
                    {
                        "name": "Yongjeong Oh"
                    },
                    {
                        "name": "Joohyuk Park"
                    },
                    {
                        "name": "Jinho Choi"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Yo-Seb Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Yo-Seb Jeon"
                },
                "author": "Yo-Seb Jeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02732v2",
                "updated": "2025-03-19T11:31:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    31,
                    31,
                    2,
                    78,
                    0
                ],
                "published": "2024-05-04T18:32:08Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    18,
                    32,
                    8,
                    5,
                    125,
                    0
                ],
                "title": "Recall Them All: Retrieval-Augmented Language Models for Long Object\n  List Extraction from Long Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recall Them All: Retrieval-Augmented Language Models for Long Object\n  List Extraction from Long Documents"
                },
                "summary": "Methods for relation extraction from text mostly focus on high precision, at\nthe cost of limited recall. High recall is crucial, though, to populate long\nlists of object entities that stand in a specific relation with a given\nsubject. Cues for relevant objects can be spread across many passages in long\ntexts. This poses the challenge of extracting long lists from long texts. We\npresent the L3X method which tackles the problem in two stages: (1)\nrecall-oriented generation using a large language model (LLM) with judicious\ntechniques for retrieval augmentation, and (2) precision-oriented\nscrutinization to validate or prune candidates. Our L3X method outperforms\nLLM-only generations by a substantial margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods for relation extraction from text mostly focus on high precision, at\nthe cost of limited recall. High recall is crucial, though, to populate long\nlists of object entities that stand in a specific relation with a given\nsubject. Cues for relevant objects can be spread across many passages in long\ntexts. This poses the challenge of extracting long lists from long texts. We\npresent the L3X method which tackles the problem in two stages: (1)\nrecall-oriented generation using a large language model (LLM) with judicious\ntechniques for retrieval augmentation, and (2) precision-oriented\nscrutinization to validate or prune candidates. Our L3X method outperforms\nLLM-only generations by a substantial margin."
                },
                "authors": [
                    {
                        "name": "Sneha Singhania"
                    },
                    {
                        "name": "Simon Razniewski"
                    },
                    {
                        "name": "Gerhard Weikum"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Weikum"
                },
                "author": "Gerhard Weikum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15117v1",
                "updated": "2025-03-19T11:21:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    21,
                    37,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:21:37Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    21,
                    37,
                    2,
                    78,
                    0
                ],
                "title": "Exploring Model Editing for LLM-based Aspect-Based Sentiment\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Model Editing for LLM-based Aspect-Based Sentiment\n  Classification"
                },
                "summary": "Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy."
                },
                "authors": [
                    {
                        "name": "Shichen Li"
                    },
                    {
                        "name": "Zhongqing Wang"
                    },
                    {
                        "name": "Zheyu Zhao"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "arxiv_comment": "AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15114v1",
                "updated": "2025-03-19T11:14:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    14,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:14:16Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    14,
                    16,
                    2,
                    78,
                    0
                ],
                "title": "DeCaFlow: A Deconfounding Causal Generative Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCaFlow: A Deconfounding Causal Generative Model"
                },
                "summary": "Causal generative models (CGMs) have recently emerged as capable approaches\nto simulate the causal mechanisms generating our observations, enabling causal\ninference. Unfortunately, existing approaches either are overly restrictive,\nassuming the absence of hidden confounders, or lack generality, being tailored\nto a particular query and graph. In this work, we introduce DeCaFlow, a CGM\nthat accounts for hidden confounders in a single amortized training process\nusing only observational data and the causal graph. Importantly, DeCaFlow can\nprovably identify all causal queries with a valid adjustment set or\nsufficiently informative proxy variables. Remarkably, for the first time to our\nknowledge, we show that a confounded counterfactual query is identifiable, and\nthus solvable by DeCaFlow, as long as its interventional counterpart is as\nwell. Our empirical results on diverse settings (including the Ecoli70 dataset,\nwith 3 independent hidden confounders, tens of observed variables and hundreds\nof causal queries) show that DeCaFlow outperforms existing approaches, while\ndemonstrating its out-of-the-box flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal generative models (CGMs) have recently emerged as capable approaches\nto simulate the causal mechanisms generating our observations, enabling causal\ninference. Unfortunately, existing approaches either are overly restrictive,\nassuming the absence of hidden confounders, or lack generality, being tailored\nto a particular query and graph. In this work, we introduce DeCaFlow, a CGM\nthat accounts for hidden confounders in a single amortized training process\nusing only observational data and the causal graph. Importantly, DeCaFlow can\nprovably identify all causal queries with a valid adjustment set or\nsufficiently informative proxy variables. Remarkably, for the first time to our\nknowledge, we show that a confounded counterfactual query is identifiable, and\nthus solvable by DeCaFlow, as long as its interventional counterpart is as\nwell. Our empirical results on diverse settings (including the Ecoli70 dataset,\nwith 3 independent hidden confounders, tens of observed variables and hundreds\nof causal queries) show that DeCaFlow outperforms existing approaches, while\ndemonstrating its out-of-the-box flexibility."
                },
                "authors": [
                    {
                        "name": "Alejandro Almodóvar"
                    },
                    {
                        "name": "Adrián Javaloy"
                    },
                    {
                        "name": "Juan Parras"
                    },
                    {
                        "name": "Santiago Zazo"
                    },
                    {
                        "name": "Isabel Valera"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Valera"
                },
                "author": "Isabel Valera",
                "arxiv_comment": "32 pages, 22 figures. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15113v1",
                "updated": "2025-03-19T11:13:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    13,
                    51,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:13:51Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    13,
                    51,
                    2,
                    78,
                    0
                ],
                "title": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles."
                },
                "authors": [
                    {
                        "name": "Benjamin Estermann"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15112v1",
                "updated": "2025-03-19T11:12:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    12,
                    53,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:12:53Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    12,
                    53,
                    2,
                    78,
                    0
                ],
                "title": "OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL\n  Generation"
                },
                "summary": "The automated generation of design RTL based on large language model (LLM)\nand natural language instructions has demonstrated great potential in agile\ncircuit design. However, the lack of datasets and benchmarks in the public\ndomain prevents the development and fair evaluation of LLM solutions. This\npaper highlights our latest advances in open datasets and benchmarks from three\nperspectives: (1) RTLLM 2.0, an updated benchmark assessing LLM's capability in\ndesign RTL generation. The benchmark is augmented to 50 hand-crafted designs.\nEach design provides the design description, test cases, and a correct RTL\ncode. (2) AssertEval, an open-source benchmark assessing the LLM's assertion\ngeneration capabilities for RTL verification. The benchmark includes 18\ndesigns, each providing specification, signal definition, and correct RTL code.\n(3) RTLCoder-Data, an extended open-source dataset with 80K instruction-code\ndata samples. Moreover, we propose a new verification-based method to verify\nthe functionality correctness of training data samples. Based on this\ntechnique, we further release a dataset with 7K verified high-quality samples.\nThese three studies are integrated into one framework, providing off-the-shelf\nsupport for the development and evaluation of LLMs for RTL code generation and\nverification. Finally, extensive experiments indicate that LLM performance can\nbe boosted by enlarging the training dataset, improving data quality, and\nimproving the training scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automated generation of design RTL based on large language model (LLM)\nand natural language instructions has demonstrated great potential in agile\ncircuit design. However, the lack of datasets and benchmarks in the public\ndomain prevents the development and fair evaluation of LLM solutions. This\npaper highlights our latest advances in open datasets and benchmarks from three\nperspectives: (1) RTLLM 2.0, an updated benchmark assessing LLM's capability in\ndesign RTL generation. The benchmark is augmented to 50 hand-crafted designs.\nEach design provides the design description, test cases, and a correct RTL\ncode. (2) AssertEval, an open-source benchmark assessing the LLM's assertion\ngeneration capabilities for RTL verification. The benchmark includes 18\ndesigns, each providing specification, signal definition, and correct RTL code.\n(3) RTLCoder-Data, an extended open-source dataset with 80K instruction-code\ndata samples. Moreover, we propose a new verification-based method to verify\nthe functionality correctness of training data samples. Based on this\ntechnique, we further release a dataset with 7K verified high-quality samples.\nThese three studies are integrated into one framework, providing off-the-shelf\nsupport for the development and evaluation of LLMs for RTL code generation and\nverification. Finally, extensive experiments indicate that LLM performance can\nbe boosted by enlarging the training dataset, improving data quality, and\nimproving the training scheme."
                },
                "authors": [
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "ICCAD'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15108v1",
                "updated": "2025-03-19T11:05:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    5,
                    42,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:05:42Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    5,
                    42,
                    2,
                    78,
                    0
                ],
                "title": "VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making"
                },
                "summary": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components."
                },
                "authors": [
                    {
                        "name": "Mohamed Salim Aissi"
                    },
                    {
                        "name": "Clemence Grislain"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Nicolas Thome"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Thome"
                },
                "author": "Nicolas Thome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15106v2",
                "updated": "2025-03-20T08:27:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    27,
                    13,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-19T11:04:37Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    4,
                    37,
                    2,
                    78,
                    0
                ],
                "title": "Distilling 3D distinctive local descriptors for 6D pose estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling 3D distinctive local descriptors for 6D pose estimation"
                },
                "summary": "Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. Can we retain GeDi's\neffectiveness while significantly improving its efficiency? In this paper, we\nexplore this question by introducing a knowledge distillation framework that\ntrains an efficient student model to regress local descriptors from a GeDi\nteacher. Our key contributions include: an efficient large-scale training\nprocedure that ensures robustness to occlusions and partial observations while\noperating under compute and storage constraints, and a novel loss formulation\nthat handles weak supervision from non-distinctive teacher descriptors. We\nvalidate our approach on five BOP Benchmark datasets and demonstrate a\nsignificant reduction in inference time while maintaining competitive\nperformance with existing methods, bringing zero-shot 6D pose estimation closer\nto real-time feasibility. Project Website: https://tev-fbk.github.io/dGeDi/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. Can we retain GeDi's\neffectiveness while significantly improving its efficiency? In this paper, we\nexplore this question by introducing a knowledge distillation framework that\ntrains an efficient student model to regress local descriptors from a GeDi\nteacher. Our key contributions include: an efficient large-scale training\nprocedure that ensures robustness to occlusions and partial observations while\noperating under compute and storage constraints, and a novel loss formulation\nthat handles weak supervision from non-distinctive teacher descriptors. We\nvalidate our approach on five BOP Benchmark datasets and demonstrate a\nsignificant reduction in inference time while maintaining competitive\nperformance with existing methods, bringing zero-shot 6D pose estimation closer\nto real-time feasibility. Project Website: https://tev-fbk.github.io/dGeDi/"
                },
                "authors": [
                    {
                        "name": "Amir Hamza"
                    },
                    {
                        "name": "Andrea Caraffa"
                    },
                    {
                        "name": "Davide Boscaini"
                    },
                    {
                        "name": "Fabio Poiesi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Poiesi"
                },
                "author": "Fabio Poiesi",
                "arxiv_comment": "Project Website: https://tev-fbk.github.io/dGeDi/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16171v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16171v3",
                "updated": "2025-03-19T10:49:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    49,
                    40,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T16:37:26Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    37,
                    26,
                    0,
                    295,
                    0
                ],
                "title": "Correcting for Selection Biases in the Determination of the Hubble\n  Constant from Time-Delay Cosmography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correcting for Selection Biases in the Determination of the Hubble\n  Constant from Time-Delay Cosmography"
                },
                "summary": "The time delay between multiple images of strongly lensed quasars has been\nused to infer the Hubble constant. The primary systematic uncertainty for\ntime-delay cosmography is the mass-sheet transform (MST), which preserves the\nlensing observables while altering the inferred $H_0$. The TDCOSMO\ncollaboration used velocity dispersion measurements of lensed quasars and\nlensed galaxies to infer that mass sheets are present, which decrease the\ninferred $H_0$ by 8$\\%$. Here, we test the assumption that the density profiles\nof galaxy-galaxy and galaxy-quasar lenses are the same. We use a composite\nstar-plus-dark-matter mass profile for the parent deflector population and\nmodel the selection function for galaxy-galaxy and galaxy-quasar lenses. We\nfind that a power-law density profile with an MST is a good approximation to a\ntwo-component mass profile around the Einstein radius, but we find that\ngalaxy-galaxy lenses have systematically higher mass-sheet components than\ngalaxy-quasar lenses. For individual systems, $\\lambda_\\mathrm{int}$ correlates\nwith the ratio of the half-light radius and Einstein radius of the lens. By\npropagating these results through the TDCOSMO methodology, we find that $H_0$\nis lowered by a further $\\sim$3\\%. Using the velocity dispersions from\n\\citet{slacs9} and our fiducial model for selection biases, we infer $H_0 =\n66\\pm4 \\ \\mathrm{(stat)} \\pm 1 \\ \\mathrm{(model \\ sys)} \\pm 2 \\\n\\mathrm{(measurement \\ sys)} \\ \\mathrm{km} \\ \\mathrm{s}^{-1} \\\n\\mathrm{Mpc}^{-1}$ for the TDCOSMO plus SLACS dataset. The first residual\nsystematic error is due to plausible alternative choices in modeling the\nselection function, and the second is an estimate of the remaining systematic\nerror in the measurement of velocity dispersions for SLACS lenses. Accurate\ntime-delay cosmography requires precise velocity dispersion measurements and\naccurate calibration of selection biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The time delay between multiple images of strongly lensed quasars has been\nused to infer the Hubble constant. The primary systematic uncertainty for\ntime-delay cosmography is the mass-sheet transform (MST), which preserves the\nlensing observables while altering the inferred $H_0$. The TDCOSMO\ncollaboration used velocity dispersion measurements of lensed quasars and\nlensed galaxies to infer that mass sheets are present, which decrease the\ninferred $H_0$ by 8$\\%$. Here, we test the assumption that the density profiles\nof galaxy-galaxy and galaxy-quasar lenses are the same. We use a composite\nstar-plus-dark-matter mass profile for the parent deflector population and\nmodel the selection function for galaxy-galaxy and galaxy-quasar lenses. We\nfind that a power-law density profile with an MST is a good approximation to a\ntwo-component mass profile around the Einstein radius, but we find that\ngalaxy-galaxy lenses have systematically higher mass-sheet components than\ngalaxy-quasar lenses. For individual systems, $\\lambda_\\mathrm{int}$ correlates\nwith the ratio of the half-light radius and Einstein radius of the lens. By\npropagating these results through the TDCOSMO methodology, we find that $H_0$\nis lowered by a further $\\sim$3\\%. Using the velocity dispersions from\n\\citet{slacs9} and our fiducial model for selection biases, we infer $H_0 =\n66\\pm4 \\ \\mathrm{(stat)} \\pm 1 \\ \\mathrm{(model \\ sys)} \\pm 2 \\\n\\mathrm{(measurement \\ sys)} \\ \\mathrm{km} \\ \\mathrm{s}^{-1} \\\n\\mathrm{Mpc}^{-1}$ for the TDCOSMO plus SLACS dataset. The first residual\nsystematic error is due to plausible alternative choices in modeling the\nselection function, and the second is an estimate of the remaining systematic\nerror in the measurement of velocity dispersions for SLACS lenses. Accurate\ntime-delay cosmography requires precise velocity dispersion measurements and\naccurate calibration of selection biases."
                },
                "authors": [
                    {
                        "name": "Tian Li"
                    },
                    {
                        "name": "Thomas E. Collett"
                    },
                    {
                        "name": "Philip J. Marshall"
                    },
                    {
                        "name": "Sydney Erickson"
                    },
                    {
                        "name": "Wolfgang Enzi"
                    },
                    {
                        "name": "Lindsay Oldham"
                    },
                    {
                        "name": "Daniel Ballard"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Ballard"
                },
                "author": "Daniel Ballard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16171v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16171v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05291v3",
                "updated": "2025-03-19T10:44:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    44,
                    50,
                    2,
                    78,
                    0
                ],
                "published": "2024-04-08T08:29:00Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    8,
                    29,
                    0,
                    0,
                    99,
                    0
                ],
                "title": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with\n  Large Language Models"
                },
                "summary": "We present a large language model (LLM) based system to empower quadrupedal\nrobots with problem-solving abilities for long-horizon tasks beyond short-term\nmotions. Long-horizon tasks for quadrupeds are challenging since they require\nboth a high-level understanding of the semantics of the problem for task\nplanning and a broad range of locomotion and manipulation skills to interact\nwith the environment. Our system builds a high-level reasoning layer with large\nlanguage models, which generates hybrid discrete-continuous plans as robot code\nfrom task descriptions. It comprises multiple LLM agents: a semantic planner\nthat sketches a plan, a parameter calculator that predicts arguments in the\nplan, a code generator that converts the plan into executable robot code, and a\nreplanner that handles execution failures or human interventions. At the low\nlevel, we adopt reinforcement learning to train a set of motion planning and\ncontrol skills to unleash the flexibility of quadrupeds for rich environment\ninteractions. Our system is tested on long-horizon tasks that are infeasible to\ncomplete with one single skill. Simulation and real-world experiments show that\nit successfully figures out multi-step strategies and demonstrates non-trivial\nbehaviors, including building tools or notifying a human for help. Demos are\navailable on our project page:\nhttps://sites.google.com/view/long-horizon-robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a large language model (LLM) based system to empower quadrupedal\nrobots with problem-solving abilities for long-horizon tasks beyond short-term\nmotions. Long-horizon tasks for quadrupeds are challenging since they require\nboth a high-level understanding of the semantics of the problem for task\nplanning and a broad range of locomotion and manipulation skills to interact\nwith the environment. Our system builds a high-level reasoning layer with large\nlanguage models, which generates hybrid discrete-continuous plans as robot code\nfrom task descriptions. It comprises multiple LLM agents: a semantic planner\nthat sketches a plan, a parameter calculator that predicts arguments in the\nplan, a code generator that converts the plan into executable robot code, and a\nreplanner that handles execution failures or human interventions. At the low\nlevel, we adopt reinforcement learning to train a set of motion planning and\ncontrol skills to unleash the flexibility of quadrupeds for rich environment\ninteractions. Our system is tested on long-horizon tasks that are infeasible to\ncomplete with one single skill. Simulation and real-world experiments show that\nit successfully figures out multi-step strategies and demonstrates non-trivial\nbehaviors, including building tools or notifying a human for help. Demos are\navailable on our project page:\nhttps://sites.google.com/view/long-horizon-robot."
                },
                "authors": [
                    {
                        "name": "Yutao Ouyang"
                    },
                    {
                        "name": "Jinhan Li"
                    },
                    {
                        "name": "Yunfei Li"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Koushil Sreenath"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16898v2",
                "updated": "2025-03-19T10:40:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    40,
                    34,
                    2,
                    78,
                    0
                ],
                "published": "2024-11-25T20:07:07Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    20,
                    7,
                    7,
                    0,
                    330,
                    0
                ],
                "title": "MonoGSDF: Exploring Monocular Geometric Cues for Gaussian\n  Splatting-Guided Implicit Surface Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoGSDF: Exploring Monocular Geometric Cues for Gaussian\n  Splatting-Guided Implicit Surface Reconstruction"
                },
                "summary": "Accurate meshing from monocular images remains a key challenge in 3D vision.\nWhile state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at\nsynthesizing photorealistic novel views through rasterization-based rendering,\ntheir reliance on sparse, explicit primitives severely limits their ability to\nrecover watertight and topologically consistent 3D surfaces.We introduce\nMonoGSDF, a novel method that couples Gaussian-based primitives with a neural\nSigned Distance Field (SDF) for high-quality reconstruction. During training,\nthe SDF guides Gaussians' spatial distribution, while at inference, Gaussians\nserve as priors to reconstruct surfaces, eliminating the need for\nmemory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a\nscaling strategy for robust generalization. A multi-resolution training scheme\nfurther refines details and monocular geometric cues from off-the-shelf\nestimators enhance reconstruction quality. Experiments on real-world datasets\nshow MonoGSDF outperforms prior methods while maintaining efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate meshing from monocular images remains a key challenge in 3D vision.\nWhile state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at\nsynthesizing photorealistic novel views through rasterization-based rendering,\ntheir reliance on sparse, explicit primitives severely limits their ability to\nrecover watertight and topologically consistent 3D surfaces.We introduce\nMonoGSDF, a novel method that couples Gaussian-based primitives with a neural\nSigned Distance Field (SDF) for high-quality reconstruction. During training,\nthe SDF guides Gaussians' spatial distribution, while at inference, Gaussians\nserve as priors to reconstruct surfaces, eliminating the need for\nmemory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a\nscaling strategy for robust generalization. A multi-resolution training scheme\nfurther refines details and monocular geometric cues from off-the-shelf\nestimators enhance reconstruction quality. Experiments on real-world datasets\nshow MonoGSDF outperforms prior methods while maintaining efficiency."
                },
                "authors": [
                    {
                        "name": "Kunyi Li"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15091v1",
                "updated": "2025-03-19T10:40:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    40,
                    28,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T10:40:28Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    40,
                    28,
                    2,
                    78,
                    0
                ],
                "title": "Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs\n  for Indoor Scenarios with the Help of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs\n  for Indoor Scenarios with the Help of LLMs"
                },
                "summary": "This paper addresses the high demand in advanced intelligent robot navigation\nfor a more holistic understanding of spatial environments, by introducing a\nnovel system that harnesses the capabilities of Large Language Models (LLMs) to\nconstruct hierarchical 3D Scene Graphs (3DSGs) for indoor scenarios. The\nproposed framework constructs 3DSGs consisting of a fundamental layer with rich\nmetric-semantic information, an object layer featuring precise point-cloud\nrepresentation of object nodes as well as visual descriptors, and higher layers\nof room, floor, and building nodes. Thanks to the innovative application of\nLLMs, not only object nodes but also nodes of higher layers, e.g., room nodes,\nare annotated in an intelligent and accurate manner. A polling mechanism for\nroom classification using LLMs is proposed to enhance the accuracy and\nreliability of the room node annotation. Thorough numerical experiments\ndemonstrate the system's ability to integrate semantic descriptions with\ngeometric data, creating an accurate and comprehensive representation of the\nenvironment instrumental for context-aware navigation and task planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the high demand in advanced intelligent robot navigation\nfor a more holistic understanding of spatial environments, by introducing a\nnovel system that harnesses the capabilities of Large Language Models (LLMs) to\nconstruct hierarchical 3D Scene Graphs (3DSGs) for indoor scenarios. The\nproposed framework constructs 3DSGs consisting of a fundamental layer with rich\nmetric-semantic information, an object layer featuring precise point-cloud\nrepresentation of object nodes as well as visual descriptors, and higher layers\nof room, floor, and building nodes. Thanks to the innovative application of\nLLMs, not only object nodes but also nodes of higher layers, e.g., room nodes,\nare annotated in an intelligent and accurate manner. A polling mechanism for\nroom classification using LLMs is proposed to enhance the accuracy and\nreliability of the room node annotation. Thorough numerical experiments\ndemonstrate the system's ability to integrate semantic descriptions with\ngeometric data, creating an accurate and comprehensive representation of the\nenvironment instrumental for context-aware navigation and task planning."
                },
                "authors": [
                    {
                        "name": "Yao Cheng"
                    },
                    {
                        "name": "Zhe Han"
                    },
                    {
                        "name": "Fengyang Jiang"
                    },
                    {
                        "name": "Huaizhen Wang"
                    },
                    {
                        "name": "Fengyu Zhou"
                    },
                    {
                        "name": "Qingshan Yin"
                    },
                    {
                        "name": "Lei Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wei"
                },
                "author": "Lei Wei",
                "arxiv_comment": "accepted by WRC SARA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15087v1",
                "updated": "2025-03-19T10:38:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    38,
                    25,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T10:38:25Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    38,
                    25,
                    2,
                    78,
                    0
                ],
                "title": "An Investigation of Beam Density on LiDAR Object Detection Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation of Beam Density on LiDAR Object Detection Performance"
                },
                "summary": "Accurate 3D object detection is a critical component of autonomous driving,\nenabling vehicles to perceive their surroundings with precision and make\ninformed decisions. LiDAR sensors, widely used for their ability to provide\ndetailed 3D measurements, are key to achieving this capability. However,\nvariations between training and inference data can cause significant\nperformance drops when object detection models are employed in different sensor\nsettings. One critical factor is beam density, as inference on sparse,\ncost-effective LiDAR sensors is often preferred in real-world applications.\nDespite previous work addressing the beam-density-induced domain gap,\nsubstantial knowledge gaps remain, particularly concerning dense 128-beam\nsensors in cross-domain scenarios. To gain better understanding of the impact\nof beam density on domain gaps, we conduct a comprehensive investigation that\nincludes an evaluation of different object detection architectures. Our\narchitecture evaluation reveals that combining voxel- and point-based\napproaches yields superior cross-domain performance by leveraging the strengths\nof both representations. Building on these findings, we analyze\nbeam-density-induced domain gaps and argue that these domain gaps must be\nevaluated in conjunction with other domain shifts. Contrary to conventional\nbeliefs, our experiments reveal that detectors benefit from training on denser\ndata and exhibit robustness to beam density variations during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate 3D object detection is a critical component of autonomous driving,\nenabling vehicles to perceive their surroundings with precision and make\ninformed decisions. LiDAR sensors, widely used for their ability to provide\ndetailed 3D measurements, are key to achieving this capability. However,\nvariations between training and inference data can cause significant\nperformance drops when object detection models are employed in different sensor\nsettings. One critical factor is beam density, as inference on sparse,\ncost-effective LiDAR sensors is often preferred in real-world applications.\nDespite previous work addressing the beam-density-induced domain gap,\nsubstantial knowledge gaps remain, particularly concerning dense 128-beam\nsensors in cross-domain scenarios. To gain better understanding of the impact\nof beam density on domain gaps, we conduct a comprehensive investigation that\nincludes an evaluation of different object detection architectures. Our\narchitecture evaluation reveals that combining voxel- and point-based\napproaches yields superior cross-domain performance by leveraging the strengths\nof both representations. Building on these findings, we analyze\nbeam-density-induced domain gaps and argue that these domain gaps must be\nevaluated in conjunction with other domain shifts. Contrary to conventional\nbeliefs, our experiments reveal that detectors benefit from training on denser\ndata and exhibit robustness to beam density variations during inference."
                },
                "authors": [
                    {
                        "name": "Christoph Griesbacher"
                    },
                    {
                        "name": "Christian Fruhwirth-Reisinger"
                    }
                ],
                "author_detail": {
                    "name": "Christian Fruhwirth-Reisinger"
                },
                "author": "Christian Fruhwirth-Reisinger",
                "arxiv_comment": "Accepted by CVWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15079v1",
                "updated": "2025-03-19T10:24:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    24,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T10:24:16Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    24,
                    16,
                    2,
                    78,
                    0
                ],
                "title": "LogiAgent: Automated Logical Testing for REST Systems with LLM-Based\n  Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogiAgent: Automated Logical Testing for REST Systems with LLM-Based\n  Multi-Agents"
                },
                "summary": "Automated testing for REST APIs has become essential for ensuring the\ncorrectness and reliability of modern web services. While existing approaches\nprimarily focus on detecting server crashes and error codes, they often\noverlook logical issues that arise due to evolving business logic and\ndomain-specific requirements. To address this limitation, we propose LogiAgent,\na novel approach for logical testing of REST systems. Built upon a large\nlanguage model (LLM)-driven multi-agent framework, LogiAgent integrates a Test\nScenario Generator, API Request Executor, and API Response Validator to\ncollaboratively generate, execute, and validate API test scenarios. Unlike\ntraditional testing methods that focus on status codes like 5xx, LogiAgent\nincorporates logical oracles that assess responses based on business logic,\nensuring more comprehensive testing. The system is further enhanced by an\nExecution Memory component that stores historical API execution data for\ncontextual consistency. We conduct extensive experiments across 12 real-world\nREST systems, demonstrating that LogiAgent effectively identifies 234 logical\nissues with an accuracy of 66.19%. Additionally, it basically excels in\ndetecting server crashes and achieves superior test coverage compared to four\nstate-of-the-art REST API testing tools. An ablation study confirms the\nsignificant contribution of LogiAgent's memory components to improving test\ncoverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated testing for REST APIs has become essential for ensuring the\ncorrectness and reliability of modern web services. While existing approaches\nprimarily focus on detecting server crashes and error codes, they often\noverlook logical issues that arise due to evolving business logic and\ndomain-specific requirements. To address this limitation, we propose LogiAgent,\na novel approach for logical testing of REST systems. Built upon a large\nlanguage model (LLM)-driven multi-agent framework, LogiAgent integrates a Test\nScenario Generator, API Request Executor, and API Response Validator to\ncollaboratively generate, execute, and validate API test scenarios. Unlike\ntraditional testing methods that focus on status codes like 5xx, LogiAgent\nincorporates logical oracles that assess responses based on business logic,\nensuring more comprehensive testing. The system is further enhanced by an\nExecution Memory component that stores historical API execution data for\ncontextual consistency. We conduct extensive experiments across 12 real-world\nREST systems, demonstrating that LogiAgent effectively identifies 234 logical\nissues with an accuracy of 66.19%. Additionally, it basically excels in\ndetecting server crashes and achieves superior test coverage compared to four\nstate-of-the-art REST API testing tools. An ablation study confirms the\nsignificant contribution of LogiAgent's memory components to improving test\ncoverage."
                },
                "authors": [
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Chenxi Zhang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "YaChen Wu"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qingshan Li"
                    },
                    {
                        "name": "Xin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Peng"
                },
                "author": "Xin Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10199v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10199v4",
                "updated": "2025-03-19T10:23:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    23,
                    49,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-13T09:34:33Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    34,
                    33,
                    3,
                    72,
                    0
                ],
                "title": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods"
                },
                "summary": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples."
                },
                "authors": [
                    {
                        "name": "Ruibiao Song"
                    },
                    {
                        "name": "Liying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liying Zhang"
                },
                "author": "Liying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10199v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10199v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07356v2",
                "updated": "2025-03-19T10:22:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    22,
                    15,
                    2,
                    78,
                    0
                ],
                "published": "2024-07-10T04:27:06Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    4,
                    27,
                    6,
                    2,
                    192,
                    0
                ],
                "title": "Video In-context Learning: Autoregressive Transformers are Zero-Shot\n  Video Imitators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video In-context Learning: Autoregressive Transformers are Zero-Shot\n  Video Imitators"
                },
                "summary": "People interact with the real-world largely dependent on visual signal, which\nare ubiquitous and illustrate detailed demonstrations. In this paper, we\nexplore utilizing visual signals as a new interface for models to interact with\nthe environment. Specifically, we choose videos as a representative visual\nsignal. And by training autoregressive Transformers on video datasets in a\nself-supervised objective, we find that the model emerges a zero-shot\ncapability to infer the semantics from a demonstration video, and imitate the\nsemantics to an unseen scenario. This allows the models to perform unseen tasks\nby watching the demonstration video in an in-context manner, without further\nfine-tuning. To validate the imitation capacity, we design various evaluation\nmetrics including both objective and subjective measures. The results show that\nour models can generate high-quality video clips that accurately align with the\nsemantic guidance provided by the demonstration videos, and we also show that\nthe imitation capacity follows the scaling law. Code and models have been\nopen-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People interact with the real-world largely dependent on visual signal, which\nare ubiquitous and illustrate detailed demonstrations. In this paper, we\nexplore utilizing visual signals as a new interface for models to interact with\nthe environment. Specifically, we choose videos as a representative visual\nsignal. And by training autoregressive Transformers on video datasets in a\nself-supervised objective, we find that the model emerges a zero-shot\ncapability to infer the semantics from a demonstration video, and imitate the\nsemantics to an unseen scenario. This allows the models to perform unseen tasks\nby watching the demonstration video in an in-context manner, without further\nfine-tuning. To validate the imitation capacity, we design various evaluation\nmetrics including both objective and subjective measures. The results show that\nour models can generate high-quality video clips that accurately align with the\nsemantic guidance provided by the demonstration videos, and we also show that\nthe imitation capacity follows the scaling law. Code and models have been\nopen-sourced."
                },
                "authors": [
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Junliang Guo"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Zhao"
                    },
                    {
                        "name": "Linli Xu"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13313v2",
                "updated": "2025-03-19T10:08:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    8,
                    35,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-17T15:52:53Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    52,
                    53,
                    0,
                    76,
                    0
                ],
                "title": "Detection of millimeter-wave coronal emission in a quasar at\n  cosmological distance using microlensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of millimeter-wave coronal emission in a quasar at\n  cosmological distance using microlensing"
                },
                "summary": "Determining the nature of emission processes at the heart of quasars is\ncritical for understanding environments of supermassive black holes. One of the\nkey open questions is the origin of long-wavelength emission from radio-quiet\nquasars. The proposed mechanisms span a broad range, from central star\nformation to dusty torus, low-power jets, or coronal emission from the\ninnermost accretion disk. Distinguishing between these scenarios requires\nprobing spatial scales $\\leq$0.01 pc, beyond the reach of any current\nmillimetre-wave telescope. Fortunately, in gravitationally lensed quasars,\ncompact mm-wave emission might be microlensed by stars in the foreground\ngalaxy, providing strong constraints on the source size. We report a striking\nchange in rest-frame 1.3-mm flux-ratios in RXJ1131-1231, a quadruply-lensed\nquasar at z = 0.658, observed by the Atacama Large Millimeter/submillimeter\nArray (ALMA) in 2015 and 2020. The observed flux-ratio variability is\nconsistent with microlensing of a very compact source with a half-light radius\n$\\leq$50 astronomical units. The compactness of the source leaves coronal\nemission as the most likely scenario. Furthermore, the inferred mm-wave and\nX-ray luminosities follow the characteristic G\\\"udel-Benz relationship for\ncoronal emission. These observations represent the first unambiguous evidence\nfor coronae as the dominant mechanism for long-wavelength emission in\nradio-quiet quasars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the nature of emission processes at the heart of quasars is\ncritical for understanding environments of supermassive black holes. One of the\nkey open questions is the origin of long-wavelength emission from radio-quiet\nquasars. The proposed mechanisms span a broad range, from central star\nformation to dusty torus, low-power jets, or coronal emission from the\ninnermost accretion disk. Distinguishing between these scenarios requires\nprobing spatial scales $\\leq$0.01 pc, beyond the reach of any current\nmillimetre-wave telescope. Fortunately, in gravitationally lensed quasars,\ncompact mm-wave emission might be microlensed by stars in the foreground\ngalaxy, providing strong constraints on the source size. We report a striking\nchange in rest-frame 1.3-mm flux-ratios in RXJ1131-1231, a quadruply-lensed\nquasar at z = 0.658, observed by the Atacama Large Millimeter/submillimeter\nArray (ALMA) in 2015 and 2020. The observed flux-ratio variability is\nconsistent with microlensing of a very compact source with a half-light radius\n$\\leq$50 astronomical units. The compactness of the source leaves coronal\nemission as the most likely scenario. Furthermore, the inferred mm-wave and\nX-ray luminosities follow the characteristic G\\\"udel-Benz relationship for\ncoronal emission. These observations represent the first unambiguous evidence\nfor coronae as the dominant mechanism for long-wavelength emission in\nradio-quiet quasars."
                },
                "authors": [
                    {
                        "name": "M. Rybak"
                    },
                    {
                        "name": "D. Sluse"
                    },
                    {
                        "name": "K. K. Gupta"
                    },
                    {
                        "name": "M. Millon"
                    },
                    {
                        "name": "E. Behar"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "J. P. McKean"
                    },
                    {
                        "name": "H. R. Stacey"
                    }
                ],
                "author_detail": {
                    "name": "H. R. Stacey"
                },
                "author": "H. R. Stacey",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12374v2",
                "updated": "2025-03-19T10:08:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    8,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-16T06:24:51Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    6,
                    24,
                    51,
                    6,
                    75,
                    0
                ],
                "title": "Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at\n  GitHub Issue Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at\n  GitHub Issue Resolution"
                },
                "summary": "AI-driven software development has rapidly advanced with the emergence of\nsoftware development agents that leverage large language models (LLMs) to\ntackle complex, repository-level software engineering tasks. These agents go\nbeyond just generation of final code; they engage in multi-step reasoning,\nutilize various tools for code modification and debugging, and interact with\nexecution environments to diagnose and iteratively resolve issues. However,\nmost existing evaluations focus primarily on static analyses of final code\noutputs, yielding limited insights into the agents' dynamic problem-solving\nprocesses. To fill this gap, we conduct an in-depth empirical study on 3,977\nsolving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked\nagents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our\nexploratory analysis shows that Python execution errors during the issue\nresolution phase correlate with lower resolution rates and increased reasoning\noverheads. We have identified the most prevalent errors -- such as\nModuleNotFoundError and TypeError -- and highlighted particularly challenging\nerrors like OSError and database-related issues (e.g., IntegrityError) that\ndemand significantly more debugging effort. Furthermore, we have discovered 3\nbugs in the SWE-Bench platform that affect benchmark fairness and accuracy;\nthese issues have been reported to and confirmed by the maintainers. To promote\ntransparency and foster future research, we publicly share our datasets and\nanalysis scripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-driven software development has rapidly advanced with the emergence of\nsoftware development agents that leverage large language models (LLMs) to\ntackle complex, repository-level software engineering tasks. These agents go\nbeyond just generation of final code; they engage in multi-step reasoning,\nutilize various tools for code modification and debugging, and interact with\nexecution environments to diagnose and iteratively resolve issues. However,\nmost existing evaluations focus primarily on static analyses of final code\noutputs, yielding limited insights into the agents' dynamic problem-solving\nprocesses. To fill this gap, we conduct an in-depth empirical study on 3,977\nsolving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked\nagents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our\nexploratory analysis shows that Python execution errors during the issue\nresolution phase correlate with lower resolution rates and increased reasoning\noverheads. We have identified the most prevalent errors -- such as\nModuleNotFoundError and TypeError -- and highlighted particularly challenging\nerrors like OSError and database-related issues (e.g., IntegrityError) that\ndemand significantly more debugging effort. Furthermore, we have discovered 3\nbugs in the SWE-Bench platform that affect benchmark fairness and accuracy;\nthese issues have been reported to and confirmed by the maintainers. To promote\ntransparency and foster future research, we publicly share our datasets and\nanalysis scripts."
                },
                "authors": [
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15045v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15045v3",
                "updated": "2025-03-19T10:05:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    5,
                    4,
                    2,
                    78,
                    0
                ],
                "published": "2024-08-27T13:13:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    13,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "DocLayLLM: An Efficient Multi-modal Extension of Large Language Models\n  for Text-rich Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocLayLLM: An Efficient Multi-modal Extension of Large Language Models\n  for Text-rich Document Understanding"
                },
                "summary": "Text-rich document understanding (TDU) requires comprehensive analysis of\ndocuments containing substantial textual content and complex layouts. While\nMultimodal Large Language Models (MLLMs) have achieved fast progress in this\ndomain, existing approaches either demand significant computational resources\nor struggle with effective multi-modal integration. In this paper, we introduce\nDocLayLLM, an efficient multi-modal extension of LLMs specifically designed for\nTDU. By lightly integrating visual patch tokens and 2D positional tokens into\nLLMs' input and encoding the document content using the LLMs themselves, we\nfully take advantage of the document comprehension capability of LLMs and\nenhance their perception of OCR information. We have also deeply considered the\nrole of chain-of-thought (CoT) and innovatively proposed the techniques of CoT\nPre-training and CoT Annealing. Our DocLayLLM can achieve remarkable\nperformances with lightweight training settings, showcasing its efficiency and\neffectiveness. Experimental results demonstrate that our DocLayLLM outperforms\nexisting OCR-dependent methods and OCR-free competitors. Code and model are\navailable at https://github.com/whlscut/DocLayLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich document understanding (TDU) requires comprehensive analysis of\ndocuments containing substantial textual content and complex layouts. While\nMultimodal Large Language Models (MLLMs) have achieved fast progress in this\ndomain, existing approaches either demand significant computational resources\nor struggle with effective multi-modal integration. In this paper, we introduce\nDocLayLLM, an efficient multi-modal extension of LLMs specifically designed for\nTDU. By lightly integrating visual patch tokens and 2D positional tokens into\nLLMs' input and encoding the document content using the LLMs themselves, we\nfully take advantage of the document comprehension capability of LLMs and\nenhance their perception of OCR information. We have also deeply considered the\nrole of chain-of-thought (CoT) and innovatively proposed the techniques of CoT\nPre-training and CoT Annealing. Our DocLayLLM can achieve remarkable\nperformances with lightweight training settings, showcasing its efficiency and\neffectiveness. Experimental results demonstrate that our DocLayLLM outperforms\nexisting OCR-dependent methods and OCR-free competitors. Code and model are\navailable at https://github.com/whlscut/DocLayLLM."
                },
                "authors": [
                    {
                        "name": "Wenhui Liao"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Hongliang Li"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "arxiv_comment": "CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15045v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15045v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15062v1",
                "updated": "2025-03-19T09:59:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    59,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T09:59:30Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    59,
                    30,
                    2,
                    78,
                    0
                ],
                "title": "A Bivariate Poisson-Gamma Distribution: Statistical Properties and\n  Practical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bivariate Poisson-Gamma Distribution: Statistical Properties and\n  Practical Applications"
                },
                "summary": "Although the specification of bivariate probability models using a collection\nof assumed conditional distributions is not a novel concept, it has received\nconsiderable attention in the last decade. In this study, a bivariate\ndistribution-the bivariate Poisson-Gamma conditional distribution-is\nintroduced, combining both univariate continuous and discrete distributions.\nThis work explores aspects of this model's structure and statistical inference\nthat have not been studied before. This paper contributes to the field of\nstatistical modeling and distribution theory through the use of maximum\nlikelihood estimation, along with simulations and analyses of real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the specification of bivariate probability models using a collection\nof assumed conditional distributions is not a novel concept, it has received\nconsiderable attention in the last decade. In this study, a bivariate\ndistribution-the bivariate Poisson-Gamma conditional distribution-is\nintroduced, combining both univariate continuous and discrete distributions.\nThis work explores aspects of this model's structure and statistical inference\nthat have not been studied before. This paper contributes to the field of\nstatistical modeling and distribution theory through the use of maximum\nlikelihood estimation, along with simulations and analyses of real data."
                },
                "authors": [
                    {
                        "name": "Indranil Ghosh"
                    },
                    {
                        "name": "Mina Norouzirad"
                    },
                    {
                        "name": "Filipe J. Marques"
                    }
                ],
                "author_detail": {
                    "name": "Filipe J. Marques"
                },
                "author": "Filipe J. Marques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15055v1",
                "updated": "2025-03-19T09:46:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    46,
                    54,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T09:46:54Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    46,
                    54,
                    2,
                    78,
                    0
                ],
                "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation"
                },
                "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains."
                },
                "authors": [
                    {
                        "name": "Arina Razmyslovich"
                    },
                    {
                        "name": "Kseniia Murasheva"
                    },
                    {
                        "name": "Sofia Sedlova"
                    },
                    {
                        "name": "Julien Capitaine"
                    },
                    {
                        "name": "Eugene Dmitriev"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Dmitriev"
                },
                "author": "Eugene Dmitriev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.15478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15478v1",
                "updated": "2025-03-19T17:55:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    55,
                    8,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:55:08Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    55,
                    8,
                    2,
                    78,
                    0
                ],
                "title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks"
                },
                "summary": "Large language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existing multi-turn RL algorithms for optimizing LLM\nagents fail to perform effective credit assignment over multiple turns while\nleveraging the generalization capabilities of LLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,\nColBench, where an LLM agent interacts with a human collaborator over multiple\nturns to solve realistic tasks in backend programming and frontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation from Training-time information), that uses a carefully\ndesigned optimization objective to train a critic model with access to\nadditional training-time information. The critic provides step-level rewards\nfor improving the policy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success and win rates on ColBench\ncompared to other state-of-the-art multi-turn RL algorithms, enabling\nLlama-3.1-8B to match or exceed the performance of GPT4-o in realistic\ncollaborative content creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existing multi-turn RL algorithms for optimizing LLM\nagents fail to perform effective credit assignment over multiple turns while\nleveraging the generalization capabilities of LLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,\nColBench, where an LLM agent interacts with a human collaborator over multiple\nturns to solve realistic tasks in backend programming and frontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation from Training-time information), that uses a carefully\ndesigned optimization objective to train a critic model with access to\nadditional training-time information. The critic provides step-level rewards\nfor improving the policy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success and win rates on ColBench\ncompared to other state-of-the-art multi-turn RL algorithms, enabling\nLlama-3.1-8B to match or exceed the performance of GPT4-o in realistic\ncollaborative content creation."
                },
                "authors": [
                    {
                        "name": "Yifei Zhou"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Xian Li"
                    }
                ],
                "author_detail": {
                    "name": "Xian Li"
                },
                "author": "Xian Li",
                "arxiv_comment": "29 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15475v1",
                "updated": "2025-03-19T17:52:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    52,
                    17,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:52:17Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    52,
                    17,
                    2,
                    78,
                    0
                ],
                "title": "Cube: A Roblox View of 3D Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cube: A Roblox View of 3D Intelligence"
                },
                "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence."
                },
                "authors": [
                    {
                        "name": "Foundation AI Team"
                    },
                    {
                        "name": "Kiran Bhat"
                    },
                    {
                        "name": "Nishchaie Khanna"
                    },
                    {
                        "name": "Karun Channa"
                    },
                    {
                        "name": "Tinghui Zhou"
                    },
                    {
                        "name": "Yiheng Zhu"
                    },
                    {
                        "name": "Xiaoxia Sun"
                    },
                    {
                        "name": "Charles Shang"
                    },
                    {
                        "name": "Anirudh Sudarshan"
                    },
                    {
                        "name": "Maurice Chu"
                    },
                    {
                        "name": "Daiqing Li"
                    },
                    {
                        "name": "Kangle Deng"
                    },
                    {
                        "name": "Jean-Philippe Fauconnier"
                    },
                    {
                        "name": "Tijmen Verhulsdonck"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Kayvon Fatahalian"
                    },
                    {
                        "name": "Alexander Weiss"
                    },
                    {
                        "name": "Christian Reiser"
                    },
                    {
                        "name": "Ravi Kiran Chirravuri"
                    },
                    {
                        "name": "Ravali Kandur"
                    },
                    {
                        "name": "Alejandro Pelaez"
                    },
                    {
                        "name": "Akash Garg"
                    },
                    {
                        "name": "Michael Palleschi"
                    },
                    {
                        "name": "Jessica Wang"
                    },
                    {
                        "name": "Skylar Litz"
                    },
                    {
                        "name": "Leon Liu"
                    },
                    {
                        "name": "Anying Li"
                    },
                    {
                        "name": "David Harmon"
                    },
                    {
                        "name": "Derek Liu"
                    },
                    {
                        "name": "Liangjun Feng"
                    },
                    {
                        "name": "Denis Goupil"
                    },
                    {
                        "name": "Lukas Kuczynski"
                    },
                    {
                        "name": "Jihyun Yoon"
                    },
                    {
                        "name": "Naveen Marri"
                    },
                    {
                        "name": "Peiye Zhuang"
                    },
                    {
                        "name": "Yinan Zhang"
                    },
                    {
                        "name": "Brian Yin"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Marcel van Workum"
                    },
                    {
                        "name": "Thomas Lane"
                    },
                    {
                        "name": "Bryce Erickson"
                    },
                    {
                        "name": "Salil Pathare"
                    },
                    {
                        "name": "Kyle Price"
                    },
                    {
                        "name": "Anupam Singh"
                    },
                    {
                        "name": "David Baszucki"
                    }
                ],
                "author_detail": {
                    "name": "David Baszucki"
                },
                "author": "David Baszucki",
                "arxiv_comment": "Our code and model weights can be found at:\n  https://github.com/Roblox/cube",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15465v1",
                "updated": "2025-03-19T17:44:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    44,
                    21,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:44:21Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    44,
                    21,
                    2,
                    78,
                    0
                ],
                "title": "FP4DiT: Towards Effective Floating Point Quantization for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FP4DiT: Towards Effective Floating Point Quantization for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Models (DM) have revolutionized the text-to-image visual generation\nprocess. However, the large computational cost and model footprint of DMs\nhinders practical deployment, especially on edge devices. Post-training\nquantization (PTQ) is a lightweight method to alleviate these burdens without\nthe need for training or fine-tuning. While recent DM PTQ methods achieve W4A8\non integer-based PTQ, two key limitations remain: First, while most existing DM\nPTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,\nwhich use convolutional U-Nets, newer Diffusion Transformer (DiT) models like\nthe PixArt series, Hunyuan and others adopt fundamentally different transformer\nbackbones to achieve superior image synthesis. Second, integer (INT)\nquantization is prevailing in DM PTQ but doesn't align well with the network\nweight and activation distribution, while Floating-Point Quantization (FPQ) is\nstill under-investigated, yet it holds the potential to better align the weight\nand activation distributions in low-bit settings for DiT. In response, we\nintroduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.\nSpecifically, we extend and generalize the Adaptive Rounding PTQ technique to\nadequately calibrate weight quantization for FPQ and demonstrate that DiT\nactivations depend on input patch data, necessitating robust online activation\nquantization techniques. Experimental results demonstrate that FP4DiT\noutperforms integer-based PTQ at W4A6 and W4A8 precision and generates\nconvincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in\nterms of several T2I metrics such as HPSv2 and CLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models (DM) have revolutionized the text-to-image visual generation\nprocess. However, the large computational cost and model footprint of DMs\nhinders practical deployment, especially on edge devices. Post-training\nquantization (PTQ) is a lightweight method to alleviate these burdens without\nthe need for training or fine-tuning. While recent DM PTQ methods achieve W4A8\non integer-based PTQ, two key limitations remain: First, while most existing DM\nPTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,\nwhich use convolutional U-Nets, newer Diffusion Transformer (DiT) models like\nthe PixArt series, Hunyuan and others adopt fundamentally different transformer\nbackbones to achieve superior image synthesis. Second, integer (INT)\nquantization is prevailing in DM PTQ but doesn't align well with the network\nweight and activation distribution, while Floating-Point Quantization (FPQ) is\nstill under-investigated, yet it holds the potential to better align the weight\nand activation distributions in low-bit settings for DiT. In response, we\nintroduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.\nSpecifically, we extend and generalize the Adaptive Rounding PTQ technique to\nadequately calibrate weight quantization for FPQ and demonstrate that DiT\nactivations depend on input patch data, necessitating robust online activation\nquantization techniques. Experimental results demonstrate that FP4DiT\noutperforms integer-based PTQ at W4A6 and W4A8 precision and generates\nconvincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in\nterms of several T2I metrics such as HPSv2 and CLIP."
                },
                "authors": [
                    {
                        "name": "Ruichen Chen"
                    },
                    {
                        "name": "Keith G. Mills"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "The code is available at https://github.com/cccrrrccc/FP4DiT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15463v1",
                "updated": "2025-03-19T17:41:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    41,
                    46,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:41:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    41,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment"
                },
                "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15461v1",
                "updated": "2025-03-19T17:40:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    40,
                    53,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:40:53Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    40,
                    53,
                    2,
                    78,
                    0
                ],
                "title": "Low Cost C-ITS Stations Using Raspberry Pi and the Open Source Software\n  OScar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Cost C-ITS Stations Using Raspberry Pi and the Open Source Software\n  OScar"
                },
                "summary": "The deployment of cooperative-intelligent transport systems (C-ITS) has\nstarted, and standardization and research activities are moving forward to\nimprove road safety and vehicular efficiency. An aspect that is still felt as a\nlimitation by the research groups active in the field, is the difficulty to\nvalidate the solutions with real hardware and software, because of the huge\ninvestments that are needed when multiple equipped vehicles need to be\nconsidered. In this work, we present a platform with low-cost hardware based on\na Raspberry Pi and a Wi-Fi module transmitting at 5.9 GHz, and on the\nopen-source software Open Stack for Car (OScar), which is compliant with the\nETSI C-ITS standards. With a limited cost in the order of 200 EUR, the platform\nrealizes a device which is standard compliant and can be used as either\non-board unit (OBU) or road side unit (RSU). The limited cost makes the testbed\nscalable to several units with limited budget and the limited size makes it\nalso deployable on mini-cars to test advanced connected and autonomous vehicle\n(CAV) networks and applications. Our tests demonstrate its interoperability\nwith other devices, compliance in terms of power spectrum, and a range of a few\nhundred meters in line-of-sight (LOS) conditions using the standard settings of\nITS-G5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of cooperative-intelligent transport systems (C-ITS) has\nstarted, and standardization and research activities are moving forward to\nimprove road safety and vehicular efficiency. An aspect that is still felt as a\nlimitation by the research groups active in the field, is the difficulty to\nvalidate the solutions with real hardware and software, because of the huge\ninvestments that are needed when multiple equipped vehicles need to be\nconsidered. In this work, we present a platform with low-cost hardware based on\na Raspberry Pi and a Wi-Fi module transmitting at 5.9 GHz, and on the\nopen-source software Open Stack for Car (OScar), which is compliant with the\nETSI C-ITS standards. With a limited cost in the order of 200 EUR, the platform\nrealizes a device which is standard compliant and can be used as either\non-board unit (OBU) or road side unit (RSU). The limited cost makes the testbed\nscalable to several units with limited budget and the limited size makes it\nalso deployable on mini-cars to test advanced connected and autonomous vehicle\n(CAV) networks and applications. Our tests demonstrate its interoperability\nwith other devices, compliance in terms of power spectrum, and a range of a few\nhundred meters in line-of-sight (LOS) conditions using the standard settings of\nITS-G5."
                },
                "authors": [
                    {
                        "name": "L. Farina"
                    },
                    {
                        "name": "M. Piccoli"
                    },
                    {
                        "name": "S. Iandolo"
                    },
                    {
                        "name": "A. Solida"
                    },
                    {
                        "name": "C. A. Grazia"
                    },
                    {
                        "name": "F. Raviglione"
                    },
                    {
                        "name": "C. Casetti"
                    },
                    {
                        "name": "A. Bazzi"
                    }
                ],
                "author_detail": {
                    "name": "A. Bazzi"
                },
                "author": "A. Bazzi",
                "arxiv_comment": "6 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15454v1",
                "updated": "2025-03-19T17:36:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    36,
                    35,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:36:35Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    36,
                    35,
                    2,
                    78,
                    0
                ],
                "title": "Evaluating Bias in Retrieval-Augmented Medical Question-Answering\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Bias in Retrieval-Augmented Medical Question-Answering\n  Systems"
                },
                "summary": "Medical QA systems powered by Retrieval-Augmented Generation (RAG) models\nsupport clinical decision-making but may introduce biases related to race,\ngender, and social determinants of health. We systematically evaluate biases in\nRAG-based LLM by examining demographic-sensitive queries and measuring\nretrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze\nretrieval overlap and correctness disparities. Our findings reveal substantial\ndemographic disparities within RAG pipelines, emphasizing the critical need for\nretrieval methods that explicitly account for fairness to ensure equitable\nclinical decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical QA systems powered by Retrieval-Augmented Generation (RAG) models\nsupport clinical decision-making but may introduce biases related to race,\ngender, and social determinants of health. We systematically evaluate biases in\nRAG-based LLM by examining demographic-sensitive queries and measuring\nretrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze\nretrieval overlap and correctness disparities. Our findings reveal substantial\ndemographic disparities within RAG pipelines, emphasizing the critical need for\nretrieval methods that explicitly account for fairness to ensure equitable\nclinical decision-making."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanshan Wang"
                },
                "author": "Yanshan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15450v1",
                "updated": "2025-03-19T17:31:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    31,
                    15,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:31:15Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    31,
                    15,
                    2,
                    78,
                    0
                ],
                "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling"
                },
                "summary": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder."
                },
                "authors": [
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "22 pages. Accepted to ICLR 2025 Workshop on Open Science for\n  Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15429v1",
                "updated": "2025-03-19T17:11:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    11,
                    8,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:11:08Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    11,
                    8,
                    2,
                    78,
                    0
                ],
                "title": "Optimum Network Slicing for Ultra-reliable Low Latency Communication\n  (URLLC) Services in Campus Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimum Network Slicing for Ultra-reliable Low Latency Communication\n  (URLLC) Services in Campus Networks"
                },
                "summary": "Within 3GPP, the campus network architecture has evolved as a deployment\noption for industries and can be provisioned using network slicing over already\ninstalled 5G public network infrastructure. In campus networks, the\nultra-reliable low latency communication (URLLC) service category is of major\ninterest for applications with strict latency and high-reliability\nrequirements. One way to achieve high reliability in a shared infrastructure is\nthrough resource isolation, whereby network slicing can be optimized to\nadequately reserve computation and transmission capacity. This paper proposes\nan approach for vertical slicing the radio access network (RAN) to enable the\ndeployment of multiple and isolated campus networks to accommodate URLLC\nservices. To this end, we model RAN function placement as a mixed integer\nlinear programming problem with URLLC-related constraints. We demonstrate that\nour approach can find optimal solutions in real-world scenarios. Furthermore,\nunlike existing solutions, our model considers the user traffic flow from a\nknown source node on the network's edge to an unknown \\textit{a priori}\ndestination node. This flexibility could be explored in industrial campus\nnetworks by allowing dynamic placement of user plane functions (UPFs) to serve\nthe URLLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within 3GPP, the campus network architecture has evolved as a deployment\noption for industries and can be provisioned using network slicing over already\ninstalled 5G public network infrastructure. In campus networks, the\nultra-reliable low latency communication (URLLC) service category is of major\ninterest for applications with strict latency and high-reliability\nrequirements. One way to achieve high reliability in a shared infrastructure is\nthrough resource isolation, whereby network slicing can be optimized to\nadequately reserve computation and transmission capacity. This paper proposes\nan approach for vertical slicing the radio access network (RAN) to enable the\ndeployment of multiple and isolated campus networks to accommodate URLLC\nservices. To this end, we model RAN function placement as a mixed integer\nlinear programming problem with URLLC-related constraints. We demonstrate that\nour approach can find optimal solutions in real-world scenarios. Furthermore,\nunlike existing solutions, our model considers the user traffic flow from a\nknown source node on the network's edge to an unknown \\textit{a priori}\ndestination node. This flexibility could be explored in industrial campus\nnetworks by allowing dynamic placement of user plane functions (UPFs) to serve\nthe URLLC."
                },
                "authors": [
                    {
                        "name": "Iulisloi Zacarias"
                    },
                    {
                        "name": "Francisco Carpio"
                    },
                    {
                        "name": "André Costa Drummond"
                    },
                    {
                        "name": "Admela Jukan"
                    }
                ],
                "author_detail": {
                    "name": "Admela Jukan"
                },
                "author": "Admela Jukan",
                "arxiv_doi": "10.1109/DRCN57075.2023.10108188",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/DRCN57075.2023.10108188",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.15429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14478v2",
                "updated": "2025-03-19T17:03:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    3,
                    25,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-18T17:51:34Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    34,
                    1,
                    77,
                    0
                ],
                "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM"
                },
                "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench."
                },
                "authors": [
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Zhijian Chen"
                    },
                    {
                        "name": "Kai Lan"
                    },
                    {
                        "name": "Lixin Ma"
                    },
                    {
                        "name": "Shengyuan Ding"
                    },
                    {
                        "name": "Yingji Liang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Farong Wen"
                    },
                    {
                        "name": "Zicheng Zhang"
                    },
                    {
                        "name": "Guofeng Zhang"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "Evaluation Code and dataset see\n  https://github.com/open-compass/Creation-MMBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15421v1",
                "updated": "2025-03-19T17:01:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    1,
                    15,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T17:01:15Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    1,
                    15,
                    2,
                    78,
                    0
                ],
                "title": "Probing the topology of the space of tokens with structured prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the topology of the space of tokens with structured prompts"
                },
                "summary": "This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article presents a general and flexible method for prompting a large\nlanguage model (LLM) to reveal its (hidden) token input embedding up to\nhomeomorphism. Moreover, this article provides strong theoretical justification\n-- a mathematical proof for generic LLMs -- for why this method should be\nexpected to work. With this method in hand, we demonstrate its effectiveness by\nrecovering the token subspace of Llemma-7B. The results of this paper apply not\nonly to LLMs but also to general nonlinear autoregressive processes."
                },
                "authors": [
                    {
                        "name": "Michael Robinson"
                    },
                    {
                        "name": "Sourya Dey"
                    },
                    {
                        "name": "Taisa Kushner"
                    }
                ],
                "author_detail": {
                    "name": "Taisa Kushner"
                },
                "author": "Taisa Kushner",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53Z50, 58Z05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13213v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13213v3",
                "updated": "2025-03-19T16:57:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    57,
                    23,
                    2,
                    78,
                    0
                ],
                "published": "2024-02-20T18:24:47Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    18,
                    24,
                    47,
                    1,
                    51,
                    0
                ],
                "title": "Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A"
                },
                "summary": "We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold."
                },
                "authors": [
                    {
                        "name": "Benjamin Plaut"
                    },
                    {
                        "name": "Nguyen X. Khanh"
                    },
                    {
                        "name": "Tu Trinh"
                    }
                ],
                "author_detail": {
                    "name": "Tu Trinh"
                },
                "author": "Tu Trinh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13213v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13213v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15402v1",
                "updated": "2025-03-19T16:43:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    43,
                    35,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    43,
                    35,
                    2,
                    78,
                    0
                ],
                "title": "Towards efficient keyword spotting using spike-based time difference\n  encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards efficient keyword spotting using spike-based time difference\n  encoders"
                },
                "summary": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns."
                },
                "authors": [
                    {
                        "name": "Alejandro Pequeño-Zurro"
                    },
                    {
                        "name": "Lyes Khacef"
                    },
                    {
                        "name": "Stefano Panzeri"
                    },
                    {
                        "name": "Elisabetta Chicca"
                    }
                ],
                "author_detail": {
                    "name": "Elisabetta Chicca"
                },
                "author": "Elisabetta Chicca",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11197v3",
                "updated": "2025-03-19T16:33:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    33,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-14T08:43:53Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    43,
                    53,
                    4,
                    73,
                    0
                ],
                "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering"
                },
                "summary": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa."
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Jizhong Liu"
                    },
                    {
                        "name": "Heinrich Dinkel"
                    },
                    {
                        "name": "Yadong Niu"
                    },
                    {
                        "name": "Junbo Zhang"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04715v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04715v4",
                "updated": "2025-03-19T16:28:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    28,
                    25,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-06T18:58:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.09% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.09% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/"
                },
                "authors": [
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shijie Xuyang"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04715v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04715v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08020v2",
                "updated": "2025-03-19T16:26:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    26,
                    10,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-11T23:40:53Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    23,
                    40,
                    53,
                    1,
                    42,
                    0
                ],
                "title": "Speculate, then Collaborate: Fusing Knowledge of Language Models during\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate, then Collaborate: Fusing Knowledge of Language Models during\n  Decoding"
                },
                "summary": "Large Language Models (LLMs) often excel in specific domains but fall short\nin others due to the limitations of their training. Thus, enabling LLMs to\nsolve problems collaboratively by integrating their complementary knowledge\npromises to improve their performance across domains. To realize this\npotential, we introduce a novel Collaborative Speculative Decoding (CoSD)\nalgorithm that enables efficient LLM knowledge fusion at test time without\nrequiring additional model training. CoSD employs a draft model to generate\ninitial sequences and an easy-to-learn rule or decision tree to decide when to\ninvoke an assistant model to improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference efficiency, is transferable across\ndomains and models, and offers greater explainability. Experimental results\ndemonstrate that CoSD improves accuracy by up to 10\\% across benchmarks\ncompared to existing methods, providing a scalable and effective solution for\nLLM-based applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often excel in specific domains but fall short\nin others due to the limitations of their training. Thus, enabling LLMs to\nsolve problems collaboratively by integrating their complementary knowledge\npromises to improve their performance across domains. To realize this\npotential, we introduce a novel Collaborative Speculative Decoding (CoSD)\nalgorithm that enables efficient LLM knowledge fusion at test time without\nrequiring additional model training. CoSD employs a draft model to generate\ninitial sequences and an easy-to-learn rule or decision tree to decide when to\ninvoke an assistant model to improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference efficiency, is transferable across\ndomains and models, and offers greater explainability. Experimental results\ndemonstrate that CoSD improves accuracy by up to 10\\% across benchmarks\ncompared to existing methods, providing a scalable and effective solution for\nLLM-based applications"
                },
                "authors": [
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Muneeza Azmat"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15374v1",
                "updated": "2025-03-19T16:12:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    12,
                    11,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T16:12:11Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    12,
                    11,
                    2,
                    78,
                    0
                ],
                "title": "Real-world validation of a multimodal LLM-powered pipeline for\n  High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world validation of a multimodal LLM-powered pipeline for\n  High-Accuracy Clinical Trial Patient Matching leveraging EHR data"
                },
                "summary": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching."
                },
                "authors": [
                    {
                        "name": "Anatole Callies"
                    },
                    {
                        "name": "Quentin Bodinier"
                    },
                    {
                        "name": "Philippe Ravaud"
                    },
                    {
                        "name": "Kourosh Davarpanah"
                    }
                ],
                "author_detail": {
                    "name": "Kourosh Davarpanah"
                },
                "arxiv_affiliation": "Inato",
                "author": "Kourosh Davarpanah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05206v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05206v3",
                "updated": "2025-03-19T16:10:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    10,
                    18,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-02T05:14:22Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    5,
                    14,
                    22,
                    6,
                    33,
                    0
                ],
                "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
                },
                "summary": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models."
                },
                "authors": [
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Ye Sun"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Hengyuan Xu"
                    },
                    {
                        "name": "Yunhao Chen"
                    },
                    {
                        "name": "Yunhan Zhao"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Jingfeng Zhang"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Baoyuan Wu"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Ruoxi Jia"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Neil Gong"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Sarah Erfani"
                    },
                    {
                        "name": "Tim Baldwin"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Masashi Sugiyama"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "James Bailey"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "47 pages, 3 figures, 11 tables; GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05206v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05206v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18750v2",
                "updated": "2025-03-19T16:08:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    8,
                    36,
                    2,
                    78,
                    0
                ],
                "published": "2024-12-25T02:48:53Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    2,
                    48,
                    53,
                    2,
                    360,
                    0
                ],
                "title": "The Impact of Input Order Bias on Large Language Models for Software\n  Fault Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Input Order Bias on Large Language Models for Software\n  Fault Localization"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications."
                },
                "authors": [
                    {
                        "name": "Md Nakhla Rafi"
                    },
                    {
                        "name": "Dong Jae Kim"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    },
                    {
                        "name": "Shaowei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Wang"
                },
                "author": "Shaowei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15369v1",
                "updated": "2025-03-19T16:07:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    7,
                    4,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T16:07:04Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    16,
                    7,
                    4,
                    2,
                    78,
                    0
                ],
                "title": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language\n  Models"
                },
                "summary": "While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model."
                },
                "authors": [
                    {
                        "name": "Yinan Liang"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15358v1",
                "updated": "2025-03-19T15:58:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    58,
                    46,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:58:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    58,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation"
                },
                "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."
                },
                "authors": [
                    {
                        "name": "Thomas Pickard"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Maggie Mi"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Dylan Phelps"
                    },
                    {
                        "name": "Carolina Scarton"
                    },
                    {
                        "name": "Marco Idiart"
                    }
                ],
                "author_detail": {
                    "name": "Marco Idiart"
                },
                "author": "Marco Idiart",
                "arxiv_comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.4.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20227v2",
                "updated": "2025-03-19T15:56:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    56,
                    49,
                    2,
                    78,
                    0
                ],
                "published": "2024-12-28T17:48:33Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    48,
                    33,
                    5,
                    363,
                    0
                ],
                "title": "LLM Reasoning Engine: Specialized Training for Enhanced Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Reasoning Engine: Specialized Training for Enhanced Mathematical\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance in various\nnatural language processing tasks but face challenges in mathematical\nreasoning, where complex problem-solving requires both linguistic understanding\nand mathematical reasoning skills. Existing approaches to address this\nchallenge often rely on ensemble methods and suffer from the problem of data\nscarcity in target domains. In this work, we present a novel method to enhance\nLLMs' capabilities in mathematical reasoning tasks. Motivated by the need to\nbridge this gap, our approach incorporates a question paraphrase strategy,\nwhich aims at diversifying the linguistic forms of mathematical questions to\nimprove generalization. Additionally, specialized training objectives are\nemployed to guide the model's learning process, focusing on enhancing its\nunderstanding of mathematical concepts and reasoning processes. We conduct\nexperiments on four datasets using different LLMs, and demonstrate the\neffectiveness of our approach in improving LLMs' performance on mathematical\nreasoning tasks. Our findings underscore the significance of our methodology in\nthe advancement of large language models and its potential implications for\nreal-world applications that require mathematical reasoning abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance in various\nnatural language processing tasks but face challenges in mathematical\nreasoning, where complex problem-solving requires both linguistic understanding\nand mathematical reasoning skills. Existing approaches to address this\nchallenge often rely on ensemble methods and suffer from the problem of data\nscarcity in target domains. In this work, we present a novel method to enhance\nLLMs' capabilities in mathematical reasoning tasks. Motivated by the need to\nbridge this gap, our approach incorporates a question paraphrase strategy,\nwhich aims at diversifying the linguistic forms of mathematical questions to\nimprove generalization. Additionally, specialized training objectives are\nemployed to guide the model's learning process, focusing on enhancing its\nunderstanding of mathematical concepts and reasoning processes. We conduct\nexperiments on four datasets using different LLMs, and demonstrate the\neffectiveness of our approach in improving LLMs' performance on mathematical\nreasoning tasks. Our findings underscore the significance of our methodology in\nthe advancement of large language models and its potential implications for\nreal-world applications that require mathematical reasoning abilities."
                },
                "authors": [
                    {
                        "name": "Shuguang Chen"
                    },
                    {
                        "name": "Guang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guang Lin"
                },
                "author": "Guang Lin",
                "arxiv_comment": "Accepted to NAACL 2025 KnowledgeNLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15351v1",
                "updated": "2025-03-19T15:48:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    48,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:48:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    48,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models"
                },
                "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals."
                },
                "authors": [
                    {
                        "name": "I-Fan Lin"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13551v2",
                "updated": "2025-03-19T15:43:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    43,
                    56,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-16T15:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    15,
                    18,
                    40,
                    6,
                    75,
                    0
                ],
                "title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models"
                },
                "summary": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate\nsteps. In this paper, we propose a novel reward model approach, Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps from fine-grained and coarse-grained level. HRM performs better in\nassessing reasoning coherence and self-reflection, particularly when the\nprevious reasoning step is incorrect. Furthermore, to address the inefficiency\nof autonomous generating PRM training data via Monte Carlo Tree Search (MCTS),\nwe introduce a lightweight and effective data augmentation strategy called\nHierarchical Node Compression (HNC) based on node merging (combining two\nconsecutive reasoning steps into one step) in the tree structure. This approach\ndiversifies MCTS results for HRM with negligible computational overhead,\nenhancing label robustness by introducing noise. Empirical results on the\nPRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves\nsuperior stability and reliability in evaluation compared to PRM. Furthermore,\ncross-domain evaluations on MATH500 and GSM8K confirm HRM's superior\ngeneralization and robustness across diverse reasoning tasks. The code for all\nexperiments will be released at https:\n//github.com/tengwang0318/hierarchial_reward_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate\nsteps. In this paper, we propose a novel reward model approach, Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps from fine-grained and coarse-grained level. HRM performs better in\nassessing reasoning coherence and self-reflection, particularly when the\nprevious reasoning step is incorrect. Furthermore, to address the inefficiency\nof autonomous generating PRM training data via Monte Carlo Tree Search (MCTS),\nwe introduce a lightweight and effective data augmentation strategy called\nHierarchical Node Compression (HNC) based on node merging (combining two\nconsecutive reasoning steps into one step) in the tree structure. This approach\ndiversifies MCTS results for HRM with negligible computational overhead,\nenhancing label robustness by introducing noise. Empirical results on the\nPRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves\nsuperior stability and reliability in evaluation compared to PRM. Furthermore,\ncross-domain evaluations on MATH500 and GSM8K confirm HRM's superior\ngeneralization and robustness across diverse reasoning tasks. The code for all\nexperiments will be released at https:\n//github.com/tengwang0318/hierarchial_reward_model."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Zhangyi Jiang"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Yanan Zheng"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Shenyang Tong"
                    },
                    {
                        "name": "Hailei Gong"
                    }
                ],
                "author_detail": {
                    "name": "Hailei Gong"
                },
                "author": "Hailei Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15342v1",
                "updated": "2025-03-19T15:41:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    41,
                    32,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:41:32Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    41,
                    32,
                    2,
                    78,
                    0
                ],
                "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TruthLens:A Training-Free Paradigm for DeepFake Detection"
                },
                "summary": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation."
                },
                "authors": [
                    {
                        "name": "Ritabrata Chakraborty"
                    },
                    {
                        "name": "Rajatsubhra Chakraborty"
                    },
                    {
                        "name": "Ali Khaleghi Rahimian"
                    },
                    {
                        "name": "Thomas MacDougall"
                    }
                ],
                "author_detail": {
                    "name": "Thomas MacDougall"
                },
                "author": "Thomas MacDougall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15341v1",
                "updated": "2025-03-19T15:40:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    40,
                    45,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:40:45Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    40,
                    45,
                    2,
                    78,
                    0
                ],
                "title": "Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Guided Chain-of-Thought for Code Generation with LLMs"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has been demonstrated as an effective\ntechnique for improving the problem-solving capabilities of large language\nmodels (LLMs) in the context of code generation. However, existing CoT methods\noften exhibit a tendency toward \"overthinking\", where the LLM consistently\napplies reasoning strategies without adequately considering the task's\nunderlying complexity. This results in the LLMs allocating excessive\ncomputational resources, in terms of tokens, to relatively simple tasks or\nproblems where the correct answer is already evident. Additionally, this\noverthinking may lead LLMs down incorrect reasoning paths, resulting in\nincorrect code generation. In this paper, we introduce UnCertainty-Aware\nChain-of-Thought (UnCert-CoT), an LLM-based approach designed to enhance code\ngeneration by incorporating an uncertainty-aware CoT reasoning mechanism, which\nfocuses computational resources on targeting points where LLMs are more prone\nto error. We propose two confidence-based uncertainty measures: Entropy-based\nand Probability Differential-based methods. When uncertainty is high,\nUnCert-CoT activates CoT-decoding to generate multiple reasoning paths and\nselects the final code that exhibits the highest likelihood of correctness. In\ncontrast, LLM directly generates the code when uncertainty is low. This\nuncertainty judgment mechanism allows LLMs to prioritize complex tasks and\navoid unnecessary steps in simpler cases, thereby improving overall efficiency\nand accuracy in code generation. Our experimental results demonstrate that\nUnCert-CoT significantly enhances code generation accuracy on challenging\nbenchmark MHPP(Mostly Hard Python Problems), it achieves improvements up to\n6.1% on PassRate accuracy, particularly in situations where traditional LLMs\nare prone to errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has been demonstrated as an effective\ntechnique for improving the problem-solving capabilities of large language\nmodels (LLMs) in the context of code generation. However, existing CoT methods\noften exhibit a tendency toward \"overthinking\", where the LLM consistently\napplies reasoning strategies without adequately considering the task's\nunderlying complexity. This results in the LLMs allocating excessive\ncomputational resources, in terms of tokens, to relatively simple tasks or\nproblems where the correct answer is already evident. Additionally, this\noverthinking may lead LLMs down incorrect reasoning paths, resulting in\nincorrect code generation. In this paper, we introduce UnCertainty-Aware\nChain-of-Thought (UnCert-CoT), an LLM-based approach designed to enhance code\ngeneration by incorporating an uncertainty-aware CoT reasoning mechanism, which\nfocuses computational resources on targeting points where LLMs are more prone\nto error. We propose two confidence-based uncertainty measures: Entropy-based\nand Probability Differential-based methods. When uncertainty is high,\nUnCert-CoT activates CoT-decoding to generate multiple reasoning paths and\nselects the final code that exhibits the highest likelihood of correctness. In\ncontrast, LLM directly generates the code when uncertainty is low. This\nuncertainty judgment mechanism allows LLMs to prioritize complex tasks and\navoid unnecessary steps in simpler cases, thereby improving overall efficiency\nand accuracy in code generation. Our experimental results demonstrate that\nUnCert-CoT significantly enhances code generation accuracy on challenging\nbenchmark MHPP(Mostly Hard Python Problems), it achieves improvements up to\n6.1% on PassRate accuracy, particularly in situations where traditional LLMs\nare prone to errors."
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hong Mei"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Yihong Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Dong"
                },
                "author": "Yihong Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08415v2",
                "updated": "2025-03-19T15:40:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    40,
                    41,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-11T13:24:39Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    24,
                    39,
                    1,
                    70,
                    0
                ],
                "title": "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems"
                },
                "summary": "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Guoyang Duan"
                    },
                    {
                        "name": "Tianle Xu"
                    },
                    {
                        "name": "Junchi Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Yongqiang Yao"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "arxiv_comment": "9 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14703v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14703v3",
                "updated": "2025-03-19T15:37:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    37,
                    42,
                    2,
                    78,
                    0
                ],
                "published": "2024-06-20T19:50:56Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    19,
                    50,
                    56,
                    3,
                    172,
                    0
                ],
                "title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction."
                },
                "authors": [
                    {
                        "name": "Seungbeen Lee"
                    },
                    {
                        "name": "Seungwon Lim"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Giyeong Oh"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Beong-woo Kwak"
                    },
                    {
                        "name": "Yeonsoo Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "Accepted to NAACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14703v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14703v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15338v1",
                "updated": "2025-03-19T15:34:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    34,
                    21,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:34:21Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    34,
                    21,
                    2,
                    78,
                    0
                ],
                "title": "Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context"
                },
                "summary": "Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio."
                },
                "authors": [
                    {
                        "name": "Junyi Ao"
                    },
                    {
                        "name": "Dekun Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Wenjie Feng"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Zhizheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhizheng Wu"
                },
                "author": "Zhizheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12167v2",
                "updated": "2025-03-19T15:23:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    23,
                    29,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-15T15:11:17Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    15,
                    11,
                    17,
                    5,
                    74,
                    0
                ],
                "title": "PLM: Efficient Peripheral Language Models Hardware-Co-Designed for\n  Ubiquitous Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLM: Efficient Peripheral Language Models Hardware-Co-Designed for\n  Ubiquitous Computing"
                },
                "summary": "While scaling laws have been continuously validated in large language models\n(LLMs) with increasing model parameters, the inherent tension between the\ninference demands of LLMs and the limited resources of edge devices poses a\ncritical challenge to the development of edge intelligence. Recently, numerous\nsmall language models have emerged, aiming to distill the capabilities of LLMs\ninto smaller footprints. However, these models often retain the fundamental\narchitectural principles of their larger counterparts, still imposing\nconsiderable strain on the storage and bandwidth capacities of edge devices. In\nthis paper, we introduce the PLM, a Peripheral Language Model, developed\nthrough a co-design process that jointly optimizes model architecture and edge\nsystem constraints. The PLM utilizes a Multi-head Latent Attention mechanism\nand employs the squared ReLU activation function to encourage sparsity, thereby\nreducing peak memory footprint during inference. During training, we collect\nand reorganize open-source datasets, implement a multi-phase training strategy,\nand empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning\nrate scheduler. Additionally, we incorporate Reinforcement Learning from Human\nFeedback (RLHF) by adopting the ARIES preference learning approach. Following a\ntwo-phase SFT process, this method yields performance gains of 2% in general\ntasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel\narchitecture, evaluation results demonstrate that PLM outperforms existing\nsmall language models trained on publicly available data while maintaining the\nlowest number of activated parameters. Furthermore, deployment across various\nedge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis,\nvalidates PLM's suitability for peripheral applications. The PLM series models\nare publicly available at https://github.com/plm-team/PLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While scaling laws have been continuously validated in large language models\n(LLMs) with increasing model parameters, the inherent tension between the\ninference demands of LLMs and the limited resources of edge devices poses a\ncritical challenge to the development of edge intelligence. Recently, numerous\nsmall language models have emerged, aiming to distill the capabilities of LLMs\ninto smaller footprints. However, these models often retain the fundamental\narchitectural principles of their larger counterparts, still imposing\nconsiderable strain on the storage and bandwidth capacities of edge devices. In\nthis paper, we introduce the PLM, a Peripheral Language Model, developed\nthrough a co-design process that jointly optimizes model architecture and edge\nsystem constraints. The PLM utilizes a Multi-head Latent Attention mechanism\nand employs the squared ReLU activation function to encourage sparsity, thereby\nreducing peak memory footprint during inference. During training, we collect\nand reorganize open-source datasets, implement a multi-phase training strategy,\nand empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning\nrate scheduler. Additionally, we incorporate Reinforcement Learning from Human\nFeedback (RLHF) by adopting the ARIES preference learning approach. Following a\ntwo-phase SFT process, this method yields performance gains of 2% in general\ntasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel\narchitecture, evaluation results demonstrate that PLM outperforms existing\nsmall language models trained on publicly available data while maintaining the\nlowest number of activated parameters. Furthermore, deployment across various\nedge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis,\nvalidates PLM's suitability for peripheral applications. The PLM series models\nare publicly available at https://github.com/plm-team/PLM."
                },
                "authors": [
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Wenxin Zhao"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel M. Ni"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09454v2",
                "updated": "2025-03-19T15:23:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    23,
                    4,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-12T14:57:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    57,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "Explicit Learning and the LLM in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Learning and the LLM in Machine Translation"
                },
                "summary": "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs."
                },
                "authors": [
                    {
                        "name": "Malik Marmonier"
                    },
                    {
                        "name": "Rachel Bawden"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15301v1",
                "updated": "2025-03-19T15:22:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    22,
                    58,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:22:58Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    22,
                    58,
                    2,
                    78,
                    0
                ],
                "title": "aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in\n  Repository-level Code Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in\n  Repository-level Code Completion"
                },
                "summary": "Repository-level code completion aims to complete code based on the long\ncontexts of the repository. Existing studies extract long contexts from the\nrepository as inputs and leverage Large Language Models (LLMs) to generate\ncode. However, we reveal a severe limitation of LLMs, i.e., LLMs may ignore the\ninformation within long contexts in code completion. In other words, even the\ncontexts contain useful information (e.g., relevant APIs or similar code), LLMs\nmay fail to utilize this information. We think this limitation is caused by an\ninherent bias in LLMs, i.e., relying on nearby contexts and ignoring long-range\ncontexts. To address this, we propose a novel fine-tuning approach named CoLT.\nThe core idea of CoLT is to provide explicit supervision signals, which\nemphasize that long-range contexts may hold relevant information. Specifically,\nCoLT proposes a reinforcement learning-based training, which explicitly\nencourages models to utilize the information within long contexts and punishes\nmodels for ignoring long contexts. To support CoLT, we release CoLT-132K, a\nlarge-scale dataset with 132k samples across four languages, each containing\nlong-context inputs. We apply CoLT to a popular LLM - aiXcoder-7B and release\naiXcoder-7B-v2. We conduct extensive experiments on CoLT-132K and a public\nbenchmark - CrossCodeEval. Our experiments yield the results: 1. Effectiveness.\nCoLT substantially improves aiXcoder-7B. aiXcoder-7B-v2 outperforms aiXcoder-7B\nby up to 44% in exact match. aiXcoder-7B-v2 becomes the state-of-the-art 7B\nmodel in code completion and even surpasses larger models. 2. Generalizability.\nThe capability learned by CoLT can generalize to new languages. Besides, CoLT\nis model-agnostic and effectively improves multiple LLMs. 3. Enhanced Context\nUtilization Capability. CoLT significantly improves the capability of LLMs in\nutilizing the relevant information within long contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code completion aims to complete code based on the long\ncontexts of the repository. Existing studies extract long contexts from the\nrepository as inputs and leverage Large Language Models (LLMs) to generate\ncode. However, we reveal a severe limitation of LLMs, i.e., LLMs may ignore the\ninformation within long contexts in code completion. In other words, even the\ncontexts contain useful information (e.g., relevant APIs or similar code), LLMs\nmay fail to utilize this information. We think this limitation is caused by an\ninherent bias in LLMs, i.e., relying on nearby contexts and ignoring long-range\ncontexts. To address this, we propose a novel fine-tuning approach named CoLT.\nThe core idea of CoLT is to provide explicit supervision signals, which\nemphasize that long-range contexts may hold relevant information. Specifically,\nCoLT proposes a reinforcement learning-based training, which explicitly\nencourages models to utilize the information within long contexts and punishes\nmodels for ignoring long contexts. To support CoLT, we release CoLT-132K, a\nlarge-scale dataset with 132k samples across four languages, each containing\nlong-context inputs. We apply CoLT to a popular LLM - aiXcoder-7B and release\naiXcoder-7B-v2. We conduct extensive experiments on CoLT-132K and a public\nbenchmark - CrossCodeEval. Our experiments yield the results: 1. Effectiveness.\nCoLT substantially improves aiXcoder-7B. aiXcoder-7B-v2 outperforms aiXcoder-7B\nby up to 44% in exact match. aiXcoder-7B-v2 becomes the state-of-the-art 7B\nmodel in code completion and even surpasses larger models. 2. Generalizability.\nThe capability learned by CoLT can generalize to new languages. Besides, CoLT\nis model-agnostic and effectively improves multiple LLMs. 3. Enhanced Context\nUtilization Capability. CoLT significantly improves the capability of LLMs in\nutilizing the relevant information within long contexts."
                },
                "authors": [
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Xianjie Shi"
                    },
                    {
                        "name": "He Zong"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Siyuan Jiang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15299v1",
                "updated": "2025-03-19T15:21:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    48,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:21:48Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    48,
                    2,
                    78,
                    0
                ],
                "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside-Out: Hidden Factual Knowledge in LLMs"
                },
                "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."
                },
                "authors": [
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Eyal Ben David"
                    },
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Eran Ofek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Idan Szpector"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13857v2",
                "updated": "2025-03-19T15:21:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    21,
                    6,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-18T03:14:23Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    3,
                    14,
                    23,
                    1,
                    77,
                    0
                ],
                "title": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations"
                },
                "summary": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate systematic incorporation of preprint\narticles in evidence-based medicine, supporting researchers in more effective\nevaluation and utilization of preprint resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate systematic incorporation of preprint\narticles in evidence-based medicine, supporting researchers in more effective\nevaluation and utilization of preprint resources."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jiayi Tong"
                    },
                    {
                        "name": "Haoyuan Wang"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Ziyang Hu"
                    },
                    {
                        "name": "Peiyu Li"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Christopher J. Lindsell"
                    },
                    {
                        "name": "Michael J. Pencina"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Chuan Hong"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Hong"
                },
                "author": "Chuan Hong",
                "arxiv_comment": "28 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15289v1",
                "updated": "2025-03-19T15:09:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    9,
                    39,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:09:39Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    9,
                    39,
                    2,
                    78,
                    0
                ],
                "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification"
                },
                "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation."
                },
                "authors": [
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Min Xiao"
                    },
                    {
                        "name": "Yining Wang"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Chengqing Zong"
                    }
                ],
                "author_detail": {
                    "name": "Chengqing Zong"
                },
                "author": "Chengqing Zong",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15282v1",
                "updated": "2025-03-19T15:02:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    2,
                    7,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T15:02:07Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    2,
                    7,
                    2,
                    78,
                    0
                ],
                "title": "SENAI: Towards Software Engineering Native Generative Artificial\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SENAI: Towards Software Engineering Native Generative Artificial\n  Intelligence"
                },
                "summary": "Large Language Models have significantly advanced the field of code\ngeneration, demonstrating the ability to produce functionally correct code\nsnippets. However, advancements in generative AI for code overlook foundational\nSoftware Engineering (SE) principles such as modularity, and single\nresponsibility, and concepts such as cohesion and coupling which are critical\nfor creating maintainable, scalable, and robust software systems. These\nconcepts are missing in pipelines that start with pre-training and end with the\nevaluation using benchmarks.\n  This vision paper argues for the integration of SE knowledge into LLMs to\nenhance their capability to understand, analyze, and generate code and other SE\nartifacts following established SE knowledge. The aim is to propose a new\ndirection where LLMs can move beyond mere functional accuracy to perform\ngenerative tasks that require adherence to SE principles and best practices. In\naddition, given the interactive nature of these conversational models, we\npropose using Bloom's Taxonomy as a framework to assess the extent to which\nthey internalize SE knowledge. The proposed evaluation framework offers a sound\nand more comprehensive evaluation technique compared to existing approaches\nsuch as linear probing. Software engineering native generative models will not\nonly overcome the shortcomings present in current models but also pave the way\nfor the next generation of generative models capable of handling real-world\nsoftware engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have significantly advanced the field of code\ngeneration, demonstrating the ability to produce functionally correct code\nsnippets. However, advancements in generative AI for code overlook foundational\nSoftware Engineering (SE) principles such as modularity, and single\nresponsibility, and concepts such as cohesion and coupling which are critical\nfor creating maintainable, scalable, and robust software systems. These\nconcepts are missing in pipelines that start with pre-training and end with the\nevaluation using benchmarks.\n  This vision paper argues for the integration of SE knowledge into LLMs to\nenhance their capability to understand, analyze, and generate code and other SE\nartifacts following established SE knowledge. The aim is to propose a new\ndirection where LLMs can move beyond mere functional accuracy to perform\ngenerative tasks that require adherence to SE principles and best practices. In\naddition, given the interactive nature of these conversational models, we\npropose using Bloom's Taxonomy as a framework to assess the extent to which\nthey internalize SE knowledge. The proposed evaluation framework offers a sound\nand more comprehensive evaluation technique compared to existing approaches\nsuch as linear probing. Software engineering native generative models will not\nonly overcome the shortcomings present in current models but also pave the way\nfor the next generation of generative models capable of handling real-world\nsoftware engineering."
                },
                "authors": [
                    {
                        "name": "Mootez Saad"
                    },
                    {
                        "name": "José Antonio Hernández López"
                    },
                    {
                        "name": "Boqi Chen"
                    },
                    {
                        "name": "Neil Ernst"
                    },
                    {
                        "name": "Dániel Varró"
                    },
                    {
                        "name": "Tushar Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Sharma"
                },
                "author": "Tushar Sharma",
                "arxiv_comment": "5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02191v3",
                "updated": "2025-03-19T14:54:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    54,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-04T02:01:37Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    2,
                    1,
                    37,
                    1,
                    63,
                    0
                ],
                "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Predicting Derailment in Toxic Conversations on GitHub"
                },
                "summary": "Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software projects thrive on the involvement and contributions of individuals\nfrom different backgrounds. However, toxic language and negative interactions\ncan hinder the participation and retention of contributors and alienate\nnewcomers. Proactive moderation strategies aim to prevent toxicity from\noccurring by addressing conversations that have derailed from their intended\npurpose. This study aims to understand and predict conversational derailment\nleading to toxicity on GitHub.\n  To facilitate this research, we curate a novel dataset comprising 202 toxic\nconversations from GitHub with annotated derailment points, along with 696\nnon-toxic conversations as a baseline. Based on this dataset, we identify\nunique characteristics of toxic conversations and derailment points, including\nlinguistic markers such as second-person pronouns, negation terms, and tones of\nBitter Frustration and Impatience, as well as patterns in conversational\ndynamics between project contributors and external participants.\n  Leveraging these empirical observations, we propose a proactive moderation\napproach to automatically detect and address potentially harmful conversations\nbefore escalation. By utilizing modern LLMs, we develop a conversation\ntrajectory summary technique that captures the evolution of discussions and\nidentifies early signs of derailment. Our experiments demonstrate that LLM\nprompts tailored to provide summaries of GitHub conversations achieve 70%\nF1-Score in predicting conversational derailment, strongly improving over a set\nof baseline approaches."
                },
                "authors": [
                    {
                        "name": "Mia Mohammad Imran"
                    },
                    {
                        "name": "Robert Zita"
                    },
                    {
                        "name": "Rebekah Copeland"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    },
                    {
                        "name": "Rahat Rizvi Rahman"
                    },
                    {
                        "name": "Kostadin Damevski"
                    }
                ],
                "author_detail": {
                    "name": "Kostadin Damevski"
                },
                "author": "Kostadin Damevski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15272v1",
                "updated": "2025-03-19T14:46:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    46,
                    53,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:46:53Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    46,
                    53,
                    2,
                    78,
                    0
                ],
                "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with\n  Multi-Agent Collaboration"
                },
                "summary": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe."
                },
                "authors": [
                    {
                        "name": "David Wan"
                    },
                    {
                        "name": "Justin Chih-Yao Chen"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15268v1",
                "updated": "2025-03-19T14:44:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    44,
                    2,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:44:02Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    44,
                    2,
                    2,
                    78,
                    0
                ],
                "title": "Do Chains-of-Thoughts of Large Language Models Suffer from\n  Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Chains-of-Thoughts of Large Language Models Suffer from\n  Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?"
                },
                "summary": "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies."
                },
                "authors": [
                    {
                        "name": "Roberto Araya"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Araya"
                },
                "author": "Roberto Araya",
                "arxiv_comment": "24 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15030v2",
                "updated": "2025-03-19T14:33:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    33,
                    50,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-20T20:34:41Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    20,
                    34,
                    41,
                    3,
                    51,
                    0
                ],
                "title": "CHOIR: Chat-based Helper for Organizational Intelligence Repository",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHOIR: Chat-based Helper for Organizational Intelligence Repository"
                },
                "summary": "Modern organizations frequently rely on chat-based platforms (e.g., Slack,\nMicrosoft Teams, and Discord) for day-to-day communication and decision-making.\nAs conversations evolve, organizational knowledge can get buried, prompting\nrepeated searches and discussions. While maintaining shared documents, such as\nWiki articles for the organization, offers a partial solution, it requires\nmanual and timely efforts to keep it up to date, and it may not effectively\npreserve the social and contextual aspect of prior discussions. Moreover,\nreaching a consensus on document updates with relevant stakeholders can be\ntime-consuming and complex. To address these challenges, we introduce CHOIR\n(Chat-based Helper for Organizational Intelligence Repository), a chatbot that\nintegrates seamlessly with chat platforms. CHOIR automatically identifies and\nproposes edits to related documents, initiates discussions with relevant team\nmembers, and preserves contextual revision histories. By embedding knowledge\nmanagement directly into chat environments and leveraging LLMs, CHOIR\nsimplifies manual updates and supports consensus-driven editing based on\nmaintained context with revision histories. We plan to design, deploy, and\nevaluate CHOIR in the context of maintaining an organizational memory for a\nresearch lab. We describe the chatbot's motivation, design, and early\nimplementation to show how CHOIR streamlines collaborative document management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern organizations frequently rely on chat-based platforms (e.g., Slack,\nMicrosoft Teams, and Discord) for day-to-day communication and decision-making.\nAs conversations evolve, organizational knowledge can get buried, prompting\nrepeated searches and discussions. While maintaining shared documents, such as\nWiki articles for the organization, offers a partial solution, it requires\nmanual and timely efforts to keep it up to date, and it may not effectively\npreserve the social and contextual aspect of prior discussions. Moreover,\nreaching a consensus on document updates with relevant stakeholders can be\ntime-consuming and complex. To address these challenges, we introduce CHOIR\n(Chat-based Helper for Organizational Intelligence Repository), a chatbot that\nintegrates seamlessly with chat platforms. CHOIR automatically identifies and\nproposes edits to related documents, initiates discussions with relevant team\nmembers, and preserves contextual revision histories. By embedding knowledge\nmanagement directly into chat environments and leveraging LLMs, CHOIR\nsimplifies manual updates and supports consensus-driven editing based on\nmaintained context with revision histories. We plan to design, deploy, and\nevaluate CHOIR in the context of maintaining an organizational memory for a\nresearch lab. We describe the chatbot's motivation, design, and early\nimplementation to show how CHOIR streamlines collaborative document management."
                },
                "authors": [
                    {
                        "name": "Sangwook Lee"
                    },
                    {
                        "name": "Adnan Abbas"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Sang Won Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Won Lee"
                },
                "author": "Sang Won Lee",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15252v1",
                "updated": "2025-03-19T14:26:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    26,
                    9,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:26:09Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    26,
                    9,
                    2,
                    78,
                    0
                ],
                "title": "Efficient allocation of image recognition and LLM tasks on multi-GPU\n  system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient allocation of image recognition and LLM tasks on multi-GPU\n  system"
                },
                "summary": "This work is concerned with the evaluation of the performance of\nparallelization of learning and tuning processes for image classification and\nlarge language models. For machine learning model in image recognition, various\nparallelization methods are developed based on different hardware and software\nscenarios: simple data parallelism, distributed data parallelism, and\ndistributed processing. A detailed description of presented strategies is\ngiven, highlighting the challenges and benefits of their application.\nFurthermore, the impact of different dataset types on the tuning process of\nlarge language models is investigated. Experiments show to what extent the task\ntype affects the iteration time in a multi-GPU environment, offering valuable\ninsights into the optimal data utilization strategies to improve model\nperformance. Furthermore, this study leverages the built-in parallelization\nmechanisms of PyTorch that can facilitate these tasks. Furthermore, performance\nprofiling is incorporated into the study to thoroughly evaluate the impact of\nmemory and communication operations during the training/tuning procedure. Test\nscenarios are developed and tested with numerous benchmarks on the NVIDIA H100\narchitecture showing efficiency through selected metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work is concerned with the evaluation of the performance of\nparallelization of learning and tuning processes for image classification and\nlarge language models. For machine learning model in image recognition, various\nparallelization methods are developed based on different hardware and software\nscenarios: simple data parallelism, distributed data parallelism, and\ndistributed processing. A detailed description of presented strategies is\ngiven, highlighting the challenges and benefits of their application.\nFurthermore, the impact of different dataset types on the tuning process of\nlarge language models is investigated. Experiments show to what extent the task\ntype affects the iteration time in a multi-GPU environment, offering valuable\ninsights into the optimal data utilization strategies to improve model\nperformance. Furthermore, this study leverages the built-in parallelization\nmechanisms of PyTorch that can facilitate these tasks. Furthermore, performance\nprofiling is incorporated into the study to thoroughly evaluate the impact of\nmemory and communication operations during the training/tuning procedure. Test\nscenarios are developed and tested with numerous benchmarks on the NVIDIA H100\narchitecture showing efficiency through selected metrics."
                },
                "authors": [
                    {
                        "name": "Marcin Lawenda"
                    },
                    {
                        "name": "Krzesimir Samborski"
                    },
                    {
                        "name": "Kyrylo Khloponin"
                    },
                    {
                        "name": "Łukasz Szustak"
                    }
                ],
                "author_detail": {
                    "name": "Łukasz Szustak"
                },
                "author": "Łukasz Szustak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14286v2",
                "updated": "2025-03-19T14:25:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    25,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-18T14:23:37Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    23,
                    37,
                    1,
                    77,
                    0
                ],
                "title": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs"
                },
                "summary": "We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance."
                },
                "authors": [
                    {
                        "name": "Nicolas Le Roux"
                    },
                    {
                        "name": "Marc G. Bellemare"
                    },
                    {
                        "name": "Jonathan Lebensold"
                    },
                    {
                        "name": "Arnaud Bergeron"
                    },
                    {
                        "name": "Joshua Greaves"
                    },
                    {
                        "name": "Alex Fréchette"
                    },
                    {
                        "name": "Carolyne Pelletier"
                    },
                    {
                        "name": "Eric Thibodeau-Laufer"
                    },
                    {
                        "name": "Sándor Toth"
                    },
                    {
                        "name": "Sam Work"
                    }
                ],
                "author_detail": {
                    "name": "Sam Work"
                },
                "author": "Sam Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15249v1",
                "updated": "2025-03-19T14:24:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    24,
                    19,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:24:19Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    24,
                    19,
                    2,
                    78,
                    0
                ],
                "title": "The Effects of iBGP Convergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effects of iBGP Convergence"
                },
                "summary": "Analyzing violations of forwarding properties is a classic networking\nproblem. However, existing work is either tailored to the steady state -- and\nnot to transient states during iBGP convergence -- or does analyze transient\nviolations but with inaccurate proxies, like control-plane convergence, or\nwithout precise control over the different impact factors.\n  We address this gap with a measurement framework that controllably and\naccurately measures transient violation times in realistic network deployments.\nThe framework relies on a programmable switch to flexibly emulate diverse\ntopologies and gain traffic visibility at all links -- enabling accurately\ninferring violation times of any forwarding property. Using the framework, we\nanalyze 50 network scenarios on a topology with 12 real routers, and show how\nfactors like the network configuration and BGP event affect transient violation\ntimes. Further, we shed light on less-known aspects of BGP convergence,\nincluding that transient violations can start before the trigger event, or that\nkeeping a backup route advertised at all times can increase violation times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing violations of forwarding properties is a classic networking\nproblem. However, existing work is either tailored to the steady state -- and\nnot to transient states during iBGP convergence -- or does analyze transient\nviolations but with inaccurate proxies, like control-plane convergence, or\nwithout precise control over the different impact factors.\n  We address this gap with a measurement framework that controllably and\naccurately measures transient violation times in realistic network deployments.\nThe framework relies on a programmable switch to flexibly emulate diverse\ntopologies and gain traffic visibility at all links -- enabling accurately\ninferring violation times of any forwarding property. Using the framework, we\nanalyze 50 network scenarios on a topology with 12 real routers, and show how\nfactors like the network configuration and BGP event affect transient violation\ntimes. Further, we shed light on less-known aspects of BGP convergence,\nincluding that transient violations can start before the trigger event, or that\nkeeping a backup route advertised at all times can increase violation times."
                },
                "authors": [
                    {
                        "name": "Roland Schmid"
                    },
                    {
                        "name": "Tibor Schneider"
                    },
                    {
                        "name": "Georgia Fragkouli"
                    },
                    {
                        "name": "Laurent Vanbever"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Vanbever"
                },
                "author": "Laurent Vanbever",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15248v1",
                "updated": "2025-03-19T14:23:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    23,
                    22,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:23:22Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    23,
                    22,
                    2,
                    78,
                    0
                ],
                "title": "Automated Non-Functional Requirements Generation in Software Engineering\n  with Large Language Models: A Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Non-Functional Requirements Generation in Software Engineering\n  with Large Language Models: A Comparative Study"
                },
                "summary": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering."
                },
                "authors": [
                    {
                        "name": "Jomar Thomas Almonte"
                    },
                    {
                        "name": "Santhosh Anitha Boominathan"
                    },
                    {
                        "name": "Nathalia Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Nathalia Nascimento"
                },
                "author": "Nathalia Nascimento",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15243v1",
                "updated": "2025-03-19T14:20:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    20,
                    24,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:20:24Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    20,
                    24,
                    2,
                    78,
                    0
                ],
                "title": "Integrating Sensing and Communications in 6G? Not Until It Is Secure to\n  Do So",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Sensing and Communications in 6G? Not Until It Is Secure to\n  Do So"
                },
                "summary": "Integrated Sensing and Communication (ISAC) is emerging as a cornerstone\ntechnology for forthcoming 6G systems, significantly improving spectrum and\nenergy efficiency. However, the commercial viability of ISAC hinges on\naddressing critical challenges surrounding security, privacy, and\ntrustworthiness. These challenges necessitate an end-to-end framework to\nsafeguards both communication data and sensing information, particularly in\nultra-low-latency and highly connected environments. Conventional solutions,\nsuch as encryption and key management, often fall short when confronted with\nISAC's dual-functional nature. In this context, the physical layer plays a\npivotal role: this article reviews emerging physical-layer strategies,\nincluding artificial noise (AN) injection, cooperative jamming, and\nconstructive interference (CI), which enhance security by mitigating\neavesdropping risks and safeguarding both communication data and sensing\ninformation. We further highlight the unique privacy issues that ISAC\nintroduces to cellular networks and outline future research directions aimed at\nensuring robust security and privacy for efficient ISAC deployment in 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Sensing and Communication (ISAC) is emerging as a cornerstone\ntechnology for forthcoming 6G systems, significantly improving spectrum and\nenergy efficiency. However, the commercial viability of ISAC hinges on\naddressing critical challenges surrounding security, privacy, and\ntrustworthiness. These challenges necessitate an end-to-end framework to\nsafeguards both communication data and sensing information, particularly in\nultra-low-latency and highly connected environments. Conventional solutions,\nsuch as encryption and key management, often fall short when confronted with\nISAC's dual-functional nature. In this context, the physical layer plays a\npivotal role: this article reviews emerging physical-layer strategies,\nincluding artificial noise (AN) injection, cooperative jamming, and\nconstructive interference (CI), which enhance security by mitigating\neavesdropping risks and safeguarding both communication data and sensing\ninformation. We further highlight the unique privacy issues that ISAC\nintroduces to cellular networks and outline future research directions aimed at\nensuring robust security and privacy for efficient ISAC deployment in 6G."
                },
                "authors": [
                    {
                        "name": "Nanchi Su"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Jiaqi Zou"
                    },
                    {
                        "name": "Christos Masouros"
                    },
                    {
                        "name": "George C. Alexandropoulos"
                    },
                    {
                        "name": "Alain Mourad"
                    },
                    {
                        "name": "Javier Lorca Hernando"
                    },
                    {
                        "name": "Qinyu Zhang"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages; 5 figures; submitted to an IEEE magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15242v1",
                "updated": "2025-03-19T14:19:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    19,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:19:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    19,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?"
                },
                "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time."
                },
                "authors": [
                    {
                        "name": "Pierre Chambon"
                    },
                    {
                        "name": "Baptiste Roziere"
                    },
                    {
                        "name": "Benoit Sagot"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Synnaeve"
                },
                "author": "Gabriel Synnaeve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13413v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13413v3",
                "updated": "2025-03-19T14:18:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    18,
                    1,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-17T17:42:51Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    42,
                    51,
                    0,
                    76,
                    0
                ],
                "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO."
                },
                "authors": [
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Jingjing Chen"
                    },
                    {
                        "name": "Libo Qin"
                    }
                ],
                "author_detail": {
                    "name": "Libo Qin"
                },
                "author": "Libo Qin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13413v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13413v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12896v2",
                "updated": "2025-03-19T14:15:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    15,
                    12,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-18T14:32:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    32,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks"
                },
                "summary": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers."
                },
                "authors": [
                    {
                        "name": "Eva Sánchez Salido"
                    },
                    {
                        "name": "Julio Gonzalo"
                    },
                    {
                        "name": "Guillermo Marco"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Marco"
                },
                "author": "Guillermo Marco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15234v1",
                "updated": "2025-03-19T14:13:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    13,
                    2,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:13:02Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    13,
                    2,
                    2,
                    78,
                    0
                ],
                "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit\n  Description and Polysemanticity Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit\n  Description and Polysemanticity Quantification"
                },
                "summary": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores."
                },
                "authors": [
                    {
                        "name": "Wenlong Yu"
                    },
                    {
                        "name": "Qilong Wang"
                    },
                    {
                        "name": "Chuang Liu"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    }
                ],
                "author_detail": {
                    "name": "Qinghua Hu"
                },
                "author": "Qinghua Hu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15235v1",
                "updated": "2025-03-19T14:13:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    13,
                    2,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:13:02Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    13,
                    2,
                    2,
                    78,
                    0
                ],
                "title": "Exploring Large Language Models for Word Games:Who is the Spy?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Models for Word Games:Who is the Spy?"
                },
                "summary": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy."
                },
                "authors": [
                    {
                        "name": "Chentian Wei"
                    },
                    {
                        "name": "Jiewei Chen"
                    },
                    {
                        "name": "Jinzhu Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhu Xu"
                },
                "author": "Jinzhu Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15231v1",
                "updated": "2025-03-19T14:08:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    8,
                    47,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:08:47Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    8,
                    47,
                    2,
                    78,
                    0
                ],
                "title": "When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code\n  Generation Just as It Helps Developers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code\n  Generation Just as It Helps Developers?"
                },
                "summary": "Retrieval-augmented generation (RAG) has increasingly shown its power in\nextending large language models' (LLMs') capability beyond their pre-trained\nknowledge. Existing works have shown that RAG can help with software\ndevelopment tasks such as code generation, code update, and test generation.\nYet, the effectiveness of adapting LLMs to fast-evolving or less common API\nlibraries using RAG remains unknown. To bridge this gap, we take an initial\nstep to study this unexplored yet practical setting - when developers code with\na less common library, they often refer to its API documentation; likewise,\nwhen LLMs are allowed to look up API documentation via RAG, to what extent can\nLLMs be advanced? To mimic such a setting, we select four less common\nopen-source Python libraries with a total of 1017 eligible APIs. We study the\nfactors that affect the effectiveness of using the documentation of less common\nAPI libraries as additional knowledge for retrieval and generation. Our\nintensive study yields interesting findings: (1) RAG helps improve LLMs'\nperformance by 83%-220%. (2) Example code contributes the most to advance LLMs,\ninstead of the descriptive texts and parameter lists in the API documentation.\n(3) LLMs could sometimes tolerate mild noises (typos in description or\nincorrect parameters) by referencing their pre-trained knowledge or document\ncontext. Finally, we suggest that developers pay more attention to the quality\nand diversity of the code examples in the API documentation. The study sheds\nlight on future low-code software development workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has increasingly shown its power in\nextending large language models' (LLMs') capability beyond their pre-trained\nknowledge. Existing works have shown that RAG can help with software\ndevelopment tasks such as code generation, code update, and test generation.\nYet, the effectiveness of adapting LLMs to fast-evolving or less common API\nlibraries using RAG remains unknown. To bridge this gap, we take an initial\nstep to study this unexplored yet practical setting - when developers code with\na less common library, they often refer to its API documentation; likewise,\nwhen LLMs are allowed to look up API documentation via RAG, to what extent can\nLLMs be advanced? To mimic such a setting, we select four less common\nopen-source Python libraries with a total of 1017 eligible APIs. We study the\nfactors that affect the effectiveness of using the documentation of less common\nAPI libraries as additional knowledge for retrieval and generation. Our\nintensive study yields interesting findings: (1) RAG helps improve LLMs'\nperformance by 83%-220%. (2) Example code contributes the most to advance LLMs,\ninstead of the descriptive texts and parameter lists in the API documentation.\n(3) LLMs could sometimes tolerate mild noises (typos in description or\nincorrect parameters) by referencing their pre-trained knowledge or document\ncontext. Finally, we suggest that developers pay more attention to the quality\nand diversity of the code examples in the API documentation. The study sheds\nlight on future low-code software development workflows."
                },
                "authors": [
                    {
                        "name": "Jingyi Chen"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Jiasi Shen"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15225v1",
                "updated": "2025-03-19T14:03:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    3,
                    20,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T14:03:20Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    3,
                    20,
                    2,
                    78,
                    0
                ],
                "title": "A Personalized Data-Driven Generative Model of Human Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Personalized Data-Driven Generative Model of Human Motion"
                },
                "summary": "The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities - such as rehabilitation therapy, sports, and\nmanufacturing - is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. However, existing\nmodels only provide simplified descriptions of human motor behavior. In this\nwork, we propose a fully data-driven approach, based on Long Short-Term Memory\nneural networks, to generate original motion that captures the unique\ncharacteristics of specific individuals. We validate the architecture using\nreal data of scalar oscillatory motion. Extensive analyses show that our model\neffectively replicates the velocity distribution and amplitude envelopes of the\nindividual it was trained on, remaining different from other individuals, and\noutperforming state-of-the-art models in terms of similarity to human data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities - such as rehabilitation therapy, sports, and\nmanufacturing - is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. However, existing\nmodels only provide simplified descriptions of human motor behavior. In this\nwork, we propose a fully data-driven approach, based on Long Short-Term Memory\nneural networks, to generate original motion that captures the unique\ncharacteristics of specific individuals. We validate the architecture using\nreal data of scalar oscillatory motion. Extensive analyses show that our model\neffectively replicates the velocity distribution and amplitude envelopes of the\nindividual it was trained on, remaining different from other individuals, and\noutperforming state-of-the-art models in terms of similarity to human data."
                },
                "authors": [
                    {
                        "name": "Angelo Di Porzio"
                    },
                    {
                        "name": "Marco Coraggio"
                    }
                ],
                "author_detail": {
                    "name": "Marco Coraggio"
                },
                "author": "Marco Coraggio",
                "arxiv_comment": "6 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.00025v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.00025v4",
                "updated": "2025-03-19T13:51:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    51,
                    24,
                    2,
                    78,
                    0
                ],
                "published": "2022-09-30T18:03:31Z",
                "published_parsed": [
                    2022,
                    9,
                    30,
                    18,
                    3,
                    31,
                    4,
                    273,
                    0
                ],
                "title": "Artificial Replay: A Meta-Algorithm for Harnessing Historical Data in\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Replay: A Meta-Algorithm for Harnessing Historical Data in\n  Bandits"
                },
                "summary": "Most real-world deployments of bandit algorithms exist somewhere in between\nthe offline and online set-up, where some historical data is available upfront\nand additional data is collected dynamically online. How best to incorporate\nhistorical data to \"warm start\" bandit algorithms is an open question: naively\ninitializing reward estimates using all historical samples can suffer from\nspurious data and imbalanced data coverage, leading to data inefficiency\n(amount of historical data used) - particularly for continuous action spaces.\nTo address these challenges, we propose ArtificialReplay, a meta-algorithm for\nincorporating historical data into any arbitrary base bandit algorithm. We show\nthat ArtificialReplay uses only a fraction of the historical data compared to a\nfull warm-start approach, while still achieving identical regret for base\nalgorithms that satisfy independence of irrelevant data (IIData), a novel and\nbroadly applicable property that we introduce. We complement these theoretical\nresults with experiments on K-armed bandits and continuous combinatorial\nbandits, on which we model green security domains using real poaching data. Our\nresults show the practical benefits of ArtificialReplay for improving data\nefficiency, including for base algorithms that do not satisfy IIData.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most real-world deployments of bandit algorithms exist somewhere in between\nthe offline and online set-up, where some historical data is available upfront\nand additional data is collected dynamically online. How best to incorporate\nhistorical data to \"warm start\" bandit algorithms is an open question: naively\ninitializing reward estimates using all historical samples can suffer from\nspurious data and imbalanced data coverage, leading to data inefficiency\n(amount of historical data used) - particularly for continuous action spaces.\nTo address these challenges, we propose ArtificialReplay, a meta-algorithm for\nincorporating historical data into any arbitrary base bandit algorithm. We show\nthat ArtificialReplay uses only a fraction of the historical data compared to a\nfull warm-start approach, while still achieving identical regret for base\nalgorithms that satisfy independence of irrelevant data (IIData), a novel and\nbroadly applicable property that we introduce. We complement these theoretical\nresults with experiments on K-armed bandits and continuous combinatorial\nbandits, on which we model green security domains using real poaching data. Our\nresults show the practical benefits of ArtificialReplay for improving data\nefficiency, including for base algorithms that do not satisfy IIData."
                },
                "authors": [
                    {
                        "name": "Siddhartha Banerjee"
                    },
                    {
                        "name": "Sean R. Sinclair"
                    },
                    {
                        "name": "Milind Tambe"
                    },
                    {
                        "name": "Lily Xu"
                    },
                    {
                        "name": "Christina Lee Yu"
                    }
                ],
                "author_detail": {
                    "name": "Christina Lee Yu"
                },
                "author": "Christina Lee Yu",
                "arxiv_comment": "55 pages (30 pages main paper), 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.00025v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.00025v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09283v2",
                "updated": "2025-03-19T13:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    41,
                    21,
                    2,
                    78,
                    0
                ],
                "published": "2024-07-12T14:13:59Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    14,
                    13,
                    59,
                    4,
                    194,
                    0
                ],
                "title": "DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection"
                },
                "summary": "Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog)."
                },
                "authors": [
                    {
                        "name": "Sangpil Youm"
                    },
                    {
                        "name": "Brodie Mather"
                    },
                    {
                        "name": "Chathuri Jayaweera"
                    },
                    {
                        "name": "Juliana Prada"
                    },
                    {
                        "name": "Bonnie Dorr"
                    }
                ],
                "author_detail": {
                    "name": "Bonnie Dorr"
                },
                "author": "Bonnie Dorr",
                "arxiv_doi": "10.1007/978-3-031-70239-6_29",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-70239-6_29",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.09283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 6 figures, Accepted to The 29th International Conference on\n  Natural Language & Information Systems (NLDB 2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15199v1",
                "updated": "2025-03-19T13:38:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    38,
                    25,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T13:38:25Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    38,
                    25,
                    2,
                    78,
                    0
                ],
                "title": "Radon: a Programming Model and Platform for Computing Continuum Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radon: a Programming Model and Platform for Computing Continuum Systems"
                },
                "summary": "Emerging compute continuum environments pose new challenges that traditional\ncloud-centric architectures struggle to address. Latency, bandwidth\nconstraints, and the heterogeneity of edge environments hinder the efficiency\nof centralized cloud solutions. While major cloud providers extend their\nplatforms to the edge, these approaches often overlook its unique\ncharacteristics, limiting its potential.\n  To tackle these challenges, we introduce Radon, a flexible programming model\nand platform designed for the edge-to-cloud continuum. Radon applications are\nstructured as atoms, isolated stateful entities that communicate through\nmessaging and can be composed into complex systems. The Radon runtime, based on\nWebAssembly (WASM), enables language- and deployment-independent execution,\nensuring portability and adaptability across heterogeneous environments. This\ndecoupling allows developers to focus on application logic while the runtime\noptimizes for diverse infrastructure conditions.\n  We present a prototype implementation of Radon and evaluate its effectiveness\nthrough a distributed key-value store case study. We analyze the implementation\nin terms of code complexity and performance. Our results demonstrate that Radon\nfacilitates the development and operation of scalable applications across the\nedge-to-cloud continuum advancing the current state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging compute continuum environments pose new challenges that traditional\ncloud-centric architectures struggle to address. Latency, bandwidth\nconstraints, and the heterogeneity of edge environments hinder the efficiency\nof centralized cloud solutions. While major cloud providers extend their\nplatforms to the edge, these approaches often overlook its unique\ncharacteristics, limiting its potential.\n  To tackle these challenges, we introduce Radon, a flexible programming model\nand platform designed for the edge-to-cloud continuum. Radon applications are\nstructured as atoms, isolated stateful entities that communicate through\nmessaging and can be composed into complex systems. The Radon runtime, based on\nWebAssembly (WASM), enables language- and deployment-independent execution,\nensuring portability and adaptability across heterogeneous environments. This\ndecoupling allows developers to focus on application logic while the runtime\noptimizes for diverse infrastructure conditions.\n  We present a prototype implementation of Radon and evaluate its effectiveness\nthrough a distributed key-value store case study. We analyze the implementation\nin terms of code complexity and performance. Our results demonstrate that Radon\nfacilitates the development and operation of scalable applications across the\nedge-to-cloud continuum advancing the current state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Luca De Martini"
                    },
                    {
                        "name": "Dario d'Abate"
                    },
                    {
                        "name": "Alessandro Margara"
                    },
                    {
                        "name": "Gianpaolo Cugola"
                    }
                ],
                "author_detail": {
                    "name": "Gianpaolo Cugola"
                },
                "author": "Gianpaolo Cugola",
                "arxiv_comment": "Submitted to EDCCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15195v1",
                "updated": "2025-03-19T13:33:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    33,
                    29,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T13:33:29Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    33,
                    29,
                    2,
                    78,
                    0
                ],
                "title": "Benchmarking Large Language Models for Handwritten Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Handwritten Text Recognition"
                },
                "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions."
                },
                "authors": [
                    {
                        "name": "Giorgia Crosilla"
                    },
                    {
                        "name": "Lukas Klic"
                    },
                    {
                        "name": "Giovanni Colavizza"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Colavizza"
                },
                "author": "Giovanni Colavizza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15191v1",
                "updated": "2025-03-19T13:21:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    21,
                    49,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T13:21:49Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    21,
                    49,
                    2,
                    78,
                    0
                ],
                "title": "Optimizing Retrieval Strategies for Financial Question Answering\n  Documents in Retrieval-Augmented Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Retrieval Strategies for Financial Question Answering\n  Documents in Retrieval-Augmented Generation Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework to\nmitigate hallucinations in Large Language Models (LLMs), yet its overall\nperformance is dependent on the underlying retrieval system. In the finance\ndomain, documents such as 10-K reports pose distinct challenges due to\ndomain-specific vocabulary and multi-hierarchical tabular data. In this work,\nwe introduce an efficient, end-to-end RAG pipeline that enhances retrieval for\nfinancial documents through a three-phase approach: pre-retrieval, retrieval,\nand post-retrieval. In the pre-retrieval phase, various query and corpus\npreprocessing techniques are employed to enrich input data. During the\nretrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with\ndomain-specific knowledge and implemented a hybrid retrieval strategy that\ncombines dense and sparse representations. Finally, the post-retrieval phase\nleverages Direct Preference Optimization (DPO) training and document selection\nmethods to further refine the results. Evaluations on seven financial question\nanswering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA,\nand MultiHiertt-demonstrate substantial improvements in retrieval performance,\nleading to more accurate and contextually appropriate generation. These\nfindings highlight the critical role of tailored retrieval techniques in\nadvancing the effectiveness of RAG systems for financial applications. A fully\nreplicable pipeline is available on GitHub:\nhttps://github.com/seohyunwoo-0407/GAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework to\nmitigate hallucinations in Large Language Models (LLMs), yet its overall\nperformance is dependent on the underlying retrieval system. In the finance\ndomain, documents such as 10-K reports pose distinct challenges due to\ndomain-specific vocabulary and multi-hierarchical tabular data. In this work,\nwe introduce an efficient, end-to-end RAG pipeline that enhances retrieval for\nfinancial documents through a three-phase approach: pre-retrieval, retrieval,\nand post-retrieval. In the pre-retrieval phase, various query and corpus\npreprocessing techniques are employed to enrich input data. During the\nretrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with\ndomain-specific knowledge and implemented a hybrid retrieval strategy that\ncombines dense and sparse representations. Finally, the post-retrieval phase\nleverages Direct Preference Optimization (DPO) training and document selection\nmethods to further refine the results. Evaluations on seven financial question\nanswering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA,\nand MultiHiertt-demonstrate substantial improvements in retrieval performance,\nleading to more accurate and contextually appropriate generation. These\nfindings highlight the critical role of tailored retrieval techniques in\nadvancing the effectiveness of RAG systems for financial applications. A fully\nreplicable pipeline is available on GitHub:\nhttps://github.com/seohyunwoo-0407/GAR."
                },
                "authors": [
                    {
                        "name": "Sejong Kim"
                    },
                    {
                        "name": "Hyunseo Song"
                    },
                    {
                        "name": "Hyunwoo Seo"
                    },
                    {
                        "name": "Hyunjun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunjun Kim"
                },
                "author": "Hyunjun Kim",
                "arxiv_comment": "15 pages, 3 figures, 11 tables. Accepted at ICLR 2025 Workshop on\n  Advances in Financial AI. Code available at\n  https://github.com/seohyunwoo-0407/GAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15185v1",
                "updated": "2025-03-19T13:14:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    14,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T13:14:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    14,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware\n  View Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware\n  View Transformation"
                },
                "summary": "The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution."
                },
                "authors": [
                    {
                        "name": "Gyeongrok Oh"
                    },
                    {
                        "name": "Sungjune Kim"
                    },
                    {
                        "name": "Heeju Ko"
                    },
                    {
                        "name": "Hyung-gun Chi"
                    },
                    {
                        "name": "Jinkyu Kim"
                    },
                    {
                        "name": "Dongwook Lee"
                    },
                    {
                        "name": "Daehyun Ji"
                    },
                    {
                        "name": "Sungjoon Choi"
                    },
                    {
                        "name": "Sujin Jang"
                    },
                    {
                        "name": "Sangpil Kim"
                    }
                ],
                "author_detail": {
                    "name": "Sangpil Kim"
                },
                "author": "Sangpil Kim",
                "arxiv_comment": "Accepted to CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15176v1",
                "updated": "2025-03-19T13:02:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    2,
                    1,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T13:02:01Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    2,
                    1,
                    2,
                    78,
                    0
                ],
                "title": "A Review on Large Language Models for Visual Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review on Large Language Models for Visual Analytics"
                },
                "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration."
                },
                "authors": [
                    {
                        "name": "Navya Sonal Agarwal"
                    },
                    {
                        "name": "Sanjay Kumar Sonbhadra"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Kumar Sonbhadra"
                },
                "author": "Sanjay Kumar Sonbhadra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15169v1",
                "updated": "2025-03-19T12:51:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    51,
                    52,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T12:51:52Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    51,
                    52,
                    2,
                    78,
                    0
                ],
                "title": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks"
                },
                "summary": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs."
                },
                "authors": [
                    {
                        "name": "Yuting Guo"
                    },
                    {
                        "name": "Abeed Sarker"
                    }
                ],
                "author_detail": {
                    "name": "Abeed Sarker"
                },
                "author": "Abeed Sarker",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15161v1",
                "updated": "2025-03-19T12:38:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    38,
                    4,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T12:38:04Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    38,
                    4,
                    2,
                    78,
                    0
                ],
                "title": "UltraFlwr -- An Efficient Federated Medical and Surgical Object\n  Detection Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltraFlwr -- An Efficient Federated Medical and Surgical Object\n  Detection Framework"
                },
                "summary": "Object detection shows promise for medical and surgical applications such as\ncell counting and tool tracking. However, its faces multiple real-world edge\ndeployment challenges including limited high-quality annotated data, data\nsharing restrictions, and computational constraints. In this work, we introduce\nUltraFlwr, a framework for federated medical and surgical object detection. By\nleveraging Federated Learning (FL), UltraFlwr enables decentralized model\ntraining across multiple sites without sharing raw data. To further enhance\nUltraFlwr's efficiency, we propose YOLO-PA, a set of novel Partial Aggregation\n(PA) strategies specifically designed for YOLO models in FL. YOLO-PA\nsignificantly reduces communication overhead by up to 83% per round while\nmaintaining performance comparable to Full Aggregation (FA) strategies. Our\nextensive experiments on BCCD and m2cai16-tool-locations datasets demonstrate\nthat YOLO-PA not only provides better client models compared to client-wise\ncentralized training and FA strategies, but also facilitates efficient training\nand deployment across resource-constrained edge devices. Further, we also\nestablish one of the first benchmarks in federated medical and surgical object\ndetection. This paper advances the feasibility of training and deploying\ndetection models on the edge, making federated object detection more practical\nfor time-critical and resource-constrained medical and surgical applications.\nUltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection shows promise for medical and surgical applications such as\ncell counting and tool tracking. However, its faces multiple real-world edge\ndeployment challenges including limited high-quality annotated data, data\nsharing restrictions, and computational constraints. In this work, we introduce\nUltraFlwr, a framework for federated medical and surgical object detection. By\nleveraging Federated Learning (FL), UltraFlwr enables decentralized model\ntraining across multiple sites without sharing raw data. To further enhance\nUltraFlwr's efficiency, we propose YOLO-PA, a set of novel Partial Aggregation\n(PA) strategies specifically designed for YOLO models in FL. YOLO-PA\nsignificantly reduces communication overhead by up to 83% per round while\nmaintaining performance comparable to Full Aggregation (FA) strategies. Our\nextensive experiments on BCCD and m2cai16-tool-locations datasets demonstrate\nthat YOLO-PA not only provides better client models compared to client-wise\ncentralized training and FA strategies, but also facilitates efficient training\nand deployment across resource-constrained edge devices. Further, we also\nestablish one of the first benchmarks in federated medical and surgical object\ndetection. This paper advances the feasibility of training and deploying\ndetection models on the edge, making federated object detection more practical\nfor time-critical and resource-constrained medical and surgical applications.\nUltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Soumya Snigdha Kundu"
                    },
                    {
                        "name": "Maxence Boels"
                    },
                    {
                        "name": "Toktam Mahmoodi"
                    },
                    {
                        "name": "Sebastien Ourselin"
                    },
                    {
                        "name": "Tom Vercauteren"
                    },
                    {
                        "name": "Prokar Dasgupta"
                    },
                    {
                        "name": "Jonathan Shapey"
                    },
                    {
                        "name": "Alejandro Granados"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Granados"
                },
                "author": "Alejandro Granados",
                "arxiv_comment": "10 pages, 2 figures, under review @ MICCAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11302v2",
                "updated": "2025-03-19T12:17:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    17,
                    11,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-14T11:11:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Are formal and functional linguistic mechanisms dissociated in language\n  models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are formal and functional linguistic mechanisms dissociated in language\n  models?"
                },
                "summary": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist."
                },
                "authors": [
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "arxiv_comment": "35 pages, 10 figures, 3 tables. Code available at\n  https://github.com/hannamw/formal-functional-dissociation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11280v2",
                "updated": "2025-03-19T12:16:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    16,
                    42,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-14T10:39:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "High-Dimensional Interlingual Representations of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Interlingual Representations of Large Language Models"
                },
                "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."
                },
                "authors": [
                    {
                        "name": "Bryan Wilie"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v3",
                "updated": "2025-03-19T12:15:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    12,
                    15,
                    10,
                    2,
                    78,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "ReAttention: Training-Free Infinite Context with Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReAttention: Training-Free Infinite Context with Finite Attention Scope"
                },
                "summary": "The long-context capability of the Large Language Models (LLM) has made\nsignificant breakthroughs, but the maximum supported context length in length\nextrapolation remains a critical bottleneck limiting their practical\napplications. The constraint of context length in LLMs arises from the\nself-attention mechanism, which cannot effectively and efficiently capture the\nsemantic relationships within infinitely long contexts via the limited\npre-trained positional information and attention scope. In this work, we\npropose ReAttention, a training-free approach enabling LLM based on the\nself-attention mechanism to support an infinite context with a finite attention\nscope under sufficient memory resources. ReAttention performs the\nposition-agnostic top-$k$ attention before the ordinary position-aware\nself-attention, freeing LLMs from the length extrapolation issue. We validate\nthe performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and\ndemonstrate that it is on par with traditional methods. Furthermore, we also\napply ReAttention on mainstream LLMs, including LLaMA3.1-8B and\nMistral-v0.3-7B, enabling them to support context lengths of at least 1M and\neven expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M\nwithout any further training in Needle-In-A-Haystack tests. We also improve the\nefficiency of ReAttention with Triton and achieve an efficient extrapolation\nwithout additional overhead. The code is available at\nhttps://github.com/OpenMOSS/ReAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long-context capability of the Large Language Models (LLM) has made\nsignificant breakthroughs, but the maximum supported context length in length\nextrapolation remains a critical bottleneck limiting their practical\napplications. The constraint of context length in LLMs arises from the\nself-attention mechanism, which cannot effectively and efficiently capture the\nsemantic relationships within infinitely long contexts via the limited\npre-trained positional information and attention scope. In this work, we\npropose ReAttention, a training-free approach enabling LLM based on the\nself-attention mechanism to support an infinite context with a finite attention\nscope under sufficient memory resources. ReAttention performs the\nposition-agnostic top-$k$ attention before the ordinary position-aware\nself-attention, freeing LLMs from the length extrapolation issue. We validate\nthe performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and\ndemonstrate that it is on par with traditional methods. Furthermore, we also\napply ReAttention on mainstream LLMs, including LLaMA3.1-8B and\nMistral-v0.3-7B, enabling them to support context lengths of at least 1M and\neven expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M\nwithout any further training in Needle-In-A-Haystack tests. We also improve the\nefficiency of ReAttention with Triton and achieve an efficient extrapolation\nwithout additional overhead. The code is available at\nhttps://github.com/OpenMOSS/ReAttention."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "21 pages, 11 figures, Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07246v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07246v4",
                "updated": "2025-03-19T11:46:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    46,
                    58,
                    2,
                    78,
                    0
                ],
                "published": "2024-08-14T01:16:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B."
                },
                "authors": [
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Xunzhi Wang"
                    },
                    {
                        "name": "Zeying Hao"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Qian Tan"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Yaotian Yang"
                    },
                    {
                        "name": "Xinrui Xiong"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "11 pages, updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07246v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07246v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15129v1",
                "updated": "2025-03-19T11:44:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    44,
                    47,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:44:47Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    44,
                    47,
                    2,
                    78,
                    0
                ],
                "title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code\n  Generation by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code\n  Generation by Large Language Models"
                },
                "summary": "This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies how AI-assisted programming and large language models\n(LLM) improve software developers' ability via AI tools (LLM agents) like\nGithub Copilot and Amazon CodeWhisperer, while integrating human feedback to\nenhance reinforcement learning (RLHF) with crowd-sourced computation to enhance\ntext-to-code generation. Additionally, we demonstrate that our Bayesian\noptimization framework supports AI alignment in code generation by distributing\nthe feedback collection burden, highlighting the value of collecting human\nfeedback of good quality. Our empirical evaluations demonstrate the efficacy of\nthis approach, showcasing how LLM agents can be effectively trained for\nimproved text-to-code generation. Our Bayesian optimization framework can be\ndesigned for general domain-specific languages, promoting the alignment of\nlarge language model capabilities with human feedback in AI-assisted\nprogramming for code generation."
                },
                "authors": [
                    {
                        "name": "Man Fai Wong"
                    },
                    {
                        "name": "Chee Wei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chee Wei Tan"
                },
                "author": "Chee Wei Tan",
                "arxiv_doi": "10.1109/TBDATA.2024.3524104",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TBDATA.2024.3524104",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.15129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15128v1",
                "updated": "2025-03-19T11:42:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    42,
                    33,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:42:33Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    42,
                    33,
                    2,
                    78,
                    0
                ],
                "title": "Increasing the Robustness of the Fine-tuned Multilingual\n  Machine-Generated Text Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing the Robustness of the Fine-tuned Multilingual\n  Machine-Generated Text Detectors"
                },
                "summary": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data."
                },
                "authors": [
                    {
                        "name": "Dominik Macko"
                    },
                    {
                        "name": "Robert Moro"
                    },
                    {
                        "name": "Ivan Srba"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Srba"
                },
                "author": "Ivan Srba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15126v1",
                "updated": "2025-03-19T11:38:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    38,
                    14,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:38:14Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    38,
                    14,
                    2,
                    78,
                    0
                ],
                "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action\n  Segmentation"
                },
                "summary": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Haoyu Ji"
                    },
                    {
                        "name": "Bowen Chen"
                    },
                    {
                        "name": "Weihong Ren"
                    },
                    {
                        "name": "Wenze Huang"
                    },
                    {
                        "name": "Zhihao Yang"
                    },
                    {
                        "name": "Zhiyong Wang"
                    },
                    {
                        "name": "Honghai Liu"
                    }
                ],
                "author_detail": {
                    "name": "Honghai Liu"
                },
                "author": "Honghai Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16457v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16457v4",
                "updated": "2025-03-19T11:37:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    37,
                    27,
                    2,
                    78,
                    0
                ],
                "published": "2025-02-23T06:16:23Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    6,
                    16,
                    23,
                    6,
                    54,
                    0
                ],
                "title": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge"
                },
                "summary": "Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science."
                },
                "authors": [
                    {
                        "name": "Heegyu Kim"
                    },
                    {
                        "name": "Taeyang Jeon"
                    },
                    {
                        "name": "Seungtaek Choi"
                    },
                    {
                        "name": "Ji Hoon Hong"
                    },
                    {
                        "name": "Dong Won Jeon"
                    },
                    {
                        "name": "Ga-Yeon Baek"
                    },
                    {
                        "name": "Gyeong-Won Kwak"
                    },
                    {
                        "name": "Dong-Hee Lee"
                    },
                    {
                        "name": "Jisu Bae"
                    },
                    {
                        "name": "Chihoon Lee"
                    },
                    {
                        "name": "Yunseo Kim"
                    },
                    {
                        "name": "Seon-Jin Choi"
                    },
                    {
                        "name": "Jin-Seong Park"
                    },
                    {
                        "name": "Sung Beom Cho"
                    },
                    {
                        "name": "Hyunsouk Cho"
                    }
                ],
                "author_detail": {
                    "name": "Hyunsouk Cho"
                },
                "author": "Hyunsouk Cho",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16457v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16457v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02732v2",
                "updated": "2025-03-19T11:31:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    31,
                    31,
                    2,
                    78,
                    0
                ],
                "published": "2024-05-04T18:32:08Z",
                "published_parsed": [
                    2024,
                    5,
                    4,
                    18,
                    32,
                    8,
                    5,
                    125,
                    0
                ],
                "title": "Recall Them All: Retrieval-Augmented Language Models for Long Object\n  List Extraction from Long Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recall Them All: Retrieval-Augmented Language Models for Long Object\n  List Extraction from Long Documents"
                },
                "summary": "Methods for relation extraction from text mostly focus on high precision, at\nthe cost of limited recall. High recall is crucial, though, to populate long\nlists of object entities that stand in a specific relation with a given\nsubject. Cues for relevant objects can be spread across many passages in long\ntexts. This poses the challenge of extracting long lists from long texts. We\npresent the L3X method which tackles the problem in two stages: (1)\nrecall-oriented generation using a large language model (LLM) with judicious\ntechniques for retrieval augmentation, and (2) precision-oriented\nscrutinization to validate or prune candidates. Our L3X method outperforms\nLLM-only generations by a substantial margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods for relation extraction from text mostly focus on high precision, at\nthe cost of limited recall. High recall is crucial, though, to populate long\nlists of object entities that stand in a specific relation with a given\nsubject. Cues for relevant objects can be spread across many passages in long\ntexts. This poses the challenge of extracting long lists from long texts. We\npresent the L3X method which tackles the problem in two stages: (1)\nrecall-oriented generation using a large language model (LLM) with judicious\ntechniques for retrieval augmentation, and (2) precision-oriented\nscrutinization to validate or prune candidates. Our L3X method outperforms\nLLM-only generations by a substantial margin."
                },
                "authors": [
                    {
                        "name": "Sneha Singhania"
                    },
                    {
                        "name": "Simon Razniewski"
                    },
                    {
                        "name": "Gerhard Weikum"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Weikum"
                },
                "author": "Gerhard Weikum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15117v1",
                "updated": "2025-03-19T11:21:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    21,
                    37,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:21:37Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    21,
                    37,
                    2,
                    78,
                    0
                ],
                "title": "Exploring Model Editing for LLM-based Aspect-Based Sentiment\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Model Editing for LLM-based Aspect-Based Sentiment\n  Classification"
                },
                "summary": "Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy."
                },
                "authors": [
                    {
                        "name": "Shichen Li"
                    },
                    {
                        "name": "Zhongqing Wang"
                    },
                    {
                        "name": "Zheyu Zhao"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "arxiv_comment": "AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15113v1",
                "updated": "2025-03-19T11:13:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    13,
                    51,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:13:51Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    13,
                    51,
                    2,
                    78,
                    0
                ],
                "title": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles."
                },
                "authors": [
                    {
                        "name": "Benjamin Estermann"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15112v1",
                "updated": "2025-03-19T11:12:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    12,
                    53,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:12:53Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    12,
                    53,
                    2,
                    78,
                    0
                ],
                "title": "OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL\n  Generation"
                },
                "summary": "The automated generation of design RTL based on large language model (LLM)\nand natural language instructions has demonstrated great potential in agile\ncircuit design. However, the lack of datasets and benchmarks in the public\ndomain prevents the development and fair evaluation of LLM solutions. This\npaper highlights our latest advances in open datasets and benchmarks from three\nperspectives: (1) RTLLM 2.0, an updated benchmark assessing LLM's capability in\ndesign RTL generation. The benchmark is augmented to 50 hand-crafted designs.\nEach design provides the design description, test cases, and a correct RTL\ncode. (2) AssertEval, an open-source benchmark assessing the LLM's assertion\ngeneration capabilities for RTL verification. The benchmark includes 18\ndesigns, each providing specification, signal definition, and correct RTL code.\n(3) RTLCoder-Data, an extended open-source dataset with 80K instruction-code\ndata samples. Moreover, we propose a new verification-based method to verify\nthe functionality correctness of training data samples. Based on this\ntechnique, we further release a dataset with 7K verified high-quality samples.\nThese three studies are integrated into one framework, providing off-the-shelf\nsupport for the development and evaluation of LLMs for RTL code generation and\nverification. Finally, extensive experiments indicate that LLM performance can\nbe boosted by enlarging the training dataset, improving data quality, and\nimproving the training scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automated generation of design RTL based on large language model (LLM)\nand natural language instructions has demonstrated great potential in agile\ncircuit design. However, the lack of datasets and benchmarks in the public\ndomain prevents the development and fair evaluation of LLM solutions. This\npaper highlights our latest advances in open datasets and benchmarks from three\nperspectives: (1) RTLLM 2.0, an updated benchmark assessing LLM's capability in\ndesign RTL generation. The benchmark is augmented to 50 hand-crafted designs.\nEach design provides the design description, test cases, and a correct RTL\ncode. (2) AssertEval, an open-source benchmark assessing the LLM's assertion\ngeneration capabilities for RTL verification. The benchmark includes 18\ndesigns, each providing specification, signal definition, and correct RTL code.\n(3) RTLCoder-Data, an extended open-source dataset with 80K instruction-code\ndata samples. Moreover, we propose a new verification-based method to verify\nthe functionality correctness of training data samples. Based on this\ntechnique, we further release a dataset with 7K verified high-quality samples.\nThese three studies are integrated into one framework, providing off-the-shelf\nsupport for the development and evaluation of LLMs for RTL code generation and\nverification. Finally, extensive experiments indicate that LLM performance can\nbe boosted by enlarging the training dataset, improving data quality, and\nimproving the training scheme."
                },
                "authors": [
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "ICCAD'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15108v1",
                "updated": "2025-03-19T11:05:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    5,
                    42,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T11:05:42Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    5,
                    42,
                    2,
                    78,
                    0
                ],
                "title": "VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making"
                },
                "summary": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components."
                },
                "authors": [
                    {
                        "name": "Mohamed Salim Aissi"
                    },
                    {
                        "name": "Clemence Grislain"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Nicolas Thome"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Thome"
                },
                "author": "Nicolas Thome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05291v3",
                "updated": "2025-03-19T10:44:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    44,
                    50,
                    2,
                    78,
                    0
                ],
                "published": "2024-04-08T08:29:00Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    8,
                    29,
                    0,
                    0,
                    99,
                    0
                ],
                "title": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with\n  Large Language Models"
                },
                "summary": "We present a large language model (LLM) based system to empower quadrupedal\nrobots with problem-solving abilities for long-horizon tasks beyond short-term\nmotions. Long-horizon tasks for quadrupeds are challenging since they require\nboth a high-level understanding of the semantics of the problem for task\nplanning and a broad range of locomotion and manipulation skills to interact\nwith the environment. Our system builds a high-level reasoning layer with large\nlanguage models, which generates hybrid discrete-continuous plans as robot code\nfrom task descriptions. It comprises multiple LLM agents: a semantic planner\nthat sketches a plan, a parameter calculator that predicts arguments in the\nplan, a code generator that converts the plan into executable robot code, and a\nreplanner that handles execution failures or human interventions. At the low\nlevel, we adopt reinforcement learning to train a set of motion planning and\ncontrol skills to unleash the flexibility of quadrupeds for rich environment\ninteractions. Our system is tested on long-horizon tasks that are infeasible to\ncomplete with one single skill. Simulation and real-world experiments show that\nit successfully figures out multi-step strategies and demonstrates non-trivial\nbehaviors, including building tools or notifying a human for help. Demos are\navailable on our project page:\nhttps://sites.google.com/view/long-horizon-robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a large language model (LLM) based system to empower quadrupedal\nrobots with problem-solving abilities for long-horizon tasks beyond short-term\nmotions. Long-horizon tasks for quadrupeds are challenging since they require\nboth a high-level understanding of the semantics of the problem for task\nplanning and a broad range of locomotion and manipulation skills to interact\nwith the environment. Our system builds a high-level reasoning layer with large\nlanguage models, which generates hybrid discrete-continuous plans as robot code\nfrom task descriptions. It comprises multiple LLM agents: a semantic planner\nthat sketches a plan, a parameter calculator that predicts arguments in the\nplan, a code generator that converts the plan into executable robot code, and a\nreplanner that handles execution failures or human interventions. At the low\nlevel, we adopt reinforcement learning to train a set of motion planning and\ncontrol skills to unleash the flexibility of quadrupeds for rich environment\ninteractions. Our system is tested on long-horizon tasks that are infeasible to\ncomplete with one single skill. Simulation and real-world experiments show that\nit successfully figures out multi-step strategies and demonstrates non-trivial\nbehaviors, including building tools or notifying a human for help. Demos are\navailable on our project page:\nhttps://sites.google.com/view/long-horizon-robot."
                },
                "authors": [
                    {
                        "name": "Yutao Ouyang"
                    },
                    {
                        "name": "Jinhan Li"
                    },
                    {
                        "name": "Yunfei Li"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Koushil Sreenath"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15091v1",
                "updated": "2025-03-19T10:40:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    40,
                    28,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T10:40:28Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    40,
                    28,
                    2,
                    78,
                    0
                ],
                "title": "Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs\n  for Indoor Scenarios with the Help of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs\n  for Indoor Scenarios with the Help of LLMs"
                },
                "summary": "This paper addresses the high demand in advanced intelligent robot navigation\nfor a more holistic understanding of spatial environments, by introducing a\nnovel system that harnesses the capabilities of Large Language Models (LLMs) to\nconstruct hierarchical 3D Scene Graphs (3DSGs) for indoor scenarios. The\nproposed framework constructs 3DSGs consisting of a fundamental layer with rich\nmetric-semantic information, an object layer featuring precise point-cloud\nrepresentation of object nodes as well as visual descriptors, and higher layers\nof room, floor, and building nodes. Thanks to the innovative application of\nLLMs, not only object nodes but also nodes of higher layers, e.g., room nodes,\nare annotated in an intelligent and accurate manner. A polling mechanism for\nroom classification using LLMs is proposed to enhance the accuracy and\nreliability of the room node annotation. Thorough numerical experiments\ndemonstrate the system's ability to integrate semantic descriptions with\ngeometric data, creating an accurate and comprehensive representation of the\nenvironment instrumental for context-aware navigation and task planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the high demand in advanced intelligent robot navigation\nfor a more holistic understanding of spatial environments, by introducing a\nnovel system that harnesses the capabilities of Large Language Models (LLMs) to\nconstruct hierarchical 3D Scene Graphs (3DSGs) for indoor scenarios. The\nproposed framework constructs 3DSGs consisting of a fundamental layer with rich\nmetric-semantic information, an object layer featuring precise point-cloud\nrepresentation of object nodes as well as visual descriptors, and higher layers\nof room, floor, and building nodes. Thanks to the innovative application of\nLLMs, not only object nodes but also nodes of higher layers, e.g., room nodes,\nare annotated in an intelligent and accurate manner. A polling mechanism for\nroom classification using LLMs is proposed to enhance the accuracy and\nreliability of the room node annotation. Thorough numerical experiments\ndemonstrate the system's ability to integrate semantic descriptions with\ngeometric data, creating an accurate and comprehensive representation of the\nenvironment instrumental for context-aware navigation and task planning."
                },
                "authors": [
                    {
                        "name": "Yao Cheng"
                    },
                    {
                        "name": "Zhe Han"
                    },
                    {
                        "name": "Fengyang Jiang"
                    },
                    {
                        "name": "Huaizhen Wang"
                    },
                    {
                        "name": "Fengyu Zhou"
                    },
                    {
                        "name": "Qingshan Yin"
                    },
                    {
                        "name": "Lei Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wei"
                },
                "author": "Lei Wei",
                "arxiv_comment": "accepted by WRC SARA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15079v1",
                "updated": "2025-03-19T10:24:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    24,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T10:24:16Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    24,
                    16,
                    2,
                    78,
                    0
                ],
                "title": "LogiAgent: Automated Logical Testing for REST Systems with LLM-Based\n  Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogiAgent: Automated Logical Testing for REST Systems with LLM-Based\n  Multi-Agents"
                },
                "summary": "Automated testing for REST APIs has become essential for ensuring the\ncorrectness and reliability of modern web services. While existing approaches\nprimarily focus on detecting server crashes and error codes, they often\noverlook logical issues that arise due to evolving business logic and\ndomain-specific requirements. To address this limitation, we propose LogiAgent,\na novel approach for logical testing of REST systems. Built upon a large\nlanguage model (LLM)-driven multi-agent framework, LogiAgent integrates a Test\nScenario Generator, API Request Executor, and API Response Validator to\ncollaboratively generate, execute, and validate API test scenarios. Unlike\ntraditional testing methods that focus on status codes like 5xx, LogiAgent\nincorporates logical oracles that assess responses based on business logic,\nensuring more comprehensive testing. The system is further enhanced by an\nExecution Memory component that stores historical API execution data for\ncontextual consistency. We conduct extensive experiments across 12 real-world\nREST systems, demonstrating that LogiAgent effectively identifies 234 logical\nissues with an accuracy of 66.19%. Additionally, it basically excels in\ndetecting server crashes and achieves superior test coverage compared to four\nstate-of-the-art REST API testing tools. An ablation study confirms the\nsignificant contribution of LogiAgent's memory components to improving test\ncoverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated testing for REST APIs has become essential for ensuring the\ncorrectness and reliability of modern web services. While existing approaches\nprimarily focus on detecting server crashes and error codes, they often\noverlook logical issues that arise due to evolving business logic and\ndomain-specific requirements. To address this limitation, we propose LogiAgent,\na novel approach for logical testing of REST systems. Built upon a large\nlanguage model (LLM)-driven multi-agent framework, LogiAgent integrates a Test\nScenario Generator, API Request Executor, and API Response Validator to\ncollaboratively generate, execute, and validate API test scenarios. Unlike\ntraditional testing methods that focus on status codes like 5xx, LogiAgent\nincorporates logical oracles that assess responses based on business logic,\nensuring more comprehensive testing. The system is further enhanced by an\nExecution Memory component that stores historical API execution data for\ncontextual consistency. We conduct extensive experiments across 12 real-world\nREST systems, demonstrating that LogiAgent effectively identifies 234 logical\nissues with an accuracy of 66.19%. Additionally, it basically excels in\ndetecting server crashes and achieves superior test coverage compared to four\nstate-of-the-art REST API testing tools. An ablation study confirms the\nsignificant contribution of LogiAgent's memory components to improving test\ncoverage."
                },
                "authors": [
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Chenxi Zhang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "YaChen Wu"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qingshan Li"
                    },
                    {
                        "name": "Xin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Peng"
                },
                "author": "Xin Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12374v2",
                "updated": "2025-03-19T10:08:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    8,
                    16,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-16T06:24:51Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    6,
                    24,
                    51,
                    6,
                    75,
                    0
                ],
                "title": "Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at\n  GitHub Issue Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at\n  GitHub Issue Resolution"
                },
                "summary": "AI-driven software development has rapidly advanced with the emergence of\nsoftware development agents that leverage large language models (LLMs) to\ntackle complex, repository-level software engineering tasks. These agents go\nbeyond just generation of final code; they engage in multi-step reasoning,\nutilize various tools for code modification and debugging, and interact with\nexecution environments to diagnose and iteratively resolve issues. However,\nmost existing evaluations focus primarily on static analyses of final code\noutputs, yielding limited insights into the agents' dynamic problem-solving\nprocesses. To fill this gap, we conduct an in-depth empirical study on 3,977\nsolving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked\nagents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our\nexploratory analysis shows that Python execution errors during the issue\nresolution phase correlate with lower resolution rates and increased reasoning\noverheads. We have identified the most prevalent errors -- such as\nModuleNotFoundError and TypeError -- and highlighted particularly challenging\nerrors like OSError and database-related issues (e.g., IntegrityError) that\ndemand significantly more debugging effort. Furthermore, we have discovered 3\nbugs in the SWE-Bench platform that affect benchmark fairness and accuracy;\nthese issues have been reported to and confirmed by the maintainers. To promote\ntransparency and foster future research, we publicly share our datasets and\nanalysis scripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-driven software development has rapidly advanced with the emergence of\nsoftware development agents that leverage large language models (LLMs) to\ntackle complex, repository-level software engineering tasks. These agents go\nbeyond just generation of final code; they engage in multi-step reasoning,\nutilize various tools for code modification and debugging, and interact with\nexecution environments to diagnose and iteratively resolve issues. However,\nmost existing evaluations focus primarily on static analyses of final code\noutputs, yielding limited insights into the agents' dynamic problem-solving\nprocesses. To fill this gap, we conduct an in-depth empirical study on 3,977\nsolving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked\nagents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our\nexploratory analysis shows that Python execution errors during the issue\nresolution phase correlate with lower resolution rates and increased reasoning\noverheads. We have identified the most prevalent errors -- such as\nModuleNotFoundError and TypeError -- and highlighted particularly challenging\nerrors like OSError and database-related issues (e.g., IntegrityError) that\ndemand significantly more debugging effort. Furthermore, we have discovered 3\nbugs in the SWE-Bench platform that affect benchmark fairness and accuracy;\nthese issues have been reported to and confirmed by the maintainers. To promote\ntransparency and foster future research, we publicly share our datasets and\nanalysis scripts."
                },
                "authors": [
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15045v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15045v3",
                "updated": "2025-03-19T10:05:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    5,
                    4,
                    2,
                    78,
                    0
                ],
                "published": "2024-08-27T13:13:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    13,
                    13,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "DocLayLLM: An Efficient Multi-modal Extension of Large Language Models\n  for Text-rich Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocLayLLM: An Efficient Multi-modal Extension of Large Language Models\n  for Text-rich Document Understanding"
                },
                "summary": "Text-rich document understanding (TDU) requires comprehensive analysis of\ndocuments containing substantial textual content and complex layouts. While\nMultimodal Large Language Models (MLLMs) have achieved fast progress in this\ndomain, existing approaches either demand significant computational resources\nor struggle with effective multi-modal integration. In this paper, we introduce\nDocLayLLM, an efficient multi-modal extension of LLMs specifically designed for\nTDU. By lightly integrating visual patch tokens and 2D positional tokens into\nLLMs' input and encoding the document content using the LLMs themselves, we\nfully take advantage of the document comprehension capability of LLMs and\nenhance their perception of OCR information. We have also deeply considered the\nrole of chain-of-thought (CoT) and innovatively proposed the techniques of CoT\nPre-training and CoT Annealing. Our DocLayLLM can achieve remarkable\nperformances with lightweight training settings, showcasing its efficiency and\neffectiveness. Experimental results demonstrate that our DocLayLLM outperforms\nexisting OCR-dependent methods and OCR-free competitors. Code and model are\navailable at https://github.com/whlscut/DocLayLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich document understanding (TDU) requires comprehensive analysis of\ndocuments containing substantial textual content and complex layouts. While\nMultimodal Large Language Models (MLLMs) have achieved fast progress in this\ndomain, existing approaches either demand significant computational resources\nor struggle with effective multi-modal integration. In this paper, we introduce\nDocLayLLM, an efficient multi-modal extension of LLMs specifically designed for\nTDU. By lightly integrating visual patch tokens and 2D positional tokens into\nLLMs' input and encoding the document content using the LLMs themselves, we\nfully take advantage of the document comprehension capability of LLMs and\nenhance their perception of OCR information. We have also deeply considered the\nrole of chain-of-thought (CoT) and innovatively proposed the techniques of CoT\nPre-training and CoT Annealing. Our DocLayLLM can achieve remarkable\nperformances with lightweight training settings, showcasing its efficiency and\neffectiveness. Experimental results demonstrate that our DocLayLLM outperforms\nexisting OCR-dependent methods and OCR-free competitors. Code and model are\navailable at https://github.com/whlscut/DocLayLLM."
                },
                "authors": [
                    {
                        "name": "Wenhui Liao"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Hongliang Li"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "arxiv_comment": "CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15045v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15045v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15055v1",
                "updated": "2025-03-19T09:46:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    46,
                    54,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T09:46:54Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    46,
                    54,
                    2,
                    78,
                    0
                ],
                "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation"
                },
                "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains."
                },
                "authors": [
                    {
                        "name": "Arina Razmyslovich"
                    },
                    {
                        "name": "Kseniia Murasheva"
                    },
                    {
                        "name": "Sofia Sedlova"
                    },
                    {
                        "name": "Julien Capitaine"
                    },
                    {
                        "name": "Eugene Dmitriev"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Dmitriev"
                },
                "author": "Eugene Dmitriev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15050v1",
                "updated": "2025-03-19T09:39:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    39,
                    32,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T09:39:32Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    39,
                    32,
                    2,
                    78,
                    0
                ],
                "title": "Studying and Understanding the Effectiveness and Failures of\n  Conversational LLM-Based Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studying and Understanding the Effectiveness and Failures of\n  Conversational LLM-Based Repair"
                },
                "summary": "Automated program repair (APR) is designed to automate the process of\nbug-fixing. In recent years, thanks to the rapid development of large language\nmodels (LLMs), automated repair has achieved remarkable progress. Advanced APR\ntechniques powered by conversational LLMs, most notably ChatGPT, have exhibited\nimpressive repair abilities and gained increasing popularity due to the\ncapabilities of the underlying LLMs in providing repair feedback and performing\niterative patch improvement. Despite the superiority, conversational APR\ntechniques still fail to repair a large number of bugs. For example, a\nstate-of-the-art conversational technique ChatRepair does not correctly repair\nover half of the single-function bugs in the Defects4J dataset. To understand\nthe effectiveness and failures of conversational LLM-based repair and provide\npossible directions for improvement, we studied the exemplary ChatRepair with a\nfocus on comparing the effectiveness of its cloze-style and full function\nrepair strategies, assessing its key iterative component for patch improvement,\nand analyzing the repair failures. Our study has led to a series of findings,\nwhich we believe provide key implications for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated program repair (APR) is designed to automate the process of\nbug-fixing. In recent years, thanks to the rapid development of large language\nmodels (LLMs), automated repair has achieved remarkable progress. Advanced APR\ntechniques powered by conversational LLMs, most notably ChatGPT, have exhibited\nimpressive repair abilities and gained increasing popularity due to the\ncapabilities of the underlying LLMs in providing repair feedback and performing\niterative patch improvement. Despite the superiority, conversational APR\ntechniques still fail to repair a large number of bugs. For example, a\nstate-of-the-art conversational technique ChatRepair does not correctly repair\nover half of the single-function bugs in the Defects4J dataset. To understand\nthe effectiveness and failures of conversational LLM-based repair and provide\npossible directions for improvement, we studied the exemplary ChatRepair with a\nfocus on comparing the effectiveness of its cloze-style and full function\nrepair strategies, assessing its key iterative component for patch improvement,\nand analyzing the repair failures. Our study has led to a series of findings,\nwhich we believe provide key implications for future research."
                },
                "authors": [
                    {
                        "name": "Aolin Chen"
                    },
                    {
                        "name": "Haojun Wu"
                    },
                    {
                        "name": "Qi Xin"
                    },
                    {
                        "name": "Steven P. Reiss"
                    },
                    {
                        "name": "Jifeng Xuan"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Xuan"
                },
                "author": "Jifeng Xuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19753v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19753v3",
                "updated": "2025-03-19T09:37:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    37,
                    10,
                    2,
                    78,
                    0
                ],
                "published": "2024-09-29T16:08:45Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    16,
                    8,
                    45,
                    6,
                    273,
                    0
                ],
                "title": "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex\n  Knowledge Graph Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex\n  Knowledge Graph Question Answering"
                },
                "summary": "Recent studies have explored the use of Large Language Models (LLMs) with\nRetrieval Augmented Generation (RAG) for Knowledge Graph Question Answering\n(KGQA). They typically require rewriting retrieved subgraphs into natural\nlanguage formats comprehensible to LLMs. However, when tackling complex\nquestions, the knowledge rewritten by existing methods may include irrelevant\ninformation, omit crucial details, or fail to align with the question's\nsemantics. To address them, we propose a novel rewriting method CoTKR,\nChain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces\nand corresponding knowledge in an interleaved manner, thereby mitigating the\nlimitations of single-step knowledge rewriting. Additionally, to bridge the\npreference gap between the knowledge rewriter and the question answering (QA)\nmodel, we propose a training strategy PAQAF, Preference Alignment from Question\nAnswering Feedback, for leveraging feedback from the QA model to further\noptimize the knowledge rewriter. We conduct experiments using various LLMs\nacross several KGQA benchmarks. Experimental results demonstrate that, compared\nwith previous knowledge rewriting methods, CoTKR generates the most beneficial\nknowledge representation for QA models, which significantly improves the\nperformance of LLMs in KGQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored the use of Large Language Models (LLMs) with\nRetrieval Augmented Generation (RAG) for Knowledge Graph Question Answering\n(KGQA). They typically require rewriting retrieved subgraphs into natural\nlanguage formats comprehensible to LLMs. However, when tackling complex\nquestions, the knowledge rewritten by existing methods may include irrelevant\ninformation, omit crucial details, or fail to align with the question's\nsemantics. To address them, we propose a novel rewriting method CoTKR,\nChain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces\nand corresponding knowledge in an interleaved manner, thereby mitigating the\nlimitations of single-step knowledge rewriting. Additionally, to bridge the\npreference gap between the knowledge rewriter and the question answering (QA)\nmodel, we propose a training strategy PAQAF, Preference Alignment from Question\nAnswering Feedback, for leveraging feedback from the QA model to further\noptimize the knowledge rewriter. We conduct experiments using various LLMs\nacross several KGQA benchmarks. Experimental results demonstrate that, compared\nwith previous knowledge rewriting methods, CoTKR generates the most beneficial\nknowledge representation for QA models, which significantly improves the\nperformance of LLMs in KGQA."
                },
                "authors": [
                    {
                        "name": "Yike Wu"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Yuncheng Hua"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19753v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19753v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15044v1",
                "updated": "2025-03-19T09:32:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    32,
                    52,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T09:32:52Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    32,
                    52,
                    2,
                    78,
                    0
                ],
                "title": "SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in\n  Machine-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in\n  Machine-Generated Text Detection"
                },
                "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue."
                },
                "authors": [
                    {
                        "name": "Haoyi Li"
                    },
                    {
                        "name": "Angela Yifei Yuan"
                    },
                    {
                        "name": "Soyeon Caren Han"
                    },
                    {
                        "name": "Christopher Leckie"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Leckie"
                },
                "author": "Christopher Leckie",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11425v2",
                "updated": "2025-03-19T09:28:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    28,
                    9,
                    2,
                    78,
                    0
                ],
                "published": "2025-01-20T11:46:04Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    11,
                    46,
                    4,
                    0,
                    20,
                    0
                ],
                "title": "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training"
                },
                "summary": "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%)."
                },
                "authors": [
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06793v2",
                "updated": "2025-03-19T09:24:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    24,
                    9,
                    2,
                    78,
                    0
                ],
                "published": "2024-08-13T10:25:13Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    25,
                    13,
                    1,
                    226,
                    0
                ],
                "title": "Layerwise Recurrent Router for Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layerwise Recurrent Router for Mixture-of-Experts"
                },
                "summary": "The scaling of large language models (LLMs) has revolutionized their\ncapabilities in various tasks, yet this growth must be matched with efficient\ncomputational strategies. The Mixture-of-Experts (MoE) architecture stands out\nfor its ability to scale model size without significantly increasing training\ncosts. Despite their advantages, current MoE models often display parameter\ninefficiency. For instance, a pre-trained MoE-based LLM with 52 billion\nparameters might perform comparably to a standard model with 6.7 billion\nparameters. Being a crucial part of MoE, current routers in different layers\nindependently assign tokens without leveraging historical routing information,\npotentially leading to suboptimal token-expert combinations and the parameter\ninefficiency problem. To alleviate this issue, we introduce the Layerwise\nRecurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated\nRecurrent Unit (GRU) to establish dependencies between routing decisions across\nconsecutive layers. Such layerwise recurrence can be efficiently parallelly\ncomputed for input tokens and introduces negotiable costs. Our extensive\nempirical evaluations demonstrate that RMoE-based language models consistently\noutperform a spectrum of baseline models. Furthermore, RMoE integrates a novel\ncomputation stage orthogonal to existing methods, allowing seamless\ncompatibility with other MoE architectures. Our analyses attribute RMoE's gains\nto its effective cross-layer information sharing, which also improves expert\nselection and diversity. Our code is at https://github.com/qiuzh20/RMoE .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scaling of large language models (LLMs) has revolutionized their\ncapabilities in various tasks, yet this growth must be matched with efficient\ncomputational strategies. The Mixture-of-Experts (MoE) architecture stands out\nfor its ability to scale model size without significantly increasing training\ncosts. Despite their advantages, current MoE models often display parameter\ninefficiency. For instance, a pre-trained MoE-based LLM with 52 billion\nparameters might perform comparably to a standard model with 6.7 billion\nparameters. Being a crucial part of MoE, current routers in different layers\nindependently assign tokens without leveraging historical routing information,\npotentially leading to suboptimal token-expert combinations and the parameter\ninefficiency problem. To alleviate this issue, we introduce the Layerwise\nRecurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated\nRecurrent Unit (GRU) to establish dependencies between routing decisions across\nconsecutive layers. Such layerwise recurrence can be efficiently parallelly\ncomputed for input tokens and introduces negotiable costs. Our extensive\nempirical evaluations demonstrate that RMoE-based language models consistently\noutperform a spectrum of baseline models. Furthermore, RMoE integrates a novel\ncomputation stage orthogonal to existing methods, allowing seamless\ncompatibility with other MoE architectures. Our analyses attribute RMoE's gains\nto its effective cross-layer information sharing, which also improves expert\nselection and diversity. Our code is at https://github.com/qiuzh20/RMoE ."
                },
                "authors": [
                    {
                        "name": "Zihan Qiu"
                    },
                    {
                        "name": "Zeyu Huang"
                    },
                    {
                        "name": "Shuang Cheng"
                    },
                    {
                        "name": "Yizhi Zhou"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ivan Titov"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "arxiv_journal_ref": "ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15019v1",
                "updated": "2025-03-19T09:16:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    16,
                    8,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T09:16:08Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    16,
                    8,
                    2,
                    78,
                    0
                ],
                "title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene"
                },
                "summary": "The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever\nrepresentation for comprehensively modeling the dynamic 4D visual real world.\nUnfortunately, current pioneering 4D-PSG research can primarily suffer from\ndata scarcity issues severely, as well as the resulting out-of-vocabulary\nproblems; also, the pipeline nature of the benchmark generation method can lead\nto suboptimal performance. To address these challenges, this paper investigates\na novel framework for 4D-PSG generation that leverages rich 2D visual scene\nannotations to enhance 4D scene learning. First, we introduce a 4D Large\nLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-end\ngeneration of 4D-PSG. A chained SG inference mechanism is further designed to\nexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive\nobject and relation labels iteratively. Most importantly, we propose a 2D-to-4D\nvisual scene transfer learning framework, where a spatial-temporal scene\ntranscending strategy effectively transfers dimension-invariant features from\nabundant 2D SG annotations to 4D scenes, effectively compensating for data\nscarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate\nthat we strikingly outperform baseline models by a large margin, highlighting\nthe effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever\nrepresentation for comprehensively modeling the dynamic 4D visual real world.\nUnfortunately, current pioneering 4D-PSG research can primarily suffer from\ndata scarcity issues severely, as well as the resulting out-of-vocabulary\nproblems; also, the pipeline nature of the benchmark generation method can lead\nto suboptimal performance. To address these challenges, this paper investigates\na novel framework for 4D-PSG generation that leverages rich 2D visual scene\nannotations to enhance 4D scene learning. First, we introduce a 4D Large\nLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-end\ngeneration of 4D-PSG. A chained SG inference mechanism is further designed to\nexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive\nobject and relation labels iteratively. Most importantly, we propose a 2D-to-4D\nvisual scene transfer learning framework, where a spatial-temporal scene\ntranscending strategy effectively transfers dimension-invariant features from\nabundant 2D SG annotations to 4D scenes, effectively compensating for data\nscarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate\nthat we strikingly outperform baseline models by a large margin, highlighting\nthe effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Shengqiong Wu"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Tat-seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-seng Chua"
                },
                "author": "Tat-seng Chua",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15003v1",
                "updated": "2025-03-19T08:52:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    52,
                    59,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T08:52:59Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    52,
                    59,
                    2,
                    78,
                    0
                ],
                "title": "LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?"
                },
                "summary": "Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language."
                },
                "authors": [
                    {
                        "name": "Amr Keleg"
                    }
                ],
                "author_detail": {
                    "name": "Amr Keleg"
                },
                "author": "Amr Keleg",
                "arxiv_comment": "Accepted to the C3NLP workshop (Co-located with NAACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14996v1",
                "updated": "2025-03-19T08:45:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    45,
                    3,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T08:45:03Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    45,
                    3,
                    2,
                    78,
                    0
                ],
                "title": "Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM\n  Evaluation in Multiple-Choice Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM\n  Evaluation in Multiple-Choice Question Answering"
                },
                "summary": "One of the most widely used tasks to evaluate Large Language Models (LLMs) is\nMultiple-Choice Question Answering (MCQA). While open-ended question answering\ntasks are more challenging to evaluate, MCQA tasks are, in principle, easier to\nassess, as the model's answer is thought to be simple to extract and is\ndirectly compared to a set of predefined choices. However, recent studies have\nstarted to question the reliability of MCQA evaluation, showing that multiple\nfactors can significantly impact the reported performance of LLMs, especially\nwhen the model generates free-form text before selecting one of the answer\nchoices. In this work, we shed light on the inconsistencies of MCQA evaluation\nstrategies, which can lead to inaccurate and misleading model comparisons. We\nsystematically analyze whether existing answer extraction methods are aligned\nwith human judgment, and how they are influenced by answer constraints in the\nprompt across different domains. Our experiments demonstrate that traditional\nevaluation strategies often underestimate LLM capabilities, while LLM-based\nanswer extractors are prone to systematic errors. Moreover, we reveal a\nfundamental trade-off between including format constraints in the prompt to\nsimplify answer extraction and allowing models to generate free-form text to\nimprove reasoning. Our findings call for standardized evaluation methodologies\nand highlight the need for more reliable and consistent MCQA evaluation\npractices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the most widely used tasks to evaluate Large Language Models (LLMs) is\nMultiple-Choice Question Answering (MCQA). While open-ended question answering\ntasks are more challenging to evaluate, MCQA tasks are, in principle, easier to\nassess, as the model's answer is thought to be simple to extract and is\ndirectly compared to a set of predefined choices. However, recent studies have\nstarted to question the reliability of MCQA evaluation, showing that multiple\nfactors can significantly impact the reported performance of LLMs, especially\nwhen the model generates free-form text before selecting one of the answer\nchoices. In this work, we shed light on the inconsistencies of MCQA evaluation\nstrategies, which can lead to inaccurate and misleading model comparisons. We\nsystematically analyze whether existing answer extraction methods are aligned\nwith human judgment, and how they are influenced by answer constraints in the\nprompt across different domains. Our experiments demonstrate that traditional\nevaluation strategies often underestimate LLM capabilities, while LLM-based\nanswer extractors are prone to systematic errors. Moreover, we reveal a\nfundamental trade-off between including format constraints in the prompt to\nsimplify answer extraction and allowing models to generate free-form text to\nimprove reasoning. Our findings call for standardized evaluation methodologies\nand highlight the need for more reliable and consistent MCQA evaluation\npractices."
                },
                "authors": [
                    {
                        "name": "Francesco Maria Molfese"
                    },
                    {
                        "name": "Luca Moroni"
                    },
                    {
                        "name": "Luca Gioffrè"
                    },
                    {
                        "name": "Alessandro Scirè"
                    },
                    {
                        "name": "Simone Conia"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "17 pages (9 main), 11 figures, 21 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14985v1",
                "updated": "2025-03-19T08:31:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    31,
                    39,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T08:31:39Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    31,
                    39,
                    2,
                    78,
                    0
                ],
                "title": "ML-Triton, A Multi-Level Compilation and Language Extension to Triton\n  GPU Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-Triton, A Multi-Level Compilation and Language Extension to Triton\n  GPU Programming"
                },
                "summary": "In the era of LLMs, dense operations such as GEMM and MHA are critical\ncomponents. These operations are well-suited for parallel execution using a\ntilebased approach. While traditional GPU programming often relies on low level\ninterfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more\nuser-friendly and portable alternative by programming at a higher level. The\ncurrent Triton starts at the workgroup (aka threadblock) level, and directly\nlowers to per-thread level. And then attempt to coalesce and amend through a\nseries of passes, promoting information from low-level representation. We\nbelieve this is pre-mature lowering based on the below observations. 1. GPU has\na hierarchical structure both physically and logically. Modern GPUs often\nfeature SIMD units capable of directly operating on tiles on a warp or\nwarpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual\nlowering can make compiler decoupled and clean by separating considerations\ninter and intra a logical layer. 3. Kernel developers often need fine control\nto get good performance on the latest hardware. FlashAttention2 advocates\nexplicit data partition between warps to make a performance boost. In this\ncontext, we propose ML-Triton which features multi-level compilation flow and\nprogramming interface. Our approach begins at the workgroup level and\nprogressively lowers to the warp and intrinsic level, implementing a multilevel\nlowering align with the hierarchical nature of GPU. Additionally, we extend\ntriton language to support user-set compiler hint and warp level programming,\nenabling researchers to get good out-of-the box performance without awaiting\ncompiler updates. Experimental results demonstrate that our approach achieves\nperformance above 95% of expert-written kernels on Intel GPU, as measured by\nthe geometric mean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of LLMs, dense operations such as GEMM and MHA are critical\ncomponents. These operations are well-suited for parallel execution using a\ntilebased approach. While traditional GPU programming often relies on low level\ninterfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more\nuser-friendly and portable alternative by programming at a higher level. The\ncurrent Triton starts at the workgroup (aka threadblock) level, and directly\nlowers to per-thread level. And then attempt to coalesce and amend through a\nseries of passes, promoting information from low-level representation. We\nbelieve this is pre-mature lowering based on the below observations. 1. GPU has\na hierarchical structure both physically and logically. Modern GPUs often\nfeature SIMD units capable of directly operating on tiles on a warp or\nwarpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual\nlowering can make compiler decoupled and clean by separating considerations\ninter and intra a logical layer. 3. Kernel developers often need fine control\nto get good performance on the latest hardware. FlashAttention2 advocates\nexplicit data partition between warps to make a performance boost. In this\ncontext, we propose ML-Triton which features multi-level compilation flow and\nprogramming interface. Our approach begins at the workgroup level and\nprogressively lowers to the warp and intrinsic level, implementing a multilevel\nlowering align with the hierarchical nature of GPU. Additionally, we extend\ntriton language to support user-set compiler hint and warp level programming,\nenabling researchers to get good out-of-the box performance without awaiting\ncompiler updates. Experimental results demonstrate that our approach achieves\nperformance above 95% of expert-written kernels on Intel GPU, as measured by\nthe geometric mean."
                },
                "authors": [
                    {
                        "name": "Dewei Wang"
                    },
                    {
                        "name": "Wei Zhu"
                    },
                    {
                        "name": "Liyang Ling"
                    },
                    {
                        "name": "Ettore Tiotto"
                    },
                    {
                        "name": "Quintin Wang"
                    },
                    {
                        "name": "Whitney Tsang"
                    },
                    {
                        "name": "Julian Opperman"
                    },
                    {
                        "name": "Jacky Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jacky Deng"
                },
                "author": "Jacky Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14975v1",
                "updated": "2025-03-19T08:10:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    10,
                    49,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T08:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    10,
                    49,
                    2,
                    78,
                    0
                ],
                "title": "Taming Flow Matching with Unbalanced Optimal Transport into Fast\n  Pansharpening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Flow Matching with Unbalanced Optimal Transport into Fast\n  Pansharpening"
                },
                "summary": "Pansharpening, a pivotal task in remote sensing for fusing high-resolution\npanchromatic and multispectral imagery, has garnered significant research\ninterest. Recent advancements employing diffusion models based on stochastic\ndifferential equations (SDEs) have demonstrated state-of-the-art performance.\nHowever, the inherent multi-step sampling process of SDEs imposes substantial\ncomputational overhead, hindering practical deployment. While existing methods\nadopt efficient samplers, knowledge distillation, or retraining to reduce\nsampling steps (e.g., from 1,000 to fewer steps), such approaches often\ncompromise fusion quality. In this work, we propose the Optimal Transport Flow\nMatching (OTFM) framework, which integrates the dual formulation of unbalanced\noptimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike\nconventional OT formulations that enforce rigid distribution alignment, UOT\nrelaxes marginal constraints to enhance modeling flexibility, accommodating the\nintrinsic spectral and spatial disparities in remote sensing data. Furthermore,\nwe incorporate task-specific regularization into the UOT objective, enhancing\nthe robustness of the flow model. The OTFM framework enables simulation-free\ntraining and single-step inference while maintaining strict adherence to\npansharpening constraints. Experimental evaluations across multiple datasets\ndemonstrate that OTFM matches or exceeds the performance of previous\nregression-based models and leading diffusion-based methods while only needing\none sampling step. Codes are available at https://github.com/294coder/PAN-OTFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pansharpening, a pivotal task in remote sensing for fusing high-resolution\npanchromatic and multispectral imagery, has garnered significant research\ninterest. Recent advancements employing diffusion models based on stochastic\ndifferential equations (SDEs) have demonstrated state-of-the-art performance.\nHowever, the inherent multi-step sampling process of SDEs imposes substantial\ncomputational overhead, hindering practical deployment. While existing methods\nadopt efficient samplers, knowledge distillation, or retraining to reduce\nsampling steps (e.g., from 1,000 to fewer steps), such approaches often\ncompromise fusion quality. In this work, we propose the Optimal Transport Flow\nMatching (OTFM) framework, which integrates the dual formulation of unbalanced\noptimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike\nconventional OT formulations that enforce rigid distribution alignment, UOT\nrelaxes marginal constraints to enhance modeling flexibility, accommodating the\nintrinsic spectral and spatial disparities in remote sensing data. Furthermore,\nwe incorporate task-specific regularization into the UOT objective, enhancing\nthe robustness of the flow model. The OTFM framework enables simulation-free\ntraining and single-step inference while maintaining strict adherence to\npansharpening constraints. Experimental evaluations across multiple datasets\ndemonstrate that OTFM matches or exceeds the performance of previous\nregression-based models and leading diffusion-based methods while only needing\none sampling step. Codes are available at https://github.com/294coder/PAN-OTFM."
                },
                "authors": [
                    {
                        "name": "Zihan Cao"
                    },
                    {
                        "name": "Yu Zhong"
                    },
                    {
                        "name": "Liang-Jian Deng"
                    }
                ],
                "author_detail": {
                    "name": "Liang-Jian Deng"
                },
                "author": "Liang-Jian Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06315v2",
                "updated": "2025-03-19T08:03:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    3,
                    55,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-08T19:46:05Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    46,
                    5,
                    1,
                    282,
                    0
                ],
                "title": "Incremental Learning for Robot Shared Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental Learning for Robot Shared Autonomy"
                },
                "summary": "Shared autonomy holds promise for improving the usability and accessibility\nof assistive robotic arms, but current methods often rely on costly expert\ndemonstrations and lack the ability to adapt post-deployment. This paper\nintroduces ILSA, an Incrementally Learned Shared Autonomy framework that\ncontinually improves its assistive control policy through repeated user\ninteractions. ILSA leverages synthetic kinematic trajectories for initial\npretraining, reducing the need for expert demonstrations, and then\nincrementally finetunes its policy after each manipulation interaction, with\nmechanisms to balance new knowledge acquisition with existing knowledge\nretention during incremental learning. We validate ILSA for complex\nlong-horizon tasks through a comprehensive ablation study and a user study with\n20 participants, demonstrating its effectiveness and robustness in both\nquantitative performance and user-reported qualitative metrics. Code and videos\nare available at https://ilsa-robo.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared autonomy holds promise for improving the usability and accessibility\nof assistive robotic arms, but current methods often rely on costly expert\ndemonstrations and lack the ability to adapt post-deployment. This paper\nintroduces ILSA, an Incrementally Learned Shared Autonomy framework that\ncontinually improves its assistive control policy through repeated user\ninteractions. ILSA leverages synthetic kinematic trajectories for initial\npretraining, reducing the need for expert demonstrations, and then\nincrementally finetunes its policy after each manipulation interaction, with\nmechanisms to balance new knowledge acquisition with existing knowledge\nretention during incremental learning. We validate ILSA for complex\nlong-horizon tasks through a comprehensive ablation study and a user study with\n20 participants, demonstrating its effectiveness and robustness in both\nquantitative performance and user-reported qualitative metrics. Code and videos\nare available at https://ilsa-robo.github.io/."
                },
                "authors": [
                    {
                        "name": "Yiran Tao"
                    },
                    {
                        "name": "Guixiu Qiao"
                    },
                    {
                        "name": "Dan Ding"
                    },
                    {
                        "name": "Zackory Erickson"
                    }
                ],
                "author_detail": {
                    "name": "Zackory Erickson"
                },
                "author": "Zackory Erickson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14957v1",
                "updated": "2025-03-19T07:49:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    7,
                    49,
                    14,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T07:49:14Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    7,
                    49,
                    14,
                    2,
                    78,
                    0
                ],
                "title": "Neuro Symbolic Knowledge Reasoning for Procedural Video Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro Symbolic Knowledge Reasoning for Procedural Video Question\n  Answering"
                },
                "summary": "This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https://github.com/LUNAProject22/KML soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https://github.com/LUNAProject22/KML soon."
                },
                "authors": [
                    {
                        "name": "Thanh-Son Nguyen"
                    },
                    {
                        "name": "Hong Yang"
                    },
                    {
                        "name": "Tzeh Yuan Neoh"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Ee Yeo Keat"
                    },
                    {
                        "name": "Basura Fernando"
                    }
                ],
                "author_detail": {
                    "name": "Basura Fernando"
                },
                "author": "Basura Fernando",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14948v1",
                "updated": "2025-03-19T07:25:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    7,
                    25,
                    21,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T07:25:21Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    7,
                    25,
                    21,
                    2,
                    78,
                    0
                ],
                "title": "ChatStitch: Visualizing Through Structures via Surround-View\n  Unsupervised Deep Image Stitching with Collaborative LLM-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatStitch: Visualizing Through Structures via Surround-View\n  Unsupervised Deep Image Stitching with Collaborative LLM-Agents"
                },
                "summary": "Collaborative perception has garnered significant attention for its ability\nto enhance the perception capabilities of individual vehicles through the\nexchange of information with surrounding vehicle-agents. However, existing\ncollaborative perception systems are limited by inefficiencies in user\ninteraction and the challenge of multi-camera photorealistic visualization. To\naddress these challenges, this paper introduces ChatStitch, the first\ncollaborative perception system capable of unveiling obscured blind spot\ninformation through natural language commands integrated with external digital\nassets. To adeptly handle complex or abstract commands, ChatStitch employs a\nmulti-agent collaborative framework based on Large Language Models. For\nachieving the most intuitive perception for humans, ChatStitch proposes\nSV-UDIS, the first surround-view unsupervised deep image stitching method under\nthe non-global-overlapping condition. We conducted extensive experiments on the\nUDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our\nSV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for\n3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,\nand SSIM improvements of 8%, 18%, and 26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative perception has garnered significant attention for its ability\nto enhance the perception capabilities of individual vehicles through the\nexchange of information with surrounding vehicle-agents. However, existing\ncollaborative perception systems are limited by inefficiencies in user\ninteraction and the challenge of multi-camera photorealistic visualization. To\naddress these challenges, this paper introduces ChatStitch, the first\ncollaborative perception system capable of unveiling obscured blind spot\ninformation through natural language commands integrated with external digital\nassets. To adeptly handle complex or abstract commands, ChatStitch employs a\nmulti-agent collaborative framework based on Large Language Models. For\nachieving the most intuitive perception for humans, ChatStitch proposes\nSV-UDIS, the first surround-view unsupervised deep image stitching method under\nthe non-global-overlapping condition. We conducted extensive experiments on the\nUDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our\nSV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for\n3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,\nand SSIM improvements of 8%, 18%, and 26%, respectively."
                },
                "authors": [
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Zhipeng Dong"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Mengyin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Mengyin Fu"
                },
                "author": "Mengyin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09020v2",
                "updated": "2025-03-19T07:24:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    7,
                    24,
                    48,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-12T03:15:46Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    15,
                    46,
                    2,
                    71,
                    0
                ],
                "title": "Enhancing High-Quality Code Generation in Large Language Models with\n  Comparative Prefix-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Quality Code Generation in Large Language Models with\n  Comparative Prefix-Tuning"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted in commercial code\ncompletion engines, significantly enhancing coding efficiency and productivity.\nHowever, LLMs may generate code with quality issues that violate coding\nstandards and best practices, such as poor code style and maintainability, even\nwhen the code is functionally correct. This necessitates additional effort from\ndevelopers to improve the code, potentially negating the efficiency gains\nprovided by LLMs. To address this problem, we propose a novel comparative\nprefix-tuning method for controllable high-quality code generation. Our method\nintroduces a single, property-specific prefix that is prepended to the\nactivations of the LLM, serving as a lightweight alternative to fine-tuning.\nUnlike existing methods that require training multiple prefixes, our approach\ntrains only one prefix and leverages pairs of high-quality and low-quality code\nsamples, introducing a sequence-level ranking loss to guide the model's\ntraining. This comparative approach enables the model to better understand the\ndifferences between high-quality and low-quality code, focusing on aspects that\nimpact code quality. Additionally, we design a data construction pipeline to\ncollect and annotate pairs of high-quality and low-quality code, facilitating\neffective training. Extensive experiments on the Code Llama 7B model\ndemonstrate that our method improves code quality by over 100% in certain task\ncategories, while maintaining functional correctness. We also conduct ablation\nstudies and generalization experiments, confirming the effectiveness of our\nmethod's components and its strong generalization capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted in commercial code\ncompletion engines, significantly enhancing coding efficiency and productivity.\nHowever, LLMs may generate code with quality issues that violate coding\nstandards and best practices, such as poor code style and maintainability, even\nwhen the code is functionally correct. This necessitates additional effort from\ndevelopers to improve the code, potentially negating the efficiency gains\nprovided by LLMs. To address this problem, we propose a novel comparative\nprefix-tuning method for controllable high-quality code generation. Our method\nintroduces a single, property-specific prefix that is prepended to the\nactivations of the LLM, serving as a lightweight alternative to fine-tuning.\nUnlike existing methods that require training multiple prefixes, our approach\ntrains only one prefix and leverages pairs of high-quality and low-quality code\nsamples, introducing a sequence-level ranking loss to guide the model's\ntraining. This comparative approach enables the model to better understand the\ndifferences between high-quality and low-quality code, focusing on aspects that\nimpact code quality. Additionally, we design a data construction pipeline to\ncollect and annotate pairs of high-quality and low-quality code, facilitating\neffective training. Extensive experiments on the Code Llama 7B model\ndemonstrate that our method improves code quality by over 100% in certain task\ncategories, while maintaining functional correctness. We also conduct ablation\nstudies and generalization experiments, confirming the effectiveness of our\nmethod's components and its strong generalization capability."
                },
                "authors": [
                    {
                        "name": "Yuan Jiang"
                    },
                    {
                        "name": "Yujian Zhang"
                    },
                    {
                        "name": "Liang Lu"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "Xiaohong Su"
                    },
                    {
                        "name": "Shan Huang"
                    },
                    {
                        "name": "Tiantian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tiantian Wang"
                },
                "author": "Tiantian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14936v1",
                "updated": "2025-03-19T06:44:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    44,
                    29,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T06:44:29Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    44,
                    29,
                    2,
                    78,
                    0
                ],
                "title": "Enhancing Code LLM Training with Programmer Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code LLM Training with Programmer Attention"
                },
                "summary": "Human attention provides valuable yet underexploited signals for code LLM\ntraining, offering a perspective beyond purely machine-driven attention.\nDespite the complexity and cost of collecting eye-tracking data, there has also\nbeen limited progress in systematically using these signals for code LLM\ntraining. To address both issues, we propose a cohesive pipeline spanning\naugmentation and reward-based fine-tuning. Specifically, we introduce (1) an\neye-tracking path augmentation method to expand programmer attention datasets,\n(2) a pattern abstraction step that refines raw fixations into learnable\nattention motifs, and (3) a reward-guided strategy for integrating these\ninsights directly into a CodeT5 supervised fine-tuning process. Our experiments\nyield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,\nunderscoring how uniting human and machine attention can boost code\nintelligence. We hope this work encourages broader exploration of human-centric\nmethods in next-generation AI4SE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human attention provides valuable yet underexploited signals for code LLM\ntraining, offering a perspective beyond purely machine-driven attention.\nDespite the complexity and cost of collecting eye-tracking data, there has also\nbeen limited progress in systematically using these signals for code LLM\ntraining. To address both issues, we propose a cohesive pipeline spanning\naugmentation and reward-based fine-tuning. Specifically, we introduce (1) an\neye-tracking path augmentation method to expand programmer attention datasets,\n(2) a pattern abstraction step that refines raw fixations into learnable\nattention motifs, and (3) a reward-guided strategy for integrating these\ninsights directly into a CodeT5 supervised fine-tuning process. Our experiments\nyield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,\nunderscoring how uniting human and machine attention can boost code\nintelligence. We hope this work encourages broader exploration of human-centric\nmethods in next-generation AI4SE."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Zachary Karas"
                    },
                    {
                        "name": "Dung Thuy Nguyen"
                    },
                    {
                        "name": "Kevin Leach"
                    },
                    {
                        "name": "Yu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Huang"
                },
                "author": "Yu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14935v1",
                "updated": "2025-03-19T06:42:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    42,
                    32,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T06:42:32Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    42,
                    32,
                    2,
                    78,
                    0
                ],
                "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion\n  Understanding"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}."
                },
                "authors": [
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "FAVOR-Bench project page: https://favor-bench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14933v1",
                "updated": "2025-03-19T06:41:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    41,
                    37,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T06:41:37Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    41,
                    37,
                    2,
                    78,
                    0
                ],
                "title": "A Language Vision Model Approach for Automated Tumor Contouring in\n  Radiation Oncology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Language Vision Model Approach for Automated Tumor Contouring in\n  Radiation Oncology"
                },
                "summary": "Background: Lung cancer ranks as the leading cause of cancer-related\nmortality worldwide. The complexity of tumor delineation, crucial for radiation\ntherapy, requires expertise often unavailable in resource-limited settings.\nArtificial Intelligence(AI), particularly with advancements in deep learning\n(DL) and natural language processing (NLP), offers potential solutions yet is\nchallenged by high false positive rates. Purpose: The Oncology Contouring\nCopilot (OCC) system is developed to leverage oncologist expertise for precise\ntumor contouring using textual descriptions, aiming to increase the efficiency\nof oncological workflows by combining the strengths of AI with human oversight.\nMethods: Our OCC system initially identifies nodule candidates from CT scans.\nEmploying Language Vision Models (LVMs) like GPT-4V, OCC then effectively\nreduces false positives with clinical descriptive texts, merging textual and\nvisual data to automate tumor delineation, designed to elevate the quality of\noncology care by incorporating knowledge from experienced domain experts.\nResults: Deployments of the OCC system resulted in a significant reduction in\nthe false discovery rate by 35.0%, a 72.4% decrease in false positives per\nscan, and an F1-score of 0.652 across our dataset for unbiased evaluation.\nConclusions: OCC represents a significant advance in oncology care,\nparticularly through the use of the latest LVMs to improve contouring results\nby (1) streamlining oncology treatment workflows by optimizing tumor\ndelineation, reducing manual processes; (2) offering a scalable and intuitive\nframework to reduce false positives in radiotherapy planning using LVMs; (3)\nintroducing novel medical language vision prompt techniques to minimize LVMs\nhallucinations with ablation study, and (4) conducting a comparative analysis\nof LVMs, highlighting their potential in addressing medical language vision\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Lung cancer ranks as the leading cause of cancer-related\nmortality worldwide. The complexity of tumor delineation, crucial for radiation\ntherapy, requires expertise often unavailable in resource-limited settings.\nArtificial Intelligence(AI), particularly with advancements in deep learning\n(DL) and natural language processing (NLP), offers potential solutions yet is\nchallenged by high false positive rates. Purpose: The Oncology Contouring\nCopilot (OCC) system is developed to leverage oncologist expertise for precise\ntumor contouring using textual descriptions, aiming to increase the efficiency\nof oncological workflows by combining the strengths of AI with human oversight.\nMethods: Our OCC system initially identifies nodule candidates from CT scans.\nEmploying Language Vision Models (LVMs) like GPT-4V, OCC then effectively\nreduces false positives with clinical descriptive texts, merging textual and\nvisual data to automate tumor delineation, designed to elevate the quality of\noncology care by incorporating knowledge from experienced domain experts.\nResults: Deployments of the OCC system resulted in a significant reduction in\nthe false discovery rate by 35.0%, a 72.4% decrease in false positives per\nscan, and an F1-score of 0.652 across our dataset for unbiased evaluation.\nConclusions: OCC represents a significant advance in oncology care,\nparticularly through the use of the latest LVMs to improve contouring results\nby (1) streamlining oncology treatment workflows by optimizing tumor\ndelineation, reducing manual processes; (2) offering a scalable and intuitive\nframework to reduce false positives in radiotherapy planning using LVMs; (3)\nintroducing novel medical language vision prompt techniques to minimize LVMs\nhallucinations with ablation study, and (4) conducting a comparative analysis\nof LVMs, highlighting their potential in addressing medical language vision\nchallenges."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Hamed Hooshangnejad"
                    },
                    {
                        "name": "Xue Feng"
                    },
                    {
                        "name": "Gaofeng Huang"
                    },
                    {
                        "name": "Xiaojian Chen"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Wil Ngwa"
                    },
                    {
                        "name": "Kai Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kai Ding"
                },
                "author": "Kai Ding",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14932v1",
                "updated": "2025-03-19T06:38:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    38,
                    51,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T06:38:51Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    38,
                    51,
                    2,
                    78,
                    0
                ],
                "title": "Prada: Black-Box LLM Adaptation with Private Data on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prada: Black-Box LLM Adaptation with Private Data on\n  Resource-Constrained Devices"
                },
                "summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nabilities in various natural language processing tasks. However, adapting these\nmodels to specialized domains using private datasets stored on\nresource-constrained edge devices, such as smartphones and personal computers,\nremains challenging due to significant privacy concerns and limited\ncomputational resources. Existing model adaptation methods either compromise\ndata privacy by requiring data transmission or jeopardize model privacy by\nexposing proprietary LLM parameters. To address these challenges, we propose\nPrada, a novel privacy-preserving and efficient black-box LLM adaptation system\nusing private on-device datasets. Prada employs a lightweight proxy model\nfine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During\ninference, Prada leverages the logits offset, i.e., difference in outputs\nbetween the base and adapted proxy models, to iteratively refine outputs from a\nremote black-box LLM. This offset-based adaptation approach preserves both data\nprivacy and model privacy, as there is no need to share sensitive data or\nproprietary model parameters. Furthermore, we incorporate speculative decoding\nto further speed up the inference process of Prada, making the system\npractically deployable on bandwidth-constrained edge devices, enabling a more\npractical deployment of Prada. Extensive experiments on various downstream\ntasks demonstrate that Prada achieves performance comparable to centralized\nfine-tuning methods while significantly reducing computational overhead by up\nto 60% and communication costs by up to 80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nabilities in various natural language processing tasks. However, adapting these\nmodels to specialized domains using private datasets stored on\nresource-constrained edge devices, such as smartphones and personal computers,\nremains challenging due to significant privacy concerns and limited\ncomputational resources. Existing model adaptation methods either compromise\ndata privacy by requiring data transmission or jeopardize model privacy by\nexposing proprietary LLM parameters. To address these challenges, we propose\nPrada, a novel privacy-preserving and efficient black-box LLM adaptation system\nusing private on-device datasets. Prada employs a lightweight proxy model\nfine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During\ninference, Prada leverages the logits offset, i.e., difference in outputs\nbetween the base and adapted proxy models, to iteratively refine outputs from a\nremote black-box LLM. This offset-based adaptation approach preserves both data\nprivacy and model privacy, as there is no need to share sensitive data or\nproprietary model parameters. Furthermore, we incorporate speculative decoding\nto further speed up the inference process of Prada, making the system\npractically deployable on bandwidth-constrained edge devices, enabling a more\npractical deployment of Prada. Extensive experiments on various downstream\ntasks demonstrate that Prada achieves performance comparable to centralized\nfine-tuning methods while significantly reducing computational overhead by up\nto 60% and communication costs by up to 80%."
                },
                "authors": [
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09985v2",
                "updated": "2025-03-19T06:27:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    27,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-13T02:50:19Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    2,
                    50,
                    19,
                    3,
                    72,
                    0
                ],
                "title": "ES-Parkour: Advanced Robot Parkour with Bio-inspired Event Camera and\n  Spiking Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ES-Parkour: Advanced Robot Parkour with Bio-inspired Event Camera and\n  Spiking Neural Network"
                },
                "summary": "In recent years, quadruped robotics has advanced significantly, particularly\nin perception and motion control via reinforcement learning, enabling complex\nmotions in challenging environments. Visual sensors like depth cameras enhance\nstability and robustness but face limitations, such as low operating\nfrequencies relative to joint control and sensitivity to lighting, which hinder\noutdoor deployment. Additionally, deep neural networks in sensor and control\nsystems increase computational demands. To address these issues, we introduce\nspiking neural networks (SNNs) and event cameras to perform a challenging\nquadruped parkour task. Event cameras capture dynamic visual data, while SNNs\nefficiently process spike sequences, mimicking biological perception.\nExperimental results demonstrate that this approach significantly outperforms\ntraditional models, achieving excellent parkour performance with just 11.7% of\nthe energy consumption of an artificial neural network (ANN)-based model,\nyielding an 88.3% energy reduction. By integrating event cameras with SNNs, our\nwork advances robotic reinforcement learning and opens new possibilities for\napplications in demanding environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, quadruped robotics has advanced significantly, particularly\nin perception and motion control via reinforcement learning, enabling complex\nmotions in challenging environments. Visual sensors like depth cameras enhance\nstability and robustness but face limitations, such as low operating\nfrequencies relative to joint control and sensitivity to lighting, which hinder\noutdoor deployment. Additionally, deep neural networks in sensor and control\nsystems increase computational demands. To address these issues, we introduce\nspiking neural networks (SNNs) and event cameras to perform a challenging\nquadruped parkour task. Event cameras capture dynamic visual data, while SNNs\nefficiently process spike sequences, mimicking biological perception.\nExperimental results demonstrate that this approach significantly outperforms\ntraditional models, achieving excellent parkour performance with just 11.7% of\nthe energy consumption of an artificial neural network (ANN)-based model,\nyielding an 88.3% energy reduction. By integrating event cameras with SNNs, our\nwork advances robotic reinforcement learning and opens new possibilities for\napplications in demanding environments."
                },
                "authors": [
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Jingkai Sun"
                    },
                    {
                        "name": "Yecheng Shao"
                    },
                    {
                        "name": "Gang Han"
                    },
                    {
                        "name": "Wen Zhao"
                    },
                    {
                        "name": "Yijie Guo"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12444v2",
                "updated": "2025-03-19T06:22:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    22,
                    38,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-16T10:48:14Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    48,
                    14,
                    2,
                    290,
                    0
                ],
                "title": "Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models"
                },
                "summary": "Service chatbots play an important role in enhancing customer support by\ndelivering timely responses to diverse queries. Traditionally, these chatbots\nrely on retrieval-based methods constrained by a predefined knowledge base of\nquestion-answer (QA) pairs to guarantee reliable responses. To effectively\nhandle varied customer inquiries, augmenting the knowledge base with similar\nquestions that maintain semantic consistency and linguistic variability is\ncrucial. This paper presents methodologies for a novel approach that utilizes\nLarge Language Models (LLMs) for generating similar questions and selecting an\noptimal subset of questions for knowledge base augmentation in industrial\nchatbots. Specifically, we define the SQG task in the context of LLM training\nand propose a one-to-many objective that incorporates contextual information.\nWe also introduce an optimization framework that selects a diverse subset of\nsimilar questions within predefined resource constraints. Experimental results\ndemonstrate significant improvements over traditional methods, achieving\ngreater semantic diversity while aligning with source QA pairs, with over 120%\nrelative improvement in meeting business-specific requirements with human\nevaluation. Combined with several best practices, we provide a robust,\napplication-driven solution for enhancing chatbot performance and improving\ncustomer service satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Service chatbots play an important role in enhancing customer support by\ndelivering timely responses to diverse queries. Traditionally, these chatbots\nrely on retrieval-based methods constrained by a predefined knowledge base of\nquestion-answer (QA) pairs to guarantee reliable responses. To effectively\nhandle varied customer inquiries, augmenting the knowledge base with similar\nquestions that maintain semantic consistency and linguistic variability is\ncrucial. This paper presents methodologies for a novel approach that utilizes\nLarge Language Models (LLMs) for generating similar questions and selecting an\noptimal subset of questions for knowledge base augmentation in industrial\nchatbots. Specifically, we define the SQG task in the context of LLM training\nand propose a one-to-many objective that incorporates contextual information.\nWe also introduce an optimization framework that selects a diverse subset of\nsimilar questions within predefined resource constraints. Experimental results\ndemonstrate significant improvements over traditional methods, achieving\ngreater semantic diversity while aligning with source QA pairs, with over 120%\nrelative improvement in meeting business-specific requirements with human\nevaluation. Combined with several best practices, we provide a robust,\napplication-driven solution for enhancing chatbot performance and improving\ncustomer service satisfaction."
                },
                "authors": [
                    {
                        "name": "Mengze Hong"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Di Jiang"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Yuanqin He"
                    },
                    {
                        "name": "Zhiyang Su"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10177v2",
                "updated": "2025-03-19T06:22:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    6,
                    22,
                    21,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-13T08:58:10Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    58,
                    10,
                    3,
                    72,
                    0
                ],
                "title": "PRISM: Preference Refinement via Implicit Scene Modeling for 3D\n  Vision-Language Preference-Based Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Preference Refinement via Implicit Scene Modeling for 3D\n  Vision-Language Preference-Based Reinforcement Learning"
                },
                "summary": "We propose PRISM, a novel framework designed to overcome the limitations of\n2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point\ncloud modeling and future-aware preference refinement. At its core, PRISM\nadopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and\nviewpoint biases, ensuring more stable and spatially consistent preference\nsignals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to\nincorporate long-horizon considerations, thereby preventing the short-sighted\nfeedback often seen in static preference comparisons. In contrast to\nconventional PBRL techniques, this integration of 3D perception and\nfuture-oriented reasoning leads to significant gains in preference agreement\nrates, faster policy convergence, and robust generalization across unseen\nrobotic environments. Our empirical results, spanning tasks such as robotic\nmanipulation and autonomous navigation, highlight PRISM's potential for\nreal-world applications where precise spatial understanding and reliable\nlong-term decision-making are critical. By bridging 3D geometric awareness with\nCoT-driven preference modeling, PRISM establishes a comprehensive foundation\nfor scalable, human-aligned reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose PRISM, a novel framework designed to overcome the limitations of\n2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point\ncloud modeling and future-aware preference refinement. At its core, PRISM\nadopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and\nviewpoint biases, ensuring more stable and spatially consistent preference\nsignals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to\nincorporate long-horizon considerations, thereby preventing the short-sighted\nfeedback often seen in static preference comparisons. In contrast to\nconventional PBRL techniques, this integration of 3D perception and\nfuture-oriented reasoning leads to significant gains in preference agreement\nrates, faster policy convergence, and robust generalization across unseen\nrobotic environments. Our empirical results, spanning tasks such as robotic\nmanipulation and autonomous navigation, highlight PRISM's potential for\nreal-world applications where precise spatial understanding and reliable\nlong-term decision-making are critical. By bridging 3D geometric awareness with\nCoT-driven preference modeling, PRISM establishes a comprehensive foundation\nfor scalable, human-aligned reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Yirong Sun"
                    },
                    {
                        "name": "Yanjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Chen"
                },
                "author": "Yanjun Chen",
                "arxiv_comment": "I withdraw arXiv:2503.10177 due to critical computational errors\n  invalidating its conclusions and the withdrawal of consent from co-author\n  Yanjun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]